{
  "date": "2026-02-26",
  "report_file": "2026-02-26_ollama_llama3.1-8b.html",
  "status": "in_progress",
  "last_updated": "2026-02-27 07:38 UTC",
  "llm_backends": [
    [
      "ollama",
      "llama3.1:8b"
    ]
  ],
  "generation_time_seconds": 0.0,
  "developer_reports": [
    {
      "name": "Alexandre Ghiti",
      "primary_email": "alexghiti@rivosinc.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Boris Burkov",
      "primary_email": "boris@bur.io",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 0/5] btrfs: fix exploits that allow malicious users to turn fs into RO mode",
          "message_id": "20260226215718.GB3111707@zen.localdomain",
          "url": "https://lore.kernel.org/all/20260226215718.GB3111707@zen.localdomain/",
          "date": "2026-02-26T21:56:26Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": []
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 1/5] btrfs: fix transaction abort on file creation due to name hash collision",
          "message_id": "20260226212912.GA3111707@zen.localdomain",
          "url": "https://lore.kernel.org/all/20260226212912.GA3111707@zen.localdomain/",
          "date": "2026-02-26T21:28:22Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": []
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH-RFC] btrfs: for unclustered allocation don't consider ffe_ctl->empty_cluster",
          "message_id": "20260226182915.GA2992537@zen.localdomain",
          "url": "https://lore.kernel.org/all/20260226182915.GA2992537@zen.localdomain/",
          "date": "2026-02-26T18:28:52Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Patch author's original patch aimed to modify btrfs allocation logic for unclustered scenarios, but a reviewer pointed out the missing RFC tag and suggested revising the approach.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Boris Burkov",
              "summary": "Acknowledged the potential bug and suggested revising the approach to clearly define relationships between variables.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledgment of potential bug",
                "request for revised patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH-RFC] btrfs: for unclustered allocation don't consider ffe_ctl->empty_cluster",
          "message_id": "20260226173746.GA2968189@zen.localdomain",
          "url": "https://lore.kernel.org/all/20260226173746.GA2968189@zen.localdomain/",
          "date": "2026-02-26T17:36:55Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Patch author's original patch aimed to modify btrfs allocation logic for unclustered scenarios, but a reviewer pointed out the missing RFC tag and suggested revising the approach.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Boris Burkov",
              "summary": "Acknowledged the potential bug and suggested revising the approach to clearly define relationships between variables.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledgment of potential bug",
                "request for revised patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On Thu, Feb 26, 2026 at 09:37:46AM -0800, Boris Burkov wrote:\n> On Thu, Feb 26, 2026 at 01:34:19PM +0200, Alex Lyakas wrote:\n> > I encountered an issue when performing unclustered allocation for metadata:\n> > \n> > free_space_ctl->free_space = 2MB\n> > ffe_ctl->num_bytes = 4096\n> > ffe_ctl->empty_cluster = 2MB\n> > ffe_ctl->empty_size = 0\n> > \n> > So early check for free_space_ctl->free_space skips the block group, even though\n> > it has enough space to do the allocation.\n> > I think when doing unclustered allocation we should not look at ffe_ctl->empty_cluster.\n> \n> I see what you are saying, and a the level of this situation and this\n> line of code, I think you are right.\n> \n> But as-is, this change does not contain enough reasoning about the\n> semantics of the allocation algorithm to realistically be anything\n> but exchanging one bug for two new ones.\n> \n> What is empty_cluster modelling? Why is it included in this calculation?\n> Why should it not be included? Where else is empty_cluster used and\n> should it change under this new interpretation? Does any of this change\n> if there is actually a cluster active for clustered allocations?\n> \n> etc. etc.\n> \n> Thanks,\n> Boris\n> \n\nI missed the RFC tag in the patch, so I would like to apologize for my\nnegativity. In my experience with other bugs, the interplay between the\nclustered algorithm and the unclustered algorithm is under-specified so\nI think it is likely you have indeed found a bug. \n\nIf you want to fix it, I would proceed along the lines I complained\nabout in my first response and try to define the relationships between\nthe variables in a consistent way that explains why we shouldn't count\nthat variable here.\n\nIf you do go through with sharpening the definition of empty_cluster,\nI would be happy to review that work and help get it in.\n\nThanks,\nBoris\n\n> > \n> > I tested this on stable kernel 6.6.\n> > \n> > Signed-off-by: Alex Lyakas <alex.lyakas@zadara.com>\n> > ---\n> >  fs/btrfs/extent-tree.c | 2 +-\n> >  1 file changed, 1 insertion(+), 1 deletion(-)\n> > \n> > diff --git a/fs/btrfs/extent-tree.c b/fs/btrfs/extent-tree.c\n> > index 03cf9f242c70..84b340a67882 100644\n> > --- a/fs/btrfs/extent-tree.c\n> > +++ b/fs/btrfs/extent-tree.c\n> > @@ -3885,7 +3885,7 @@ static int find_free_extent_unclustered(struct btrfs_block_group *bg,\n> >  \t\tfree_space_ctl = bg->free_space_ctl;\n> >  \t\tspin_lock(&free_space_ctl->tree_lock);\n> >  \t\tif (free_space_ctl->free_space <\n> > -\t\t    ffe_ctl->num_bytes + ffe_ctl->empty_cluster +\n> > +\t\t    ffe_ctl->num_bytes +\n> >  \t\t    ffe_ctl->empty_size) {\n> >  \t\t\tffe_ctl->total_free_space = max_t(u64,\n> >  \t\t\t\t\tffe_ctl->total_free_space,\n> > -- \n> > 2.43.0\n> > \n\n",
              "reply_to": "",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH 0/5] btrfs: fix exploits that allow malicious users to turn fs into RO mode",
          "message_id": "20260226191009.GB2996252@zen.localdomain",
          "url": "https://lore.kernel.org/all/20260226191009.GB2996252@zen.localdomain/",
          "date": "2026-02-26T19:09:18Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": []
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH 1/5] btrfs: fix transaction abort on file creation due to name hash collision",
          "message_id": "20260226185506.GA2996252@zen.localdomain",
          "url": "https://lore.kernel.org/all/20260226185506.GA2996252@zen.localdomain/",
          "date": "2026-02-26T18:54:15Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": []
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH] btrfs: remove the folio ref count ASSERT() from btrfs_free_comp_folio()",
          "message_id": "20260226004344.GA2580843@zen.localdomain",
          "url": "https://lore.kernel.org/all/20260226004344.GA2580843@zen.localdomain/",
          "date": "2026-02-26T00:42:51Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": []
        }
      ],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Dmitry Ilvokhin",
      "primary_email": "d@ilvokhin.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 5/5] mm: add tracepoints for zone lock",
          "message_id": "378089dd269249d3d7981fe10eb8b49ad551d353.1772129168.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/378089dd269249d3d7981fe10eb8b49ad551d353.1772129168.git.d@ilvokhin.com/",
          "date": "2026-02-26T18:26:59Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch adds tracepoint instrumentation to zone lock acquire and release operations in the Linux kernel. The implementation uses a lightweight inline helper that checks whether tracing is enabled, and calls an out-of-line helper when tracing is active. When CONFIG_TRACING is disabled, the helpers compile to empty inline stubs, leaving the fast path unaffected. This allows for easier debugging and analysis of zone lock behavior.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted a minor issue with the zone lock tracing implementation, specifically mentioning an empty inline stub in the !CONFIG_TRACING branch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "minor issue"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "One nit below other than that:\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n\n[...]",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-26",
              "message_id": "aaCbMwGNuUO1SbUb@linux.dev",
              "analysis_source": "llm"
            },
            {
              "author": "Andrew Morton",
              "summary": "Reviewer Andrew Morton questioned the necessity of exporting several files, specifically mentioning include/linux/mmzone.h and mm/compaction.c among others.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "export",
                "necessity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Do we need the exports at all?\n\ninclude/linux/mmzone.h\ninclude/linux/zone_lock.h\ninclude/trace/events/zone_lock.h\nMAINTAINERS\nmm/compaction.c\nmm/internal.h\nmm/Makefile\nmm/memory_hotplug.c\nmm/mm_init.c\nmm/page_alloc.c\nmm/page_isolation.c\nmm/page_owner.c\nmm/page_reporting.c\nmm/show_mem.c\nmm/vmscan.c\nmm/vmstat.c\nmm/zone_lock.c",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-26",
              "message_id": "20260226132501.5b70914daf9438a1103189ee@linux-foundation.org",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that zone lock tracepoints may require exports like mmap_lock wrappers to prevent drivers from taking the lock directly",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential locking issue",
                "export requirements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Very good point and we don't. I think this might just be copying the mmap_lock\ntracepoint wrappers which might need the exports as some drivers might be taking\nthe mmap_lock.\n\nDmitry, please confirm (test) and let us know.",
              "reply_to": "Andrew Morton",
              "message_date": "2026-02-26",
              "message_id": "aaC7S2ensAGTyujc@linux.dev",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 3/5] mm: convert compaction to zone lock wrappers",
          "message_id": "01729baf359e4c6612aead53f1fcb644f782d1de.1772129168.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/01729baf359e4c6612aead53f1fcb644f782d1de.1772129168.git.d@ilvokhin.com/",
          "date": "2026-02-26T18:26:57Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch adds tracepoint instrumentation to zone lock acquire and release operations in the Linux kernel. The implementation uses a lightweight inline helper that checks whether tracing is enabled, and calls an out-of-line helper when tracing is active. When CONFIG_TRACING is disabled, the helpers compile to empty inline stubs, leaving the fast path unaffected. This allows for easier debugging and analysis of zone lock behavior.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted a minor issue with the zone lock tracing implementation, specifically mentioning an empty inline stub in the !CONFIG_TRACING branch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "minor issue"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "One nit below other than that:\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n\n[...]",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Andrew Morton",
              "summary": "Reviewer Andrew Morton questioned the necessity of exporting several files, specifically mentioning include/linux/mmzone.h and mm/compaction.c among others.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "export",
                "necessity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Do we need the exports at all?\n\ninclude/linux/mmzone.h\ninclude/linux/zone_lock.h\ninclude/trace/events/zone_lock.h\nMAINTAINERS\nmm/compaction.c\nmm/internal.h\nmm/Makefile\nmm/memory_hotplug.c\nmm/mm_init.c\nmm/page_alloc.c\nmm/page_isolation.c\nmm/page_owner.c\nmm/page_reporting.c\nmm/show_mem.c\nmm/vmscan.c\nmm/vmstat.c\nmm/zone_lock.c",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that zone lock tracepoints may require exports like mmap_lock wrappers to prevent drivers from taking the lock directly",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential locking issue",
                "export requirements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Very good point and we don't. I think this might just be copying the mmap_lock\ntracepoint wrappers which might need the exports as some drivers might be taking\nthe mmap_lock.\n\nDmitry, please confirm (test) and let us know.",
              "reply_to": "Andrew Morton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 4/5] mm: rename zone->lock to zone->_lock",
          "message_id": "1221b8e7fa9f5694f3c4e411f01581b5aba9bc63.1772129168.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/1221b8e7fa9f5694f3c4e411f01581b5aba9bc63.1772129168.git.d@ilvokhin.com/",
          "date": "2026-02-26T18:26:57Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch adds tracepoint instrumentation to zone lock acquire and release operations in the Linux kernel. The implementation uses a lightweight inline helper that checks whether tracing is enabled, and calls an out-of-line helper when tracing is active. When CONFIG_TRACING is disabled, the helpers compile to empty inline stubs, leaving the fast path unaffected. This allows for easier debugging and analysis of zone lock behavior.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted a minor issue with the zone lock tracing implementation, specifically mentioning an empty inline stub in the !CONFIG_TRACING branch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "minor issue"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "One nit below other than that:\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n\n[...]",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Andrew Morton",
              "summary": "Reviewer Andrew Morton questioned the necessity of exporting several files, specifically mentioning include/linux/mmzone.h and mm/compaction.c among others.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "export",
                "necessity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Do we need the exports at all?\n\ninclude/linux/mmzone.h\ninclude/linux/zone_lock.h\ninclude/trace/events/zone_lock.h\nMAINTAINERS\nmm/compaction.c\nmm/internal.h\nmm/Makefile\nmm/memory_hotplug.c\nmm/mm_init.c\nmm/page_alloc.c\nmm/page_isolation.c\nmm/page_owner.c\nmm/page_reporting.c\nmm/show_mem.c\nmm/vmscan.c\nmm/vmstat.c\nmm/zone_lock.c",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that zone lock tracepoints may require exports like mmap_lock wrappers to prevent drivers from taking the lock directly",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential locking issue",
                "export requirements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Very good point and we don't. I think this might just be copying the mmap_lock\ntracepoint wrappers which might need the exports as some drivers might be taking\nthe mmap_lock.\n\nDmitry, please confirm (test) and let us know.",
              "reply_to": "Andrew Morton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 1/5] mm: introduce zone lock wrappers",
          "message_id": "5bcc39cd3a227944d0fbe75ff86cdac92b38d4ca.1772129168.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/5bcc39cd3a227944d0fbe75ff86cdac92b38d4ca.1772129168.git.d@ilvokhin.com/",
          "date": "2026-02-26T18:26:56Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch adds tracepoint instrumentation to zone lock acquire and release operations in the Linux kernel. The implementation uses a lightweight inline helper that checks whether tracing is enabled, and calls an out-of-line helper when tracing is active. When CONFIG_TRACING is disabled, the helpers compile to empty inline stubs, leaving the fast path unaffected. This allows for easier debugging and analysis of zone lock behavior.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted a minor issue with the zone lock tracing implementation, specifically mentioning an empty inline stub in the !CONFIG_TRACING branch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "minor issue"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "One nit below other than that:\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n\n[...]",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Andrew Morton",
              "summary": "Reviewer Andrew Morton questioned the necessity of exporting several files, specifically mentioning include/linux/mmzone.h and mm/compaction.c among others.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "export",
                "necessity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Do we need the exports at all?\n\ninclude/linux/mmzone.h\ninclude/linux/zone_lock.h\ninclude/trace/events/zone_lock.h\nMAINTAINERS\nmm/compaction.c\nmm/internal.h\nmm/Makefile\nmm/memory_hotplug.c\nmm/mm_init.c\nmm/page_alloc.c\nmm/page_isolation.c\nmm/page_owner.c\nmm/page_reporting.c\nmm/show_mem.c\nmm/vmscan.c\nmm/vmstat.c\nmm/zone_lock.c",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that zone lock tracepoints may require exports like mmap_lock wrappers to prevent drivers from taking the lock directly",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential locking issue",
                "export requirements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Very good point and we don't. I think this might just be copying the mmap_lock\ntracepoint wrappers which might need the exports as some drivers might be taking\nthe mmap_lock.\n\nDmitry, please confirm (test) and let us know.",
              "reply_to": "Andrew Morton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 0/5] mm: zone lock tracepoint instrumentation",
          "message_id": "cover.1772129168.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/cover.1772129168.git.d@ilvokhin.com/",
          "date": "2026-02-26T18:26:56Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch adds tracepoint instrumentation to zone lock acquire and release operations in the Linux kernel. The implementation uses a lightweight inline helper that checks whether tracing is enabled, and calls an out-of-line helper when tracing is active. When CONFIG_TRACING is disabled, the helpers compile to empty inline stubs, leaving the fast path unaffected. This allows for easier debugging and analysis of zone lock behavior.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted a minor issue with the zone lock tracing implementation, specifically mentioning an empty inline stub in the !CONFIG_TRACING branch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "minor issue"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "One nit below other than that:\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n\n[...]",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Andrew Morton",
              "summary": "Reviewer Andrew Morton questioned the necessity of exporting several files, specifically mentioning include/linux/mmzone.h and mm/compaction.c among others.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "export",
                "necessity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Do we need the exports at all?\n\ninclude/linux/mmzone.h\ninclude/linux/zone_lock.h\ninclude/trace/events/zone_lock.h\nMAINTAINERS\nmm/compaction.c\nmm/internal.h\nmm/Makefile\nmm/memory_hotplug.c\nmm/mm_init.c\nmm/page_alloc.c\nmm/page_isolation.c\nmm/page_owner.c\nmm/page_reporting.c\nmm/show_mem.c\nmm/vmscan.c\nmm/vmstat.c\nmm/zone_lock.c",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that zone lock tracepoints may require exports like mmap_lock wrappers to prevent drivers from taking the lock directly",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential locking issue",
                "export requirements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Very good point and we don't. I think this might just be copying the mmap_lock\ntracepoint wrappers which might need the exports as some drivers might be taking\nthe mmap_lock.\n\nDmitry, please confirm (test) and let us know.",
              "reply_to": "Andrew Morton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 2/5] mm: convert zone lock users to wrappers",
          "message_id": "e5324d64361f86d930d940a5b49235f7996efe53.1772129168.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/e5324d64361f86d930d940a5b49235f7996efe53.1772129168.git.d@ilvokhin.com/",
          "date": "2026-02-26T18:26:56Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch adds tracepoint instrumentation to zone lock acquire and release operations in the Linux kernel. The implementation uses a lightweight inline helper that checks whether tracing is enabled, and calls an out-of-line helper when tracing is active. When CONFIG_TRACING is disabled, the helpers compile to empty inline stubs, leaving the fast path unaffected. This allows for easier debugging and analysis of zone lock behavior.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted a minor issue with the zone lock tracing implementation, specifically mentioning an empty inline stub in the !CONFIG_TRACING branch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "minor issue"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "One nit below other than that:\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n\n[...]",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Andrew Morton",
              "summary": "Reviewer Andrew Morton questioned the necessity of exporting several files, specifically mentioning include/linux/mmzone.h and mm/compaction.c among others.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "export",
                "necessity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Do we need the exports at all?\n\ninclude/linux/mmzone.h\ninclude/linux/zone_lock.h\ninclude/trace/events/zone_lock.h\nMAINTAINERS\nmm/compaction.c\nmm/internal.h\nmm/Makefile\nmm/memory_hotplug.c\nmm/mm_init.c\nmm/page_alloc.c\nmm/page_isolation.c\nmm/page_owner.c\nmm/page_reporting.c\nmm/show_mem.c\nmm/vmscan.c\nmm/vmstat.c\nmm/zone_lock.c",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that zone lock tracepoints may require exports like mmap_lock wrappers to prevent drivers from taking the lock directly",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential locking issue",
                "export requirements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Very good point and we don't. I think this might just be copying the mmap_lock\ntracepoint wrappers which might need the exports as some drivers might be taking\nthe mmap_lock.\n\nDmitry, please confirm (test) and let us know.",
              "reply_to": "Andrew Morton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v2 4/4] mm: add tracepoints for zone lock",
          "message_id": "bde161acf827852ef19de51e91caf5c9f7df81bd.1772030186.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/bde161acf827852ef19de51e91caf5c9f7df81bd.1772030186.git.d@ilvokhin.com/",
          "date": "2026-02-25T14:44:17Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-25",
          "patch_summary": "This patch series adds tracepoint instrumentation to zone lock acquire and release operations, following the mmap_lock tracepoint pattern. The implementation includes lightweight inline helpers that check whether tracing is enabled and call out-of-line helpers when active. When CONFIG_TRACING is disabled, these helpers compile to empty inline stubs.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin",
              "summary": "No comments from the author in this thread.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v2 3/4] mm: convert compaction to zone lock wrappers",
          "message_id": "9710c3448c6c984164c93d7c6c0283e06ff987bf.1772030186.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/9710c3448c6c984164c93d7c6c0283e06ff987bf.1772030186.git.d@ilvokhin.com/",
          "date": "2026-02-25T14:44:16Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-25",
          "patch_summary": "This patch series adds tracepoint instrumentation to zone lock acquire and release operations, following the mmap_lock tracepoint pattern. The implementation includes lightweight inline helpers that check whether tracing is enabled and call out-of-line helpers when active. When CONFIG_TRACING is disabled, these helpers compile to empty inline stubs.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin",
              "summary": "No comments from the author in this thread.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On Wed, Feb 25, 2026 at 12:12:52PM -0800, Andrew Morton wrote:\n> On Wed, 25 Feb 2026 14:43:05 +0000 Dmitry Ilvokhin <d@ilvokhin.com> wrote:\n> \n> > Compaction uses compact_lock_irqsave(), which currently operates\n> > on a raw spinlock_t pointer so that it can be used for both\n> > zone->lock and lru_lock. Since zone lock operations are now wrapped,\n> > compact_lock_irqsave() can no longer operate directly on a spinlock_t\n> > when the lock belongs to a zone.\n> > \n> > Introduce struct compact_lock to abstract the underlying lock type. The\n> > structure carries a lock type enum and a union holding either a zone\n> > pointer or a raw spinlock_t pointer, and dispatches to the appropriate\n> > lock/unlock helper.\n> \n> It's regrettable that adds overhead - increased .text, increased\n> instructions.\n> \n> Thing is, compact_lock_irqsave() has only two callsites.  One knows\n> that it's dealing with the zone lock, the other knows that it's dealing\n> with the lruvec lock.\n> \n> Would it not be simpler and more efficient to copy/paste/edit two\n> versions of compact_lock_irqsave()?  A compact_zone_lock_irqsave() and a\n> compact_lruvec_lock_irqsave()?\n>\n\nThanks for the feedback, Andrew.\n\nMy initial goal was to reduce code duplication by keeping the logic\ncentralized, but your rationale makes sense. Given that there are only\ntwo call sites and both statically know the lock type, splitting the\nhelper avoids unnecessary abstraction.\n\nI'll introduce compact_zone_lock_irqsave() and\ncompact_lruvec_lock_irqsave() in v3.\n\n",
              "reply_to": "",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v2 2/4] mm: convert zone lock users to wrappers",
          "message_id": "e5324d64361f86d930d940a5b49235f7996efe53.1772030186.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/e5324d64361f86d930d940a5b49235f7996efe53.1772030186.git.d@ilvokhin.com/",
          "date": "2026-02-25T14:44:16Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-25",
          "patch_summary": "This patch series adds tracepoint instrumentation to zone lock acquire and release operations, following the mmap_lock tracepoint pattern. The implementation includes lightweight inline helpers that check whether tracing is enabled and call out-of-line helpers when active. When CONFIG_TRACING is disabled, these helpers compile to empty inline stubs.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin",
              "summary": "No comments from the author in this thread.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v2 1/4] mm: introduce zone lock wrappers",
          "message_id": "5bcc39cd3a227944d0fbe75ff86cdac92b38d4ca.1772030186.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/5bcc39cd3a227944d0fbe75ff86cdac92b38d4ca.1772030186.git.d@ilvokhin.com/",
          "date": "2026-02-25T14:44:15Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-25",
          "patch_summary": "This patch series adds tracepoint instrumentation to zone lock acquire and release operations, following the mmap_lock tracepoint pattern. The implementation includes lightweight inline helpers that check whether tracing is enabled and call out-of-line helpers when active. When CONFIG_TRACING is disabled, these helpers compile to empty inline stubs.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin",
              "summary": "No comments from the author in this thread.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On Wed, Feb 25, 2026 at 12:14:21PM -0800, Andrew Morton wrote:\n> On Wed, 25 Feb 2026 14:43:03 +0000 Dmitry Ilvokhin <d@ilvokhin.com> wrote:\n> \n> > Add thin wrappers around zone lock acquire/release operations. This\n> > prepares the code for future tracepoint instrumentation without\n> > modifying individual call sites.\n> > \n> > Centralizing zone lock operations behind wrappers allows future\n> > instrumentation or debugging hooks to be added without touching\n> > all users.\n> > \n> > No functional change intended. The wrappers are introduced in\n> > preparation for subsequent patches and are not yet used.\n> > \n> > ...\n> >\n> > +static inline void zone_lock_init(struct zone *zone)\n> > +{\n> > +\tspin_lock_init(&zone->lock);\n> > +}\n> \n> Please consider renaming zone.lock to something else (_lock would be\n> conventional) so that any present and future and out-of-tree\n> unconverted code won't compile.\n>\n\nThanks for suggestion, Andrew! Makes total sense to me, I'll rename lock\nto _lock in v3.\n",
              "reply_to": "",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v2 0/4] mm: zone lock tracepoint instrumentation",
          "message_id": "cover.1772030186.git.d@ilvokhin.com",
          "url": "https://lore.kernel.org/all/cover.1772030186.git.d@ilvokhin.com/",
          "date": "2026-02-25T14:44:15Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-25",
          "patch_summary": "This patch series adds tracepoint instrumentation to zone lock acquire and release operations, following the mmap_lock tracepoint pattern. The implementation includes lightweight inline helpers that check whether tracing is enabled and call out-of-line helpers when active. When CONFIG_TRACING is disabled, these helpers compile to empty inline stubs.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin",
              "summary": "No comments from the author in this thread.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 1/4] mm: introduce zone lock wrappers",
          "message_id": "aaCI_-bNm-YGGXI1@shell.ilvokhin.com",
          "url": "https://lore.kernel.org/all/aaCI_-bNm-YGGXI1@shell.ilvokhin.com/",
          "date": "2026-02-26T17:55:14Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Introduction of zone lock wrappers for simplification and tracepoint addition.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin",
              "summary": "Expressed support for the idea of zone lock wrappers but suggested keeping changes mechanical and focused on introducing wrappers and tracepoints. Proposed a separate patch to explore adding guards for zone locks.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v2 3/4] mm: convert compaction to zone lock wrappers",
          "message_id": "aaCGrl96pD-0_ts4@shell.ilvokhin.com",
          "url": "https://lore.kernel.org/all/aaCGrl96pD-0_ts4@shell.ilvokhin.com/",
          "date": "2026-02-26T17:45:21Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Author is revising the patch to introduce two new helper functions instead of a single abstraction.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin",
              "summary": "Acknowledged Andrew's feedback on unnecessary abstraction, agreed to introduce two new helper functions in v3.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed with reviewer"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v2 1/4] mm: introduce zone lock wrappers",
          "message_id": "aaBk1sVcy87EPQ2D@shell.ilvokhin.com",
          "url": "https://lore.kernel.org/all/aaBk1sVcy87EPQ2D@shell.ilvokhin.com/",
          "date": "2026-02-26T15:21:03Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Author is revising the patch to introduce two new helper functions instead of a single abstraction.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Dmitry Ilvokhin",
              "summary": "Acknowledged Andrew's feedback on unnecessary abstraction, agreed to introduce two new helper functions in v3.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed with reviewer"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Gregory Price",
      "primary_email": "gourry@gourry.net",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[RFC PATCH v4 27/27] cxl: add cxl_compression PCI driver",
          "message_id": "20260222084842.1824063-28-gourry@gourry.net",
          "url": "https://lore.kernel.org/all/20260222084842.1824063-28-gourry@gourry.net/",
          "date": "2026-02-22T08:50:38Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-22",
          "patch_summary": "This patch introduces a new PCI driver called cxl_compression, which is part of a larger series that adds support for Private Memory Nodes (PMNs) and Compressed RAM. The PMN feature allows for the creation of isolated NUMA nodes, while Compressed RAM enables the compression of memory pages to reduce memory usage. The cxl_compression driver is designed to work with CXL (Compute Express Link) devices, which are used to manage compressed memory. This patch adds the necessary infrastructure to support the cxl_compression driver and allows for the creation of PMNs and Compressed RAM services.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the node_private infrastructure being mutually exclusive with N_MEMORY, explained that it's intended for memory nodes not meant for general consumption, and confirmed that Zonelist construction changes are deferred to a subsequent commit.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "N_MEMORY nodes are intended to contain general System RAM. Today, some\ndevice drivers hotplug their memory (marked Specific Purpose or Reserved)\nto get access to mm/ services, but don't intend it for general consumption.\n\nCreate N_MEMORY_PRIVATE for memory nodes whose memory is not intended for\ngeneral consumption. This state is mutually exclusive with N_MEMORY.\n\nAdd the node_private infrastructure for N_MEMORY_PRIVATE nodes:\n\n  - struct node_private: Per-node container stored in NODE_DATA(nid),\n    holding driver callbacks (ops), owner, and refcount.\n\n  - struct node_private_ops: Initial structure with void *reserved\n    placeholder and flags field.  Callbacks will be added by subsequent\n    commits as each consumer is wired up.\n\n  - folio_is_private_node() / page_is_private_node(): check if a\n    folio/page resides on a private node.\n\n  - folio_node_private_ops() / node_private_flags(): retrieve the ops\n    vtable or flags for a folio's node.\n\n  - Registration API: node_private_register()/unregister() for drivers\n    to register callbacks for private nodes. Only one driver callback\n    can be registered per node - attempting to register different ops\n    returns -EBUSY.\n\n  - sysfs attribute exposing N_MEMORY_PRIVATE node state.\n\nZonelist construction changes for private nodes are deferred to a\nsubsequent commit.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          | 197 ++++++++++++++++++++++++++++++++\n include/linux/mmzone.h       |   4 +\n include/linux/node_private.h | 210 +++++++++++++++++++++++++++++++++++\n include/linux/nodemask.h     |   1 +\n 4 files changed, 412 insertions(+)\n create mode 100644 include/linux/node_private.h\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex 00cf4532f121..646dc48a23b5 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -22,6 +22,7 @@\n #include <linux/swap.h>\n #include <linux/slab.h>\n #include <linux/memblock.h>\n+#include <linux/node_private.h>\n \n static const struct bus_type node_subsys = {\n \t.name = \"node\",\n@@ -861,6 +862,198 @@ void register_memory_blocks_under_node_hotplug(int nid, unsigned long start_pfn,\n \t\t\t   (void *)&nid, register_mem_block_under_node_hotplug);\n \treturn;\n }\n+\n+static DEFINE_MUTEX(node_private_lock);\n+static bool node_private_initialized;\n+\n+/**\n+ * node_private_register - Register a private node\n+ * @nid: Node identifier\n+ * @np: The node_private structure (driver-allocated, driver-owned)\n+ *\n+ * Register a driver for a private node. Only one driver can register\n+ * per node. If another driver has already registered (with different np),\n+ * -EBUSY is returned. Re-registration with the same np is allowed.\n+ *\n+ * The driver owns the node_private memory and must ensure it remains valid\n+ * until refcount reaches 0 after node_private_unregister().\n+ *\n+ * Returns 0 on success, negative errno on failure.\n+ */\n+int node_private_register(int nid, struct node_private *np)\n+{\n+\tstruct node_private *existing;\n+\tpg_data_t *pgdat;\n+\tint ret = 0;\n+\n+\tif (!np || !node_possible(nid))\n+\t\treturn -EINVAL;\n+\n+\tif (!node_private_initialized)\n+\t\treturn -ENODEV;\n+\n+\tmutex_lock(&node_private_lock);\n+\tmem_hotplug_begin();\n+\n+\t/* N_MEMORY_PRIVATE and N_MEMORY are mutually exclusive */\n+\tif (node_state(nid, N_MEMORY)) {\n+\t\tret = -EBUSY;\n+\t\tgoto out;\n+\t}\n+\n+\tpgdat = NODE_DATA(nid);\n+\texisting = rcu_dereference_protected(pgdat->node_private,\n+\t\t\t\t\t     lockdep_is_held(&node_private_lock));\n+\n+\t/* Only one source my register this node */\n+\tif (existing) {\n+\t\tif (existing != np) {\n+\t\t\tret = -EBUSY;\n+\t\t\tgoto out;\n+\t\t}\n+\t\tgoto out;\n+\t}\n+\n+\trefcount_set(&np->refcount, 1);\n+\tinit_completion(&np->released);\n+\n+\trcu_assign_pointer(pgdat->node_private, np);\n+\tpgdat->private = true;\n+\n+out:\n+\tmem_hotplug_done();\n+\tmutex_unlock(&node_private_lock);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(node_private_register);\n+\n+/**\n+ * node_private_set_ops - Set service callbacks on a registered private node\n+ * @nid: Node identifier\n+ * @ops: Service callbacks and flags (driver-owned, must outlive registration)\n+ *\n+ * Validates flag dependencies and sets the ops on the node's node_private.\n+ * The node must already be registered via node_private_register().\n+ *\n+ * Returns 0 on success, -EINVAL for invalid flag combinations,\n+ * -ENODEV if no node_private is registered on @nid.\n+ */\n+int node_private_set_ops(int nid, const struct node_private_ops *ops)\n+{\n+\tstruct node_private *np;\n+\tint ret = 0;\n+\n+\tif (!ops)\n+\t\treturn -EINVAL;\n+\n+\tif (!node_possible(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&node_private_lock);\n+\tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n+\t\t\t\t       lockdep_is_held(&node_private_lock));\n+\tif (!np)\n+\t\tret = -ENODEV;\n+\telse\n+\t\tnp->ops = ops;\n+\tmutex_unlock(&node_private_lock);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(node_private_set_ops);\n+\n+/**\n+ * node_private_clear_ops - Clear service callbacks from a private node\n+ * @nid: Node identifier\n+ * @ops: Expected ops pointer (must match current ops)\n+ *\n+ * Clears the ops only if @ops matches the currently registered ops,\n+ * preventing one service from accidentally clearing another's callbacks.\n+ *\n+ * Returns 0 on success, -ENODEV if no node_private is registered,\n+ * -EINVAL if @ops does not match.\n+ */\n+int node_private_clear_ops(int nid, const struct node_private_ops *ops)\n+{\n+\tstruct node_private *np;\n+\tint ret = 0;\n+\n+\tif (!node_possible(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&node_private_lock);\n+\tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n+\t\t\t\t       lockdep_is_held(&node_private_lock));\n+\tif (!np)\n+\t\tret = -ENODEV;\n+\telse if (np->ops != ops)\n+\t\tret = -EINVAL;\n+\telse\n+\t\tnp->ops = NULL;\n+\tmutex_unlock(&node_private_lock);\n+\treturn ret;\n+}\n+EXPORT_SYMBOL_GPL(node_private_clear_ops);\n+\n+/**\n+ * node_private_unregister - Unregister a private node\n+ * @nid: Node identifier\n+ *\n+ * Unregister the driver from a private node. Only succeeds if all memory\n+ * has been offlined and the node is no longer N_MEMORY_PRIVATE.\n+ * When successful, drops the refcount to 0 indicating the driver can\n+ * free its context.\n+ *\n+ * N_MEMORY_PRIVATE state is cleared by offline_pages() when the last\n+ * memory is offlined, not by this function.\n+ *\n+ * Return: 0 if unregistered, -EBUSY if N_MEMORY_PRIVATE is still set\n+ * (other memory blocks remain on this node).\n+ */\n+int node_private_unregister(int nid)\n+{\n+\tstruct node_private *np;\n+\tpg_data_t *pgdat;\n+\n+\tif (!node_possible(nid))\n+\t\treturn 0;\n+\n+\tmutex_lock(&node_private_lock);\n+\tmem_hotplug_begin();\n+\n+\tpgdat = NODE_DATA(nid);\n+\tnp = rcu_dereference_protected(pgdat->node_private,\n+\t\t\t\t       lockdep_is_held(&node_private_lock));\n+\tif (!np) {\n+\t\tmem_hotplug_done();\n+\t\tmutex_unlock(&node_private_lock);\n+\t\treturn 0;\n+\t}\n+\n+\t/*\n+\t * Only unregister if all memory is offline and N_MEMORY_PRIVATE is\n+\t * cleared. N_MEMORY_PRIVATE is cleared by offline_pages() when the\n+\t * last memory block is offlined.\n+\t */\n+\tif (node_state(nid, N_MEMORY_PRIVATE)) {\n+\t\tmem_hotplug_done();\n+\t\tmutex_unlock(&node_private_lock);\n+\t\treturn -EBUSY;\n+\t}\n+\n+\trcu_assign_pointer(pgdat->node_private, NULL);\n+\tpgdat->private = false;\n+\n+\tmem_hotplug_done();\n+\tmutex_unlock(&node_private_lock);\n+\n+\tsynchronize_rcu();\n+\n+\tif (!refcount_dec_and_test(&np->refcount))\n+\t\twait_for_completion(&np->released);\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(node_private_unregister);\n+\n #endif /* CONFIG_MEMORY_HOTPLUG */\n \n /**\n@@ -959,6 +1152,7 @@ static struct node_attr node_state_attr[] = {\n \t[N_HIGH_MEMORY] = _NODE_ATTR(has_high_memory, N_HIGH_MEMORY),\n #endif\n \t[N_MEMORY] = _NODE_ATTR(has_memory, N_MEMORY),\n+\t[N_MEMORY_PRIVATE] = _NODE_ATTR(has_private_memory, N_MEMORY_PRIVATE),\n \t[N_CPU] = _NODE_ATTR(has_cpu, N_CPU),\n \t[N_GENERIC_INITIATOR] = _NODE_ATTR(has_generic_initiator,\n \t\t\t\t\t   N_GENERIC_INITIATOR),\n@@ -972,6 +1166,7 @@ static struct attribute *node_state_attrs[] = {\n \t&node_state_attr[N_HIGH_MEMORY].attr.attr,\n #endif\n \t&node_state_attr[N_MEMORY].attr.attr,\n+\t&node_state_attr[N_MEMORY_PRIVATE].attr.attr,\n \t&node_state_attr[N_CPU].attr.attr,\n \t&node_state_attr[N_GENERIC_INITIATOR].attr.attr,\n \tNULL\n@@ -1007,5 +1202,7 @@ void __init node_dev_init(void)\n \t\t\tpanic(\"%s() failed to add node: %d\\n\", __func__, ret);\n \t}\n \n+\tnode_private_initialized = true;\n+\n \tregister_memory_blocks_under_nodes();\n }\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex b01cb1e49896..992eb1c5a2c6 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -25,6 +25,8 @@\n #include <linux/zswap.h>\n #include <asm/page.h>\n \n+struct node_private;\n+\n /* Free memory management - zoned buddy allocator.  */\n #ifndef CONFIG_ARCH_FORCE_MAX_ORDER\n #define MAX_PAGE_ORDER 10\n@@ -1514,6 +1516,8 @@ typedef struct pglist_data {\n \tatomic_long_t\t\tvm_stat[NR_VM_NODE_STAT_ITEMS];\n #ifdef CONFIG_NUMA\n \tstruct memory_tier __rcu *memtier;\n+\tstruct node_private __rcu *node_private;\n+\tbool private;\n #endif\n #ifdef CONFIG_MEMORY_FAILURE\n \tstruct memory_failure_stats mf_stats;\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nnew file mode 100644\nindex 000000000000..6a70ec39d569\n--- /dev/null\n+++ b/include/linux/node_private.h\n@@ -0,0 +1,210 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#ifndef _LINUX_NODE_PRIVATE_H\n+#define _LINUX_NODE_PRIVATE_H\n+\n+#include <linux/completion.h>\n+#include <linux/mm.h>\n+#include <linux/nodemask.h>\n+#include <linux/rcupdate.h>\n+#include <linux/refcount.h>\n+\n+struct page;\n+struct vm_area_struct;\n+struct vm_fault;\n+\n+/**\n+ * struct node_private_ops - Callbacks for private node services\n+ *\n+ * Services register these callbacks to intercept MM operations that affect\n+ * their private nodes.\n+ *\n+ * Flag bits control which MM subsystems may operate on folios on this node.\n+ *\n+ * The pgdat->node_private pointer is RCU-protected.  Callbacks fall into\n+ * three categories based on their calling context:\n+ *\n+ * Folio-referenced callbacks (RCU released before callback):\n+ *   The caller holds a reference to a folio on the private node, which\n+ *   pins the node's memory online and prevents node_private teardown.\n+ *\n+ * Refcounted callbacks (RCU released before callback):\n+ *   The caller has no folio on the private node (e.g., folios are on a\n+ *   source node being migrated TO this node).  A temporary refcount is\n+ *   taken on node_private under rcu_read_lock to keep the structure (and\n+ *   the service module) alive across the callback.  node_private_unregister\n+ *   waits for all temporary references to drain before returning.\n+ *\n+ * Non-folio callbacks (rcu_read_lock held during callback):\n+ *   No folio reference exists, so rcu_read_lock is held across the\n+ *   callback to prevent node_private from being freed.\n+ *   These callbacks MUST NOT sleep.\n+ *\n+ * @flags: Operation exclusion flags (NP_OPS_* constants).\n+ *\n+ */\n+struct node_private_ops {\n+\tunsigned long flags;\n+};\n+\n+/**\n+ * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n+ *\n+ * This structure is allocated by the driver and passed to node_private_register().\n+ * The driver owns the memory and must ensure it remains valid until after\n+ * node_private_unregister() returns with the reference count dropped to 0.\n+ *\n+ * @owner: Opaque driver identifier\n+ * @refcount: Reference count (1 = registered; temporary refs for non-folio\n+ *\t\tcallbacks that may sleep; 0 = fully released)\n+ * @released: Signaled when refcount drops to 0; unregister waits on this\n+ * @ops: Service callbacks and exclusion flags (NULL until service registers)\n+ */\n+struct node_private {\n+\tvoid *owner;\n+\trefcount_t refcount;\n+\tstruct completion released;\n+\tconst struct node_private_ops *ops;\n+};\n+\n+#ifdef CONFIG_NUMA\n+\n+#include <linux/mmzone.h>\n+\n+/**\n+ * folio_is_private_node - Check if folio is on an N_MEMORY_PRIVATE node\n+ * @folio: The folio to check\n+ *\n+ * Returns true if the folio resides on a private node.\n+ */\n+static inline bool folio_is_private_node(struct folio *folio)\n+{\n+\treturn node_state(folio_nid(folio), N_MEMORY_PRIVATE);\n+}\n+\n+/**\n+ * page_is_private_node - Check if page is on an N_MEMORY_PRIVATE node\n+ * @page: The page to check\n+ *\n+ * Returns true if the page resides on a private node.\n+ */\n+static inline bool page_is_private_node(struct page *page)\n+{\n+\treturn node_state(page_to_nid(page), N_MEMORY_PRIVATE);\n+}\n+\n+static inline const struct node_private_ops *\n+folio_node_private_ops(struct folio *folio)\n+{\n+\tconst struct node_private_ops *ops;\n+\tstruct node_private *np;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(folio_nid(folio))->node_private);\n+\tops = np ? np->ops : NULL;\n+\trcu_read_unlock();\n+\n+\treturn ops;\n+}\n+\n+static inline unsigned long node_private_flags(int nid)\n+{\n+\tstruct node_private *np;\n+\tunsigned long flags;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tflags = (np && np->ops) ? np->ops->flags : 0;\n+\trcu_read_unlock();\n+\n+\treturn flags;\n+}\n+\n+static inline bool folio_private_flags(struct folio *f, unsigned long flag)\n+{\n+\treturn node_private_flags(folio_nid(f)) & flag;\n+}\n+\n+static inline bool node_private_has_flag(int nid, unsigned long flag)\n+{\n+\treturn node_private_flags(nid) & flag;\n+}\n+\n+static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n+{\n+\treturn node_private_flags(zone_to_nid(z)) & flag;\n+}\n+\n+#else /* !CONFIG_NUMA */\n+\n+static inline bool folio_is_private_node(struct folio *folio)\n+{\n+\treturn false;\n+}\n+\n+static inline bool page_is_private_node(struct page *page)\n+{\n+\treturn false;\n+}\n+\n+static inline const struct node_private_ops *\n+folio_node_private_ops(struct folio *folio)\n+{\n+\treturn NULL;\n+}\n+\n+static inline unsigned long node_private_flags(int nid)\n+{\n+\treturn 0;\n+}\n+\n+static inline bool folio_private_flags(struct folio *f, unsigned long flag)\n+{\n+\treturn false;\n+}\n+\n+static inline bool node_private_has_flag(int nid, unsigned long flag)\n+{\n+\treturn false;\n+}\n+\n+static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n+{\n+\treturn false;\n+}\n+\n+#endif /* CONFIG_NUMA */\n+\n+#if defined(CONFIG_NUMA) && defined(CONFIG_MEMORY_HOTPLUG)\n+\n+int node_private_register(int nid, struct node_private *np);\n+int node_private_unregister(int nid);\n+int node_private_set_ops(int nid, const struct node_private_ops *ops);\n+int node_private_clear_ops(int nid, const struct node_private_ops *ops);\n+\n+#else /* !CONFIG_NUMA || !CONFIG_MEMORY_HOTPLUG */\n+\n+static inline int node_private_register(int nid, struct node_private *np)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int node_private_unregister(int nid)\n+{\n+\treturn 0;\n+}\n+\n+static inline int node_private_set_ops(int nid,\n+\t\t\t\t       const struct node_private_ops *ops)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int node_private_clear_ops(int nid,\n+\t\t\t\t\t const struct node_private_ops *ops)\n+{\n+\treturn -ENODEV;\n+}\n+\n+#endif /* CONFIG_NUMA && CONFIG_MEMORY_HOTPLUG */\n+\n+#endif /* _LINUX_NODE_PRIVATE_H */\ndiff --git a/include/linux/nodemask.h b/include/linux/nodemask.h\nindex bd38648c998d..c9bcfd5a9a06 100644\n--- a/include/linux/nodemask.h\n+++ b/include/linux/nodemask.h\n@@ -391,6 +391,7 @@ enum node_states {\n \tN_HIGH_MEMORY = N_NORMAL_MEMORY,\n #endif\n \tN_MEMORY,\t\t/* The node has memory(regular, high, movable) */\n+\tN_MEMORY_PRIVATE,\t/* The node's memory is private */\n \tN_CPU,\t\t/* The node has one or more cpus */\n \tN_GENERIC_INITIATOR,\t/* The node has one or more Generic Initiators */\n \tNR_NODE_STATES\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about general allocations landing on private nodes without explicit permission by introducing __GFP_PRIVATE and updating cpuset_current_node_allowed() to filter out N_MEMORY_PRIVATE nodes unless this flag is set.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "N_MEMORY_PRIVATE nodes hold device-managed memory that should not be\nused for general allocations. Without a gating mechanism, any allocation\ncould land on a private node if it appears in the task's mems_allowed.\n\nIntroduce __GFP_PRIVATE that explicitly opts in to allocation from\nN_MEMORY_PRIVATE nodes.\n\nAdd the GFP_PRIVATE compound mask (__GFP_PRIVATE | __GFP_THISNODE)\nfor callers that explicitly target private nodes to help prevent\nfallback allocations from DRAM.\n\nUpdate cpuset_current_node_allowed() to filter out N_MEMORY_PRIVATE\nnodes unless __GFP_PRIVATE is set.\n\nIn interrupt context, only N_MEMORY nodes are valid.\n\nUpdate cpuset_handle_hotplug() to include N_MEMORY_PRIVATE nodes in\nthe effective mems set, allowing cgroup-level control over private\nnode access.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/gfp_types.h      | 15 +++++++++++++--\n include/trace/events/mmflags.h |  4 ++--\n kernel/cgroup/cpuset.c         | 32 ++++++++++++++++++++++++++++----\n 3 files changed, 43 insertions(+), 8 deletions(-)\n\ndiff --git a/include/linux/gfp_types.h b/include/linux/gfp_types.h\nindex 3de43b12209e..ac375f9a0fc2 100644\n--- a/include/linux/gfp_types.h\n+++ b/include/linux/gfp_types.h\n@@ -33,7 +33,7 @@ enum {\n \t___GFP_IO_BIT,\n \t___GFP_FS_BIT,\n \t___GFP_ZERO_BIT,\n-\t___GFP_UNUSED_BIT,\t/* 0x200u unused */\n+\t___GFP_PRIVATE_BIT,\n \t___GFP_DIRECT_RECLAIM_BIT,\n \t___GFP_KSWAPD_RECLAIM_BIT,\n \t___GFP_WRITE_BIT,\n@@ -69,7 +69,7 @@ enum {\n #define ___GFP_IO\t\tBIT(___GFP_IO_BIT)\n #define ___GFP_FS\t\tBIT(___GFP_FS_BIT)\n #define ___GFP_ZERO\t\tBIT(___GFP_ZERO_BIT)\n-/* 0x200u unused */\n+#define ___GFP_PRIVATE\t\tBIT(___GFP_PRIVATE_BIT)\n #define ___GFP_DIRECT_RECLAIM\tBIT(___GFP_DIRECT_RECLAIM_BIT)\n #define ___GFP_KSWAPD_RECLAIM\tBIT(___GFP_KSWAPD_RECLAIM_BIT)\n #define ___GFP_WRITE\t\tBIT(___GFP_WRITE_BIT)\n@@ -139,6 +139,11 @@ enum {\n  * %__GFP_ACCOUNT causes the allocation to be accounted to kmemcg.\n  *\n  * %__GFP_NO_OBJ_EXT causes slab allocation to have no object extension.\n+ *\n+ * %__GFP_PRIVATE allows allocation from N_MEMORY_PRIVATE nodes (e.g., compressed\n+ * memory, accelerator memory). Without this flag, allocations are restricted\n+ * to N_MEMORY nodes only. Used by migration/demotion paths when explicitly\n+ * targeting private nodes.\n  */\n #define __GFP_RECLAIMABLE ((__force gfp_t)___GFP_RECLAIMABLE)\n #define __GFP_WRITE\t((__force gfp_t)___GFP_WRITE)\n@@ -146,6 +151,7 @@ enum {\n #define __GFP_THISNODE\t((__force gfp_t)___GFP_THISNODE)\n #define __GFP_ACCOUNT\t((__force gfp_t)___GFP_ACCOUNT)\n #define __GFP_NO_OBJ_EXT   ((__force gfp_t)___GFP_NO_OBJ_EXT)\n+#define __GFP_PRIVATE\t((__force gfp_t)___GFP_PRIVATE)\n \n /**\n  * DOC: Watermark modifiers\n@@ -367,6 +373,10 @@ enum {\n  * available and will not wake kswapd/kcompactd on failure. The _LIGHT\n  * version does not attempt reclaim/compaction at all and is by default used\n  * in page fault path, while the non-light is used by khugepaged.\n+ *\n+ * %GFP_PRIVATE adds %__GFP_THISNODE by default to prevent any fallback\n+ * allocations to other nodes, given that the caller was already attempting\n+ * to access driver-managed memory explicitly.\n  */\n #define GFP_ATOMIC\t(__GFP_HIGH|__GFP_KSWAPD_RECLAIM)\n #define GFP_KERNEL\t(__GFP_RECLAIM | __GFP_IO | __GFP_FS)\n@@ -382,5 +392,6 @@ enum {\n #define GFP_TRANSHUGE_LIGHT\t((GFP_HIGHUSER_MOVABLE | __GFP_COMP | \\\n \t\t\t __GFP_NOMEMALLOC | __GFP_NOWARN) & ~__GFP_RECLAIM)\n #define GFP_TRANSHUGE\t(GFP_TRANSHUGE_LIGHT | __GFP_DIRECT_RECLAIM)\n+#define GFP_PRIVATE\t(__GFP_PRIVATE | __GFP_THISNODE)\n \n #endif /* __LINUX_GFP_TYPES_H */\ndiff --git a/include/trace/events/mmflags.h b/include/trace/events/mmflags.h\nindex a6e5a44c9b42..f042cd848451 100644\n--- a/include/trace/events/mmflags.h\n+++ b/include/trace/events/mmflags.h\n@@ -37,7 +37,8 @@\n \tTRACE_GFP_EM(HARDWALL)\t\t\t\\\n \tTRACE_GFP_EM(THISNODE)\t\t\t\\\n \tTRACE_GFP_EM(ACCOUNT)\t\t\t\\\n-\tTRACE_GFP_EM(ZEROTAGS)\n+\tTRACE_GFP_EM(ZEROTAGS)\t\t\t\\\n+\tTRACE_GFP_EM(PRIVATE)\n \n #ifdef CONFIG_KASAN_HW_TAGS\n # define TRACE_GFP_FLAGS_KASAN\t\t\t\\\n@@ -73,7 +74,6 @@\n TRACE_GFP_FLAGS\n \n /* Just in case these are ever used */\n-TRACE_DEFINE_ENUM(___GFP_UNUSED_BIT);\n TRACE_DEFINE_ENUM(___GFP_LAST_BIT);\n \n #define gfpflag_string(flag) {(__force unsigned long)flag, #flag}\ndiff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c\nindex 473aa9261e16..1a597f0c7c6c 100644\n--- a/kernel/cgroup/cpuset.c\n+++ b/kernel/cgroup/cpuset.c\n@@ -444,21 +444,32 @@ static void guarantee_active_cpus(struct task_struct *tsk,\n }\n \n /*\n- * Return in *pmask the portion of a cpusets's mems_allowed that\n+ * Return in *pmask the portion of a cpuset's mems_allowed that\n  * are online, with memory.  If none are online with memory, walk\n  * up the cpuset hierarchy until we find one that does have some\n  * online mems.  The top cpuset always has some mems online.\n  *\n  * One way or another, we guarantee to return some non-empty subset\n- * of node_states[N_MEMORY].\n+ * of node_states[N_MEMORY].  N_MEMORY_PRIVATE nodes from the\n+ * original cpuset are preserved, but only N_MEMORY nodes are\n+ * pulled from ancestors.\n  *\n  * Call with callback_lock or cpuset_mutex held.\n  */\n static void guarantee_online_mems(struct cpuset *cs, nodemask_t *pmask)\n {\n+\tstruct cpuset *orig_cs = cs;\n+\tint nid;\n+\n \twhile (!nodes_intersects(cs->effective_mems, node_states[N_MEMORY]))\n \t\tcs = parent_cs(cs);\n+\n \tnodes_and(*pmask, cs->effective_mems, node_states[N_MEMORY]);\n+\n+\tfor_each_node_state(nid, N_MEMORY_PRIVATE) {\n+\t\tif (node_isset(nid, orig_cs->effective_mems))\n+\t\t\tnode_set(nid, *pmask);\n+\t}\n }\n \n /**\n@@ -4075,7 +4086,9 @@ static void cpuset_handle_hotplug(void)\n \n \t/* fetch the available cpus/mems and find out which changed how */\n \tcpumask_copy(&new_cpus, cpu_active_mask);\n-\tnew_mems = node_states[N_MEMORY];\n+\n+\t/* Include N_MEMORY_PRIVATE so cpuset controls access the same way */\n+\tnodes_or(new_mems, node_states[N_MEMORY], node_states[N_MEMORY_PRIVATE]);\n \n \t/*\n \t * If subpartitions_cpus is populated, it is likely that the check\n@@ -4488,10 +4501,21 @@ bool cpuset_node_allowed(struct cgroup *cgroup, int nid)\n  * __alloc_pages() will include all nodes.  If the slab allocator\n  * is passed an offline node, it will fall back to the local node.\n  * See kmem_cache_alloc_node().\n+ *\n+ *\n+ * Private nodes aren't eligible for these allocations, so skip them.\n+ * guarantee_online_mems guaranttes at least one N_MEMORY node is set.\n  */\n static int cpuset_spread_node(int *rotor)\n {\n-\treturn *rotor = next_node_in(*rotor, current->mems_allowed);\n+\tint node;\n+\n+\tdo {\n+\t\tnode = next_node_in(*rotor, current->mems_allowed);\n+\t\t*rotor = node;\n+\t} while (node_state(node, N_MEMORY_PRIVATE));\n+\n+\treturn node;\n }\n \n /**\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern that the open-coded cpuset filtering in mm/ does not account for N_MEMORY_PRIVATE nodes on systems without cpusets, which can lead to private-node zones leaking into allocation paths. The author added a new helper function numa_zone_allowed() and replaced the open-coded patterns with it.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Various locations in mm/ open-code cpuset filtering with:\n\n  cpusets_enabled() && ALLOC_CPUSET && !__cpuset_zone_allowed()\n\nThis pattern does not account for N_MEMORY_PRIVATE nodes on systems\nwithout cpusets, so private-node zones can leak into allocation\npaths that should only see general-purpose memory.\n\nAdd numa_zone_allowed() which consolidates zone filtering. It checks\ncpuset membership when cpusets are enabled, and otherwise gates\nN_MEMORY_PRIVATE zones behind __GFP_PRIVATE globally.\n\nReplace the open-coded patterns in mm/ with the new helper.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/compaction.c |  6 ++----\n mm/hugetlb.c    |  2 +-\n mm/internal.h   |  7 +++++++\n mm/page_alloc.c | 31 ++++++++++++++++++++-----------\n mm/slub.c       |  3 ++-\n 5 files changed, 32 insertions(+), 17 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..6a65145b03d8 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -2829,10 +2829,8 @@ enum compact_result try_to_compact_pages(gfp_t gfp_mask, unsigned int order,\n \t\t\t\t\tac->highest_zoneidx, ac->nodemask) {\n \t\tenum compact_result status;\n \n-\t\tif (cpusets_enabled() &&\n-\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n-\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n-\t\t\t\tcontinue;\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n+\t\t\tcontinue;\n \n \t\tif (prio > MIN_COMPACT_PRIORITY\n \t\t\t\t\t&& compaction_deferred(zone, order)) {\ndiff --git a/mm/hugetlb.c b/mm/hugetlb.c\nindex 51273baec9e5..f2b914ab5910 100644\n--- a/mm/hugetlb.c\n+++ b/mm/hugetlb.c\n@@ -1353,7 +1353,7 @@ static struct folio *dequeue_hugetlb_folio_nodemask(struct hstate *h, gfp_t gfp_\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist, gfp_zone(gfp_mask), nmask) {\n \t\tstruct folio *folio;\n \n-\t\tif (!cpuset_zone_allowed(zone, gfp_mask))\n+\t\tif (!numa_zone_alloc_allowed(ALLOC_CPUSET, zone, gfp_mask))\n \t\t\tcontinue;\n \t\t/*\n \t\t * no need to ask again on the same node. Pool is node rather than\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 23ee14790227..97023748e6a9 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1206,6 +1206,8 @@ extern int node_reclaim_mode;\n \n extern int node_reclaim(struct pglist_data *, gfp_t, unsigned int);\n extern int find_next_best_node(int node, nodemask_t *used_node_mask);\n+extern bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n+\t\t\t      gfp_t gfp_mask);\n #else\n #define node_reclaim_mode 0\n \n@@ -1218,6 +1220,11 @@ static inline int find_next_best_node(int node, nodemask_t *used_node_mask)\n {\n \treturn NUMA_NO_NODE;\n }\n+static inline bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n+\t\t\t\t     gfp_t gfp_mask)\n+{\n+\treturn true;\n+}\n #endif\n \n static inline bool node_reclaim_enabled(void)\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 2facee0805da..47f2619d3840 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -3690,6 +3690,21 @@ static bool zone_allows_reclaim(struct zone *local_zone, struct zone *zone)\n \treturn node_distance(zone_to_nid(local_zone), zone_to_nid(zone)) <=\n \t\t\t\tnode_reclaim_distance;\n }\n+\n+/* Returns true if allocation from this zone is permitted */\n+bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone, gfp_t gfp_mask)\n+{\n+\t/* Gate N_MEMORY_PRIVATE zones behind __GFP_PRIVATE */\n+\tif (!(gfp_mask & __GFP_PRIVATE) &&\n+\t    node_state(zone_to_nid(zone), N_MEMORY_PRIVATE))\n+\t\treturn false;\n+\n+\t/* If cpusets is being used, check mems_allowed */\n+\tif (cpusets_enabled() && (alloc_flags & ALLOC_CPUSET))\n+\t\treturn cpuset_zone_allowed(zone, gfp_mask);\n+\n+\treturn true;\n+}\n #else\t/* CONFIG_NUMA */\n static bool zone_allows_reclaim(struct zone *local_zone, struct zone *zone)\n {\n@@ -3781,10 +3796,8 @@ get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,\n \t\tstruct page *page;\n \t\tunsigned long mark;\n \n-\t\tif (cpusets_enabled() &&\n-\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n-\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n-\t\t\t\tcontinue;\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n+\t\t\tcontinue;\n \t\t/*\n \t\t * When allocating a page cache page for writing, we\n \t\t * want to get it from a node that is within its dirty\n@@ -4585,10 +4598,8 @@ should_reclaim_retry(gfp_t gfp_mask, unsigned order,\n \t\tunsigned long min_wmark = min_wmark_pages(zone);\n \t\tbool wmark;\n \n-\t\tif (cpusets_enabled() &&\n-\t\t\t(alloc_flags & ALLOC_CPUSET) &&\n-\t\t\t!__cpuset_zone_allowed(zone, gfp_mask))\n-\t\t\t\tcontinue;\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n+\t\t\tcontinue;\n \n \t\tavailable = reclaimable = zone_reclaimable_pages(zone);\n \t\tavailable += zone_page_state_snapshot(zone, NR_FREE_PAGES);\n@@ -5084,10 +5095,8 @@ unsigned long alloc_pages_bulk_noprof(gfp_t gfp, int preferred_nid,\n \tfor_next_zone_zonelist_nodemask(zone, z, ac.highest_zoneidx, ac.nodemask) {\n \t\tunsigned long mark;\n \n-\t\tif (cpusets_enabled() && (alloc_flags & ALLOC_CPUSET) &&\n-\t\t    !__cpuset_zone_allowed(zone, gfp)) {\n+\t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp))\n \t\t\tcontinue;\n-\t\t}\n \n \t\tif (nr_online_nodes > 1 && zone != zonelist_zone(ac.preferred_zoneref) &&\n \t\t    zone_to_nid(zone) != zonelist_node_idx(ac.preferred_zoneref)) {\ndiff --git a/mm/slub.c b/mm/slub.c\nindex 861592ac5425..e4bd6ede81d1 100644\n--- a/mm/slub.c\n+++ b/mm/slub.c\n@@ -3595,7 +3595,8 @@ static struct slab *get_any_partial(struct kmem_cache *s,\n \n \t\t\tn = get_node(s, zone_to_nid(zone));\n \n-\t\t\tif (n && cpuset_zone_allowed(zone, pc->flags) &&\n+\t\t\tif (n && numa_zone_alloc_allowed(ALLOC_CPUSET, zone,\n+\t\t\t\t\t\t   pc->flags) &&\n \t\t\t\t\tn->nr_partial > s->min_partial) {\n \t\t\t\tslab = get_partial_node(s, n, pc);\n \t\t\t\tif (slab) {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about N_MEMORY fallback lists including N_MEMORY_PRIVATE nodes, explaining that this would allow allocations from private nodes in some scenarios and cause unnecessary iterations over ineligible nodes. The author provided a patch to fix the issue by adding private nodes as fallbacks for kernel allocations on behalf of the private node.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "N_MEMORY fallback lists should not include N_MEMORY_PRIVATE nodes, at\nworst this would allow allocation from them in some scenarios, and at\nbest it causes iterations over nodes that aren't eligible.\n\nPrivate node primary fallback lists do include N_MEMORY nodes so\nkernel/slab allocations made on behalf of the private node can\nfall back to DRAM when __GFP_PRIVATE is not set.\n\nThe nofallback list contains only the node's own zones, restricting\n__GFP_THISNODE allocations to the private node.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/page_alloc.c | 20 ++++++++++++++++++++\n 1 file changed, 20 insertions(+)\n\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 47f2619d3840..5a1b35421d78 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -5683,6 +5683,26 @@ static void build_zonelists(pg_data_t *pgdat)\n \tlocal_node = pgdat->node_id;\n \tprev_node = local_node;\n \n+\t/*\n+\t * Private nodes need N_MEMORY nodes as fallback for kernel allocations\n+\t * (e.g., slab objects allocated on behalf of this node).\n+\t */\n+\tif (node_state(local_node, N_MEMORY_PRIVATE)) {\n+\t\tnode_order[nr_nodes++] = local_node;\n+\t\tnode_set(local_node, used_mask);\n+\n+\t\twhile ((node = find_next_best_node(local_node, &used_mask)) >= 0)\n+\t\t\tnode_order[nr_nodes++] = node;\n+\n+\t\tbuild_zonelists_in_node_order(pgdat, node_order, nr_nodes);\n+\t\tbuild_thisnode_zonelists(pgdat);\n+\t\tpr_info(\"Fallback order for Node %d (private):\", local_node);\n+\t\tfor (node = 0; node < nr_nodes; node++)\n+\t\t\tpr_cont(\" %d\", node_order[node]);\n+\t\tpr_cont(\"\\n\");\n+\t\treturn;\n+\t}\n+\n \tmemset(node_order, 0, sizeof(node_order));\n \twhile ((node = find_next_best_node(local_node, &used_mask)) >= 0) {\n \t\t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author addressed a concern about the need for a unified predicate to exclude both N_MEMORY_PRIVATE and ZONE_DEVICE folios from MM operations, and provided a patch that adds the folio_is_private_managed() function to achieve this.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix is needed",
                "provided a patch"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Multiple mm/ subsystems already skip operations for ZONE_DEVICE folios,\nand N_MEMORY_PRIVATE folios share the checkpoints for ZONE_DEVICE pages.\n\nAdd folio_is_private_managed() as a unified predicate that returns true\nfor folios on N_MEMORY_PRIVATE nodes or in ZONE_DEVICE.\n\nThis predicate replaces folio_is_zone_device at skip sites where both\nfolio types should be excluded from an MM operation.\n\nAt some locations, explicit zone_device vs private_node checks are more\nappropriate when the operations between the two fundamentally differ.\n\nThe !CONFIG_NUMA stubs fall through to folio_is_zone_device() only,\npreserving existing behavior when NUMA is disabled.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h | 20 ++++++++++++++++++++\n 1 file changed, 20 insertions(+)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 6a70ec39d569..7687a4cf990c 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -92,6 +92,16 @@ static inline bool page_is_private_node(struct page *page)\n \treturn node_state(page_to_nid(page), N_MEMORY_PRIVATE);\n }\n \n+static inline bool folio_is_private_managed(struct folio *folio)\n+{\n+\treturn folio_is_zone_device(folio) || folio_is_private_node(folio);\n+}\n+\n+static inline bool page_is_private_managed(struct page *page)\n+{\n+\treturn folio_is_private_managed(page_folio(page));\n+}\n+\n static inline const struct node_private_ops *\n folio_node_private_ops(struct folio *folio)\n {\n@@ -146,6 +156,16 @@ static inline bool page_is_private_node(struct page *page)\n \treturn false;\n }\n \n+static inline bool folio_is_private_managed(struct folio *folio)\n+{\n+\treturn folio_is_zone_device(folio);\n+}\n+\n+static inline bool page_is_private_managed(struct page *page)\n+{\n+\treturn folio_is_private_managed(page_folio(page));\n+}\n+\n static inline const struct node_private_ops *\n folio_node_private_ops(struct folio *folio)\n {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about mlocking private node folios, explaining that they should not be locked and citing the existing folio_is_zone_device check as sufficient to handle this case. The author extended this check to include private nodes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "addressed_concern",
                "explained_reasoning"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios are managed by device drivers and should not be\nmlocked.  The existing folio_is_zone_device check is already correctly\nplaced to handle this - simply extend it for private nodes.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/mlock.c | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a/mm/mlock.c b/mm/mlock.c\nindex 2f699c3497a5..c56159253e45 100644\n--- a/mm/mlock.c\n+++ b/mm/mlock.c\n@@ -25,6 +25,7 @@\n #include <linux/memcontrol.h>\n #include <linux/mm_inline.h>\n #include <linux/secretmem.h>\n+#include <linux/node_private.h>\n \n #include \"internal.h\"\n \n@@ -366,7 +367,7 @@ static int mlock_pte_range(pmd_t *pmd, unsigned long addr,\n \t\tif (is_huge_zero_pmd(*pmd))\n \t\t\tgoto out;\n \t\tfolio = pmd_folio(*pmd);\n-\t\tif (folio_is_zone_device(folio))\n+\t\tif (unlikely(folio_is_private_managed(folio)))\n \t\t\tgoto out;\n \t\tif (vma->vm_flags & VM_LOCKED)\n \t\t\tmlock_folio(folio);\n@@ -386,7 +387,7 @@ static int mlock_pte_range(pmd_t *pmd, unsigned long addr,\n \t\tif (!pte_present(ptent))\n \t\t\tcontinue;\n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n-\t\tif (!folio || folio_is_zone_device(folio))\n+\t\tif (!folio || unlikely(folio_is_private_managed(folio)))\n \t\t\tcontinue;\n \n \t\tstep = folio_mlock_step(folio, pte, addr, end);\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author acknowledged a concern that madvise and pageout operations should not interfere with device driver-managed private node folios, agreed to extend the zone_device check to cover private nodes, and made corresponding changes to mm/madvise.c.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a concern",
                "agreed to make changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios are managed by device drivers and should not be\nsubjectto madvise cold/pageout/free operations that would interfere\nwith the driver's memory management.\n\nExtend the existing zone_device check to cover private nodes.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/madvise.c | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a/mm/madvise.c b/mm/madvise.c\nindex b617b1be0f53..3aac105e840b 100644\n--- a/mm/madvise.c\n+++ b/mm/madvise.c\n@@ -32,6 +32,7 @@\n #include <linux/leafops.h>\n #include <linux/shmem_fs.h>\n #include <linux/mmu_notifier.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlb.h>\n \n@@ -475,7 +476,7 @@ static int madvise_cold_or_pageout_pte_range(pmd_t *pmd,\n \t\t\tcontinue;\n \n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n-\t\tif (!folio || folio_is_zone_device(folio))\n+\t\tif (!folio || unlikely(folio_is_private_managed(folio)))\n \t\t\tcontinue;\n \n \t\t/*\n@@ -704,7 +705,7 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,\n \t\t}\n \n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n-\t\tif (!folio || folio_is_zone_device(folio))\n+\t\tif (!folio || unlikely(folio_is_private_managed(folio)))\n \t\t\tcontinue;\n \n \t\t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private node folios participating in KSM merging by default, agreeing that this can interfere with driver operations. The author extended existing checks to exclude private node folios from KSM merging.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios should not participate in KSM merging by default.\nThe driver manages the memory lifecycle and KSM's page sharing can\ninterfere with driver operations.\n\nExtend the existing zone_device checks in get_mergeable_page and\nksm_next_page_pmd_entry to cover private node folios as well.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/ksm.c | 9 ++++++---\n 1 file changed, 6 insertions(+), 3 deletions(-)\n\ndiff --git a/mm/ksm.c b/mm/ksm.c\nindex 2d89a7c8b4eb..c48e95a6fff9 100644\n--- a/mm/ksm.c\n+++ b/mm/ksm.c\n@@ -40,6 +40,7 @@\n #include <linux/oom.h>\n #include <linux/numa.h>\n #include <linux/pagewalk.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n #include \"internal.h\"\n@@ -808,7 +809,7 @@ static struct page *get_mergeable_page(struct ksm_rmap_item *rmap_item)\n \n \tfolio = folio_walk_start(&fw, vma, addr, 0);\n \tif (folio) {\n-\t\tif (!folio_is_zone_device(folio) &&\n+\t\tif (!folio_is_private_managed(folio) &&\n \t\t    folio_test_anon(folio)) {\n \t\t\tfolio_get(folio);\n \t\t\tpage = fw.page;\n@@ -2521,7 +2522,8 @@ static int ksm_next_page_pmd_entry(pmd_t *pmdp, unsigned long addr, unsigned lon\n \t\t\t\tgoto not_found_unlock;\n \t\t\tfolio = page_folio(page);\n \n-\t\t\tif (folio_is_zone_device(folio) || !folio_test_anon(folio))\n+\t\t\tif (unlikely(folio_is_private_managed(folio)) ||\n+\t\t\t    !folio_test_anon(folio))\n \t\t\t\tgoto not_found_unlock;\n \n \t\t\tpage += ((addr & (PMD_SIZE - 1)) >> PAGE_SHIFT);\n@@ -2545,7 +2547,8 @@ static int ksm_next_page_pmd_entry(pmd_t *pmdp, unsigned long addr, unsigned lon\n \t\t\tcontinue;\n \t\tfolio = page_folio(page);\n \n-\t\tif (folio_is_zone_device(folio) || !folio_test_anon(folio))\n+\t\tif (unlikely(folio_is_private_managed(folio)) ||\n+\t\t    !folio_test_anon(folio))\n \t\t\tcontinue;\n \t\tgoto found_unlock;\n \t}\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about collapse operations on private nodes potentially promoting pages to local nodes and inverting LRU order, agreeing that handling this like zone_device is the best approach for now.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "A collapse operation allocates a new large folio and migrates the\nsmaller folios into it.  This is an issue for private nodes:\n\n  1. The private node service may not support migration\n  2. Collapse may promotes pages from the private node to a local node,\n     which may result in an LRU inversion that defeats memory tiering.\n\nHandle this just like zone_device for now.\n\nIt may be possible to support this later for some private node services\nthat report explicit support for collapse (and migration).\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n mm/khugepaged.c | 7 ++++---\n 1 file changed, 4 insertions(+), 3 deletions(-)\n\ndiff --git a/mm/khugepaged.c b/mm/khugepaged.c\nindex 97d1b2824386..36f6bc5da53c 100644\n--- a/mm/khugepaged.c\n+++ b/mm/khugepaged.c\n@@ -21,6 +21,7 @@\n #include <linux/shmem_fs.h>\n #include <linux/dax.h>\n #include <linux/ksm.h>\n+#include <linux/node_private.h>\n #include <linux/pgalloc.h>\n \n #include <asm/tlb.h>\n@@ -571,7 +572,7 @@ static int __collapse_huge_page_isolate(struct vm_area_struct *vma,\n \t\t\tgoto out;\n \t\t}\n \t\tpage = vm_normal_page(vma, addr, pteval);\n-\t\tif (unlikely(!page) || unlikely(is_zone_device_page(page))) {\n+\t\tif (unlikely(!page) || unlikely(page_is_private_managed(page))) {\n \t\t\tresult = SCAN_PAGE_NULL;\n \t\t\tgoto out;\n \t\t}\n@@ -1323,7 +1324,7 @@ static int hpage_collapse_scan_pmd(struct mm_struct *mm,\n \t\t}\n \n \t\tpage = vm_normal_page(vma, addr, pteval);\n-\t\tif (unlikely(!page) || unlikely(is_zone_device_page(page))) {\n+\t\tif (unlikely(!page) || unlikely(page_is_private_managed(page))) {\n \t\t\tresult = SCAN_PAGE_NULL;\n \t\t\tgoto out_unmap;\n \t\t}\n@@ -1575,7 +1576,7 @@ int collapse_pte_mapped_thp(struct mm_struct *mm, unsigned long addr,\n \t\t}\n \n \t\tpage = vm_normal_page(vma, addr, ptent);\n-\t\tif (WARN_ON_ONCE(page && is_zone_device_page(page)))\n+\t\tif (WARN_ON_ONCE(page && page_is_private_managed(page)))\n \t\t\tpage = NULL;\n \t\t/*\n \t\t * Note that uprobe, debugger, or MAP_PRIVATE may change the\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about cleanup when a folio's refcount drops to zero, explaining that the service may need to perform cleanup before the page returns to the buddy allocator. They added a new function `folio_managed_on_free()` to wrap both zone_device and private node semantics for this operation. The function will return true if the folio is fully handled (zone_device) or false if the callback ran but the folio should continue through the normal free path (private_node).",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "added new function to address concern"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "When a folio's refcount drops to zero, the service may need to perform\ncleanup before the page returns to the buddy allocator (e.g. zeroing\npages to scrub stale compressed data / release compression ratio).\n\nAdd folio_managed_on_free() to wrap both zone_device and private node\nsemantics for this operation since they are the same.\n\nOne difference between zone_device and private node folios:\n  - private nodes may choose to either take a reference and return true\n    (\"handled\"), or return false to return it back to the buddy.\n\n  - zone_device returns the page to the buddy (always returns true)\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h |  6 ++++++\n mm/internal.h                | 30 ++++++++++++++++++++++++++++++\n mm/swap.c                    | 21 ++++++++++-----------\n 3 files changed, 46 insertions(+), 11 deletions(-)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 7687a4cf990c..09ea7c4cb13c 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -39,10 +39,16 @@ struct vm_fault;\n  *   callback to prevent node_private from being freed.\n  *   These callbacks MUST NOT sleep.\n  *\n+ * @free_folio: Called when a folio refcount drops to 0\n+ *   [folio-referenced callback]\n+ *   Returns: true if handled (skip return to buddy)\n+ *            false if no op (return to buddy)\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n struct node_private_ops {\n+\tbool (*free_folio)(struct folio *folio);\n \tunsigned long flags;\n };\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex 97023748e6a9..658da41cdb8e 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1412,6 +1412,36 @@ int numa_migrate_check(struct folio *folio, struct vm_fault *vmf,\n void free_zone_device_folio(struct folio *folio);\n int migrate_device_coherent_folio(struct folio *folio);\n \n+/**\n+ * folio_managed_on_free - Notify managed-memory service that folio\n+ *                         refcount reached zero.\n+ * @folio: the folio being freed\n+ *\n+ * Returns true if the folio is fully handled (zone_device -- caller\n+ * must return immediately).  Returns false if the callback ran but\n+ * the folio should continue through the normal free path\n+ * (private_node -- pages go back to buddy).\n+ *\n+ * Returns false for normal folios (no-op).\n+ */\n+static inline bool folio_managed_on_free(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio)) {\n+\t\tfree_zone_device_folio(folio);\n+\t\treturn true;\n+\t}\n+\tif (folio_is_private_node(folio)) {\n+\t\tconst struct node_private_ops *ops =\n+\t\t\tfolio_node_private_ops(folio);\n+\n+\t\tif (ops && ops->free_folio) {\n+\t\t\tif (ops->free_folio(folio))\n+\t\t\t\treturn true;\n+\t\t}\n+\t}\n+\treturn false;\n+}\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/swap.c b/mm/swap.c\nindex 2260dcd2775e..dca306e1ae6d 100644\n--- a/mm/swap.c\n+++ b/mm/swap.c\n@@ -37,6 +37,7 @@\n #include <linux/page_idle.h>\n #include <linux/local_lock.h>\n #include <linux/buffer_head.h>\n+#include <linux/node_private.h>\n \n #include \"internal.h\"\n \n@@ -96,10 +97,9 @@ static void page_cache_release(struct folio *folio)\n \n void __folio_put(struct folio *folio)\n {\n-\tif (unlikely(folio_is_zone_device(folio))) {\n-\t\tfree_zone_device_folio(folio);\n-\t\treturn;\n-\t}\n+\tif (unlikely(folio_is_private_managed(folio)))\n+\t\tif (folio_managed_on_free(folio))\n+\t\t\treturn;\n \n \tif (folio_test_hugetlb(folio)) {\n \t\tfree_huge_folio(folio);\n@@ -961,19 +961,18 @@ void folios_put_refs(struct folio_batch *folios, unsigned int *refs)\n \t\tif (is_huge_zero_folio(folio))\n \t\t\tcontinue;\n \n-\t\tif (folio_is_zone_device(folio)) {\n+\t\tif (!folio_ref_sub_and_test(folio, nr_refs))\n+\t\t\tcontinue;\n+\n+\t\tif (unlikely(folio_is_private_managed(folio))) {\n \t\t\tif (lruvec) {\n \t\t\t\tunlock_page_lruvec_irqrestore(lruvec, flags);\n \t\t\t\tlruvec = NULL;\n \t\t\t}\n-\t\t\tif (folio_ref_sub_and_test(folio, nr_refs))\n-\t\t\t\tfree_zone_device_folio(folio);\n-\t\t\tcontinue;\n+\t\t\tif (folio_managed_on_free(folio))\n+\t\t\t\tcontinue;\n \t\t}\n \n-\t\tif (!folio_ref_sub_and_test(folio, nr_refs))\n-\t\t\tcontinue;\n-\n \t\t/* hugetlb has its own memcg */\n \t\tif (folio_test_hugetlb(folio)) {\n \t\t\tif (lruvec) {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about notifying private node services when a THP folio is split by adding an optional callback to the ops struct and updating the folio_split path in huge_memory.c. The author confirmed that this change will be included in the next version of the patch series.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a concern",
                "confirmed a fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Some private node services may need to update internal metadata when\na THP folio is split.  ZONE_DEVICE already has a split callback via\npgmap->ops; private nodes can provide the same capability.\n\nJust like zone_device, some private node services may want to know\nabout a folio being split.  Add this optional callback to the ops\nstruct and add a wrapper for zone_device and private node callback\ndispatch to be consolidated.\n\nWire this into __folio_split() where the zone_device check was made.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h | 33 +++++++++++++++++++++++++++++++++\n mm/huge_memory.c             |  6 ++++--\n 2 files changed, 37 insertions(+), 2 deletions(-)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 09ea7c4cb13c..f9dd2d25c8a5 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -3,6 +3,7 @@\n #define _LINUX_NODE_PRIVATE_H\n \n #include <linux/completion.h>\n+#include <linux/memremap.h>\n #include <linux/mm.h>\n #include <linux/nodemask.h>\n #include <linux/rcupdate.h>\n@@ -44,11 +45,19 @@ struct vm_fault;\n  *   Returns: true if handled (skip return to buddy)\n  *            false if no op (return to buddy)\n  *\n+ * @folio_split: Notification that a folio on this private node is being split.\n+ *    [folio-referenced callback]\n+ *     Called from the folio split path via folio_managed_split_cb().\n+ *     @folio is the original folio; @new_folio is the newly created folio,\n+ *     or NULL when called for the final (original) folio after all sub-folios\n+ *     have been split off.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n struct node_private_ops {\n \tbool (*free_folio)(struct folio *folio);\n+\tvoid (*folio_split)(struct folio *folio, struct folio *new_folio);\n \tunsigned long flags;\n };\n \n@@ -150,6 +159,24 @@ static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n \treturn node_private_flags(zone_to_nid(z)) & flag;\n }\n \n+static inline void node_private_split_cb(struct folio *folio,\n+\t\t\t\t\t struct folio *new_folio)\n+{\n+\tconst struct node_private_ops *ops = folio_node_private_ops(folio);\n+\n+\tif (ops && ops->folio_split)\n+\t\tops->folio_split(folio, new_folio);\n+}\n+\n+static inline void folio_managed_split_cb(struct folio *original_folio,\n+\t\t\t\t\t  struct folio *new_folio)\n+{\n+\tif (folio_is_zone_device(original_folio))\n+\t\tzone_device_private_split_cb(original_folio, new_folio);\n+\telse if (folio_is_private_node(original_folio))\n+\t\tnode_private_split_cb(original_folio, new_folio);\n+}\n+\n #else /* !CONFIG_NUMA */\n \n static inline bool folio_is_private_node(struct folio *folio)\n@@ -198,6 +225,12 @@ static inline bool zone_private_flags(struct zone *z, unsigned long flag)\n \treturn false;\n }\n \n+static inline void folio_managed_split_cb(struct folio *original_folio,\n+\t\t\t\t\t  struct folio *new_folio)\n+{\n+\tif (folio_is_zone_device(original_folio))\n+\t\tzone_device_private_split_cb(original_folio, new_folio);\n+}\n #endif /* CONFIG_NUMA */\n \n #if defined(CONFIG_NUMA) && defined(CONFIG_MEMORY_HOTPLUG)\ndiff --git a/mm/huge_memory.c b/mm/huge_memory.c\nindex 40cf59301c21..2ecae494291a 100644\n--- a/mm/huge_memory.c\n+++ b/mm/huge_memory.c\n@@ -24,6 +24,7 @@\n #include <linux/freezer.h>\n #include <linux/mman.h>\n #include <linux/memremap.h>\n+#include <linux/node_private.h>\n #include <linux/pagemap.h>\n #include <linux/debugfs.h>\n #include <linux/migrate.h>\n@@ -3850,7 +3851,7 @@ static int __folio_freeze_and_split_unmapped(struct folio *folio, unsigned int n\n \n \t\t\tnext = folio_next(new_folio);\n \n-\t\t\tzone_device_private_split_cb(folio, new_folio);\n+\t\t\tfolio_managed_split_cb(folio, new_folio);\n \n \t\t\tfolio_ref_unfreeze(new_folio,\n \t\t\t\t\t   folio_cache_ref_count(new_folio) + 1);\n@@ -3889,7 +3890,8 @@ static int __folio_freeze_and_split_unmapped(struct folio *folio, unsigned int n\n \t\t\tfolio_put_refs(new_folio, nr_pages);\n \t\t}\n \n-\t\tzone_device_private_split_cb(folio, NULL);\n+\t\tfolio_managed_split_cb(folio, NULL);\n+\n \t\t/*\n \t\t * Unfreeze @folio only after all page cache entries, which\n \t\t * used to point to it, have been updated with new folios.\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about user-driven migration to private nodes, explaining that ZONE_DEVICE always rejects it but private nodes should be able to opt in. They added the NP_OPS_MIGRATION flag and folio_managed_user_migrate() wrapper to dispatch migration requests, allowing migrate_pages syscall to target private nodes.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "added new functionality",
                "acknowledged reviewer feedback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node services may want to support user-driven migration\n(migrate_pages syscall, mbind) to allow data movement between regular\nand private nodes.\n\nZONE_DEVICE always rejects user migration, but private nodes should\nbe able to opt in.\n\nAdd NP_OPS_MIGRATION flag and folio_managed_user_migrate() wrapper that\ndispatches migration requests.  Private nodes can either set the flag\nand provide a custom migrate_to callback for driver-managed migration.\n\nIn migrate_to_node(), allows GFP_PRIVATE when the destination node\nsupports NP_OPS_MIGRATION, enabling migrate_pages syscall to target\nprivate nodes.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |   4 ++\n include/linux/migrate.h      |  10 +++\n include/linux/node_private.h | 122 +++++++++++++++++++++++++++++++++++\n mm/damon/paddr.c             |   3 +\n mm/internal.h                |  24 +++++++\n mm/mempolicy.c               |  10 +--\n mm/migrate.c                 |  49 ++++++++++----\n mm/rmap.c                    |   4 +-\n 8 files changed, 206 insertions(+), 20 deletions(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex 646dc48a23b5..e587f5781135 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -949,6 +949,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \tif (!node_possible(nid))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_MIGRATION) &&\n+\t    (!ops->migrate_to || !ops->folio_migrate))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/migrate.h b/include/linux/migrate.h\nindex 26ca00c325d9..7b2da3875ff2 100644\n--- a/include/linux/migrate.h\n+++ b/include/linux/migrate.h\n@@ -71,6 +71,9 @@ void folio_migrate_flags(struct folio *newfolio, struct folio *folio);\n int folio_migrate_mapping(struct address_space *mapping,\n \t\tstruct folio *newfolio, struct folio *folio, int extra_count);\n int set_movable_ops(const struct movable_operations *ops, enum pagetype type);\n+int migrate_folios_to_node(struct list_head *folios, int nid,\n+\t\t\t\t    enum migrate_mode mode,\n+\t\t\t\t    enum migrate_reason reason);\n \n #else\n \n@@ -96,6 +99,13 @@ static inline int set_movable_ops(const struct movable_operations *ops, enum pag\n {\n \treturn -ENOSYS;\n }\n+static inline int migrate_folios_to_node(struct list_head *folios,\n+\t\t\t\t\t\t  int nid,\n+\t\t\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t\t\t  enum migrate_reason reason)\n+{\n+\treturn -ENOSYS;\n+}\n \n #endif /* CONFIG_MIGRATION */\n \ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex f9dd2d25c8a5..0c5be1ee6e60 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -4,6 +4,7 @@\n \n #include <linux/completion.h>\n #include <linux/memremap.h>\n+#include <linux/migrate_mode.h>\n #include <linux/mm.h>\n #include <linux/nodemask.h>\n #include <linux/rcupdate.h>\n@@ -52,15 +53,40 @@ struct vm_fault;\n  *     or NULL when called for the final (original) folio after all sub-folios\n  *     have been split off.\n  *\n+ * @migrate_to: Migrate folios TO this node.\n+ *\t[refcounted callback]\n+ *\tReturns: 0 on full success, >0 = number of folios that failed to\n+ *\t\t migrate, <0 = error.  Matches migrate_pages() semantics.\n+ *\t\t @nr_succeeded is set to the number of successfully migrated\n+ *\t\t folios (may be NULL if caller doesn't need it).\n+ *\n+ * @folio_migrate: Post-migration notification that a folio on this private node\n+ *    changed physical location (on the same node or a different node).\n+ *    [folio-referenced callback]\n+ *     Called from migrate_folio_move() after data has been copied but before\n+ *     migration entries are replaced with real PTEs.  Both @src and @dst are\n+ *     locked.  Faults block in migration_entry_wait() until\n+ *     remove_migration_ptes() runs, so the service can safely update\n+ *     PFN-based metadata (compression tables, device page tables, DMA\n+ *     mappings, etc.) before any access through the page tables.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n struct node_private_ops {\n \tbool (*free_folio)(struct folio *folio);\n \tvoid (*folio_split)(struct folio *folio, struct folio *new_folio);\n+\tint (*migrate_to)(struct list_head *folios, int nid,\n+\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t  enum migrate_reason reason,\n+\t\t\t\t  unsigned int *nr_succeeded);\n+\tvoid (*folio_migrate)(struct folio *src, struct folio *dst);\n \tunsigned long flags;\n };\n \n+/* Allow user/kernel migration; requires migrate_to and folio_migrate */\n+#define NP_OPS_MIGRATION\t\tBIT(0)\n+\n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n  *\n@@ -177,6 +203,81 @@ static inline void folio_managed_split_cb(struct folio *original_folio,\n \t\tnode_private_split_cb(original_folio, new_folio);\n }\n \n+#ifdef CONFIG_MEMORY_HOTPLUG\n+static inline int folio_managed_allows_user_migrate(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio))\n+\t\treturn -ENOENT;\n+\treturn node_private_has_flag(folio_nid(folio), NP_OPS_MIGRATION) ?\n+\t       folio_nid(folio) : -ENOENT;\n+}\n+\n+/**\n+ * folio_managed_allows_migrate - Check if a managed folio supports migration\n+ * @folio: The folio to check\n+ *\n+ * Returns true if the folio can be migrated.  For zone_device folios, only\n+ * device_private and device_coherent support migration.  For private node\n+ * folios, migration requires NP_OPS_MIGRATION.  Normal folios always\n+ * return true.\n+ */\n+static inline bool folio_managed_allows_migrate(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio))\n+\t\treturn folio_is_device_private(folio) ||\n+\t\t       folio_is_device_coherent(folio);\n+\tif (folio_is_private_node(folio))\n+\t\treturn folio_private_flags(folio, NP_OPS_MIGRATION);\n+\treturn true;\n+}\n+\n+/**\n+ * node_private_migrate_to - Attempt service-specific migration to a private node\n+ * @folios: list of folios to migrate (may sleep)\n+ * @nid: target node\n+ * @mode: migration mode (MIGRATE_ASYNC, MIGRATE_SYNC, etc.)\n+ * @reason: migration reason (MR_DEMOTION, MR_SYSCALL, etc.)\n+ * @nr_succeeded: optional output for number of successfully migrated folios\n+ *\n+ * If @nid is an N_MEMORY_PRIVATE node with a migrate_to callback,\n+ * invokes the callback and returns the result with migrate_pages()\n+ * semantics (0 = full success, >0 = failure count, <0 = error).\n+ * Returns -ENODEV if the node is not private or the service is being\n+ * torn down.\n+ *\n+ * The source folios are on other nodes, so they do not pin the target\n+ * node's node_private.  A temporary refcount is taken under rcu_read_lock\n+ * to keep node_private (and the service module) alive across the callback.\n+ */\n+static inline int node_private_migrate_to(struct list_head *folios, int nid,\n+\t\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t\t  enum migrate_reason reason,\n+\t\t\t\t\t  unsigned int *nr_succeeded)\n+{\n+\tint (*fn)(struct list_head *, int, enum migrate_mode,\n+\t\t  enum migrate_reason, unsigned int *);\n+\tstruct node_private *np;\n+\tint ret;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tif (!np || !np->ops || !np->ops->migrate_to ||\n+\t    !refcount_inc_not_zero(&np->refcount)) {\n+\t\trcu_read_unlock();\n+\t\treturn -ENODEV;\n+\t}\n+\tfn = np->ops->migrate_to;\n+\trcu_read_unlock();\n+\n+\tret = fn(folios, nid, mode, reason, nr_succeeded);\n+\n+\tif (refcount_dec_and_test(&np->refcount))\n+\t\tcomplete(&np->released);\n+\n+\treturn ret;\n+}\n+#endif /* CONFIG_MEMORY_HOTPLUG */\n+\n #else /* !CONFIG_NUMA */\n \n static inline bool folio_is_private_node(struct folio *folio)\n@@ -242,6 +343,27 @@ int node_private_clear_ops(int nid, const struct node_private_ops *ops);\n \n #else /* !CONFIG_NUMA || !CONFIG_MEMORY_HOTPLUG */\n \n+static inline int folio_managed_allows_user_migrate(struct folio *folio)\n+{\n+\treturn -ENOENT;\n+}\n+\n+static inline bool folio_managed_allows_migrate(struct folio *folio)\n+{\n+\tif (folio_is_zone_device(folio))\n+\t\treturn folio_is_device_private(folio) ||\n+\t\t       folio_is_device_coherent(folio);\n+\treturn true;\n+}\n+\n+static inline int node_private_migrate_to(struct list_head *folios, int nid,\n+\t\t\t\t\t  enum migrate_mode mode,\n+\t\t\t\t\t  enum migrate_reason reason,\n+\t\t\t\t\t  unsigned int *nr_succeeded)\n+{\n+\treturn -ENODEV;\n+}\n+\n static inline int node_private_register(int nid, struct node_private *np)\n {\n \treturn -ENODEV;\ndiff --git a/mm/damon/paddr.c b/mm/damon/paddr.c\nindex 07a8aead439e..532b8e2c62b0 100644\n--- a/mm/damon/paddr.c\n+++ b/mm/damon/paddr.c\n@@ -277,6 +277,9 @@ static unsigned long damon_pa_migrate(struct damon_region *r,\n \t\telse\n \t\t\t*sz_filter_passed += folio_size(folio) / addr_unit;\n \n+\t\tif (!folio_managed_allows_migrate(folio))\n+\t\t\tgoto put_folio;\n+\n \t\tif (!folio_isolate_lru(folio))\n \t\t\tgoto put_folio;\n \t\tlist_add(&folio->lru, &folio_list);\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 658da41cdb8e..6ab4679fe943 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1442,6 +1442,30 @@ static inline bool folio_managed_on_free(struct folio *folio)\n \treturn false;\n }\n \n+/**\n+ * folio_managed_migrate_notify - Notify service that a folio changed location\n+ * @src: the old folio (about to be freed)\n+ * @dst: the new folio (data already copied, migration entries still in place)\n+ *\n+ * Called from migrate_folio_move() after data has been copied but before\n+ * remove_migration_ptes() installs real PTEs pointing to @dst.  While\n+ * migration entries are in place, faults block in migration_entry_wait(),\n+ * so the service can safely update PFN-based metadata before any access\n+ * through the page tables.  Both @src and @dst are locked.\n+ */\n+static inline void folio_managed_migrate_notify(struct folio *src,\n+\t\t\t\t\t\tstruct folio *dst)\n+{\n+\tconst struct node_private_ops *ops;\n+\n+\tif (!folio_is_private_node(src))\n+\t\treturn;\n+\n+\tops = folio_node_private_ops(src);\n+\tif (ops && ops->folio_migrate)\n+\t\tops->folio_migrate(src, dst);\n+}\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 68a98ba57882..2b0f9762d171 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -111,6 +111,7 @@\n #include <linux/mmu_notifier.h>\n #include <linux/printk.h>\n #include <linux/leafops.h>\n+#include <linux/node_private.h>\n #include <linux/gcd.h>\n \n #include <asm/tlbflush.h>\n@@ -1282,11 +1283,6 @@ static long migrate_to_node(struct mm_struct *mm, int source, int dest,\n \tLIST_HEAD(pagelist);\n \tlong nr_failed;\n \tlong err = 0;\n-\tstruct migration_target_control mtc = {\n-\t\t.nid = dest,\n-\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n-\t\t.reason = MR_SYSCALL,\n-\t};\n \n \tnodes_clear(nmask);\n \tnode_set(source, nmask);\n@@ -1311,8 +1307,8 @@ static long migrate_to_node(struct mm_struct *mm, int source, int dest,\n \tmmap_read_unlock(mm);\n \n \tif (!list_empty(&pagelist)) {\n-\t\terr = migrate_pages(&pagelist, alloc_migration_target, NULL,\n-\t\t\t(unsigned long)&mtc, MIGRATE_SYNC, MR_SYSCALL, NULL);\n+\t\terr = migrate_folios_to_node(&pagelist, dest, MIGRATE_SYNC,\n+\t\t\t\t\t     MR_SYSCALL);\n \t\tif (err)\n \t\t\tputback_movable_pages(&pagelist);\n \t}\ndiff --git a/mm/migrate.c b/mm/migrate.c\nindex 5169f9717f60..a54d4af04df3 100644\n--- a/mm/migrate.c\n+++ b/mm/migrate.c\n@@ -43,6 +43,7 @@\n #include <linux/sched/sysctl.h>\n #include <linux/memory-tiers.h>\n #include <linux/pagewalk.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1387,6 +1388,8 @@ static int migrate_folio_move(free_folio_t put_new_folio, unsigned long private,\n \tif (old_page_state & PAGE_WAS_MLOCKED)\n \t\tlru_add_drain();\n \n+\tfolio_managed_migrate_notify(src, dst);\n+\n \tif (old_page_state & PAGE_WAS_MAPPED)\n \t\tremove_migration_ptes(src, dst, 0);\n \n@@ -2165,6 +2168,7 @@ int migrate_pages(struct list_head *from, new_folio_t get_new_folio,\n \n \treturn rc_gather;\n }\n+EXPORT_SYMBOL_GPL(migrate_pages);\n \n struct folio *alloc_migration_target(struct folio *src, unsigned long private)\n {\n@@ -2204,6 +2208,31 @@ struct folio *alloc_migration_target(struct folio *src, unsigned long private)\n \n \treturn __folio_alloc(gfp_mask, order, nid, mtc->nmask);\n }\n+EXPORT_SYMBOL_GPL(alloc_migration_target);\n+\n+static int __migrate_folios_to_node(struct list_head *folios, int nid,\n+\t\t\t\t    enum migrate_mode mode,\n+\t\t\t\t    enum migrate_reason reason)\n+{\n+\tstruct migration_target_control mtc = {\n+\t\t.nid = nid,\n+\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n+\t\t.reason = reason,\n+\t};\n+\n+\treturn migrate_pages(folios, alloc_migration_target, NULL,\n+\t\t\t     (unsigned long)&mtc, mode, reason, NULL);\n+}\n+\n+int migrate_folios_to_node(struct list_head *folios, int nid,\n+\t\t\t   enum migrate_mode mode,\n+\t\t\t   enum migrate_reason reason)\n+{\n+\tif (node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn node_private_migrate_to(folios, nid, mode,\n+\t\t\t\t\t       reason, NULL);\n+\treturn __migrate_folios_to_node(folios, nid, mode, reason);\n+}\n \n #ifdef CONFIG_NUMA\n \n@@ -2221,14 +2250,8 @@ static int store_status(int __user *status, int start, int value, int nr)\n static int do_move_pages_to_node(struct list_head *pagelist, int node)\n {\n \tint err;\n-\tstruct migration_target_control mtc = {\n-\t\t.nid = node,\n-\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE,\n-\t\t.reason = MR_SYSCALL,\n-\t};\n \n-\terr = migrate_pages(pagelist, alloc_migration_target, NULL,\n-\t\t(unsigned long)&mtc, MIGRATE_SYNC, MR_SYSCALL, NULL);\n+\terr = migrate_folios_to_node(pagelist, node, MIGRATE_SYNC, MR_SYSCALL);\n \tif (err)\n \t\tputback_movable_pages(pagelist);\n \treturn err;\n@@ -2240,7 +2263,7 @@ static int __add_folio_for_migration(struct folio *folio, int node,\n \tif (is_zero_folio(folio) || is_huge_zero_folio(folio))\n \t\treturn -EFAULT;\n \n-\tif (folio_is_zone_device(folio))\n+\tif (!folio_managed_allows_migrate(folio))\n \t\treturn -ENOENT;\n \n \tif (folio_nid(folio) == node)\n@@ -2364,7 +2387,8 @@ static int do_pages_move(struct mm_struct *mm, nodemask_t task_nodes,\n \t\terr = -ENODEV;\n \t\tif (node < 0 || node >= MAX_NUMNODES)\n \t\t\tgoto out_flush;\n-\t\tif (!node_state(node, N_MEMORY))\n+\t\tif (!node_state(node, N_MEMORY) &&\n+\t\t    !node_state(node, N_MEMORY_PRIVATE))\n \t\t\tgoto out_flush;\n \n \t\terr = -EACCES;\n@@ -2449,8 +2473,8 @@ static void do_pages_stat_array(struct mm_struct *mm, unsigned long nr_pages,\n \t\tif (folio) {\n \t\t\tif (is_zero_folio(folio) || is_huge_zero_folio(folio))\n \t\t\t\terr = -EFAULT;\n-\t\t\telse if (folio_is_zone_device(folio))\n-\t\t\t\terr = -ENOENT;\n+\t\t\telse if (unlikely(folio_is_private_managed(folio)))\n+\t\t\t\terr = folio_managed_allows_user_migrate(folio);\n \t\t\telse\n \t\t\t\terr = folio_nid(folio);\n \t\t\tfolio_walk_end(&fw, vma);\n@@ -2660,6 +2684,9 @@ int migrate_misplaced_folio_prepare(struct folio *folio,\n \tint nr_pages = folio_nr_pages(folio);\n \tpg_data_t *pgdat = NODE_DATA(node);\n \n+\tif (!folio_managed_allows_migrate(folio))\n+\t\treturn -ENOENT;\n+\n \tif (folio_is_file_lru(folio)) {\n \t\t/*\n \t\t * Do not migrate file folios that are mapped in multiple\ndiff --git a/mm/rmap.c b/mm/rmap.c\nindex f955f02d570e..805f9ceb82f3 100644\n--- a/mm/rmap.c\n+++ b/mm/rmap.c\n@@ -72,6 +72,7 @@\n #include <linux/backing-dev.h>\n #include <linux/page_idle.h>\n #include <linux/memremap.h>\n+#include <linux/node_private.h>\n #include <linux/userfaultfd_k.h>\n #include <linux/mm_inline.h>\n #include <linux/oom.h>\n@@ -2616,8 +2617,7 @@ void try_to_migrate(struct folio *folio, enum ttu_flags flags)\n \t\t\t\t\tTTU_SYNC | TTU_BATCH_FLUSH)))\n \t\treturn;\n \n-\tif (folio_is_zone_device(folio) &&\n-\t    (!folio_is_device_private(folio) && !folio_is_device_coherent(folio)))\n+\tif (!folio_managed_allows_migrate(folio))\n \t\treturn;\n \n \t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about allowing userland to directly allocate from private nodes via set_mempolicy() and mbind(), but not wanting those nodes as normal allocable system memory in the fallback lists. The author added a flag NP_OPS_MEMPOLICY requiring NP_OPS_MIGRATION, updated sysfs 'has_memory' attribute, and modified mempolicy migration sites to use __GFP_PRIVATE.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Some private nodes want userland to directly allocate from the node\nvia set_mempolicy() and mbind() - but don't want that node as normal\nallocable system memory in the fallback lists.\n\nAdd NP_OPS_MEMPOLICY flag requiring NP_OPS_MIGRATION (since mbind can\ndrive migrations).  Only allow private nodes in policy nodemasks if\nall private nodes in the mask support NP_OPS_MEMPOLICY. This prevents\n__GFP_PRIVATE from unlocking nodes without NP_OPS_MEMPOLICY support.\n\nAdd __GFP_PRIVATE to mempolicy migration sites so moves to opted-in\nprivate nodes succeed.\n\nUpdate the sysfs \"has_memory\" attribute to include N_MEMORY_PRIVATE\nnodes with NP_OPS_MEMPOLICY set, allowing existing numactl userland\ntools to work without modification.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c            | 22 +++++++++++++-\n include/linux/node_private.h   | 40 +++++++++++++++++++++++++\n include/uapi/linux/mempolicy.h |  1 +\n mm/mempolicy.c                 | 54 ++++++++++++++++++++++++++++++----\n mm/page_alloc.c                |  5 ++++\n 5 files changed, 116 insertions(+), 6 deletions(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex e587f5781135..c08b5a948779 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -953,6 +953,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    (!ops->migrate_to || !ops->folio_migrate))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_MEMPOLICY) &&\n+\t    !(ops->flags & NP_OPS_MIGRATION))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\n@@ -1145,6 +1149,21 @@ static ssize_t show_node_state(struct device *dev,\n \t\t\t  nodemask_pr_args(&node_states[na->state]));\n }\n \n+/* has_memory includes N_MEMORY + N_MEMORY_PRIVATE that support mempolicy. */\n+static ssize_t show_has_memory(struct device *dev,\n+\t\t\t       struct device_attribute *attr, char *buf)\n+{\n+\tnodemask_t mask = node_states[N_MEMORY];\n+\tint nid;\n+\n+\tfor_each_node_state(nid, N_MEMORY_PRIVATE) {\n+\t\tif (node_private_has_flag(nid, NP_OPS_MEMPOLICY))\n+\t\t\tnode_set(nid, mask);\n+\t}\n+\n+\treturn sysfs_emit(buf, \"%*pbl\\n\", nodemask_pr_args(&mask));\n+}\n+\n #define _NODE_ATTR(name, state) \\\n \t{ __ATTR(name, 0444, show_node_state, NULL), state }\n \n@@ -1155,7 +1174,8 @@ static struct node_attr node_state_attr[] = {\n #ifdef CONFIG_HIGHMEM\n \t[N_HIGH_MEMORY] = _NODE_ATTR(has_high_memory, N_HIGH_MEMORY),\n #endif\n-\t[N_MEMORY] = _NODE_ATTR(has_memory, N_MEMORY),\n+\t[N_MEMORY] = { __ATTR(has_memory, 0444, show_has_memory, NULL),\n+\t\t       N_MEMORY },\n \t[N_MEMORY_PRIVATE] = _NODE_ATTR(has_private_memory, N_MEMORY_PRIVATE),\n \t[N_CPU] = _NODE_ATTR(has_cpu, N_CPU),\n \t[N_GENERIC_INITIATOR] = _NODE_ATTR(has_generic_initiator,\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 0c5be1ee6e60..e9b58afa366b 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -86,6 +86,8 @@ struct node_private_ops {\n \n /* Allow user/kernel migration; requires migrate_to and folio_migrate */\n #define NP_OPS_MIGRATION\t\tBIT(0)\n+/* Allow mempolicy-directed allocation and mbind migration to this node */\n+#define NP_OPS_MEMPOLICY\t\tBIT(1)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n@@ -276,6 +278,34 @@ static inline int node_private_migrate_to(struct list_head *folios, int nid,\n \n \treturn ret;\n }\n+\n+static inline bool node_mpol_eligible(int nid)\n+{\n+\tbool ret;\n+\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn node_state(nid, N_MEMORY);\n+\n+\trcu_read_lock();\n+\tret = node_private_has_flag(nid, NP_OPS_MEMPOLICY);\n+\trcu_read_unlock();\n+\treturn ret;\n+}\n+\n+static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n+{\n+\tint nid;\n+\tbool eligible = false;\n+\n+\tfor_each_node_mask(nid, *nodes) {\n+\t\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\t\tcontinue;\n+\t\tif (!node_mpol_eligible(nid))\n+\t\t\treturn false;\n+\t\teligible = true;\n+\t}\n+\treturn eligible;\n+}\n #endif /* CONFIG_MEMORY_HOTPLUG */\n \n #else /* !CONFIG_NUMA */\n@@ -364,6 +394,16 @@ static inline int node_private_migrate_to(struct list_head *folios, int nid,\n \treturn -ENODEV;\n }\n \n+static inline bool node_mpol_eligible(int nid)\n+{\n+\treturn false;\n+}\n+\n+static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n+{\n+\treturn false;\n+}\n+\n static inline int node_private_register(int nid, struct node_private *np)\n {\n \treturn -ENODEV;\ndiff --git a/include/uapi/linux/mempolicy.h b/include/uapi/linux/mempolicy.h\nindex 8fbbe613611a..b606eae983c8 100644\n--- a/include/uapi/linux/mempolicy.h\n+++ b/include/uapi/linux/mempolicy.h\n@@ -64,6 +64,7 @@ enum {\n #define MPOL_F_SHARED  (1 << 0)\t/* identify shared policies */\n #define MPOL_F_MOF\t(1 << 3) /* this policy wants migrate on fault */\n #define MPOL_F_MORON\t(1 << 4) /* Migrate On protnone Reference On Node */\n+#define MPOL_F_PRIVATE\t(1 << 5) /* policy targets private node; use __GFP_PRIVATE */\n \n /*\n  * Enabling zone reclaim means the page allocator will attempt to fulfill\ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 2b0f9762d171..8ac014950e88 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -406,8 +406,6 @@ static int mpol_new_preferred(struct mempolicy *pol, const nodemask_t *nodes)\n static int mpol_set_nodemask(struct mempolicy *pol,\n \t\t     const nodemask_t *nodes, struct nodemask_scratch *nsc)\n {\n-\tint ret;\n-\n \t/*\n \t * Default (pol==NULL) resp. local memory policies are not a\n \t * subject of any remapping. They also do not need any special\n@@ -416,9 +414,12 @@ static int mpol_set_nodemask(struct mempolicy *pol,\n \tif (!pol || pol->mode == MPOL_LOCAL)\n \t\treturn 0;\n \n-\t/* Check N_MEMORY */\n+\t/* Check N_MEMORY and N_MEMORY_PRIVATE*/\n \tnodes_and(nsc->mask1,\n \t\t  cpuset_current_mems_allowed, node_states[N_MEMORY]);\n+\tnodes_and(nsc->mask2, cpuset_current_mems_allowed,\n+\t\t  node_states[N_MEMORY_PRIVATE]);\n+\tnodes_or(nsc->mask1, nsc->mask1, nsc->mask2);\n \n \tVM_BUG_ON(!nodes);\n \n@@ -432,8 +433,13 @@ static int mpol_set_nodemask(struct mempolicy *pol,\n \telse\n \t\tpol->w.cpuset_mems_allowed = cpuset_current_mems_allowed;\n \n-\tret = mpol_ops[pol->mode].create(pol, &nsc->mask2);\n-\treturn ret;\n+\t/* All private nodes in the mask must have NP_OPS_MEMPOLICY. */\n+\tif (nodes_private_mpol_allowed(&nsc->mask2))\n+\t\tpol->flags |= MPOL_F_PRIVATE;\n+\telse if (nodes_intersects(nsc->mask2, node_states[N_MEMORY_PRIVATE]))\n+\t\treturn -EINVAL;\n+\n+\treturn mpol_ops[pol->mode].create(pol, &nsc->mask2);\n }\n \n /*\n@@ -500,6 +506,7 @@ static void mpol_rebind_default(struct mempolicy *pol, const nodemask_t *nodes)\n static void mpol_rebind_nodemask(struct mempolicy *pol, const nodemask_t *nodes)\n {\n \tnodemask_t tmp;\n+\tint nid;\n \n \tif (pol->flags & MPOL_F_STATIC_NODES)\n \t\tnodes_and(tmp, pol->w.user_nodemask, *nodes);\n@@ -514,6 +521,21 @@ static void mpol_rebind_nodemask(struct mempolicy *pol, const nodemask_t *nodes)\n \tif (nodes_empty(tmp))\n \t\ttmp = *nodes;\n \n+\t/*\n+\t * Drop private nodes that don't have mempolicy support.\n+\t * cpusets guarantees at least one N_MEMORY node in effective_mems\n+\t * and mems_allowed, so dropping private nodes here is safe.\n+\t */\n+\tfor_each_node_mask(nid, tmp) {\n+\t\tif (node_state(nid, N_MEMORY_PRIVATE) &&\n+\t\t    !node_private_has_flag(nid, NP_OPS_MEMPOLICY))\n+\t\t\tnode_clear(nid, tmp);\n+\t}\n+\tif (nodes_intersects(tmp, node_states[N_MEMORY_PRIVATE]))\n+\t\tpol->flags |= MPOL_F_PRIVATE;\n+\telse\n+\t\tpol->flags &= ~MPOL_F_PRIVATE;\n+\n \tpol->nodes = tmp;\n }\n \n@@ -661,6 +683,9 @@ static void queue_folios_pmd(pmd_t *pmd, struct mm_walk *walk)\n \t}\n \tif (!queue_folio_required(folio, qp))\n \t\treturn;\n+\tif (folio_is_private_node(folio) &&\n+\t    !folio_private_flags(folio, NP_OPS_MIGRATION))\n+\t\treturn;\n \tif (!(qp->flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ||\n \t    !vma_migratable(walk->vma) ||\n \t    !migrate_folio_add(folio, qp->pagelist, qp->flags))\n@@ -717,6 +742,9 @@ static int queue_folios_pte_range(pmd_t *pmd, unsigned long addr,\n \t\tfolio = vm_normal_folio(vma, addr, ptent);\n \t\tif (!folio || folio_is_zone_device(folio))\n \t\t\tcontinue;\n+\t\tif (folio_is_private_node(folio) &&\n+\t\t    !folio_private_flags(folio, NP_OPS_MIGRATION))\n+\t\t\tcontinue;\n \t\tif (folio_test_large(folio) && max_nr != 1)\n \t\t\tnr = folio_pte_batch(folio, pte, ptent, max_nr);\n \t\t/*\n@@ -1451,6 +1479,9 @@ static struct folio *alloc_migration_target_by_mpol(struct folio *src,\n \telse\n \t\tgfp = GFP_HIGHUSER_MOVABLE | __GFP_RETRY_MAYFAIL | __GFP_COMP;\n \n+\tif (pol->flags & MPOL_F_PRIVATE)\n+\t\tgfp |= __GFP_PRIVATE;\n+\n \treturn folio_alloc_mpol(gfp, order, pol, ilx, nid);\n }\n #else\n@@ -2280,6 +2311,15 @@ static nodemask_t *policy_nodemask(gfp_t gfp, struct mempolicy *pol,\n \t\t\tnodemask = &pol->nodes;\n \t\tif (pol->home_node != NUMA_NO_NODE)\n \t\t\t*nid = pol->home_node;\n+\t\telse if ((pol->flags & MPOL_F_PRIVATE) &&\n+\t\t\t !node_isset(*nid, pol->nodes)) {\n+\t\t\t/*\n+\t\t\t * Private nodes are not in N_MEMORY nodes' zonelists.\n+\t\t\t * When the preferred nid (usually numa_node_id()) can't\n+\t\t\t * reach the policy nodes, start from a policy node.\n+\t\t\t */\n+\t\t\t*nid = first_node(pol->nodes);\n+\t\t}\n \t\t/*\n \t\t * __GFP_THISNODE shouldn't even be used with the bind policy\n \t\t * because we might easily break the expectation to stay on the\n@@ -2533,6 +2573,10 @@ struct folio *vma_alloc_folio_noprof(gfp_t gfp, int order, struct vm_area_struct\n \t\tgfp |= __GFP_NOWARN;\n \n \tpol = get_vma_policy(vma, addr, order, &ilx);\n+\n+\tif (pol->flags & MPOL_F_PRIVATE)\n+\t\tgfp |= __GFP_PRIVATE;\n+\n \tfolio = folio_alloc_mpol_noprof(gfp, order, pol, ilx, numa_node_id());\n \tmpol_cond_put(pol);\n \treturn folio;\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 5a1b35421d78..ec6c1f8e85d8 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -3849,8 +3849,13 @@ get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,\n \t\t * if another process has NUMA bindings and is causing\n \t\t * kswapd wakeups on only some nodes. Avoid accidental\n \t\t * \"node_reclaim_mode\"-like behavior in this case.\n+\t\t *\n+\t\t * Nodes without kswapd (some private nodes) are never\n+\t\t * skipped - this causes some mempolicies to silently\n+\t\t * fall back to DRAM even if the node is eligible.\n \t\t */\n \t\tif (skip_kswapd_nodes &&\n+\t\t    zone->zone_pgdat->kswapd &&\n \t\t    !waitqueue_active(&zone->zone_pgdat->kswapd_wait)) {\n \t\t\tskipped_kswapd_nodes = true;\n \t\t\tcontinue;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private nodes being used as demotion targets in the memory-tiers subsystem, agreeing that they should be added to the demotion target mask and implementing backpressure support to allow vmscan to fall back to swap. The author also acknowledged that the current demotion logic induces LRU inversions and suggested re-doing it to allow less fallback and kick kswapd instead.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "agreed with approach"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The memory-tier subsystem needs to know which private nodes should\nappear as demotion targets.\n\nAdd NP_OPS_DEMOTION (BIT(2)):\n   Node can be added as a demotion target by memory-tiers.\n\nAdd demotion backpressure support so private nodes can reject\nnew demotions cleanly, allowing vmscan to fall back to swap.\n\nIn the demotion path, try demotion to private nodes invididually,\nthen clear private nodes from the demotion target mask until a\nnon-private node is found, then fall back to the remaining mask.\nThis prevents LRU inversion while still allowing forward progress.\n\nThis is the closest match to the current behavior without making\nprivate nodes inaccessible or preventing forward progress. We\nshould probably completely re-do the demotion logic to allow less\nfallback and kick kswapd instead - right now we induce LRU\ninversions by simply falling back to any node in the demotion list.\n\nAdd memory_tier_refresh_demotion() export for services to trigger\nre-evaluation of demotion targets after changing their flags.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/memory-tiers.h |  9 +++++++\n include/linux/node_private.h | 22 +++++++++++++++++\n mm/internal.h                |  7 ++++++\n mm/memory-tiers.c            | 46 ++++++++++++++++++++++++++++++++----\n mm/page_alloc.c              | 12 +++++++---\n mm/vmscan.c                  | 30 ++++++++++++++++++++++-\n 6 files changed, 117 insertions(+), 9 deletions(-)\n\ndiff --git a/include/linux/memory-tiers.h b/include/linux/memory-tiers.h\nindex 3e1159f6762c..e1476432e359 100644\n--- a/include/linux/memory-tiers.h\n+++ b/include/linux/memory-tiers.h\n@@ -58,6 +58,7 @@ struct memory_dev_type *mt_get_memory_type(int adist);\n int next_demotion_node(int node);\n void node_get_allowed_targets(pg_data_t *pgdat, nodemask_t *targets);\n bool node_is_toptier(int node);\n+void memory_tier_refresh_demotion(void);\n #else\n static inline int next_demotion_node(int node)\n {\n@@ -73,6 +74,10 @@ static inline bool node_is_toptier(int node)\n {\n \treturn true;\n }\n+\n+static inline void memory_tier_refresh_demotion(void)\n+{\n+}\n #endif\n \n #else\n@@ -106,6 +111,10 @@ static inline bool node_is_toptier(int node)\n \treturn true;\n }\n \n+static inline void memory_tier_refresh_demotion(void)\n+{\n+}\n+\n static inline int register_mt_adistance_algorithm(struct notifier_block *nb)\n {\n \treturn 0;\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex e9b58afa366b..e254e36056cd 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -88,6 +88,8 @@ struct node_private_ops {\n #define NP_OPS_MIGRATION\t\tBIT(0)\n /* Allow mempolicy-directed allocation and mbind migration to this node */\n #define NP_OPS_MEMPOLICY\t\tBIT(1)\n+/* Node participates as a demotion target in memory-tiers */\n+#define NP_OPS_DEMOTION\t\t\tBIT(2)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n@@ -101,12 +103,14 @@ struct node_private_ops {\n  *\t\tcallbacks that may sleep; 0 = fully released)\n  * @released: Signaled when refcount drops to 0; unregister waits on this\n  * @ops: Service callbacks and exclusion flags (NULL until service registers)\n+ * @migration_blocked: Service signals migrations should pause\n  */\n struct node_private {\n \tvoid *owner;\n \trefcount_t refcount;\n \tstruct completion released;\n \tconst struct node_private_ops *ops;\n+\tbool migration_blocked;\n };\n \n #ifdef CONFIG_NUMA\n@@ -306,6 +310,19 @@ static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n \t}\n \treturn eligible;\n }\n+\n+static inline bool node_private_migration_blocked(int nid)\n+{\n+\tstruct node_private *np;\n+\tbool blocked;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tblocked = np && READ_ONCE(np->migration_blocked);\n+\trcu_read_unlock();\n+\n+\treturn blocked;\n+}\n #endif /* CONFIG_MEMORY_HOTPLUG */\n \n #else /* !CONFIG_NUMA */\n@@ -404,6 +421,11 @@ static inline bool nodes_private_mpol_allowed(const nodemask_t *nodes)\n \treturn false;\n }\n \n+static inline bool node_private_migration_blocked(int nid)\n+{\n+\treturn false;\n+}\n+\n static inline int node_private_register(int nid, struct node_private *np)\n {\n \treturn -ENODEV;\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 6ab4679fe943..5950e20d4023 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1206,6 +1206,8 @@ extern int node_reclaim_mode;\n \n extern int node_reclaim(struct pglist_data *, gfp_t, unsigned int);\n extern int find_next_best_node(int node, nodemask_t *used_node_mask);\n+extern int find_next_best_node_in(int node, nodemask_t *used_node_mask,\n+\t\t\t\t  const nodemask_t *candidates);\n extern bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n \t\t\t      gfp_t gfp_mask);\n #else\n@@ -1220,6 +1222,11 @@ static inline int find_next_best_node(int node, nodemask_t *used_node_mask)\n {\n \treturn NUMA_NO_NODE;\n }\n+static inline int find_next_best_node_in(int node, nodemask_t *used_node_mask,\n+\t\t\t\t\t const nodemask_t *candidates)\n+{\n+\treturn NUMA_NO_NODE;\n+}\n static inline bool numa_zone_alloc_allowed(int alloc_flags, struct zone *zone,\n \t\t\t\t     gfp_t gfp_mask)\n {\ndiff --git a/mm/memory-tiers.c b/mm/memory-tiers.c\nindex 9c742e18e48f..434190fdc078 100644\n--- a/mm/memory-tiers.c\n+++ b/mm/memory-tiers.c\n@@ -3,6 +3,7 @@\n #include <linux/lockdep.h>\n #include <linux/sysfs.h>\n #include <linux/kobject.h>\n+#include <linux/node_private.h>\n #include <linux/memory.h>\n #include <linux/memory-tiers.h>\n #include <linux/notifier.h>\n@@ -380,6 +381,8 @@ static void disable_all_demotion_targets(void)\n \t\tif (memtier)\n \t\t\tmemtier->lower_tier_mask = NODE_MASK_NONE;\n \t}\n+\tfor_each_node_state(node, N_MEMORY_PRIVATE)\n+\t\tnode_demotion[node].preferred = NODE_MASK_NONE;\n \t/*\n \t * Ensure that the \"disable\" is visible across the system.\n \t * Readers will see either a combination of before+disable\n@@ -421,6 +424,7 @@ static void establish_demotion_targets(void)\n \tint target = NUMA_NO_NODE, node;\n \tint distance, best_distance;\n \tnodemask_t tier_nodes, lower_tier;\n+\tnodemask_t all_memory;\n \n \tlockdep_assert_held_once(&memory_tier_lock);\n \n@@ -429,6 +433,13 @@ static void establish_demotion_targets(void)\n \n \tdisable_all_demotion_targets();\n \n+\t/* Include private nodes that have opted in to demotion. */\n+\tall_memory = node_states[N_MEMORY];\n+\tfor_each_node_state(node, N_MEMORY_PRIVATE) {\n+\t\tif (node_private_has_flag(node, NP_OPS_DEMOTION))\n+\t\t\tnode_set(node, all_memory);\n+\t}\n+\n \tfor_each_node_state(node, N_MEMORY) {\n \t\tbest_distance = -1;\n \t\tnd = &node_demotion[node];\n@@ -442,12 +453,12 @@ static void establish_demotion_targets(void)\n \t\tmemtier = list_next_entry(memtier, list);\n \t\ttier_nodes = get_memtier_nodemask(memtier);\n \t\t/*\n-\t\t * find_next_best_node, use 'used' nodemask as a skip list.\n+\t\t * find_next_best_node_in, use 'used' nodemask as a skip list.\n \t\t * Add all memory nodes except the selected memory tier\n \t\t * nodelist to skip list so that we find the best node from the\n \t\t * memtier nodelist.\n \t\t */\n-\t\tnodes_andnot(tier_nodes, node_states[N_MEMORY], tier_nodes);\n+\t\tnodes_andnot(tier_nodes, all_memory, tier_nodes);\n \n \t\t/*\n \t\t * Find all the nodes in the memory tier node list of same best distance.\n@@ -455,7 +466,8 @@ static void establish_demotion_targets(void)\n \t\t * in the preferred mask when allocating pages during demotion.\n \t\t */\n \t\tdo {\n-\t\t\ttarget = find_next_best_node(node, &tier_nodes);\n+\t\t\ttarget = find_next_best_node_in(node, &tier_nodes,\n+\t\t\t\t\t\t\t&all_memory);\n \t\t\tif (target == NUMA_NO_NODE)\n \t\t\t\tbreak;\n \n@@ -495,7 +507,7 @@ static void establish_demotion_targets(void)\n \t * allocation to a set of nodes that is closer the above selected\n \t * preferred node.\n \t */\n-\tlower_tier = node_states[N_MEMORY];\n+\tlower_tier = all_memory;\n \tlist_for_each_entry(memtier, &memory_tiers, list) {\n \t\t/*\n \t\t * Keep removing current tier from lower_tier nodes,\n@@ -542,7 +554,7 @@ static struct memory_tier *set_node_memory_tier(int node)\n \n \tlockdep_assert_held_once(&memory_tier_lock);\n \n-\tif (!node_state(node, N_MEMORY))\n+\tif (!node_state(node, N_MEMORY) && !node_state(node, N_MEMORY_PRIVATE))\n \t\treturn ERR_PTR(-EINVAL);\n \n \tmt_calc_adistance(node, &adist);\n@@ -865,6 +877,30 @@ int mt_calc_adistance(int node, int *adist)\n }\n EXPORT_SYMBOL_GPL(mt_calc_adistance);\n \n+/**\n+ * memory_tier_refresh_demotion() - Re-establish demotion targets\n+ *\n+ * Called by services after registering or unregistering ops->migrate_to on\n+ * a private node, so that establish_demotion_targets() picks up the change.\n+ */\n+void memory_tier_refresh_demotion(void)\n+{\n+\tint nid;\n+\n+\tmutex_lock(&memory_tier_lock);\n+\t/*\n+\t * Ensure private nodes are registered with a tier, otherwise\n+\t * they won't show up in any node's demotion targets nodemask.\n+\t */\n+\tfor_each_node_state(nid, N_MEMORY_PRIVATE) {\n+\t\tif (!__node_get_memory_tier(nid))\n+\t\t\tset_node_memory_tier(nid);\n+\t}\n+\testablish_demotion_targets();\n+\tmutex_unlock(&memory_tier_lock);\n+}\n+EXPORT_SYMBOL_GPL(memory_tier_refresh_demotion);\n+\n static int __meminit memtier_hotplug_callback(struct notifier_block *self,\n \t\t\t\t\t      unsigned long action, void *_arg)\n {\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex ec6c1f8e85d8..e272dfdc6b00 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -5589,7 +5589,8 @@ static int node_load[MAX_NUMNODES];\n  *\n  * Return: node id of the found node or %NUMA_NO_NODE if no node is found.\n  */\n-int find_next_best_node(int node, nodemask_t *used_node_mask)\n+int find_next_best_node_in(int node, nodemask_t *used_node_mask,\n+\t\t\t   const nodemask_t *candidates)\n {\n \tint n, val;\n \tint min_val = INT_MAX;\n@@ -5599,12 +5600,12 @@ int find_next_best_node(int node, nodemask_t *used_node_mask)\n \t * Use the local node if we haven't already, but for memoryless local\n \t * node, we should skip it and fall back to other nodes.\n \t */\n-\tif (!node_isset(node, *used_node_mask) && node_state(node, N_MEMORY)) {\n+\tif (!node_isset(node, *used_node_mask) && node_isset(node, *candidates)) {\n \t\tnode_set(node, *used_node_mask);\n \t\treturn node;\n \t}\n \n-\tfor_each_node_state(n, N_MEMORY) {\n+\tfor_each_node_mask(n, *candidates) {\n \n \t\t/* Don't want a node to appear more than once */\n \t\tif (node_isset(n, *used_node_mask))\n@@ -5636,6 +5637,11 @@ int find_next_best_node(int node, nodemask_t *used_node_mask)\n \treturn best_node;\n }\n \n+int find_next_best_node(int node, nodemask_t *used_node_mask)\n+{\n+\treturn find_next_best_node_in(node, used_node_mask,\n+\t\t\t\t      &node_states[N_MEMORY]);\n+}\n \n /*\n  * Build zonelists ordered by node and zones within node.\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 6113be4d3519..0f534428ea88 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -355,6 +356,10 @@ static bool can_demote(int nid, struct scan_control *sc,\n \tif (demotion_nid == NUMA_NO_NODE)\n \t\treturn false;\n \n+\t/* Don't demote when the target's service signals backpressure */\n+\tif (node_private_migration_blocked(demotion_nid))\n+\t\treturn false;\n+\n \t/* If demotion node isn't in the cgroup's mems_allowed, fall back */\n \treturn mem_cgroup_node_allowed(memcg, demotion_nid);\n }\n@@ -1022,8 +1027,10 @@ static unsigned int demote_folio_list(struct list_head *demote_folios,\n \t\t\t\t     struct pglist_data *pgdat)\n {\n \tint target_nid = next_demotion_node(pgdat->node_id);\n-\tunsigned int nr_succeeded;\n+\tint first_nid = target_nid;\n+\tunsigned int nr_succeeded = 0;\n \tnodemask_t allowed_mask;\n+\tint ret;\n \n \tstruct migration_target_control mtc = {\n \t\t/*\n@@ -1046,6 +1053,27 @@ static unsigned int demote_folio_list(struct list_head *demote_folios,\n \n \tnode_get_allowed_targets(pgdat, &allowed_mask);\n \n+\t/* Try private node targets until we find non-private node */\n+\twhile (node_state(target_nid, N_MEMORY_PRIVATE)) {\n+\t\tunsigned int nr = 0;\n+\n+\t\tret = node_private_migrate_to(demote_folios, target_nid,\n+\t\t\t\t\t      MIGRATE_ASYNC, MR_DEMOTION,\n+\t\t\t\t\t      &nr);\n+\t\tnr_succeeded += nr;\n+\t\tif (ret == 0 || list_empty(demote_folios))\n+\t\t\treturn nr_succeeded;\n+\n+\t\ttarget_nid = next_node_in(target_nid, allowed_mask);\n+\t\tif (target_nid == first_nid)\n+\t\t\treturn nr_succeeded;\n+\t\tif (!node_state(target_nid, N_MEMORY_PRIVATE))\n+\t\t\tbreak;\n+\t}\n+\n+\t/* target_nid is a non-private node; use standard migration */\n+\tmtc.nid = target_nid;\n+\n \t/* Demotion ignores all cpuset and mempolicy settings */\n \tmigrate_pages(demote_folios, alloc_demote_folio, NULL,\n \t\t      (unsigned long)&mtc, MIGRATE_ASYNC, MR_DEMOTION,\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about write faults on private nodes by adding a new operation flag NP_OPS_PROTECT_WRITE and modifying several functions to prevent PTEs from being upgraded to writable when the node is write-protected.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Services that intercept write faults (e.g., for promotion tracking)\nneed PTEs to stay read-only. This requires preventing mprotect\nfrom silently upgrade the PTE, bypassing the service's handle_fault\ncallback.\n\nAdd NP_OPS_PROTECT_WRITE and folio_managed_wrprotect().\n\nIn change_pte_range() and change_huge_pmd(), suppress PTE write-upgrade\nwhen MM_CP_TRY_CHANGE_WRITABLE is sees the folio is write-protected.\n\nIn handle_pte_fault() and do_huge_pmd_wp_page(), dispatch to the node's\nops->handle_fault callback when set, allowing the service to handle write\nfaults with promotion or other custom logic.\n\nNP_OPS_MEMPOLICY is incompatible with NP_OPS_PROTECT_WRITE to avoid the\nfootgun of binding a writable VMA to a write-protected node.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |  4 ++\n include/linux/node_private.h | 22 ++++++++\n mm/huge_memory.c             | 17 ++++++-\n mm/internal.h                | 99 ++++++++++++++++++++++++++++++++++++\n mm/memory.c                  | 15 ++++++\n mm/migrate.c                 | 14 +----\n mm/mprotect.c                |  4 +-\n 7 files changed, 159 insertions(+), 16 deletions(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex c08b5a948779..a4955b9b5b93 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -957,6 +957,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    !(ops->flags & NP_OPS_MIGRATION))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_MEMPOLICY) &&\n+\t    (ops->flags & NP_OPS_PROTECT_WRITE))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex e254e36056cd..27d6e5d84e61 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -70,6 +70,24 @@ struct vm_fault;\n  *     PFN-based metadata (compression tables, device page tables, DMA\n  *     mappings, etc.) before any access through the page tables.\n  *\n+ * @handle_fault: Handle fault on folio on this private node.\n+ *   [folio-referenced callback, PTL held on entry]\n+ *\n+ *   Called from handle_pte_fault() (PTE level) or do_huge_pmd_wp_page()\n+ *   (PMD level) after lock acquisition and entry verification.\n+ *   @folio is the faulting folio, @level indicates the page table level.\n+ *\n+ *   For PGTABLE_LEVEL_PTE: vmf->pte is mapped and vmf->ptl is the\n+ *   PTE lock.  Release via pte_unmap_unlock(vmf->pte, vmf->ptl).\n+ *\n+ *   For PGTABLE_LEVEL_PMD: vmf->pte is NULL and vmf->ptl is the\n+ *   PMD lock.  Release via spin_unlock(vmf->ptl).\n+ *\n+ *   The callback MUST release PTL on ALL paths.\n+ *   The caller will NOT touch the page table entry after this returns.\n+ *\n+ *   Returns: vm_fault_t result (0, VM_FAULT_RETRY, etc.)\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n@@ -81,6 +99,8 @@ struct node_private_ops {\n \t\t\t\t  enum migrate_reason reason,\n \t\t\t\t  unsigned int *nr_succeeded);\n \tvoid (*folio_migrate)(struct folio *src, struct folio *dst);\n+\tvm_fault_t (*handle_fault)(struct folio *folio, struct vm_fault *vmf,\n+\t\t\t\t   enum pgtable_level level);\n \tunsigned long flags;\n };\n \n@@ -90,6 +110,8 @@ struct node_private_ops {\n #define NP_OPS_MEMPOLICY\t\tBIT(1)\n /* Node participates as a demotion target in memory-tiers */\n #define NP_OPS_DEMOTION\t\t\tBIT(2)\n+/* Prevent mprotect/NUMA from upgrading PTEs to writable on this node */\n+#define NP_OPS_PROTECT_WRITE\t\tBIT(3)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\ndiff --git a/mm/huge_memory.c b/mm/huge_memory.c\nindex 2ecae494291a..d9ba6593244d 100644\n--- a/mm/huge_memory.c\n+++ b/mm/huge_memory.c\n@@ -2063,12 +2063,14 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf)\n \tstruct page *page;\n \tunsigned long haddr = vmf->address & HPAGE_PMD_MASK;\n \tpmd_t orig_pmd = vmf->orig_pmd;\n+\tvm_fault_t ret;\n+\n \n \tvmf->ptl = pmd_lockptr(vma->vm_mm, vmf->pmd);\n \tVM_BUG_ON_VMA(!vma->anon_vma, vma);\n \n \tif (is_huge_zero_pmd(orig_pmd)) {\n-\t\tvm_fault_t ret = do_huge_zero_wp_pmd(vmf);\n+\t\tret = do_huge_zero_wp_pmd(vmf);\n \n \t\tif (!(ret & VM_FAULT_FALLBACK))\n \t\t\treturn ret;\n@@ -2088,6 +2090,13 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf)\n \tfolio = page_folio(page);\n \tVM_BUG_ON_PAGE(!PageHead(page), page);\n \n+\t/* Private-managed write-protect: let the service handle the fault */\n+\tif (unlikely(folio_is_private_managed(folio))) {\n+\t\tif (folio_managed_handle_fault(folio, vmf,\n+\t\t\t\t\t      PGTABLE_LEVEL_PMD, &ret))\n+\t\t\treturn ret;\n+\t}\n+\n \t/* Early check when only holding the PT lock. */\n \tif (PageAnonExclusive(page))\n \t\tgoto reuse;\n@@ -2633,7 +2642,8 @@ int change_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,\n \n \t/* See change_pte_range(). */\n \tif ((cp_flags & MM_CP_TRY_CHANGE_WRITABLE) && !pmd_write(entry) &&\n-\t    can_change_pmd_writable(vma, addr, entry))\n+\t    can_change_pmd_writable(vma, addr, entry) &&\n+\t    !folio_managed_wrprotect(pmd_folio(entry)))\n \t\tentry = pmd_mkwrite(entry, vma);\n \n \tret = HPAGE_PMD_NR;\n@@ -4943,6 +4953,9 @@ void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)\n \tif (folio_test_dirty(folio) && softleaf_is_migration_dirty(entry))\n \t\tpmde = pmd_mkdirty(pmde);\n \n+\tif (folio_managed_wrprotect(folio))\n+\t\tpmde = pmd_wrprotect(pmde);\n+\n \tif (folio_is_device_private(folio)) {\n \t\tswp_entry_t entry;\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex 5950e20d4023..ae4ff86e8dc6 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -11,6 +11,7 @@\n #include <linux/khugepaged.h>\n #include <linux/mm.h>\n #include <linux/mm_inline.h>\n+#include <linux/node_private.h>\n #include <linux/pagemap.h>\n #include <linux/pagewalk.h>\n #include <linux/rmap.h>\n@@ -18,6 +19,7 @@\n #include <linux/leafops.h>\n #include <linux/swap_cgroup.h>\n #include <linux/tracepoint-defs.h>\n+#include <linux/node_private.h>\n \n /* Internal core VMA manipulation functions. */\n #include \"vma.h\"\n@@ -1449,6 +1451,103 @@ static inline bool folio_managed_on_free(struct folio *folio)\n \treturn false;\n }\n \n+/*\n+ * folio_managed_handle_fault - Dispatch fault on managed-memory folio\n+ * @folio: the faulting folio (must not be NULL)\n+ * @vmf: the vm_fault descriptor (PTL held: vmf->ptl locked)\n+ * @level: page table level (PGTABLE_LEVEL_PTE or PGTABLE_LEVEL_PMD)\n+ * @ret: output fault result if handled\n+ *\n+ * Called with PTL held.  If a handle_fault callback exists, it is invoked\n+ * with PTL still held.  The callback is responsible for releasing PTL on\n+ * all paths.\n+ *\n+ * Returns true if the service handled the fault (PTL released by callback,\n+ * caller returns *ret).  Returns false if no handler exists (PTL still held,\n+ * caller continues with normal fault handling).\n+ */\n+static inline bool folio_managed_handle_fault(struct folio *folio,\n+\t\t\t\t\t      struct vm_fault *vmf,\n+\t\t\t\t\t      enum pgtable_level level,\n+\t\t\t\t\t      vm_fault_t *ret)\n+{\n+\t/* Zone device pages use swap entries; handled in do_swap_page */\n+\tif (folio_is_zone_device(folio))\n+\t\treturn false;\n+\n+\tif (folio_is_private_node(folio)) {\n+\t\tconst struct node_private_ops *ops =\n+\t\t\tfolio_node_private_ops(folio);\n+\n+\t\tif (ops && ops->handle_fault) {\n+\t\t\t*ret = ops->handle_fault(folio, vmf, level);\n+\t\t\treturn true;\n+\t\t}\n+\t}\n+\treturn false;\n+}\n+\n+/**\n+ * folio_managed_wrprotect - Should this folio's mappings stay write-protected?\n+ * @folio: the folio to check\n+ *\n+ * Returns true if the folio is on a private node with NP_OPS_PROTECT_WRITE,\n+ * meaning page table entries (PTE or PMD) should not be made writable.\n+ * Write faults are intercepted by the service's handle_fault callback\n+ * to promote the folio to DRAM.\n+ *\n+ * Used by:\n+ *   - change_pte_range() / change_huge_pmd(): prevent mprotect write-upgrade\n+ *   - remove_migration_pte() / remove_migration_pmd(): strip write after migration\n+ *   - do_huge_pmd_wp_page(): dispatch to fault handler instead of reuse\n+ */\n+static inline bool folio_managed_wrprotect(struct folio *folio)\n+{\n+\treturn unlikely(folio_is_private_node(folio) &&\n+\t\t\tfolio_private_flags(folio, NP_OPS_PROTECT_WRITE));\n+}\n+\n+/**\n+ * folio_managed_fixup_migration_pte - Fixup PTE after migration for\n+ *                                     managed memory pages.\n+ * @new: the destination page\n+ * @pte: the PTE being installed (normal PTE built by caller)\n+ * @old_pte: the original PTE (before migration, for swap entry flags)\n+ * @vma: the VMA\n+ *\n+ * For MEMORY_DEVICE_PRIVATE pages: replaces the PTE with a device-private\n+ * swap entry, preserving soft_dirty and uffd_wp from old_pte.\n+ *\n+ * For N_MEMORY_PRIVATE pages with NP_OPS_PROTECT_WRITE: strips the write\n+ * bit so the next write triggers the fault handler for promotion.\n+ *\n+ * For normal pages: returns pte unmodified.\n+ */\n+static inline pte_t folio_managed_fixup_migration_pte(struct page *new,\n+\t\t\t\t\t\t      pte_t pte,\n+\t\t\t\t\t\t      pte_t old_pte,\n+\t\t\t\t\t\t      struct vm_area_struct *vma)\n+{\n+\tif (unlikely(is_device_private_page(new))) {\n+\t\tsoftleaf_t entry;\n+\n+\t\tif (pte_write(pte))\n+\t\t\tentry = make_writable_device_private_entry(\n+\t\t\t\t\t\tpage_to_pfn(new));\n+\t\telse\n+\t\t\tentry = make_readable_device_private_entry(\n+\t\t\t\t\t\tpage_to_pfn(new));\n+\t\tpte = softleaf_to_pte(entry);\n+\t\tif (pte_swp_soft_dirty(old_pte))\n+\t\t\tpte = pte_swp_mksoft_dirty(pte);\n+\t\tif (pte_swp_uffd_wp(old_pte))\n+\t\t\tpte = pte_swp_mkuffd_wp(pte);\n+\t} else if (folio_managed_wrprotect(page_folio(new))) {\n+\t\tpte = pte_wrprotect(pte);\n+\t}\n+\treturn pte;\n+}\n+\n /**\n  * folio_managed_migrate_notify - Notify service that a folio changed location\n  * @src: the old folio (about to be freed)\ndiff --git a/mm/memory.c b/mm/memory.c\nindex 2a55edc48a65..0f78988befef 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -6079,6 +6079,10 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \t * Make it present again, depending on how arch implements\n \t * non-accessible ptes, some can allow access by kernel mode.\n \t */\n+\tif (unlikely(folio && folio_managed_wrprotect(folio))) {\n+\t\twritable = false;\n+\t\tignore_writable = true;\n+\t}\n \tif (folio && folio_test_large(folio))\n \t\tnuma_rebuild_large_mapping(vmf, vma, folio, pte, ignore_writable,\n \t\t\t\t\t   pte_write_upgrade);\n@@ -6228,6 +6232,7 @@ static void fix_spurious_fault(struct vm_fault *vmf,\n  */\n static vm_fault_t handle_pte_fault(struct vm_fault *vmf)\n {\n+\tstruct folio *folio;\n \tpte_t entry;\n \n \tif (unlikely(pmd_none(*vmf->pmd))) {\n@@ -6284,6 +6289,16 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)\n \t\tupdate_mmu_tlb(vmf->vma, vmf->address, vmf->pte);\n \t\tgoto unlock;\n \t}\n+\n+\tfolio = vm_normal_folio(vmf->vma, vmf->address, entry);\n+\tif (unlikely(folio && folio_is_private_managed(folio))) {\n+\t\tvm_fault_t fault_ret;\n+\n+\t\tif (folio_managed_handle_fault(folio, vmf, PGTABLE_LEVEL_PTE,\n+\t\t\t\t\t       &fault_ret))\n+\t\t\treturn fault_ret;\n+\t}\n+\n \tif (vmf->flags & (FAULT_FLAG_WRITE|FAULT_FLAG_UNSHARE)) {\n \t\tif (!pte_write(entry))\n \t\t\treturn do_wp_page(vmf);\ndiff --git a/mm/migrate.c b/mm/migrate.c\nindex a54d4af04df3..f632e8b03504 100644\n--- a/mm/migrate.c\n+++ b/mm/migrate.c\n@@ -398,19 +398,7 @@ static bool remove_migration_pte(struct folio *folio,\n \t\tif (folio_test_anon(folio) && !softleaf_is_migration_read(entry))\n \t\t\trmap_flags |= RMAP_EXCLUSIVE;\n \n-\t\tif (unlikely(is_device_private_page(new))) {\n-\t\t\tif (pte_write(pte))\n-\t\t\t\tentry = make_writable_device_private_entry(\n-\t\t\t\t\t\t\tpage_to_pfn(new));\n-\t\t\telse\n-\t\t\t\tentry = make_readable_device_private_entry(\n-\t\t\t\t\t\t\tpage_to_pfn(new));\n-\t\t\tpte = softleaf_to_pte(entry);\n-\t\t\tif (pte_swp_soft_dirty(old_pte))\n-\t\t\t\tpte = pte_swp_mksoft_dirty(pte);\n-\t\t\tif (pte_swp_uffd_wp(old_pte))\n-\t\t\t\tpte = pte_swp_mkuffd_wp(pte);\n-\t\t}\n+\t\tpte = folio_managed_fixup_migration_pte(new, pte, old_pte, vma);\n \n #ifdef CONFIG_HUGETLB_PAGE\n \t\tif (folio_test_hugetlb(folio)) {\ndiff --git a/mm/mprotect.c b/mm/mprotect.c\nindex 283889e4f1ce..830be609bc24 100644\n--- a/mm/mprotect.c\n+++ b/mm/mprotect.c\n@@ -30,6 +30,7 @@\n #include <linux/mm_inline.h>\n #include <linux/pgtable.h>\n #include <linux/userfaultfd_k.h>\n+#include <linux/node_private.h>\n #include <uapi/linux/mman.h>\n #include <asm/cacheflush.h>\n #include <asm/mmu_context.h>\n@@ -290,7 +291,8 @@ static long change_pte_range(struct mmu_gather *tlb,\n \t\t\t * COW or special handling is required.\n \t\t\t */\n \t\t\tif ((cp_flags & MM_CP_TRY_CHANGE_WRITABLE) &&\n-\t\t\t     !pte_write(ptent))\n+\t\t\t     !pte_write(ptent) &&\n+\t\t\t     !(folio && folio_managed_wrprotect(folio)))\n \t\t\t\tset_write_prot_commit_flush_ptes(vma, folio, page,\n \t\t\t\taddr, pte, oldpte, ptent, nr_ptes, tlb);\n \t\t\telse\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the reclaim policy for private nodes in boosted reclaim mode, explaining that it needs to allow swap and writepage operations. They proposed adding a reclaim_policy callback to struct node_private_ops and a struct node_reclaim_policy to configure these policies. The author also added zone_reclaim_allowed() to filter private nodes that have not opted into reclaim.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix is needed",
                "proposed changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node services that drive kswapd via watermark_boost need\ncontrol over the reclaim policy.  There are three problems:\n\n1) Boosted reclaim suppresses may_swap and may_writepage.  When\n   demotion is not possible, swap is the only evict path, so kswapd\n   cannot make progress and pages are stranded.\n\n2) __setup_per_zone_wmarks() unconditionally zeros watermark_boost,\n   killing the service's pressure signal.\n\n3) Not all private nodes want reclaim to touch their pages.\n\nAdd a reclaim_policy callback to struct node_private_ops and a\nstruct node_reclaim_policy with:\n\n  - active:             set by the helper when a callback was invoked\n  - may_swap:           allow swap writeback during boosted reclaim\n  - may_writepage:      allow writepage during boosted reclaim\n  - managed_watermarks: service owns watermark_boost lifecycle\n\nWe do not allow disabling swap/writepage, as core MM may have\nexplicitly enabled them on a non-boosted pass.\n\nWe only allow enablign swap/writepage, so that the supression during\na boost can be overridden.  This allows a device to force evictions\neven when the system otherwise would not percieve pressure.\n\nThis is important for a service like compressed RAM, as device capacity\nmay differ from reported capacity, and device may want to relieve real\npressure (poor compression ratio) as opposed to percieved pressure\n(i.e. how many pages are in use).\n\nAdd zone_reclaim_allowed() to filter private nodes that have not\nopted into reclaim.\n\nRegular nodes fall through to cpuset_zone_allowed() unchanged.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h | 28 ++++++++++++++++++++++++++++\n mm/internal.h                | 36 ++++++++++++++++++++++++++++++++++++\n mm/page_alloc.c              | 11 ++++++++++-\n mm/vmscan.c                  | 25 +++++++++++++++++++++++--\n 4 files changed, 97 insertions(+), 3 deletions(-)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 27d6e5d84e61..34be52383255 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -14,6 +14,24 @@ struct page;\n struct vm_area_struct;\n struct vm_fault;\n \n+/**\n+ * struct node_reclaim_policy - Reclaim policy overrides for private nodes\n+ * @active: set by node_private_reclaim_policy() when a callback was invoked\n+ * @may_swap: allow swap writeback during boosted reclaim\n+ * @may_writepage: allow writepage during boosted reclaim\n+ * @managed_watermarks: service owns watermark_boost lifecycle; kswapd must\n+ *                      not clear it after boosted reclaim\n+ *\n+ * Passed to the reclaim_policy callback so each private node service can\n+ * inject its own reclaim policy before kswapd runs boosted reclaim.\n+ */\n+struct node_reclaim_policy {\n+\tbool active;\n+\tbool may_swap;\n+\tbool may_writepage;\n+\tbool managed_watermarks;\n+};\n+\n /**\n  * struct node_private_ops - Callbacks for private node services\n  *\n@@ -88,6 +106,13 @@ struct vm_fault;\n  *\n  *   Returns: vm_fault_t result (0, VM_FAULT_RETRY, etc.)\n  *\n+ * @reclaim_policy: Configure reclaim policy for boosted reclaim.\n+ *   [called hodling rcu_read_lock, MUST NOT sleep]\n+ *   Called by kswapd before boosted reclaim to let the service override\n+ *   may_swap / may_writepage.  If provided, the service also owns the\n+ *   watermark_boost lifecycle (kswapd will not clear it).\n+ *   If NULL, normal boost policy applies.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n@@ -101,6 +126,7 @@ struct node_private_ops {\n \tvoid (*folio_migrate)(struct folio *src, struct folio *dst);\n \tvm_fault_t (*handle_fault)(struct folio *folio, struct vm_fault *vmf,\n \t\t\t\t   enum pgtable_level level);\n+\tvoid (*reclaim_policy)(int nid, struct node_reclaim_policy *policy);\n \tunsigned long flags;\n };\n \n@@ -112,6 +138,8 @@ struct node_private_ops {\n #define NP_OPS_DEMOTION\t\t\tBIT(2)\n /* Prevent mprotect/NUMA from upgrading PTEs to writable on this node */\n #define NP_OPS_PROTECT_WRITE\t\tBIT(3)\n+/* Kernel reclaim (kswapd, direct reclaim, OOM) operates on this node */\n+#define NP_OPS_RECLAIM\t\t\tBIT(4)\n \n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\ndiff --git a/mm/internal.h b/mm/internal.h\nindex ae4ff86e8dc6..db32cb2d7a29 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1572,6 +1572,42 @@ static inline void folio_managed_migrate_notify(struct folio *src,\n \t\tops->folio_migrate(src, dst);\n }\n \n+/**\n+ * node_private_reclaim_policy - invoke the service's reclaim policy callback\n+ * @nid: NUMA node id\n+ * @policy: reclaim policy struct to fill in\n+ *\n+ * Called by kswapd before boosted reclaim.  Zeroes @policy, then if the\n+ * private node service provides a reclaim_policy callback, invokes it\n+ * and sets policy->active to true.\n+ */\n+#ifdef CONFIG_NUMA\n+static inline void node_private_reclaim_policy(int nid,\n+\t\t\t\t\t       struct node_reclaim_policy *policy)\n+{\n+\tstruct node_private *np;\n+\n+\tmemset(policy, 0, sizeof(*policy));\n+\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn;\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\tif (np && np->ops && np->ops->reclaim_policy) {\n+\t\tnp->ops->reclaim_policy(nid, policy);\n+\t\tpolicy->active = true;\n+\t}\n+\trcu_read_unlock();\n+}\n+#else\n+static inline void node_private_reclaim_policy(int nid,\n+\t\t\t\t\t       struct node_reclaim_policy *policy)\n+{\n+\tmemset(policy, 0, sizeof(*policy));\n+}\n+#endif\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e272dfdc6b00..9692048ab5fb 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -55,6 +55,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/node_private.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -6437,6 +6438,8 @@ static void __setup_per_zone_wmarks(void)\n \tunsigned long lowmem_pages = 0;\n \tstruct zone *zone;\n \tunsigned long flags;\n+\tstruct node_reclaim_policy rp;\n+\tint prev_nid = NUMA_NO_NODE;\n \n \t/* Calculate total number of !ZONE_HIGHMEM and !ZONE_MOVABLE pages */\n \tfor_each_zone(zone) {\n@@ -6446,6 +6449,7 @@ static void __setup_per_zone_wmarks(void)\n \n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n+\t\tint nid = zone_to_nid(zone);\n \n \t\tspin_lock_irqsave(&zone->lock, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n@@ -6482,7 +6486,12 @@ static void __setup_per_zone_wmarks(void)\n \t\t\t    mult_frac(zone_managed_pages(zone),\n \t\t\t\t      watermark_scale_factor, 10000));\n \n-\t\tzone->watermark_boost = 0;\n+\t\tif (nid != prev_nid) {\n+\t\t\tnode_private_reclaim_policy(nid, &rp);\n+\t\t\tprev_nid = nid;\n+\t\t}\n+\t\tif (!rp.managed_watermarks)\n+\t\t\tzone->watermark_boost = 0;\n \t\tzone->_watermark[WMARK_LOW]  = min_wmark_pages(zone) + tmp;\n \t\tzone->_watermark[WMARK_HIGH] = low_wmark_pages(zone) + tmp;\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 0f534428ea88..07de666c1276 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -73,6 +73,13 @@\n #define CREATE_TRACE_POINTS\n #include <trace/events/vmscan.h>\n \n+static inline bool zone_reclaim_allowed(struct zone *zone, gfp_t gfp_mask)\n+{\n+\tif (node_state(zone_to_nid(zone), N_MEMORY_PRIVATE))\n+\t\treturn zone_private_flags(zone, NP_OPS_RECLAIM);\n+\treturn cpuset_zone_allowed(zone, gfp_mask);\n+}\n+\n struct scan_control {\n \t/* How many pages shrink_list() should reclaim */\n \tunsigned long nr_to_reclaim;\n@@ -6274,7 +6281,7 @@ static void shrink_zones(struct zonelist *zonelist, struct scan_control *sc)\n \t\t * to global LRU.\n \t\t */\n \t\tif (!cgroup_reclaim(sc)) {\n-\t\t\tif (!cpuset_zone_allowed(zone,\n+\t\t\tif (!zone_reclaim_allowed(zone,\n \t\t\t\t\t\t GFP_KERNEL | __GFP_HARDWALL))\n \t\t\t\tcontinue;\n \n@@ -6992,6 +6999,7 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \tunsigned long zone_boosts[MAX_NR_ZONES] = { 0, };\n \tbool boosted;\n \tstruct zone *zone;\n+\tstruct node_reclaim_policy policy;\n \tstruct scan_control sc = {\n \t\t.gfp_mask = GFP_KERNEL,\n \t\t.order = order,\n@@ -7016,6 +7024,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \t}\n \tboosted = nr_boost_reclaim;\n \n+\t/* Query/cache private node reclaim policy once per balance() */\n+\tnode_private_reclaim_policy(pgdat->node_id, &policy);\n+\n restart:\n \tset_reclaim_active(pgdat, highest_zoneidx);\n \tsc.priority = DEF_PRIORITY;\n@@ -7083,6 +7094,12 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \t\tsc.may_writepage = !laptop_mode && !nr_boost_reclaim;\n \t\tsc.may_swap = !nr_boost_reclaim;\n \n+\t\t/* Private nodes may enable swap/writepage when using boost */\n+\t\tif (policy.active) {\n+\t\t\tsc.may_swap |= policy.may_swap;\n+\t\t\tsc.may_writepage |= policy.may_writepage;\n+\t\t}\n+\n \t\t/*\n \t\t * Do some background aging, to give pages a chance to be\n \t\t * referenced before reclaiming. All pages are rotated\n@@ -7176,6 +7193,10 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \t\t\tif (!zone_boosts[i])\n \t\t\t\tcontinue;\n \n+\t\t\t/* Some private nodes may own the\\ boost lifecycle */\n+\t\t\tif (policy.managed_watermarks)\n+\t\t\t\tcontinue;\n+\n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n \t\t\tspin_lock_irqsave(&zone->lock, flags);\n@@ -7406,7 +7427,7 @@ void wakeup_kswapd(struct zone *zone, gfp_t gfp_flags, int order,\n \tif (!managed_zone(zone))\n \t\treturn;\n \n-\tif (!cpuset_zone_allowed(zone, gfp_flags))\n+\tif (!zone_reclaim_allowed(zone, gfp_flags))\n \t\treturn;\n \n \tpgdat = zone->zone_pgdat;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed the concern that the OOM killer may select an undeserving victim if it doesn't know whether killing a task can actually free memory on a private node. The author introduced NP_OPS_OOM_ELIGIBLE and helpers to check if a private node is OOM-eligible, and updated constrained_alloc() to use these checks.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "The OOM killer must know whether killing a task can actually free\nmemory such that pressure is reduced.\n\nA private node only contributes to relieving pressure if it participates\nin both reclaim and demotion. Without this check, the check, the OOM\nkiller may select an undeserving victim.\n\nIntroduce NP_OPS_OOM_ELIGIBLE and helpers node_oom_eligible() and\nzone_oom_eligible().\n\nReplace cpuset_mems_allowed_intersects() in oom_cpuset_eligible()\nwith oom_mems_intersect() that iterates N_MEMORY nodes and skips\nineligible private nodes.\n\nUpdate constrained_alloc() to use zone_oom_eligible() for constraint\ndetection and node_oom_eligible() to exclude ineligible nodes from\ntotalpages accounting.\n\nRemove cpuset_mems_allowed_intersects() as it has no remaining callers.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/cpuset.h       |  9 -------\n include/linux/node_private.h |  3 +++\n kernel/cgroup/cpuset.c       | 17 ------------\n mm/oom_kill.c                | 52 ++++++++++++++++++++++++++++++++----\n 4 files changed, 50 insertions(+), 31 deletions(-)\n\ndiff --git a/include/linux/cpuset.h b/include/linux/cpuset.h\nindex 7b2f3f6b68a9..53ccfb00b277 100644\n--- a/include/linux/cpuset.h\n+++ b/include/linux/cpuset.h\n@@ -97,9 +97,6 @@ static inline bool cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)\n \treturn true;\n }\n \n-extern int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n-\t\t\t\t\t  const struct task_struct *tsk2);\n-\n #ifdef CONFIG_CPUSETS_V1\n #define cpuset_memory_pressure_bump() \t\t\t\t\\\n \tdo {\t\t\t\t\t\t\t\\\n@@ -241,12 +238,6 @@ static inline bool cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)\n \treturn true;\n }\n \n-static inline int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n-\t\t\t\t\t\t const struct task_struct *tsk2)\n-{\n-\treturn 1;\n-}\n-\n static inline void cpuset_memory_pressure_bump(void) {}\n \n static inline void cpuset_task_status_allowed(struct seq_file *m,\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 34be52383255..34d862f09e24 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -141,6 +141,9 @@ struct node_private_ops {\n /* Kernel reclaim (kswapd, direct reclaim, OOM) operates on this node */\n #define NP_OPS_RECLAIM\t\t\tBIT(4)\n \n+/* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n+#define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\n+\n /**\n  * struct node_private - Per-node container for N_MEMORY_PRIVATE nodes\n  *\ndiff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c\nindex 1a597f0c7c6c..29789d544fd5 100644\n--- a/kernel/cgroup/cpuset.c\n+++ b/kernel/cgroup/cpuset.c\n@@ -4530,23 +4530,6 @@ int cpuset_mem_spread_node(void)\n \treturn cpuset_spread_node(&current->cpuset_mem_spread_rotor);\n }\n \n-/**\n- * cpuset_mems_allowed_intersects - Does @tsk1's mems_allowed intersect @tsk2's?\n- * @tsk1: pointer to task_struct of some task.\n- * @tsk2: pointer to task_struct of some other task.\n- *\n- * Description: Return true if @tsk1's mems_allowed intersects the\n- * mems_allowed of @tsk2.  Used by the OOM killer to determine if\n- * one of the task's memory usage might impact the memory available\n- * to the other.\n- **/\n-\n-int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n-\t\t\t\t   const struct task_struct *tsk2)\n-{\n-\treturn nodes_intersects(tsk1->mems_allowed, tsk2->mems_allowed);\n-}\n-\n /**\n  * cpuset_print_current_mems_allowed - prints current's cpuset and mems_allowed\n  *\ndiff --git a/mm/oom_kill.c b/mm/oom_kill.c\nindex 5eb11fbba704..cd0d65ccd1e8 100644\n--- a/mm/oom_kill.c\n+++ b/mm/oom_kill.c\n@@ -74,7 +74,45 @@ static inline bool is_memcg_oom(struct oom_control *oc)\n \treturn oc->memcg != NULL;\n }\n \n+/* Private nodes are only eligible if they support both reclaim and demotion */\n+static inline bool node_oom_eligible(int nid)\n+{\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn true;\n+\treturn (node_private_flags(nid) & NP_OPS_OOM_ELIGIBLE) ==\n+\t\tNP_OPS_OOM_ELIGIBLE;\n+}\n+\n+static inline bool zone_oom_eligible(struct zone *zone, gfp_t gfp_mask)\n+{\n+\tif (!node_oom_eligible(zone_to_nid(zone)))\n+\t\treturn false;\n+\treturn cpuset_zone_allowed(zone, gfp_mask);\n+}\n+\n #ifdef CONFIG_NUMA\n+/*\n+ * Killing a task can only relieve system pressure if freed memory can be\n+ * demoted there and reclaim can operate on the node's pages, so we\n+ * omit private nodes that aren't eligible.\n+ */\n+static bool oom_mems_intersect(const struct task_struct *tsk1,\n+\t\t\t       const struct task_struct *tsk2)\n+{\n+\tint nid;\n+\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tif (!node_isset(nid, tsk1->mems_allowed))\n+\t\t\tcontinue;\n+\t\tif (!node_isset(nid, tsk2->mems_allowed))\n+\t\t\tcontinue;\n+\t\tif (!node_oom_eligible(nid))\n+\t\t\tcontinue;\n+\t\treturn true;\n+\t}\n+\treturn false;\n+}\n+\n /**\n  * oom_cpuset_eligible() - check task eligibility for kill\n  * @start: task struct of which task to consider\n@@ -107,9 +145,10 @@ static bool oom_cpuset_eligible(struct task_struct *start,\n \t\t} else {\n \t\t\t/*\n \t\t\t * This is not a mempolicy constrained oom, so only\n-\t\t\t * check the mems of tsk's cpuset.\n+\t\t\t * check the mems of tsk's cpuset, excluding private\n+\t\t\t * nodes that do not participate in kernel reclaim.\n \t\t\t */\n-\t\t\tret = cpuset_mems_allowed_intersects(current, tsk);\n+\t\t\tret = oom_mems_intersect(current, tsk);\n \t\t}\n \t\tif (ret)\n \t\t\tbreak;\n@@ -291,16 +330,19 @@ static enum oom_constraint constrained_alloc(struct oom_control *oc)\n \t\treturn CONSTRAINT_MEMORY_POLICY;\n \t}\n \n-\t/* Check this allocation failure is caused by cpuset's wall function */\n+\t/* Check this allocation failure is caused by cpuset or private node constraints */\n \tfor_each_zone_zonelist_nodemask(zone, z, oc->zonelist,\n \t\t\thighest_zoneidx, oc->nodemask)\n-\t\tif (!cpuset_zone_allowed(zone, oc->gfp_mask))\n+\t\tif (!zone_oom_eligible(zone, oc->gfp_mask))\n \t\t\tcpuset_limited = true;\n \n \tif (cpuset_limited) {\n \t\toc->totalpages = total_swap_pages;\n-\t\tfor_each_node_mask(nid, cpuset_current_mems_allowed)\n+\t\tfor_each_node_mask(nid, cpuset_current_mems_allowed) {\n+\t\t\tif (!node_oom_eligible(nid))\n+\t\t\t\tcontinue;\n \t\t\toc->totalpages += node_present_pages(nid);\n+\t\t}\n \t\treturn CONSTRAINT_CPUSET;\n \t}\n \treturn CONSTRAINT_NONE;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private nodes engaging in NUMA balancing faults by introducing an opt-in method (NP_OPS_NUMA_BALANCING) and adding a helper function to filter for private nodes that have opted in. The author also added code to enforce write-protection on failed or skipped migrations.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a concern",
                "added new functionality"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Not all private nodes may wish to engage in NUMA balancing faults.\n\nAdd the NP_OPS_NUMA_BALANCING flag (BIT(5)) as an opt-in method.\n\nIntroduce folio_managed_allows_numa() helper:\n   ZONE_DEVICE folios always return false (never NUMA-scanned)\n   NP_OPS_NUMA_BALANCING filters for private nodes\n\nIn do_numa_page(), if a private-node folio with NP_OPS_PROTECT_WRITE\nis still on its node after a failed/skipped migration, enforce\nwrite-protection so the next write triggers handle_fault.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |  4 ++++\n include/linux/node_private.h | 16 ++++++++++++++++\n mm/memory.c                  | 11 +++++++++++\n mm/mempolicy.c               |  5 ++++-\n 4 files changed, 35 insertions(+), 1 deletion(-)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex a4955b9b5b93..88aaac45e814 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -961,6 +961,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    (ops->flags & NP_OPS_PROTECT_WRITE))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_NUMA_BALANCING) &&\n+\t    !(ops->flags & NP_OPS_MIGRATION))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 34d862f09e24..5ac60db1f044 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -140,6 +140,8 @@ struct node_private_ops {\n #define NP_OPS_PROTECT_WRITE\t\tBIT(3)\n /* Kernel reclaim (kswapd, direct reclaim, OOM) operates on this node */\n #define NP_OPS_RECLAIM\t\t\tBIT(4)\n+/* Allow NUMA balancing to scan and migrate folios on this node */\n+#define NP_OPS_NUMA_BALANCING\t\tBIT(5)\n \n /* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n #define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\n@@ -263,6 +265,15 @@ static inline void folio_managed_split_cb(struct folio *original_folio,\n }\n \n #ifdef CONFIG_MEMORY_HOTPLUG\n+static inline bool folio_managed_allows_numa(struct folio *folio)\n+{\n+\tif (!folio_is_private_managed(folio))\n+\t\treturn true;\n+\tif (folio_is_zone_device(folio))\n+\t\treturn false;\n+\treturn folio_private_flags(folio, NP_OPS_NUMA_BALANCING);\n+}\n+\n static inline int folio_managed_allows_user_migrate(struct folio *folio)\n {\n \tif (folio_is_zone_device(folio))\n@@ -443,6 +454,11 @@ int node_private_clear_ops(int nid, const struct node_private_ops *ops);\n \n #else /* !CONFIG_NUMA || !CONFIG_MEMORY_HOTPLUG */\n \n+static inline bool folio_managed_allows_numa(struct folio *folio)\n+{\n+\treturn !folio_is_zone_device(folio);\n+}\n+\n static inline int folio_managed_allows_user_migrate(struct folio *folio)\n {\n \treturn -ENOENT;\ndiff --git a/mm/memory.c b/mm/memory.c\nindex 0f78988befef..88a581baae40 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -78,6 +78,7 @@\n #include <linux/sched/sysctl.h>\n #include <linux/pgalloc.h>\n #include <linux/uaccess.h>\n+#include <linux/node_private.h>\n \n #include <trace/events/kmem.h>\n \n@@ -6041,6 +6042,12 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \tif (!folio || folio_is_zone_device(folio))\n \t\tgoto out_map;\n \n+\t/*\n+\t * We do not need to check private-node folios here because the private\n+\t * memory service either never opted in to NUMA balancing, or it did\n+\t * and we need to restore private PTE controls on the failure path.\n+\t */\n+\n \tnid = folio_nid(folio);\n \tnr_pages = folio_nr_pages(folio);\n \n@@ -6078,6 +6085,10 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \t/*\n \t * Make it present again, depending on how arch implements\n \t * non-accessible ptes, some can allow access by kernel mode.\n+\t *\n+\t * If the folio is still on a private node with NP_OPS_PROTECT_WRITE,\n+\t * enforce write-protection so the next write triggers handle_fault.\n+\t * This covers migration-failed and migration-skipped paths.\n \t */\n \tif (unlikely(folio && folio_managed_wrprotect(folio))) {\n \t\twritable = false;\ndiff --git a/mm/mempolicy.c b/mm/mempolicy.c\nindex 8ac014950e88..8a3a9916ab59 100644\n--- a/mm/mempolicy.c\n+++ b/mm/mempolicy.c\n@@ -861,7 +861,10 @@ bool folio_can_map_prot_numa(struct folio *folio, struct vm_area_struct *vma,\n {\n \tint nid;\n \n-\tif (!folio || folio_is_zone_device(folio) || folio_test_ksm(folio))\n+\tif (!folio || folio_test_ksm(folio))\n+\t\treturn false;\n+\n+\tif (unlikely(!folio_managed_allows_numa(folio)))\n \t\treturn false;\n \n \t/* Also skip shared copy-on-write folios */\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about compaction on private nodes, explaining that it requires migration and services may have PFN-based metadata to update. They added a folio_migrate callback, zone_supports_compaction function, and filtered three direct compaction zone loops. The service is responsible for starting kcompactd on its node.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node zones should not be compacted unless the service explicitly\nopts in - as compaction requires migration and services may have\nPFN-based metadata that needs updating.\n\nAdd a folio_migrate callback which fires from migrate_folio_move() for\neach relocated folio before faults are unblocked.\n\nAdd zone_supports_compaction() which returns true for normal zones and\nchecks NP_OPS_COMPACTION for N_MEMORY_PRIVATE zones.\n\nFilter three direct compaction zone loops:\n  - compaction_zonelist_suitable() (reclaimer eligibility)\n  - try_to_compact_pages()         (direct compaction)\n  - compact_node()                 (proactive/manual compaction)\n\nkcompactd paths are intentionally unfiltered -- the service is\nresponsible for starting kcompactd on its node.\n\nNP_OPS_COMPACTION requires NP_OPS_MIGRATION.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          |  4 ++++\n include/linux/node_private.h |  2 ++\n mm/compaction.c              | 26 ++++++++++++++++++++++++++\n 3 files changed, 32 insertions(+)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex 88aaac45e814..da523aca18fa 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -965,6 +965,10 @@ int node_private_set_ops(int nid, const struct node_private_ops *ops)\n \t    !(ops->flags & NP_OPS_MIGRATION))\n \t\treturn -EINVAL;\n \n+\tif ((ops->flags & NP_OPS_COMPACTION) &&\n+\t    !(ops->flags & NP_OPS_MIGRATION))\n+\t\treturn -EINVAL;\n+\n \tmutex_lock(&node_private_lock);\n \tnp = rcu_dereference_protected(NODE_DATA(nid)->node_private,\n \t\t\t\t       lockdep_is_held(&node_private_lock));\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 5ac60db1f044..fe0336773ddb 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -142,6 +142,8 @@ struct node_private_ops {\n #define NP_OPS_RECLAIM\t\t\tBIT(4)\n /* Allow NUMA balancing to scan and migrate folios on this node */\n #define NP_OPS_NUMA_BALANCING\t\tBIT(5)\n+/* Allow compaction to run on the node.  Service must start kcompactd. */\n+#define NP_OPS_COMPACTION\t\tBIT(6)\n \n /* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n #define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 6a65145b03d8..d8532b957ec6 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,9 +24,26 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/node_private.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n+\n+/*\n+ * Private node zones require NP_OPS_COMPACTION to opt in.  Normal zones\n+ * always support compaction.\n+ */\n+static inline bool zone_supports_compaction(struct zone *zone)\n+{\n+#ifdef CONFIG_NUMA\n+\tif (!node_state(zone_to_nid(zone), N_MEMORY_PRIVATE))\n+\t\treturn true;\n+\treturn zone_private_flags(zone, NP_OPS_COMPACTION);\n+#else\n+\treturn true;\n+#endif\n+}\n+\n /*\n  * Fragmentation score check interval for proactive compaction purposes.\n  */\n@@ -2443,6 +2460,9 @@ bool compaction_zonelist_suitable(struct alloc_context *ac, int order,\n \t\t\t\tac->highest_zoneidx, ac->nodemask) {\n \t\tunsigned long available;\n \n+\t\tif (!zone_supports_compaction(zone))\n+\t\t\tcontinue;\n+\n \t\t/*\n \t\t * Do not consider all the reclaimable memory because we do not\n \t\t * want to trash just for a single high order allocation which\n@@ -2832,6 +2852,9 @@ enum compact_result try_to_compact_pages(gfp_t gfp_mask, unsigned int order,\n \t\tif (!numa_zone_alloc_allowed(alloc_flags, zone, gfp_mask))\n \t\t\tcontinue;\n \n+\t\tif (!zone_supports_compaction(zone))\n+\t\t\tcontinue;\n+\n \t\tif (prio > MIN_COMPACT_PRIORITY\n \t\t\t\t\t&& compaction_deferred(zone, order)) {\n \t\t\trc = max_t(enum compact_result, COMPACT_DEFERRED, rc);\n@@ -2906,6 +2929,9 @@ static int compact_node(pg_data_t *pgdat, bool proactive)\n \t\tif (!populated_zone(zone))\n \t\t\tcontinue;\n \n+\t\tif (!zone_supports_compaction(zone))\n+\t\t\tcontinue;\n+\n \t\tif (fatal_signal_pending(current))\n \t\t\treturn -EINTR;\n \n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private node folios being longterm-pinnable by default, explaining that this would prevent services from controlling the memory during pinning. They added an NP_OPS_LONGTERM_PIN flag for services to opt-in and modified the folio_is_longterm_pinnable() function in mm.h to check for this flag.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Private node folios should not be longterm-pinnable by default.\nA pinned folio is frozen in place, no migration, compaction, or\nreclaim, so the service loses control for the duration of the pin.\n\nSome services may depend on hot-unplugability and must disallow\nlongterm pinning.  Others (accelerators with shared CPU-device state)\nneed pinning to work.\n\nAdd NP_OPS_LONGTERM_PIN flag for services to opt in with. Hook into\nfolio_is_longterm_pinnable() in mm.h, which all GUP callers\nout-of-line helper, node_private_allows_longterm_pin(),  called\nonly for N_MEMORY_PRIVATE nodes.\n\nWithout the flag: folio_is_longterm_pinnable() returns false, migration\nfails (no __GFP_PRIVATE in GFP mask) and pin_user_pages(FOLL_LONGTERM)\nreturns -ENOMEM.\n\nWith the flag: pin succeeds and the folio stays on the private node.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/base/node.c          | 15 +++++++++++++++\n include/linux/mm.h           | 22 ++++++++++++++++++++++\n include/linux/node_private.h |  2 ++\n 3 files changed, 39 insertions(+)\n\ndiff --git a/drivers/base/node.c b/drivers/base/node.c\nindex da523aca18fa..5d2487fd54f4 100644\n--- a/drivers/base/node.c\n+++ b/drivers/base/node.c\n@@ -866,6 +866,21 @@ void register_memory_blocks_under_node_hotplug(int nid, unsigned long start_pfn,\n static DEFINE_MUTEX(node_private_lock);\n static bool node_private_initialized;\n \n+/**\n+ * node_private_allows_longterm_pin - Check if a private node allows longterm pinning\n+ * @nid: Node identifier\n+ *\n+ * Out-of-line helper for folio_is_longterm_pinnable() since mm.h cannot\n+ * include node_private.h (circular dependency).\n+ *\n+ * Returns true if the node has NP_OPS_LONGTERM_PIN set.\n+ */\n+bool node_private_allows_longterm_pin(int nid)\n+{\n+\treturn node_private_has_flag(nid, NP_OPS_LONGTERM_PIN);\n+}\n+EXPORT_SYMBOL_GPL(node_private_allows_longterm_pin);\n+\n /**\n  * node_private_register - Register a private node\n  * @nid: Node identifier\ndiff --git a/include/linux/mm.h b/include/linux/mm.h\nindex fb1819ad42c3..9088fd08aeb9 100644\n--- a/include/linux/mm.h\n+++ b/include/linux/mm.h\n@@ -2192,6 +2192,13 @@ static inline bool is_zero_folio(const struct folio *folio)\n \n /* MIGRATE_CMA and ZONE_MOVABLE do not allow pin folios */\n #ifdef CONFIG_MIGRATION\n+\n+#ifdef CONFIG_NUMA\n+bool node_private_allows_longterm_pin(int nid);\n+#else\n+static inline bool node_private_allows_longterm_pin(int nid) { return false; }\n+#endif\n+\n static inline bool folio_is_longterm_pinnable(struct folio *folio)\n {\n #ifdef CONFIG_CMA\n@@ -2215,6 +2222,21 @@ static inline bool folio_is_longterm_pinnable(struct folio *folio)\n \tif (folio_is_fsdax(folio))\n \t\treturn false;\n \n+\t/*\n+\t * Private node folios are not longterm pinnable by default.\n+\t * Services that support pinning opt in via NP_OPS_LONGTERM_PIN.\n+\t * node_private_allows_longterm_pin() is out-of-line because\n+\t * node_private.h includes mm.h (circular dependency).\n+\t *\n+\t * Guarded by CONFIG_NUMA because on !CONFIG_NUMA the single-node\n+\t * node_state() stub returns true for node 0, which would make\n+\t * all folios non-pinnable via the false-returning stub.\n+\t */\n+#ifdef CONFIG_NUMA\n+\tif (node_state(folio_nid(folio), N_MEMORY_PRIVATE))\n+\t\treturn node_private_allows_longterm_pin(folio_nid(folio));\n+#endif\n+\n \t/* Otherwise, non-movable zone folios can be pinned. */\n \treturn !folio_is_zone_movable(folio);\n \ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex fe0336773ddb..7a7438fb9eda 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -144,6 +144,8 @@ struct node_private_ops {\n #define NP_OPS_NUMA_BALANCING\t\tBIT(5)\n /* Allow compaction to run on the node.  Service must start kcompactd. */\n #define NP_OPS_COMPACTION\t\tBIT(6)\n+/* Allow longterm DMA pinning (RDMA, VFIO, etc.) of folios on this node */\n+#define NP_OPS_LONGTERM_PIN\t\tBIT(7)\n \n /* Private node is OOM-eligible: reclaim can run and pages can be demoted here */\n #define NP_OPS_OOM_ELIGIBLE\t\t(NP_OPS_RECLAIM | NP_OPS_DEMOTION)\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about notifying private-node services of hardware errors on their nodes by adding a memory_failure callback to struct node_private_ops, which will be called after TestSetPageHWPoison succeeds and before get_hwpoison_page. The kernel always proceeds with standard hwpoison handling for online pages.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged fix",
                "added callback"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a void memory_failure notification callback to struct\nnode_private_ops so services managing N_MEMORY_PRIVATE nodes notified\nwhen a page on their node experiences a hardware error.\n\nThe callback is notification only -- the kernel always proceeds with\nstandard hwpoison handling for online pages.\n\nThe notification hook fires after TestSetPageHWPoison succeeds and\nbefore get_hwpoison_page giving the service a chance to clean up.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/node_private.h |  6 ++++++\n mm/internal.h                | 16 ++++++++++++++++\n mm/memory-failure.c          | 15 +++++++++++++++\n 3 files changed, 37 insertions(+)\n\ndiff --git a/include/linux/node_private.h b/include/linux/node_private.h\nindex 7a7438fb9eda..d2669f68ac20 100644\n--- a/include/linux/node_private.h\n+++ b/include/linux/node_private.h\n@@ -113,6 +113,10 @@ struct node_reclaim_policy {\n  *   watermark_boost lifecycle (kswapd will not clear it).\n  *   If NULL, normal boost policy applies.\n  *\n+ * @memory_failure: Notification of hardware error on a page on this node.\n+ *   [folio-referenced callback]\n+ *   Notification only, kernel always handles the failure.\n+ *\n  * @flags: Operation exclusion flags (NP_OPS_* constants).\n  *\n  */\n@@ -127,6 +131,8 @@ struct node_private_ops {\n \tvm_fault_t (*handle_fault)(struct folio *folio, struct vm_fault *vmf,\n \t\t\t\t   enum pgtable_level level);\n \tvoid (*reclaim_policy)(int nid, struct node_reclaim_policy *policy);\n+\tvoid (*memory_failure)(struct folio *folio, unsigned long pfn,\n+\t\t\t       int mf_flags);\n \tunsigned long flags;\n };\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex db32cb2d7a29..64467ca774f1 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1608,6 +1608,22 @@ static inline void node_private_reclaim_policy(int nid,\n }\n #endif\n \n+static inline void folio_managed_memory_failure(struct folio *folio,\n+\t\t\t\t\t\tunsigned long pfn,\n+\t\t\t\t\t\tint mf_flags)\n+{\n+\t/* Zone device pages handle memory failure via dev_pagemap_ops */\n+\tif (folio_is_zone_device(folio))\n+\t\treturn;\n+\tif (folio_is_private_node(folio)) {\n+\t\tconst struct node_private_ops *ops =\n+\t\t\tfolio_node_private_ops(folio);\n+\n+\t\tif (ops && ops->memory_failure)\n+\t\t\tops->memory_failure(folio, pfn, mf_flags);\n+\t}\n+}\n+\n struct vm_struct *__get_vm_area_node(unsigned long size,\n \t\t\t\t     unsigned long align, unsigned long shift,\n \t\t\t\t     unsigned long vm_flags, unsigned long start,\ndiff --git a/mm/memory-failure.c b/mm/memory-failure.c\nindex c80c2907da33..79c91d44ec1e 100644\n--- a/mm/memory-failure.c\n+++ b/mm/memory-failure.c\n@@ -2379,6 +2379,15 @@ int memory_failure(unsigned long pfn, int flags)\n \t\tgoto unlock_mutex;\n \t}\n \n+\t/*\n+\t * Notify private-node services about the hardware error so they\n+\t * can update internal tracking (e.g., CXL poison lists, stop\n+\t * demoting to failing DIMMs).  This is notification only -- the\n+\t * kernel proceeds with standard hwpoison handling regardless.\n+\t */\n+\tif (unlikely(page_is_private_managed(p)))\n+\t\tfolio_managed_memory_failure(page_folio(p), pfn, flags);\n+\n \t/*\n \t * We need/can do nothing about count=0 pages.\n \t * 1) it's a free page, and therefore in safe hand:\n@@ -2825,6 +2834,12 @@ static int soft_offline_in_use_page(struct page *page)\n \t\treturn 0;\n \t}\n \n+\tif (!folio_managed_allows_migrate(folio)) {\n+\t\tpr_info(\"%#lx: cannot migrate private node folio\\n\", pfn);\n+\t\tfolio_put(folio);\n+\t\treturn -EBUSY;\n+\t}\n+\n \tisolated = isolate_folio_to_list(folio, &pagelist);\n \n \t/*\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the ordering of registering private regions and hotplugging memory, explaining that their new function combines these two steps to ensure proper ordering. The function first registers the private region, then hotplugs the memory, and on failure, unregisters the private region. They also added checks for migration support and online status when removing the last of memory.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarifying explanation",
                "no clear resolution signal"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a new function for drivers to hotplug memory as N_MEMORY_PRIVATE.\n\nThis function combines node_private_region_register() with\n__add_memory_driver_managed() to ensure proper ordering:\n\n1. Register the private region first (sets private node context)\n2. Then hotplug the memory (sets N_MEMORY_PRIVATE)\n3. On failure, unregister the private region to avoid leaving the\n   node in an inconsistent state.\n\nWhen the last of memory is removed, hotplug also removes the private\nnode context. If migration is not supported and the node is still\nonline, fire a warning (likely bug in the driver).\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/memory_hotplug.h |  11 +++\n include/linux/mmzone.h         |  12 ++++\n mm/memory_hotplug.c            | 122 ++++++++++++++++++++++++++++++---\n 3 files changed, 135 insertions(+), 10 deletions(-)\n\ndiff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h\nindex 1f19f08552ea..e5abade9450a 100644\n--- a/include/linux/memory_hotplug.h\n+++ b/include/linux/memory_hotplug.h\n@@ -293,6 +293,7 @@ extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n extern int remove_memory(u64 start, u64 size);\n extern void __remove_memory(u64 start, u64 size);\n extern int offline_and_remove_memory(u64 start, u64 size);\n+extern int offline_and_remove_private_memory(int nid, u64 start, u64 size);\n \n #else\n static inline void try_offline_node(int nid) {}\n@@ -309,6 +310,12 @@ static inline int remove_memory(u64 start, u64 size)\n }\n \n static inline void __remove_memory(u64 start, u64 size) {}\n+\n+static inline int offline_and_remove_private_memory(int nid, u64 start,\n+\t\t\t\t\t\t    u64 size)\n+{\n+\treturn -EOPNOTSUPP;\n+}\n #endif /* CONFIG_MEMORY_HOTREMOVE */\n \n #ifdef CONFIG_MEMORY_HOTPLUG\n@@ -326,6 +333,10 @@ int __add_memory_driver_managed(int nid, u64 start, u64 size,\n extern int add_memory_driver_managed(int nid, u64 start, u64 size,\n \t\t\t\t     const char *resource_name,\n \t\t\t\t     mhp_t mhp_flags);\n+int add_private_memory_driver_managed(int nid, u64 start, u64 size,\n+\t\t\t\t      const char *resource_name,\n+\t\t\t\t      mhp_t mhp_flags, enum mmop online_type,\n+\t\t\t\t      struct node_private *np);\n extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,\n \t\t\t\t   unsigned long nr_pages,\n \t\t\t\t   struct vmem_altmap *altmap, int migratetype,\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 992eb1c5a2c6..cc532b67ad3f 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -1524,6 +1524,18 @@ typedef struct pglist_data {\n #endif\n } pg_data_t;\n \n+#ifdef CONFIG_NUMA\n+static inline bool pgdat_is_private(pg_data_t *pgdat)\n+{\n+\treturn pgdat->private;\n+}\n+#else\n+static inline bool pgdat_is_private(pg_data_t *pgdat)\n+{\n+\treturn false;\n+}\n+#endif\n+\n #define node_present_pages(nid)\t(NODE_DATA(nid)->node_present_pages)\n #define node_spanned_pages(nid)\t(NODE_DATA(nid)->node_spanned_pages)\n \ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex d2dc527bd5b0..9d72f44a30dc 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/node_private.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1173,8 +1174,7 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \tmove_pfn_range_to_zone(zone, pfn, nr_pages, NULL, MIGRATE_MOVABLE,\n \t\t\t       true);\n \n-\tif (!node_state(nid, N_MEMORY)) {\n-\t\t/* Adding memory to the node for the first time */\n+\tif (!node_state(nid, N_MEMORY) && !node_state(nid, N_MEMORY_PRIVATE)) {\n \t\tnode_arg.nid = nid;\n \t\tret = node_notify(NODE_ADDING_FIRST_MEMORY, &node_arg);\n \t\tret = notifier_to_errno(ret);\n@@ -1208,8 +1208,12 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \tonline_pages_range(pfn, nr_pages);\n \tadjust_present_page_count(pfn_to_page(pfn), group, nr_pages);\n \n-\tif (node_arg.nid >= 0)\n-\t\tnode_set_state(nid, N_MEMORY);\n+\tif (node_arg.nid >= 0) {\n+\t\tif (pgdat_is_private(NODE_DATA(nid)))\n+\t\t\tnode_set_state(nid, N_MEMORY_PRIVATE);\n+\t\telse\n+\t\t\tnode_set_state(nid, N_MEMORY);\n+\t}\n \tif (need_zonelists_rebuild)\n \t\tbuild_all_zonelists(NULL);\n \n@@ -1227,8 +1231,14 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t/* reinitialise watermarks and update pcp limits */\n \tinit_per_zone_wmark_min();\n \n-\tkswapd_run(nid);\n-\tkcompactd_run(nid);\n+\t/*\n+\t * Don't start reclaim/compaction daemons for private nodes.\n+\t * Private node services will decide whether to start these services.\n+\t */\n+\tif (!pgdat_is_private(NODE_DATA(nid))) {\n+\t\tkswapd_run(nid);\n+\t\tkcompactd_run(nid);\n+\t}\n \n \tif (node_arg.nid >= 0)\n \t\t/* First memory added successfully. Notify consumers. */\n@@ -1722,6 +1732,54 @@ int add_memory_driver_managed(int nid, u64 start, u64 size,\n }\n EXPORT_SYMBOL_GPL(add_memory_driver_managed);\n \n+/**\n+ * add_private_memory_driver_managed - add driver-managed N_MEMORY_PRIVATE memory\n+ * @nid: NUMA node ID (or memory group ID when MHP_NID_IS_MGID is set)\n+ * @start: Start physical address\n+ * @size: Size in bytes\n+ * @resource_name: \"System RAM ($DRIVER)\" format\n+ * @mhp_flags: Memory hotplug flags\n+ * @online_type: MMOP_* online type\n+ * @np: Driver-owned node_private structure (owner, refcount)\n+ *\n+ * Registers node_private first, then hotplugs the memory.\n+ *\n+ * On failure, unregisters the node_private.\n+ */\n+int add_private_memory_driver_managed(int nid, u64 start, u64 size,\n+\t\t\t\t      const char *resource_name,\n+\t\t\t\t      mhp_t mhp_flags, enum mmop online_type,\n+\t\t\t\t      struct node_private *np)\n+{\n+\tstruct memory_group *group;\n+\tint real_nid = nid;\n+\tint rc;\n+\n+\tif (!np)\n+\t\treturn -EINVAL;\n+\n+\tif (mhp_flags & MHP_NID_IS_MGID) {\n+\t\tgroup = memory_group_find_by_id(nid);\n+\t\tif (!group)\n+\t\t\treturn -EINVAL;\n+\t\treal_nid = group->nid;\n+\t}\n+\n+\trc = node_private_register(real_nid, np);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\trc = __add_memory_driver_managed(nid, start, size, resource_name,\n+\t\t\t\t\t mhp_flags, online_type);\n+\tif (rc) {\n+\t\tnode_private_unregister(real_nid);\n+\t\treturn rc;\n+\t}\n+\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(add_private_memory_driver_managed);\n+\n /*\n  * Platforms should define arch_get_mappable_range() that provides\n  * maximum possible addressable physical memory range for which the\n@@ -1872,6 +1930,15 @@ static void do_migrate_range(unsigned long start_pfn, unsigned long end_pfn)\n \t\t\tgoto put_folio;\n \t\t}\n \n+\t\t/* Private nodes w/o migration must ensure folios are offline */\n+\t\tif (folio_is_private_node(folio) &&\n+\t\t    !folio_private_flags(folio, NP_OPS_MIGRATION)) {\n+\t\t\tWARN_ONCE(1, \"hot-unplug on non-migratable node %d pfn %lx\\n\",\n+\t\t\t\t  folio_nid(folio), pfn);\n+\t\t\tpfn = folio_pfn(folio) + folio_nr_pages(folio) - 1;\n+\t\t\tgoto put_folio;\n+\t\t}\n+\n \t\tif (!isolate_folio_to_list(folio, &source)) {\n \t\t\tif (__ratelimit(&migrate_rs)) {\n \t\t\t\tpr_warn(\"failed to isolate pfn %lx\\n\",\n@@ -2014,8 +2081,8 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \n \t/*\n \t * Check whether the node will have no present pages after we offline\n-\t * 'nr_pages' more. If so, we know that the node will become empty, and\n-\t * so we will clear N_MEMORY for it.\n+\t * 'nr_pages' more. If so, send pre-notification for last memory removal.\n+\t * We will clear N_MEMORY(_PRIVATE) if this is the case.\n \t */\n \tif (nr_pages >= pgdat->node_present_pages) {\n \t\tnode_arg.nid = node;\n@@ -2108,8 +2175,12 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * Make sure to mark the node as memory-less before rebuilding the zone\n \t * list. Otherwise this node would still appear in the fallback lists.\n \t */\n-\tif (node_arg.nid >= 0)\n-\t\tnode_clear_state(node, N_MEMORY);\n+\tif (node_arg.nid >= 0) {\n+\t\tif (node_state(node, N_MEMORY))\n+\t\t\tnode_clear_state(node, N_MEMORY);\n+\t\telse if (node_state(node, N_MEMORY_PRIVATE))\n+\t\t\tnode_clear_state(node, N_MEMORY_PRIVATE);\n+\t}\n \tif (!populated_zone(zone)) {\n \t\tzone_pcp_reset(zone);\n \t\tbuild_all_zonelists(NULL);\n@@ -2461,4 +2532,35 @@ int offline_and_remove_memory(u64 start, u64 size)\n \treturn rc;\n }\n EXPORT_SYMBOL_GPL(offline_and_remove_memory);\n+\n+/**\n+ * offline_and_remove_private_memory - offline, remove, and unregister private memory\n+ * @nid: NUMA node ID of the private memory\n+ * @start: Start physical address\n+ * @size: Size in bytes\n+ *\n+ * Counterpart to add_private_memory_driver_managed().  Offlines and removes\n+ * the memory range, then attempts to unregister the node_private.\n+ *\n+ * offline_and_remove_memory() clears N_MEMORY_PRIVATE when the last block\n+ * is offlined, which allows node_private_unregister() to clear the\n+ * pgdat->node_private pointer.  If other private memory ranges remain on\n+ * the node, node_private_unregister() returns -EBUSY (N_MEMORY_PRIVATE\n+ * is still set) and the node_private remains registered.\n+ *\n+ * Return: 0 on full success (memory removed and node_private unregistered),\n+ *         -EBUSY if memory was removed but node still has other private memory,\n+ *         other negative error code if offline/remove failed.\n+ */\n+int offline_and_remove_private_memory(int nid, u64 start, u64 size)\n+{\n+\tint rc;\n+\n+\trc = offline_and_remove_memory(start, size);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\treturn node_private_unregister(nid);\n+}\n+EXPORT_SYMBOL_GPL(offline_and_remove_private_memory);\n #endif /* CONFIG_MEMORY_HOTREMOVE */\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the lack of locking in the swapoff path, acknowledged that the per-vswap spinlock needs to be dropped before calling try_to_unmap(), and agreed to restructure in v2.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged fix needed",
                "agreed to restructure"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add the CRAM (Compressed RAM) subsystem that manages folios demoted\nto N_MEMORY_PRIVATE nodes via the standard kernel LRU.\n\nWe limit entry into CRAM by demotion in to provide devices a way for\ndrivers to close access - which allows the system to stabiliz under\nmemory pressure (the device can run out of real memory when compression\nratios drop too far).\n\nWe utilize write-protect to prevent unbounded writes to compressed\nmemory pages, which may cause run-away compression ratio loss without\na reliable way to prevent the degenerate case (cascading poisons).\n\nCRAM provides the bridge between the mm/ private node infrastructure\nand compressed memory hardware.  Folios are aged by kswapd on the\nprivate node and reclaimed to swap when the device signals pressure.\n\nWrite faults trigger promotion back to regular DRAM via the\nops->handle_fault callback.\n\nDevice pressure is communicated via watermark_boost on the private\nnode's zone.\n\nCRAM registers node_private_ops with:\n  - handle_fault:   promotes folio back to DRAM on write\n  - migrate_to:     custom demotion to the CRAM node\n  - folio_migrate:  (no-op)\n  - free_folio:     zeroes pages on free to scrub stale data\n  - reclaim_policy: provides mayswap/writeback/boost overrides\n  - flags: NP_OPS_MIGRATION | NP_OPS_DEMOTION |\n\t   NP_OPS_NUMA_BALANCING | NP_OPS_PROTECT_WRITE\n           NP_OPS_RECLAIM\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n include/linux/cram.h |  66 ++++++\n mm/Kconfig           |  10 +\n mm/Makefile          |   1 +\n mm/cram.c            | 508 +++++++++++++++++++++++++++++++++++++++++++\n 4 files changed, 585 insertions(+)\n create mode 100644 include/linux/cram.h\n create mode 100644 mm/cram.c\n\ndiff --git a/include/linux/cram.h b/include/linux/cram.h\nnew file mode 100644\nindex 000000000000..a3c10362fd4f\n--- /dev/null\n+++ b/include/linux/cram.h\n@@ -0,0 +1,66 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#ifndef _LINUX_CRAM_H\n+#define _LINUX_CRAM_H\n+\n+#include <linux/mm_types.h>\n+\n+struct folio;\n+struct list_head;\n+struct vm_fault;\n+\n+#define CRAM_PRESSURE_MAX\t1000\n+\n+/**\n+ * cram_flush_cb_t - Driver callback invoked when a folio on a private node\n+ *                   is freed (refcount reaches zero).\n+ * @folio: the folio being freed\n+ * @private: opaque driver data passed at registration\n+ *\n+ * Return:\n+ *   0: Flush resolved -- page should return to buddy allocator (e.g., flush\n+ *      record bit was set, meaning this free is from our own flush resolution)\n+ *   1: Page deferred -- driver took a reference, page will be flushed later.\n+ *      Do NOT return to buddy allocator.\n+ *   2: Buffer full -- caller should zero the page and return to buddy.\n+ */\n+typedef int (*cram_flush_cb_t)(struct folio *folio, void *private);\n+\n+#ifdef CONFIG_CRAM\n+\n+int cram_register_private_node(int nid, void *owner,\n+\t\t\t       cram_flush_cb_t flush_cb, void *flush_data);\n+int cram_unregister_private_node(int nid);\n+int cram_unpurge(int nid);\n+void cram_set_pressure(int nid, unsigned int pressure);\n+void cram_clear_pressure(int nid);\n+\n+#else /* !CONFIG_CRAM */\n+\n+static inline int cram_register_private_node(int nid, void *owner,\n+\t\t\t\t\t     cram_flush_cb_t flush_cb,\n+\t\t\t\t\t     void *flush_data)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int cram_unregister_private_node(int nid)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline int cram_unpurge(int nid)\n+{\n+\treturn -ENODEV;\n+}\n+\n+static inline void cram_set_pressure(int nid, unsigned int pressure)\n+{\n+}\n+\n+static inline void cram_clear_pressure(int nid)\n+{\n+}\n+\n+#endif /* CONFIG_CRAM */\n+\n+#endif /* _LINUX_CRAM_H */\ndiff --git a/mm/Kconfig b/mm/Kconfig\nindex bd0ea5454af8..054462b954d8 100644\n--- a/mm/Kconfig\n+++ b/mm/Kconfig\n@@ -662,6 +662,16 @@ config MIGRATION\n config DEVICE_MIGRATION\n \tdef_bool MIGRATION && ZONE_DEVICE\n \n+config CRAM\n+\tbool \"Compressed RAM - private node memory management\"\n+\tdepends on NUMA\n+\tdepends on MIGRATION\n+\tdepends on MEMORY_HOTPLUG\n+\thelp\n+\t  Enables management of N_MEMORY_PRIVATE nodes for compressed RAM\n+\t  and similar use cases. Provides demotion, promotion, and lifecycle\n+\t  management for private memory nodes.\n+\n config ARCH_ENABLE_HUGEPAGE_MIGRATION\n \tbool\n \ndiff --git a/mm/Makefile b/mm/Makefile\nindex 2d0570a16e5b..0e1421512643 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -98,6 +98,7 @@ obj-$(CONFIG_MEMTEST)\t\t+= memtest.o\n obj-$(CONFIG_MIGRATION) += migrate.o\n obj-$(CONFIG_NUMA) += memory-tiers.o\n obj-$(CONFIG_DEVICE_MIGRATION) += migrate_device.o\n+obj-$(CONFIG_CRAM) += cram.o\n obj-$(CONFIG_TRANSPARENT_HUGEPAGE) += huge_memory.o khugepaged.o\n obj-$(CONFIG_PAGE_COUNTER) += page_counter.o\n obj-$(CONFIG_LIVEUPDATE) += memfd_luo.o\ndiff --git a/mm/cram.c b/mm/cram.c\nnew file mode 100644\nindex 000000000000..6709e61f5b9d\n--- /dev/null\n+++ b/mm/cram.c\n@@ -0,0 +1,508 @@\n+// SPDX-License-Identifier: GPL-2.0\n+/*\n+ * mm/cram.c - Compressed RAM / private node memory management\n+ *\n+ * Copyright 2026 Meta Technologies Inc.\n+ *   Author: Gregory Price <gourry@gourry.net>\n+ *\n+ * Manages folios demoted to N_MEMORY_PRIVATE nodes via the standard kernel\n+ * LRU.  Folios are aged by kswapd on the private node and reclaimed to swap\n+ * (demotion is suppressed for private nodes).  Write faults trigger promotion\n+ * back to regular DRAM via the ops->handle_fault callback.\n+ *\n+ * All reclaim/demotion uses the standard vmscan infrastructure. Device pressure\n+ * is communicated via watermark_boost on the private node's zone.\n+ */\n+\n+#include <linux/atomic.h>\n+#include <linux/cpuset.h>\n+#include <linux/cram.h>\n+#include <linux/errno.h>\n+#include <linux/gfp.h>\n+#include <linux/jiffies.h>\n+#include <linux/highmem.h>\n+#include <linux/memory-tiers.h>\n+#include <linux/list.h>\n+#include <linux/migrate.h>\n+#include <linux/mm.h>\n+#include <linux/huge_mm.h>\n+#include <linux/mmzone.h>\n+#include <linux/mutex.h>\n+#include <linux/nodemask.h>\n+#include <linux/node_private.h>\n+#include <linux/pagemap.h>\n+#include <linux/rcupdate.h>\n+#include <linux/refcount.h>\n+#include <linux/swap.h>\n+\n+#include \"internal.h\"\n+\n+struct cram_node {\n+\tvoid\t\t*owner;\n+\tbool\t\tpurged;\t\t/* node is being torn down */\n+\tunsigned int\tpressure;\n+\trefcount_t\trefcount;\n+\tcram_flush_cb_t\tflush_cb;\t/* optional driver flush callback */\n+\tvoid\t\t*flush_data;\t/* opaque data for flush_cb */\n+};\n+\n+static struct cram_node *cram_nodes[MAX_NUMNODES];\n+static DEFINE_MUTEX(cram_mutex);\n+\n+static inline bool cram_valid_nid(int nid)\n+{\n+\treturn nid >= 0 && nid < MAX_NUMNODES;\n+}\n+\n+static inline struct cram_node *get_cram_node(int nid)\n+{\n+\tstruct cram_node *cn;\n+\n+\tif (!cram_valid_nid(nid))\n+\t\treturn NULL;\n+\n+\trcu_read_lock();\n+\tcn = rcu_dereference(cram_nodes[nid]);\n+\tif (cn && !refcount_inc_not_zero(&cn->refcount))\n+\t\tcn = NULL;\n+\trcu_read_unlock();\n+\n+\treturn cn;\n+}\n+\n+static inline void put_cram_node(struct cram_node *cn)\n+{\n+\tif (cn)\n+\t\trefcount_dec(&cn->refcount);\n+}\n+\n+static void cram_zero_folio(struct folio *folio)\n+{\n+\tunsigned int i, nr = folio_nr_pages(folio);\n+\n+\tif (want_init_on_free())\n+\t\treturn;\n+\n+\tfor (i = 0; i < nr; i++)\n+\t\tclear_highpage(folio_page(folio, i));\n+}\n+\n+static bool cram_free_folio_cb(struct folio *folio)\n+{\n+\tint nid = folio_nid(folio);\n+\tstruct cram_node *cn;\n+\tint ret;\n+\n+\tcn = get_cram_node(nid);\n+\tif (!cn)\n+\t\tgoto zero_and_free;\n+\n+\tif (!cn->flush_cb)\n+\t\tgoto zero_and_free_put;\n+\n+\tret = cn->flush_cb(folio, cn->flush_data);\n+\tput_cram_node(cn);\n+\n+\tswitch (ret) {\n+\tcase 0:\n+\t\t/* Flush resolved: return to buddy (already zeroed by device) */\n+\t\treturn false;\n+\tcase 1:\n+\t\t/* Deferred: driver holds a ref, do not free to buddy */\n+\t\treturn true;\n+\tcase 2:\n+\tdefault:\n+\t\t/* Buffer full or unknown: zero locally, return to buddy */\n+\t\tgoto zero_and_free;\n+\t}\n+\n+zero_and_free_put:\n+\tput_cram_node(cn);\n+zero_and_free:\n+\tcram_zero_folio(folio);\n+\treturn false;\n+}\n+\n+static struct folio *alloc_cram_folio(struct folio *src, unsigned long private)\n+{\n+\tint nid = (int)private;\n+\tunsigned int order = folio_order(src);\n+\tgfp_t gfp = GFP_PRIVATE | __GFP_KSWAPD_RECLAIM |\n+\t\t     __GFP_HIGHMEM | __GFP_MOVABLE |\n+\t\t     __GFP_NOWARN | __GFP_NORETRY;\n+\n+\t/* Stop allocating if backpressure fired mid-batch */\n+\tif (node_private_migration_blocked(nid))\n+\t\treturn NULL;\n+\n+\tif (order)\n+\t\tgfp |= __GFP_COMP;\n+\n+\treturn __folio_alloc_node(gfp, order, nid);\n+}\n+\n+static void cram_put_new_folio(struct folio *folio, unsigned long private)\n+{\n+\tcram_zero_folio(folio);\n+\tfolio_put(folio);\n+}\n+\n+/*\n+ * Allocate a DRAM folio for promotion out of a private node.\n+ *\n+ * Unlike alloc_migration_target(), this does NOT strip __GFP_RECLAIM for\n+ * large folios, the generic helper does that because THP allocations are\n+ * opportunistic, but promotion from a private node is mandatory: the page\n+ * MUST move to DRAM or the process cannot make forward progress.\n+ *\n+ * __GFP_RETRY_MAYFAIL tells the allocator to try hard (multiple reclaim\n+ * rounds, wait for writeback) before giving up.\n+ */\n+static struct folio *alloc_cram_promote_folio(struct folio *src,\n+\t\t\t\t\t      unsigned long private)\n+{\n+\tint nid = (int)private;\n+\tunsigned int order = folio_order(src);\n+\tgfp_t gfp = GFP_HIGHUSER_MOVABLE | __GFP_RETRY_MAYFAIL;\n+\n+\tif (order)\n+\t\tgfp |= __GFP_COMP;\n+\n+\treturn __folio_alloc(gfp, order, nid, NULL);\n+}\n+\n+static int cram_migrate_to(struct list_head *demote_folios, int to_nid,\n+\t\t\t   enum migrate_mode mode,\n+\t\t\t   enum migrate_reason reason,\n+\t\t\t   unsigned int *nr_succeeded)\n+{\n+\tstruct cram_node *cn;\n+\tunsigned int nr_success = 0;\n+\tint ret = 0;\n+\n+\tcn = get_cram_node(to_nid);\n+\tif (!cn)\n+\t\treturn -ENODEV;\n+\n+\tif (cn->purged) {\n+\t\tret = -ENODEV;\n+\t\tgoto out;\n+\t}\n+\n+\t/* Block new demotions at maximum pressure */\n+\tif (READ_ONCE(cn->pressure) >= CRAM_PRESSURE_MAX) {\n+\t\tret = -ENOSPC;\n+\t\tgoto out;\n+\t}\n+\n+\tret = migrate_pages(demote_folios, alloc_cram_folio, cram_put_new_folio,\n+\t\t\t    (unsigned long)to_nid, mode, reason,\n+\t\t\t    &nr_success);\n+\n+\t/*\n+\t * migrate_folio_move() calls folio_add_lru() for each migrated\n+\t * folio, but that only adds the folio to a per-CPU batch, \n+\t * PG_lru is not set until the batch is drained.  Drain now so\n+\t * that cram_fault() can isolate these folios immediately.\n+\t *\n+\t * Use lru_add_drain_all() because migrate_pages() may process\n+\t * folios across CPUs, and the local drain might miss batches\n+\t * filled on other CPUs.\n+\t */\n+\tif (nr_success)\n+\t\tlru_add_drain_all();\n+out:\n+\tput_cram_node(cn);\n+\tif (nr_succeeded)\n+\t\t*nr_succeeded = nr_success;\n+\treturn ret;\n+}\n+\n+static void cram_release_ptl(struct vm_fault *vmf, enum pgtable_level level)\n+{\n+\tif (level == PGTABLE_LEVEL_PTE)\n+\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n+\telse\n+\t\tspin_unlock(vmf->ptl);\n+}\n+\n+static vm_fault_t cram_fault(struct folio *folio, struct vm_fault *vmf,\n+\t\t\t     enum pgtable_level level)\n+{\n+\tstruct folio *f, *f2;\n+\tstruct cram_node *cn;\n+\tunsigned int nr_succeeded = 0;\n+\tint nid;\n+\tLIST_HEAD(folios);\n+\n+\tnid = folio_nid(folio);\n+\n+\tcn = get_cram_node(nid);\n+\tif (!cn) {\n+\t\tcram_release_ptl(vmf, level);\n+\t\treturn 0;\n+\t}\n+\n+\t/*\n+\t * Isolate from LRU while holding PTL.  This serializes against\n+\t * other CPUs faulting on the same folio: only one CPU can clear\n+\t * PG_lru under the PTL, and it proceeds to migration.  Other\n+\t * CPUs find the folio already isolated and bail out, preventing\n+\t * the refcount pile-up that causes migrate_pages() to fail with\n+\t * -EAGAIN.\n+\t *\n+\t * No explicit folio_get() is needed: the page table entry holds\n+\t * a reference (we still hold PTL), and folio_isolate_lru() takes\n+\t * its own reference.  This matches do_numa_page()'s pattern.\n+\t *\n+\t * PG_lru should already be set: cram_migrate_to() drains per-CPU\n+\t * LRU batches after migration, and the failure path below\n+\t * drains after putback.\n+\t */\n+\tif (!folio_isolate_lru(folio)) {\n+\t\tput_cram_node(cn);\n+\t\tcram_release_ptl(vmf, level);\n+\t\tcond_resched();\n+\t\treturn 0;\n+\t}\n+\n+\t/* Folio isolated, release PTL, proceed to migration */\n+\tcram_release_ptl(vmf, level);\n+\n+\tnode_stat_mod_folio(folio,\n+\t\t\t    NR_ISOLATED_ANON + folio_is_file_lru(folio),\n+\t\t\t    folio_nr_pages(folio));\n+\tlist_add(&folio->lru, &folios);\n+\n+\tmigrate_pages(&folios, alloc_cram_promote_folio, NULL,\n+\t\t      (unsigned long)numa_node_id(),\n+\t\t      MIGRATE_SYNC, MR_NUMA_MISPLACED, &nr_succeeded);\n+\n+\t/* Put failed folios back on LRU; retry on next fault */\n+\tlist_for_each_entry_safe(f, f2, &folios, lru) {\n+\t\tlist_del(&f->lru);\n+\t\tnode_stat_mod_folio(f,\n+\t\t\t\t    NR_ISOLATED_ANON + folio_is_file_lru(f),\n+\t\t\t\t    -folio_nr_pages(f));\n+\t\tfolio_putback_lru(f);\n+\t}\n+\n+\t/*\n+\t * If migration failed, folio_putback_lru() batched the folio\n+\t * into this CPU's per-CPU LRU cache (PG_lru not yet set).\n+\t * Drain now so the folio is immediately visible on the LRU,\n+\t * the next fault can then isolate it without an IPI storm\n+\t * via lru_add_drain_all().\n+\t *\n+\t * Return VM_FAULT_RETRY after releasing the fault lock so the\n+\t * arch handler retries from scratch.  Without this, returning 0\n+\t * causes a tight livelock: the process immediately re-faults on\n+\t * the same write-protected entry, alloc fails again, and\n+\t * VM_FAULT_OOM eventually leaks out through a stale path.\n+\t * VM_FAULT_RETRY gives the system breathing room to reclaim.\n+\t */\n+\tif (!nr_succeeded) {\n+\t\tlru_add_drain();\n+\t\tcond_resched();\n+\t\tput_cram_node(cn);\n+\t\trelease_fault_lock(vmf);\n+\t\treturn VM_FAULT_RETRY;\n+\t}\n+\n+\tcond_resched();\n+\tput_cram_node(cn);\n+\treturn 0;\n+}\n+\n+static void cram_folio_migrate(struct folio *src, struct folio *dst)\n+{\n+}\n+\n+static void cram_reclaim_policy(int nid, struct node_reclaim_policy *policy)\n+{\n+\tpolicy->may_swap = true;\n+\tpolicy->may_writepage = true;\n+\tpolicy->managed_watermarks = true;\n+}\n+\n+static vm_fault_t cram_handle_fault(struct folio *folio, struct vm_fault *vmf,\n+\t\t\t\t    enum pgtable_level level)\n+{\n+\treturn cram_fault(folio, vmf, level);\n+}\n+\n+static const struct node_private_ops cram_ops = {\n+\t.handle_fault\t\t= cram_handle_fault,\n+\t.migrate_to\t\t= cram_migrate_to,\n+\t.folio_migrate\t\t= cram_folio_migrate,\n+\t.free_folio\t\t= cram_free_folio_cb,\n+\t.reclaim_policy\t\t= cram_reclaim_policy,\n+\t.flags\t\t\t= NP_OPS_MIGRATION | NP_OPS_DEMOTION |\n+\t\t\t\t  NP_OPS_NUMA_BALANCING | NP_OPS_PROTECT_WRITE |\n+\t\t\t\t  NP_OPS_RECLAIM,\n+};\n+\n+int cram_register_private_node(int nid, void *owner,\n+\t\t\t       cram_flush_cb_t flush_cb, void *flush_data)\n+{\n+\tstruct cram_node *cn;\n+\tint ret;\n+\n+\tif (!node_state(nid, N_MEMORY_PRIVATE))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&cram_mutex);\n+\n+\tcn = cram_nodes[nid];\n+\tif (cn) {\n+\t\tif (cn->owner != owner) {\n+\t\t\tmutex_unlock(&cram_mutex);\n+\t\t\treturn -EBUSY;\n+\t\t}\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn 0;\n+\t}\n+\n+\tcn = kzalloc(sizeof(*cn), GFP_KERNEL);\n+\tif (!cn) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn -ENOMEM;\n+\t}\n+\n+\tcn->owner = owner;\n+\tcn->pressure = 0;\n+\tcn->flush_cb = flush_cb;\n+\tcn->flush_data = flush_data;\n+\trefcount_set(&cn->refcount, 1);\n+\n+\tret = node_private_set_ops(nid, &cram_ops);\n+\tif (ret) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\tkfree(cn);\n+\t\treturn ret;\n+\t}\n+\n+\trcu_assign_pointer(cram_nodes[nid], cn);\n+\n+\t/* Start kswapd on the private node for LRU aging and reclaim */\n+\tkswapd_run(nid);\n+\n+\tmutex_unlock(&cram_mutex);\n+\n+\t/* Now that ops->migrate_to is set, refresh demotion targets */\n+\tmemory_tier_refresh_demotion();\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(cram_register_private_node);\n+\n+int cram_unregister_private_node(int nid)\n+{\n+\tstruct cram_node *cn;\n+\n+\tif (!cram_valid_nid(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&cram_mutex);\n+\n+\tcn = cram_nodes[nid];\n+\tif (!cn) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn -ENODEV;\n+\t}\n+\n+\tkswapd_stop(nid);\n+\n+\tWARN_ON(node_private_clear_ops(nid, &cram_ops));\n+\trcu_assign_pointer(cram_nodes[nid], NULL);\n+\tmutex_unlock(&cram_mutex);\n+\n+\t/* ops->migrate_to cleared, refresh demotion targets */\n+\tmemory_tier_refresh_demotion();\n+\n+\tsynchronize_rcu();\n+\twhile (!refcount_dec_if_one(&cn->refcount))\n+\t\tcond_resched();\n+\tkfree(cn);\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(cram_unregister_private_node);\n+\n+int cram_unpurge(int nid)\n+{\n+\tstruct cram_node *cn;\n+\n+\tif (!cram_valid_nid(nid))\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&cram_mutex);\n+\n+\tcn = cram_nodes[nid];\n+\tif (!cn) {\n+\t\tmutex_unlock(&cram_mutex);\n+\t\treturn -ENODEV;\n+\t}\n+\n+\tcn->purged = false;\n+\n+\tmutex_unlock(&cram_mutex);\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_GPL(cram_unpurge);\n+\n+void cram_set_pressure(int nid, unsigned int pressure)\n+{\n+\tstruct cram_node *cn;\n+\tstruct node_private *np;\n+\tstruct zone *zone;\n+\tunsigned long managed, boost;\n+\n+\tcn = get_cram_node(nid);\n+\tif (!cn)\n+\t\treturn;\n+\n+\tif (pressure > CRAM_PRESSURE_MAX)\n+\t\tpressure = CRAM_PRESSURE_MAX;\n+\n+\tWRITE_ONCE(cn->pressure, pressure);\n+\n+\trcu_read_lock();\n+\tnp = rcu_dereference(NODE_DATA(nid)->node_private);\n+\t/* Block demotions only at maximum pressure */\n+\tif (np)\n+\t\tWRITE_ONCE(np->migration_blocked,\n+\t\t\t   pressure >= CRAM_PRESSURE_MAX);\n+\trcu_read_unlock();\n+\n+\tzone = NULL;\n+\tfor (int i = 0; i < MAX_NR_ZONES; i++) {\n+\t\tstruct zone *z = &NODE_DATA(nid)->node_zones[i];\n+\n+\t\tif (zone_managed_pages(z) > 0) {\n+\t\t\tzone = z;\n+\t\t\tbreak;\n+\t\t}\n+\t}\n+\tif (!zone) {\n+\t\tput_cram_node(cn);\n+\t\treturn;\n+\t}\n+\tmanaged = zone_managed_pages(zone);\n+\n+\t/* Boost proportional to pressure. 0:no boost, 1000:full managed */\n+\tboost = (managed * (unsigned long)pressure) / CRAM_PRESSURE_MAX;\n+\tWRITE_ONCE(zone->watermark_boost, boost);\n+\n+\tif (boost) {\n+\t\tset_bit(ZONE_BOOSTED_WATERMARK, &zone->flags);\n+\t\twakeup_kswapd(zone, GFP_KERNEL, 0, ZONE_MOVABLE);\n+\t}\n+\n+\tput_cram_node(cn);\n+}\n+EXPORT_SYMBOL_GPL(cram_set_pressure);\n+\n+void cram_clear_pressure(int nid)\n+{\n+\tcram_set_pressure(nid, 0);\n+}\n+EXPORT_SYMBOL_GPL(cram_clear_pressure);\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author is addressing a concern about the need for a sysram region to directly perform memory hotplug operations, which would eliminate the intermediate dax_region/dax device layer. The author agrees that this feature is necessary and explains how it will work, including its key features such as supporting memory tier integration and automatically hotplugging memory on probe if online type is configured.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "agreed to implement a new feature",
                "explained the benefits of the feature"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add the CXL sysram region for direct memory hotplug of CXL RAM regions.\n\nThis region eliminates the intermediate dax_region/dax device layer by\ndirectly performing memory hotplug operations.\n\nKey features:\n- Supports memory tier integration for proper NUMA placement\n- Uses the CXL_SYSRAM_ONLINE_* Kconfig options for default online type\n- Automatically hotplugs memory on probe if online type is configured\n- Will be extended to support private memory nodes in the future\n\nThe driver registers a sysram_regionN device as a child of the CXL\nregion, managing the memory hotplug lifecycle through device add/remove.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/Makefile        |   1 +\n drivers/cxl/core/core.h          |   4 +\n drivers/cxl/core/port.c          |   2 +\n drivers/cxl/core/region_sysram.c | 351 +++++++++++++++++++++++++++++++\n drivers/cxl/cxl.h                |  48 +++++\n 5 files changed, 406 insertions(+)\n create mode 100644 drivers/cxl/core/region_sysram.c\n\ndiff --git a/drivers/cxl/core/Makefile b/drivers/cxl/core/Makefile\nindex d3ec8aea64c5..d7ce52c50810 100644\n--- a/drivers/cxl/core/Makefile\n+++ b/drivers/cxl/core/Makefile\n@@ -18,6 +18,7 @@ cxl_core-$(CONFIG_TRACING) += trace.o\n cxl_core-$(CONFIG_CXL_REGION) += region.o\n cxl_core-$(CONFIG_CXL_REGION) += region_dax.o\n cxl_core-$(CONFIG_CXL_REGION) += region_pmem.o\n+cxl_core-$(CONFIG_CXL_REGION) += region_sysram.o\n cxl_core-$(CONFIG_CXL_MCE) += mce.o\n cxl_core-$(CONFIG_CXL_FEATURES) += features.o\n cxl_core-$(CONFIG_CXL_EDAC_MEM_FEATURES) += edac.o\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex 6e1f695fd155..973bbcae43f7 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -35,6 +35,7 @@ extern struct device_attribute dev_attr_delete_region;\n extern struct device_attribute dev_attr_region;\n extern const struct device_type cxl_pmem_region_type;\n extern const struct device_type cxl_dax_region_type;\n+extern const struct device_type cxl_sysram_type;\n extern const struct device_type cxl_region_type;\n \n int cxl_decoder_detach(struct cxl_region *cxlr,\n@@ -46,6 +47,7 @@ int cxl_decoder_detach(struct cxl_region *cxlr,\n #define SET_CXL_REGION_ATTR(x) (&dev_attr_##x.attr),\n #define CXL_PMEM_REGION_TYPE(x) (&cxl_pmem_region_type)\n #define CXL_DAX_REGION_TYPE(x) (&cxl_dax_region_type)\n+#define CXL_SYSRAM_TYPE(x) (&cxl_sysram_type)\n int cxl_region_init(void);\n void cxl_region_exit(void);\n int cxl_get_poison_by_endpoint(struct cxl_port *port);\n@@ -54,6 +56,7 @@ u64 cxl_dpa_to_hpa(struct cxl_region *cxlr, const struct cxl_memdev *cxlmd,\n \t\t   u64 dpa);\n int devm_cxl_add_dax_region(struct cxl_region *cxlr, enum dax_driver_type);\n int devm_cxl_add_pmem_region(struct cxl_region *cxlr);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n \n #else\n static inline u64 cxl_dpa_to_hpa(struct cxl_region *cxlr,\n@@ -88,6 +91,7 @@ static inline void cxl_region_exit(void)\n #define SET_CXL_REGION_ATTR(x)\n #define CXL_PMEM_REGION_TYPE(x) NULL\n #define CXL_DAX_REGION_TYPE(x) NULL\n+#define CXL_SYSRAM_TYPE(x) NULL\n #endif\n \n struct cxl_send_command;\ndiff --git a/drivers/cxl/core/port.c b/drivers/cxl/core/port.c\nindex 5c82e6f32572..d6e82b3c2b64 100644\n--- a/drivers/cxl/core/port.c\n+++ b/drivers/cxl/core/port.c\n@@ -66,6 +66,8 @@ static int cxl_device_id(const struct device *dev)\n \t\treturn CXL_DEVICE_PMEM_REGION;\n \tif (dev->type == CXL_DAX_REGION_TYPE())\n \t\treturn CXL_DEVICE_DAX_REGION;\n+\tif (dev->type == CXL_SYSRAM_TYPE())\n+\t\treturn CXL_DEVICE_SYSRAM;\n \tif (is_cxl_port(dev)) {\n \t\tif (is_cxl_root(to_cxl_port(dev)))\n \t\t\treturn CXL_DEVICE_ROOT;\ndiff --git a/drivers/cxl/core/region_sysram.c b/drivers/cxl/core/region_sysram.c\nnew file mode 100644\nindex 000000000000..47a415deb352\n--- /dev/null\n+++ b/drivers/cxl/core/region_sysram.c\n@@ -0,0 +1,351 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2026 Meta Platforms, Inc. All rights reserved. */\n+/*\n+ * CXL Sysram Region - Direct memory hotplug for CXL RAM regions\n+ *\n+ * This interface directly performs memory hotplug for CXL RAM regions,\n+ * eliminating the indirection through DAX.\n+ */\n+\n+#include <linux/memory_hotplug.h>\n+#include <linux/memory-tiers.h>\n+#include <linux/memory.h>\n+#include <linux/device.h>\n+#include <linux/slab.h>\n+#include <linux/mm.h>\n+#include <cxlmem.h>\n+#include <cxl.h>\n+#include \"core.h\"\n+\n+static const char *sysram_res_name = \"System RAM (CXL)\";\n+\n+/**\n+ * cxl_region_find_sysram - Find the sysram device associated with a region\n+ * @cxlr: The CXL region\n+ *\n+ * Finds and returns the sysram child device of a CXL region.\n+ * The caller must release the device reference with put_device()\n+ * when done with the returned pointer.\n+ *\n+ * Return: Pointer to cxl_sysram, or NULL if not found\n+ */\n+struct cxl_sysram *cxl_region_find_sysram(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_sysram *sysram;\n+\tstruct device *sdev;\n+\tchar sname[32];\n+\n+\tsnprintf(sname, sizeof(sname), \"sysram_region%d\", cxlr->id);\n+\tsdev = device_find_child_by_name(&cxlr->dev, sname);\n+\tif (!sdev)\n+\t\treturn NULL;\n+\n+\tsysram = to_cxl_sysram(sdev);\n+\treturn sysram;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_region_find_sysram, \"CXL\");\n+\n+static int sysram_get_numa_node(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_region_params *p = &cxlr->params;\n+\tint nid;\n+\n+\tnid = phys_to_target_node(p->res->start);\n+\tif (nid == NUMA_NO_NODE)\n+\t\tnid = memory_add_physaddr_to_nid(p->res->start);\n+\n+\treturn nid;\n+}\n+\n+static int sysram_hotplug_add(struct cxl_sysram *sysram, enum mmop online_type)\n+{\n+\tstruct resource *res;\n+\tmhp_t mhp_flags;\n+\tint rc;\n+\n+\tif (sysram->res)\n+\t\treturn -EBUSY;\n+\n+\tres = request_mem_region(sysram->hpa_range.start,\n+\t\t\t\t range_len(&sysram->hpa_range),\n+\t\t\t\t sysram->res_name);\n+\tif (!res)\n+\t\treturn -EBUSY;\n+\n+\tsysram->res = res;\n+\n+\t/*\n+\t * Set flags appropriate for System RAM. Leave ..._BUSY clear\n+\t * so that add_memory() can add a child resource.\n+\t */\n+\tres->flags = IORESOURCE_SYSTEM_RAM;\n+\n+\tmhp_flags = MHP_NID_IS_MGID;\n+\n+\t/*\n+\t * Ensure that future kexec'd kernels will not treat\n+\t * this as RAM automatically.\n+\t */\n+\trc = __add_memory_driver_managed(sysram->mgid,\n+\t\t\t\t\t sysram->hpa_range.start,\n+\t\t\t\t\t range_len(&sysram->hpa_range),\n+\t\t\t\t\t sysram_res_name, mhp_flags,\n+\t\t\t\t\t online_type);\n+\tif (rc) {\n+\t\tremove_resource(res);\n+\t\tkfree(res);\n+\t\tsysram->res = NULL;\n+\t\treturn rc;\n+\t}\n+\n+\treturn 0;\n+}\n+\n+static int sysram_hotplug_remove(struct cxl_sysram *sysram)\n+{\n+\tint rc;\n+\n+\tif (!sysram->res)\n+\t\treturn 0;\n+\n+\trc = offline_and_remove_memory(sysram->hpa_range.start,\n+\t\t\t\t       range_len(&sysram->hpa_range));\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tif (sysram->res) {\n+\t\tremove_resource(sysram->res);\n+\t\tkfree(sysram->res);\n+\t\tsysram->res = NULL;\n+\t}\n+\n+\treturn 0;\n+}\n+\n+int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram)\n+{\n+\treturn sysram_hotplug_remove(sysram);\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_sysram_offline_and_remove, \"CXL\");\n+\n+static void cxl_sysram_release(struct device *dev)\n+{\n+\tstruct cxl_sysram *sysram = to_cxl_sysram(dev);\n+\n+\tif (sysram->res)\n+\t\tsysram_hotplug_remove(sysram);\n+\n+\tkfree(sysram->res_name);\n+\n+\tif (sysram->mgid >= 0)\n+\t\tmemory_group_unregister(sysram->mgid);\n+\n+\tif (sysram->mtype)\n+\t\tclear_node_memory_type(sysram->numa_node, sysram->mtype);\n+\n+\tkfree(sysram);\n+}\n+\n+static ssize_t hotplug_store(struct device *dev,\n+\t\t\t     struct device_attribute *attr,\n+\t\t\t     const char *buf, size_t len)\n+{\n+\tstruct cxl_sysram *sysram = to_cxl_sysram(dev);\n+\tint online_type, rc;\n+\n+\tonline_type = mhp_online_type_from_str(buf);\n+\tif (online_type < 0)\n+\t\treturn online_type;\n+\n+\tif (online_type == MMOP_OFFLINE)\n+\t\trc = sysram_hotplug_remove(sysram);\n+\telse\n+\t\trc = sysram_hotplug_add(sysram, online_type);\n+\n+\tif (rc)\n+\t\tdev_warn(dev, \"hotplug %s failed: %d\\n\",\n+\t\t\t online_type == MMOP_OFFLINE ? \"offline\" : \"online\", rc);\n+\n+\treturn rc ? rc : len;\n+}\n+static DEVICE_ATTR_WO(hotplug);\n+\n+static struct attribute *cxl_sysram_attrs[] = {\n+\t&dev_attr_hotplug.attr,\n+\tNULL\n+};\n+\n+static const struct attribute_group cxl_sysram_attribute_group = {\n+\t.attrs = cxl_sysram_attrs,\n+};\n+\n+static const struct attribute_group *cxl_sysram_attribute_groups[] = {\n+\t&cxl_base_attribute_group,\n+\t&cxl_sysram_attribute_group,\n+\tNULL\n+};\n+\n+const struct device_type cxl_sysram_type = {\n+\t.name = \"cxl_sysram\",\n+\t.release = cxl_sysram_release,\n+\t.groups = cxl_sysram_attribute_groups,\n+};\n+\n+static bool is_cxl_sysram(struct device *dev)\n+{\n+\treturn dev->type == &cxl_sysram_type;\n+}\n+\n+struct cxl_sysram *to_cxl_sysram(struct device *dev)\n+{\n+\tif (dev_WARN_ONCE(dev, !is_cxl_sysram(dev),\n+\t\t\t  \"not a cxl_sysram device\\n\"))\n+\t\treturn NULL;\n+\treturn container_of(dev, struct cxl_sysram, dev);\n+}\n+EXPORT_SYMBOL_NS_GPL(to_cxl_sysram, \"CXL\");\n+\n+struct device *cxl_sysram_dev(struct cxl_sysram *sysram)\n+{\n+\treturn &sysram->dev;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_sysram_dev, \"CXL\");\n+\n+static struct lock_class_key cxl_sysram_key;\n+\n+static enum mmop cxl_sysram_get_default_online_type(void)\n+{\n+\tif (IS_ENABLED(CONFIG_CXL_SYSRAM_ONLINE_TYPE_SYSTEM_DEFAULT))\n+\t\treturn mhp_get_default_online_type();\n+\tif (IS_ENABLED(CONFIG_CXL_SYSRAM_ONLINE_TYPE_MOVABLE))\n+\t\treturn MMOP_ONLINE_MOVABLE;\n+\tif (IS_ENABLED(CONFIG_CXL_SYSRAM_ONLINE_TYPE_NORMAL))\n+\t\treturn MMOP_ONLINE;\n+\treturn MMOP_OFFLINE;\n+}\n+\n+static struct cxl_sysram *cxl_sysram_alloc(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_sysram *sysram __free(kfree) = NULL;\n+\tstruct device *dev;\n+\n+\tsysram = kzalloc(sizeof(*sysram), GFP_KERNEL);\n+\tif (!sysram)\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\tsysram->online_type = cxl_sysram_get_default_online_type();\n+\tsysram->last_hotplug_cmd = MMOP_OFFLINE;\n+\tsysram->numa_node = -1;\n+\tsysram->mgid = -1;\n+\n+\tdev = &sysram->dev;\n+\tsysram->cxlr = cxlr;\n+\tdevice_initialize(dev);\n+\tlockdep_set_class(&dev->mutex, &cxl_sysram_key);\n+\tdevice_set_pm_not_required(dev);\n+\tdev->parent = &cxlr->dev;\n+\tdev->bus = &cxl_bus_type;\n+\tdev->type = &cxl_sysram_type;\n+\n+\treturn_ptr(sysram);\n+}\n+\n+static void sysram_unregister(void *_sysram)\n+{\n+\tstruct cxl_sysram *sysram = _sysram;\n+\n+\tdevice_unregister(&sysram->dev);\n+}\n+\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type)\n+{\n+\tstruct cxl_sysram *sysram __free(put_cxl_sysram) = NULL;\n+\tstruct memory_dev_type *mtype;\n+\tstruct range hpa_range;\n+\tstruct device *dev;\n+\tint adist = MEMTIER_DEFAULT_LOWTIER_ADISTANCE;\n+\tint numa_node;\n+\tint rc;\n+\n+\trc = cxl_region_get_hpa_range(cxlr, &hpa_range);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\thpa_range = memory_block_align_range(&hpa_range);\n+\tif (hpa_range.start >= hpa_range.end) {\n+\t\tdev_warn(&cxlr->dev, \"region too small after alignment\\n\");\n+\t\treturn -ENOSPC;\n+\t}\n+\n+\tsysram = cxl_sysram_alloc(cxlr);\n+\tif (IS_ERR(sysram))\n+\t\treturn PTR_ERR(sysram);\n+\n+\tsysram->hpa_range = hpa_range;\n+\n+\tsysram->res_name = kasprintf(GFP_KERNEL, \"cxl_sysram%d\", cxlr->id);\n+\tif (!sysram->res_name)\n+\t\treturn -ENOMEM;\n+\n+\t/* Override default online type if caller specified one */\n+\tif (online_type >= 0)\n+\t\tsysram->online_type = online_type;\n+\n+\tdev = &sysram->dev;\n+\n+\trc = dev_set_name(dev, \"sysram_region%d\", cxlr->id);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\t/* Setup memory tier before adding device */\n+\tnuma_node = sysram_get_numa_node(cxlr);\n+\tif (numa_node < 0) {\n+\t\tdev_warn(&cxlr->dev, \"rejecting region with invalid node: %d\\n\",\n+\t\t\t numa_node);\n+\t\treturn -EINVAL;\n+\t}\n+\tsysram->numa_node = numa_node;\n+\n+\tmt_calc_adistance(numa_node, &adist);\n+\tmtype = mt_get_memory_type(adist);\n+\tif (IS_ERR(mtype))\n+\t\treturn PTR_ERR(mtype);\n+\tsysram->mtype = mtype;\n+\n+\tinit_node_memory_type(numa_node, mtype);\n+\n+\t/* Register memory group for this region */\n+\trc = memory_group_register_static(numa_node,\n+\t\t\t\t\t  PFN_UP(range_len(&hpa_range)));\n+\tif (rc < 0)\n+\t\treturn rc;\n+\tsysram->mgid = rc;\n+\n+\trc = device_add(dev);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tdev_dbg(&cxlr->dev, \"%s: register %s\\n\", dev_name(dev->parent),\n+\t\tdev_name(dev));\n+\n+\t/*\n+\t * Dynamic capacity regions (DCD) will have memory added later.\n+\t * For static RAM regions, hotplug the entire range now.\n+\t */\n+\tif (cxlr->mode != CXL_PARTMODE_RAM)\n+\t\tgoto out;\n+\n+\t/* If default online_type is a valid online mode, immediately hotplug */\n+\tif (sysram->online_type > MMOP_OFFLINE) {\n+\t\trc = sysram_hotplug_add(sysram, sysram->online_type);\n+\t\tif (rc)\n+\t\t\tdev_warn(dev, \"hotplug failed: %d\\n\", rc);\n+\t\telse\n+\t\t\tsysram->last_hotplug_cmd = sysram->online_type;\n+\t}\n+\n+out:\n+\treturn devm_add_action_or_reset(&cxlr->dev, sysram_unregister,\n+\t\t\t\t\tno_free_ptr(sysram));\n+}\n+EXPORT_SYMBOL_NS_GPL(devm_cxl_add_sysram, \"CXL\");\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex f899f240f229..8e8342fd4fde 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -607,6 +607,34 @@ struct cxl_dax_region {\n \tenum dax_driver_type dax_driver;\n };\n \n+/**\n+ * struct cxl_sysram - CXL SysRAM region for system memory hotplug\n+ * @dev: device for this sysram\n+ * @cxlr: parent cxl_region\n+ * @online_type: Default memory online type for new hotplug ops (MMOP_* value)\n+ * @last_hotplug_cmd: Last hotplug command submitted (MMOP_* value)\n+ * @hpa_range: Host physical address range for the region\n+ * @res_name: Resource name for the memory region\n+ * @res: Memory resource (set when hotplugged)\n+ * @mgid: Memory group id\n+ * @mtype: Memory tier type\n+ * @numa_node: NUMA node for this memory\n+ *\n+ * Device that directly performs memory hotplug for CXL RAM regions.\n+ */\n+struct cxl_sysram {\n+\tstruct device dev;\n+\tstruct cxl_region *cxlr;\n+\tenum mmop online_type;\n+\tint last_hotplug_cmd;\n+\tstruct range hpa_range;\n+\tconst char *res_name;\n+\tstruct resource *res;\n+\tint mgid;\n+\tstruct memory_dev_type *mtype;\n+\tint numa_node;\n+};\n+\n /**\n  * struct cxl_port - logical collection of upstream port devices and\n  *\t\t     downstream port devices to construct a CXL memory\n@@ -807,6 +835,7 @@ DEFINE_FREE(put_cxl_port, struct cxl_port *, if (!IS_ERR_OR_NULL(_T)) put_device\n DEFINE_FREE(put_cxl_root_decoder, struct cxl_root_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxlsd.cxld.dev))\n DEFINE_FREE(put_cxl_region, struct cxl_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n DEFINE_FREE(put_cxl_dax_region, struct cxl_dax_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n+DEFINE_FREE(put_cxl_sysram, struct cxl_sysram *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n \n int devm_cxl_enumerate_ports(struct cxl_memdev *cxlmd);\n void cxl_bus_rescan(void);\n@@ -889,6 +918,7 @@ void cxl_destroy_region(struct cxl_region *cxlr);\n struct device *cxl_region_dev(struct cxl_region *cxlr);\n enum cxl_partition_mode cxl_region_mode(struct cxl_region *cxlr);\n int cxl_get_region_range(struct cxl_region *cxlr, struct range *range);\n+struct cxl_sysram *cxl_region_find_sysram(struct cxl_region *cxlr);\n int cxl_get_committed_regions(struct cxl_memdev *cxlmd,\n \t\t\t      struct cxl_region **regions, int max_regions);\n struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n@@ -936,6 +966,7 @@ void cxl_driver_unregister(struct cxl_driver *cxl_drv);\n #define CXL_DEVICE_PMEM_REGION\t\t7\n #define CXL_DEVICE_DAX_REGION\t\t8\n #define CXL_DEVICE_PMU\t\t\t9\n+#define CXL_DEVICE_SYSRAM\t\t10\n \n #define MODULE_ALIAS_CXL(type) MODULE_ALIAS(\"cxl:t\" __stringify(type) \"*\")\n #define CXL_MODALIAS_FMT \"cxl:t%d\"\n@@ -954,6 +985,10 @@ bool is_cxl_pmem_region(struct device *dev);\n struct cxl_pmem_region *to_cxl_pmem_region(struct device *dev);\n int cxl_add_to_region(struct cxl_endpoint_decoder *cxled);\n struct cxl_dax_region *to_cxl_dax_region(struct device *dev);\n+struct cxl_sysram *to_cxl_sysram(struct device *dev);\n+struct device *cxl_sysram_dev(struct cxl_sysram *sysram);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n+int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram);\n u64 cxl_port_get_spa_cache_alias(struct cxl_port *endpoint, u64 spa);\n #else\n static inline bool is_cxl_pmem_region(struct device *dev)\n@@ -972,6 +1007,19 @@ static inline struct cxl_dax_region *to_cxl_dax_region(struct device *dev)\n {\n \treturn NULL;\n }\n+static inline struct cxl_sysram *to_cxl_sysram(struct device *dev)\n+{\n+\treturn NULL;\n+}\n+static inline int devm_cxl_add_sysram(struct cxl_region *cxlr,\n+\t\t\t\t      enum mmop online_type)\n+{\n+\treturn -ENXIO;\n+}\n+static inline int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram)\n+{\n+\treturn -ENXIO;\n+}\n static inline u64 cxl_port_get_spa_cache_alias(struct cxl_port *endpoint,\n \t\t\t\t\t       u64 spa)\n {\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about private memory regions being isolated from normal allocations and reclaim by adding support for N_MEMORY_PRIVATE hotplug via add_private_memory_driver_managed(). They modified the cxl_sysram region to register as a private node when private=true is passed to devm_cxl_add_sysram(), allowing callers to isolate their memory. A fix is planned.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a concern",
                "planned a fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Extend the cxl_sysram region to support N_MEMORY_PRIVATE hotplug\nvia add_private_memory_driver_managed(). When a caller passes\nprivate=true to devm_cxl_add_sysram(), the memory is registered\nas a private node, isolating it from normal allocations and reclaim.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/core/core.h          |  2 +-\n drivers/cxl/core/region_sysram.c | 50 +++++++++++++++++++++++++-------\n drivers/cxl/cxl.h                |  9 ++++--\n 3 files changed, 48 insertions(+), 13 deletions(-)\n\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex 973bbcae43f7..8ca3d6d41fe4 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -56,7 +56,7 @@ u64 cxl_dpa_to_hpa(struct cxl_region *cxlr, const struct cxl_memdev *cxlmd,\n \t\t   u64 dpa);\n int devm_cxl_add_dax_region(struct cxl_region *cxlr, enum dax_driver_type);\n int devm_cxl_add_pmem_region(struct cxl_region *cxlr);\n-int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private, enum mmop online_type);\n \n #else\n static inline u64 cxl_dpa_to_hpa(struct cxl_region *cxlr,\ndiff --git a/drivers/cxl/core/region_sysram.c b/drivers/cxl/core/region_sysram.c\nindex 47a415deb352..77aaa52e7332 100644\n--- a/drivers/cxl/core/region_sysram.c\n+++ b/drivers/cxl/core/region_sysram.c\n@@ -85,12 +85,23 @@ static int sysram_hotplug_add(struct cxl_sysram *sysram, enum mmop online_type)\n \t/*\n \t * Ensure that future kexec'd kernels will not treat\n \t * this as RAM automatically.\n+\t *\n+\t * For private regions, use add_private_memory_driver_managed()\n+\t * to register as N_MEMORY_PRIVATE which isolates the memory from\n+\t * normal allocations and reclaim.\n \t */\n-\trc = __add_memory_driver_managed(sysram->mgid,\n-\t\t\t\t\t sysram->hpa_range.start,\n-\t\t\t\t\t range_len(&sysram->hpa_range),\n-\t\t\t\t\t sysram_res_name, mhp_flags,\n-\t\t\t\t\t online_type);\n+\tif (sysram->private)\n+\t\trc = add_private_memory_driver_managed(sysram->mgid,\n+\t\t\t\t\t\t       sysram->hpa_range.start,\n+\t\t\t\t\t\t       range_len(&sysram->hpa_range),\n+\t\t\t\t\t\t       sysram_res_name, mhp_flags,\n+\t\t\t\t\t\t       online_type, &sysram->np);\n+\telse\n+\t\trc = __add_memory_driver_managed(sysram->mgid,\n+\t\t\t\t\t\t sysram->hpa_range.start,\n+\t\t\t\t\t\t range_len(&sysram->hpa_range),\n+\t\t\t\t\t\t sysram_res_name, mhp_flags,\n+\t\t\t\t\t\t online_type);\n \tif (rc) {\n \t\tremove_resource(res);\n \t\tkfree(res);\n@@ -108,10 +119,23 @@ static int sysram_hotplug_remove(struct cxl_sysram *sysram)\n \tif (!sysram->res)\n \t\treturn 0;\n \n-\trc = offline_and_remove_memory(sysram->hpa_range.start,\n-\t\t\t\t       range_len(&sysram->hpa_range));\n-\tif (rc)\n-\t\treturn rc;\n+\tif (sysram->private) {\n+\t\trc = offline_and_remove_private_memory(sysram->numa_node,\n+\t\t\t\t\t\t       sysram->hpa_range.start,\n+\t\t\t\t\t\t       range_len(&sysram->hpa_range));\n+\t\t/*\n+\t\t * -EBUSY means memory was removed but node_private_unregister()\n+\t\t * could not complete because other regions share the node.\n+\t\t * Continue to resource cleanup since the memory is gone.\n+\t\t */\n+\t\tif (rc && rc != -EBUSY)\n+\t\t\treturn rc;\n+\t} else {\n+\t\trc = offline_and_remove_memory(sysram->hpa_range.start,\n+\t\t\t\t\t       range_len(&sysram->hpa_range));\n+\t\tif (rc)\n+\t\t\treturn rc;\n+\t}\n \n \tif (sysram->res) {\n \t\tremove_resource(sysram->res);\n@@ -257,7 +281,8 @@ static void sysram_unregister(void *_sysram)\n \tdevice_unregister(&sysram->dev);\n }\n \n-int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type)\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private,\n+\t\t\tenum mmop online_type)\n {\n \tstruct cxl_sysram *sysram __free(put_cxl_sysram) = NULL;\n \tstruct memory_dev_type *mtype;\n@@ -291,6 +316,11 @@ int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type)\n \tif (online_type >= 0)\n \t\tsysram->online_type = online_type;\n \n+\t/* Set up private node registration if requested */\n+\tsysram->private = private;\n+\tif (private)\n+\t\tsysram->np.owner = sysram;\n+\n \tdev = &sysram->dev;\n \n \trc = dev_set_name(dev, \"sysram_region%d\", cxlr->id);\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex 8e8342fd4fde..54e5f9ac59dc 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -10,6 +10,7 @@\n #include <linux/bitops.h>\n #include <linux/log2.h>\n #include <linux/node.h>\n+#include <linux/node_private.h>\n #include <linux/io.h>\n #include <linux/range.h>\n #include <linux/dax.h>\n@@ -619,6 +620,8 @@ struct cxl_dax_region {\n  * @mgid: Memory group id\n  * @mtype: Memory tier type\n  * @numa_node: NUMA node for this memory\n+ * @private: true if this region uses N_MEMORY_PRIVATE hotplug\n+ * @np: private node registration state (valid when @private is true)\n  *\n  * Device that directly performs memory hotplug for CXL RAM regions.\n  */\n@@ -633,6 +636,8 @@ struct cxl_sysram {\n \tint mgid;\n \tstruct memory_dev_type *mtype;\n \tint numa_node;\n+\tbool private;\n+\tstruct node_private np;\n };\n \n /**\n@@ -987,7 +992,7 @@ int cxl_add_to_region(struct cxl_endpoint_decoder *cxled);\n struct cxl_dax_region *to_cxl_dax_region(struct device *dev);\n struct cxl_sysram *to_cxl_sysram(struct device *dev);\n struct device *cxl_sysram_dev(struct cxl_sysram *sysram);\n-int devm_cxl_add_sysram(struct cxl_region *cxlr, enum mmop online_type);\n+int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private, enum mmop online_type);\n int cxl_sysram_offline_and_remove(struct cxl_sysram *sysram);\n u64 cxl_port_get_spa_cache_alias(struct cxl_port *endpoint, u64 spa);\n #else\n@@ -1011,7 +1016,7 @@ static inline struct cxl_sysram *to_cxl_sysram(struct device *dev)\n {\n \treturn NULL;\n }\n-static inline int devm_cxl_add_sysram(struct cxl_region *cxlr,\n+static inline int devm_cxl_add_sysram(struct cxl_region *cxlr, bool private,\n \t\t\t\t      enum mmop online_type)\n {\n \treturn -ENXIO;\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author addressed a concern about the driver's interaction with the migration target control, explaining that they moved struct migration_target_control to include/linux/migrate.h so the driver can use alloc_migration_target() without depending on mm-internal headers.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a sample CXL type-3 driver that registers device memory as\nprivate-node NUMA memory reachable only via explicit mempolicy\n(set_mempolicy / mbind).\n\nProbe flow:\n  1. Call cxl_pci_type3_probe_init() for standard CXL device setup\n  2. Look for pre-committed RAM regions; if none exist, create one\n     using cxl_get_hpa_freespace() + cxl_request_dpa() +\n     cxl_create_region()\n  3. Convert the region to sysram via devm_cxl_add_sysram() with\n     private=true and MMOP_ONLINE_MOVABLE\n  4. Register node_private_ops with NP_OPS_MIGRATION | NP_OPS_MEMPOLICY\n     so the node is excluded from default allocations\n\nThe migrate_to callback uses alloc_migration_target() with\n__GFP_THISNODE | __GFP_PRIVATE to keep pages on the target node.\n\nMove struct migration_target_control from mm/internal.h to\ninclude/linux/migrate.h so the driver can use alloc_migration_target()\nwithout depending on mm-internal headers.\n\nUsage:\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_mempolicy/bind\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/Kconfig                           |   2 +\n drivers/cxl/Makefile                          |   2 +\n drivers/cxl/type3_drivers/Kconfig             |   2 +\n drivers/cxl/type3_drivers/Makefile            |   2 +\n .../cxl/type3_drivers/cxl_mempolicy/Kconfig   |  16 +\n .../cxl/type3_drivers/cxl_mempolicy/Makefile  |   4 +\n .../type3_drivers/cxl_mempolicy/mempolicy.c   | 297 ++++++++++++++++++\n include/linux/migrate.h                       |   7 +-\n mm/internal.h                                 |   7 -\n 9 files changed, 331 insertions(+), 8 deletions(-)\n create mode 100644 drivers/cxl/type3_drivers/Kconfig\n create mode 100644 drivers/cxl/type3_drivers/Makefile\n create mode 100644 drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\n create mode 100644 drivers/cxl/type3_drivers/cxl_mempolicy/Makefile\n create mode 100644 drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c\n\ndiff --git a/drivers/cxl/Kconfig b/drivers/cxl/Kconfig\nindex f99aa7274d12..1648cdeaa0c9 100644\n--- a/drivers/cxl/Kconfig\n+++ b/drivers/cxl/Kconfig\n@@ -278,4 +278,6 @@ config CXL_ATL\n \tdepends on CXL_REGION\n \tdepends on ACPI_PRMT && AMD_NB\n \n+source \"drivers/cxl/type3_drivers/Kconfig\"\n+\n endif\ndiff --git a/drivers/cxl/Makefile b/drivers/cxl/Makefile\nindex 2caa90fa4bf2..94d2b2233bf8 100644\n--- a/drivers/cxl/Makefile\n+++ b/drivers/cxl/Makefile\n@@ -19,3 +19,5 @@ cxl_acpi-y := acpi.o\n cxl_pmem-y := pmem.o security.o\n cxl_mem-y := mem.o\n cxl_pci-y := pci.o\n+\n+obj-y += type3_drivers/\ndiff --git a/drivers/cxl/type3_drivers/Kconfig b/drivers/cxl/type3_drivers/Kconfig\nnew file mode 100644\nindex 000000000000..369b21763856\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/Kconfig\n@@ -0,0 +1,2 @@\n+# SPDX-License-Identifier: GPL-2.0\n+source \"drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\"\ndiff --git a/drivers/cxl/type3_drivers/Makefile b/drivers/cxl/type3_drivers/Makefile\nnew file mode 100644\nindex 000000000000..2b82265ff118\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/Makefile\n@@ -0,0 +1,2 @@\n+# SPDX-License-Identifier: GPL-2.0\n+obj-$(CONFIG_CXL_MEMPOLICY) += cxl_mempolicy/\ndiff --git a/drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig b/drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\nnew file mode 100644\nindex 000000000000..3c45da237b9f\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\n@@ -0,0 +1,16 @@\n+config CXL_MEMPOLICY\n+\ttristate \"CXL Private Memory with Mempolicy Support\"\n+\tdepends on CXL_PCI\n+\tdepends on CXL_REGION\n+\tdepends on NUMA\n+\tdepends on MIGRATION\n+\thelp\n+\t  Minimal driver for CXL memory devices that registers memory as\n+\t  N_MEMORY_PRIVATE with mempolicy support.  The memory is isolated\n+\t  from default allocations and can only be reached via explicit\n+\t  mempolicy (set_mempolicy or mbind).\n+\n+\t  No compression, no PTE controls, the memory behaves like normal\n+\t  DRAM but is excluded from fallback allocations.\n+\n+\t  If unsure say 'n'.\ndiff --git a/drivers/cxl/type3_drivers/cxl_mempolicy/Makefile b/drivers/cxl/type3_drivers/cxl_mempolicy/Makefile\nnew file mode 100644\nindex 000000000000..dfb58fc88ad9\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_mempolicy/Makefile\n@@ -0,0 +1,4 @@\n+# SPDX-License-Identifier: GPL-2.0\n+obj-$(CONFIG_CXL_MEMPOLICY) += cxl_mempolicy.o\n+cxl_mempolicy-y := mempolicy.o\n+ccflags-y += -I$(srctree)/drivers/cxl\ndiff --git a/drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c b/drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c\nnew file mode 100644\nindex 000000000000..1c19818eb268\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_mempolicy/mempolicy.c\n@@ -0,0 +1,297 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2026 Meta Platforms, Inc. All rights reserved. */\n+/*\n+ * CXL Mempolicy Driver\n+ *\n+ * Minimal driver for CXL memory devices that registers memory as\n+ * N_MEMORY_PRIVATE with mempolicy support but no PTE controls.  The\n+ * memory behaves like normal DRAM but is isolated from default allocations,\n+ * it can only be reached via explicit mempolicy (set_mempolicy/mbind).\n+ *\n+ * Usage:\n+ *   1. Unbind device from cxl_pci:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n+ *   2. Bind to cxl_mempolicy:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_mempolicy/bind\n+ */\n+\n+#include <linux/module.h>\n+#include <linux/pci.h>\n+#include <linux/xarray.h>\n+#include <linux/node_private.h>\n+#include <linux/migrate.h>\n+#include <cxl/mailbox.h>\n+#include \"cxlmem.h\"\n+#include \"cxl.h\"\n+\n+struct cxl_mempolicy_ctx {\n+\tstruct cxl_region *cxlr;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tint nid;\n+};\n+\n+static DEFINE_XARRAY(ctx_xa);\n+\n+static struct cxl_mempolicy_ctx *memdev_to_ctx(struct cxl_memdev *cxlmd)\n+{\n+\tstruct pci_dev *pdev = to_pci_dev(cxlmd->dev.parent);\n+\n+\treturn xa_load(&ctx_xa, (unsigned long)pdev);\n+}\n+\n+static int cxl_mempolicy_migrate_to(struct list_head *folios, int nid,\n+\t\t\t\t    enum migrate_mode mode,\n+\t\t\t\t    enum migrate_reason reason,\n+\t\t\t\t    unsigned int *nr_succeeded)\n+{\n+\tstruct migration_target_control mtc = {\n+\t\t.nid = nid,\n+\t\t.gfp_mask = GFP_HIGHUSER_MOVABLE | __GFP_THISNODE |\n+\t\t\t    __GFP_PRIVATE,\n+\t\t.reason = reason,\n+\t};\n+\n+\treturn migrate_pages(folios, alloc_migration_target, NULL,\n+\t\t\t     (unsigned long)&mtc, mode, reason, nr_succeeded);\n+}\n+\n+static void cxl_mempolicy_folio_migrate(struct folio *src, struct folio *dst)\n+{\n+}\n+\n+static const struct node_private_ops cxl_mempolicy_ops = {\n+\t.migrate_to\t= cxl_mempolicy_migrate_to,\n+\t.folio_migrate\t= cxl_mempolicy_folio_migrate,\n+\t.flags = NP_OPS_MIGRATION | NP_OPS_MEMPOLICY,\n+};\n+\n+static struct cxl_region *create_ram_region(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx = memdev_to_ctx(cxlmd);\n+\tstruct cxl_root_decoder *cxlrd;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tstruct cxl_region *cxlr;\n+\tresource_size_t ram_size, avail;\n+\n+\tram_size = cxl_ram_size(cxlmd->cxlds);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev, \"no RAM capacity available\\n\");\n+\t\treturn ERR_PTR(-ENODEV);\n+\t}\n+\n+\tram_size = ALIGN_DOWN(ram_size, SZ_256M);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev,\n+\t\t\t \"RAM capacity too small (< 256M)\\n\");\n+\t\treturn ERR_PTR(-ENOSPC);\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"creating RAM region for %lld MB\\n\",\n+\t\t ram_size >> 20);\n+\n+\tcxlrd = cxl_get_hpa_freespace(cxlmd, ram_size, &avail);\n+\tif (IS_ERR(cxlrd)) {\n+\t\tdev_err(&cxlmd->dev, \"no HPA freespace: %ld\\n\",\n+\t\t\tPTR_ERR(cxlrd));\n+\t\treturn ERR_CAST(cxlrd);\n+\t}\n+\n+\tcxled = cxl_request_dpa(cxlmd, CXL_PARTMODE_RAM, ram_size);\n+\tif (IS_ERR(cxled)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to request DPA: %ld\\n\",\n+\t\t\tPTR_ERR(cxled));\n+\t\tcxl_put_root_decoder(cxlrd);\n+\t\treturn ERR_CAST(cxled);\n+\t}\n+\n+\tcxlr = cxl_create_region(cxlrd, &cxled, 1);\n+\tcxl_put_root_decoder(cxlrd);\n+\tif (IS_ERR(cxlr)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to create region: %ld\\n\",\n+\t\t\tPTR_ERR(cxlr));\n+\t\tcxl_dpa_free(cxled);\n+\t\treturn cxlr;\n+\t}\n+\n+\tctx->cxled = cxled;\n+\tdev_info(&cxlmd->dev, \"created region %s\\n\",\n+\t\t dev_name(cxl_region_dev(cxlr)));\n+\treturn cxlr;\n+}\n+\n+static int setup_private_node(struct cxl_memdev *cxlmd,\n+\t\t\t      struct cxl_region *cxlr)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx = memdev_to_ctx(cxlmd);\n+\tstruct range hpa_range;\n+\tint rc;\n+\n+\tdevice_release_driver(cxl_region_dev(cxlr));\n+\n+\trc = devm_cxl_add_sysram(cxlr, true, MMOP_ONLINE_MOVABLE);\n+\tif (rc) {\n+\t\tdev_err(cxl_region_dev(cxlr),\n+\t\t\t\"failed to add sysram: %d\\n\", rc);\n+\t\tif (device_attach(cxl_region_dev(cxlr)) < 0)\n+\t\t\tdev_warn(cxl_region_dev(cxlr),\n+\t\t\t\t \"failed to re-attach driver\\n\");\n+\t\treturn rc;\n+\t}\n+\n+\trc = cxl_get_region_range(cxlr, &hpa_range);\n+\tif (rc) {\n+\t\tdev_err(cxl_region_dev(cxlr),\n+\t\t\t\"failed to get region range: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\tctx->nid = phys_to_target_node(hpa_range.start);\n+\tif (ctx->nid == NUMA_NO_NODE)\n+\t\tctx->nid = memory_add_physaddr_to_nid(hpa_range.start);\n+\n+\trc = node_private_set_ops(ctx->nid, &cxl_mempolicy_ops);\n+\tif (rc) {\n+\t\tdev_err(cxl_region_dev(cxlr),\n+\t\t\t\"failed to set ops on node %d: %d\\n\", ctx->nid, rc);\n+\t\tctx->nid = NUMA_NO_NODE;\n+\t\treturn rc;\n+\t}\n+\n+\tdev_info(&cxlmd->dev,\n+\t\t \"node %d registered as private mempolicy memory\\n\", ctx->nid);\n+\treturn 0;\n+}\n+\n+static int cxl_mempolicy_attach_probe(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_region *regions[8];\n+\tstruct cxl_region *cxlr;\n+\tint nr, i;\n+\tint rc;\n+\n+\tdev_info(&cxlmd->dev,\n+\t\t \"cxl_mempolicy attach: looking for regions\\n\");\n+\n+\t/* Phase 1: look for pre-committed RAM regions */\n+\tnr = cxl_get_committed_regions(cxlmd, regions, ARRAY_SIZE(regions));\n+\tfor (i = 0; i < nr; i++) {\n+\t\tif (cxl_region_mode(regions[i]) != CXL_PARTMODE_RAM) {\n+\t\t\tput_device(cxl_region_dev(regions[i]));\n+\t\t\tcontinue;\n+\t\t}\n+\n+\t\tcxlr = regions[i];\n+\t\trc = setup_private_node(cxlmd, cxlr);\n+\t\tput_device(cxl_region_dev(cxlr));\n+\t\tif (rc == 0) {\n+\t\t\t/* Release remaining region references */\n+\t\t\tfor (i++; i < nr; i++)\n+\t\t\t\tput_device(cxl_region_dev(regions[i]));\n+\t\t\treturn 0;\n+\t\t}\n+\t}\n+\n+\t/* Phase 2: no committed regions, create one */\n+\tdev_info(&cxlmd->dev,\n+\t\t \"no existing regions, creating RAM region\\n\");\n+\n+\tcxlr = create_ram_region(cxlmd);\n+\tif (IS_ERR(cxlr)) {\n+\t\trc = PTR_ERR(cxlr);\n+\t\tif (rc == -ENODEV) {\n+\t\t\tdev_info(&cxlmd->dev,\n+\t\t\t\t \"no RAM capacity: %d\\n\", rc);\n+\t\t\treturn 0;\n+\t\t}\n+\t\treturn rc;\n+\t}\n+\n+\trc = setup_private_node(cxlmd, cxlr);\n+\tif (rc) {\n+\t\tdev_err(&cxlmd->dev,\n+\t\t\t\"failed to setup private node: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\t/* Only take ownership of regions we created (Phase 2) */\n+\tmemdev_to_ctx(cxlmd)->cxlr = cxlr;\n+\n+\treturn 0;\n+}\n+\n+static const struct cxl_memdev_attach cxl_mempolicy_attach = {\n+\t.probe = cxl_mempolicy_attach_probe,\n+};\n+\n+static int cxl_mempolicy_probe(struct pci_dev *pdev,\n+\t\t\t       const struct pci_device_id *id)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx;\n+\tstruct cxl_memdev *cxlmd;\n+\tint rc;\n+\n+\tdev_info(&pdev->dev, \"cxl_mempolicy: probing device\\n\");\n+\n+\tctx = devm_kzalloc(&pdev->dev, sizeof(*ctx), GFP_KERNEL);\n+\tif (!ctx)\n+\t\treturn -ENOMEM;\n+\tctx->nid = NUMA_NO_NODE;\n+\n+\trc = xa_insert(&ctx_xa, (unsigned long)pdev, ctx, GFP_KERNEL);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tcxlmd = cxl_pci_type3_probe_init(pdev, &cxl_mempolicy_attach);\n+\tif (IS_ERR(cxlmd)) {\n+\t\txa_erase(&ctx_xa, (unsigned long)pdev);\n+\t\treturn PTR_ERR(cxlmd);\n+\t}\n+\n+\tdev_info(&pdev->dev, \"cxl_mempolicy: probe complete\\n\");\n+\treturn 0;\n+}\n+\n+static void cxl_mempolicy_remove(struct pci_dev *pdev)\n+{\n+\tstruct cxl_mempolicy_ctx *ctx = xa_erase(&ctx_xa, (unsigned long)pdev);\n+\n+\tdev_info(&pdev->dev, \"cxl_mempolicy: removing device\\n\");\n+\n+\tif (!ctx)\n+\t\treturn;\n+\n+\tif (ctx->nid != NUMA_NO_NODE)\n+\t\tWARN_ON(node_private_clear_ops(ctx->nid, &cxl_mempolicy_ops));\n+\n+\tif (ctx->cxlr) {\n+\t\tcxl_destroy_region(ctx->cxlr);\n+\t\tctx->cxlr = NULL;\n+\t}\n+\n+\tif (ctx->cxled) {\n+\t\tcxl_dpa_free(ctx->cxled);\n+\t\tctx->cxled = NULL;\n+\t}\n+}\n+\n+static const struct pci_device_id cxl_mempolicy_pci_tbl[] = {\n+\t{ PCI_DEVICE(PCI_VENDOR_ID_INTEL, 0x0d93) },\n+\t{ },\n+};\n+MODULE_DEVICE_TABLE(pci, cxl_mempolicy_pci_tbl);\n+\n+static struct pci_driver cxl_mempolicy_driver = {\n+\t.name\t\t= KBUILD_MODNAME,\n+\t.id_table\t= cxl_mempolicy_pci_tbl,\n+\t.probe\t\t= cxl_mempolicy_probe,\n+\t.remove\t\t= cxl_mempolicy_remove,\n+\t.driver\t= {\n+\t\t.probe_type\t= PROBE_PREFER_ASYNCHRONOUS,\n+\t},\n+};\n+\n+module_pci_driver(cxl_mempolicy_driver);\n+\n+MODULE_DESCRIPTION(\"CXL: Private Memory with Mempolicy Support\");\n+MODULE_LICENSE(\"GPL v2\");\n+MODULE_IMPORT_NS(\"CXL\");\ndiff --git a/include/linux/migrate.h b/include/linux/migrate.h\nindex 7b2da3875ff2..1f9fb61f3932 100644\n--- a/include/linux/migrate.h\n+++ b/include/linux/migrate.h\n@@ -10,7 +10,12 @@\n typedef struct folio *new_folio_t(struct folio *folio, unsigned long private);\n typedef void free_folio_t(struct folio *folio, unsigned long private);\n \n-struct migration_target_control;\n+struct migration_target_control {\n+\tint nid;\t\t/* preferred node id */\n+\tnodemask_t *nmask;\n+\tgfp_t gfp_mask;\n+\tenum migrate_reason reason;\n+};\n \n /**\n  * struct movable_operations - Driver page migration\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 64467ca774f1..85cd11189854 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1352,13 +1352,6 @@ extern const struct trace_print_flags gfpflag_names[];\n \n void setup_zone_pageset(struct zone *zone);\n \n-struct migration_target_control {\n-\tint nid;\t\t/* preferred node id */\n-\tnodemask_t *nmask;\n-\tgfp_t gfp_mask;\n-\tenum migrate_reason reason;\n-};\n-\n /*\n  * mm/filemap.c\n  */\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author is addressing a concern about the cxl_compression driver's page reclamation using the CXL Media Operations Zero command (opcode 0x4402). The author explains that if the device does not support this command, the driver falls back to inline CPU zeroing.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Add a generic CXL type-3 driver for compressed memory controllers.\n\nThe driver provides an alternative PCI binding that converts CXL\nRAM regions to private-node sysram and registers them with the\nCRAM subsystem for transparent demotion/promotion.\n\nProbe flow:\n  1. cxl_pci_type3_probe_init() for standard CXL device setup\n  2. Discover/convert auto-RAM regions or create a RAM region\n  3. Convert to private-node sysram via devm_cxl_add_sysram()\n  4. Register with CRAM via cram_register_private_node()\n\nPage flush pipeline:\n  When a CRAM folio is freed, the CRAM free_folio   callback buffers\n  it into a per-CPU RCU-protected flush buffer to offload the operation.\n\n  A periodic kthread swaps the per-CPU buffers under RCU, then sends\n  batched Sanitize-Zero commands so the device can zero pages.\n\n  A flush_record bitmap tracks in-flight pages to avoid re-buffering on\n  the second free_folio entry after folio_put().\n\n  Overflow from full buffers is handled by a per-CPU workqueue fallback.\n\nWatermark interrupts:\n  MSI-X vector 12 - delivers \"Low\" watermark interrupts\n  MSI-X vector 13 - delivers \"High\" watermark interrupts\n  This adjusts CRAM pressure:\n\tLow  - increases pressure.\n  \tHigh - reduces pressure.\n\n  A dynamic watermark mode cycles through four phases with\n  progressively tighter thresholds.\n\n  Static watermark mode sets pressure 0 or MAX respectively.\n\nTeardown ordering:\n  pre_teardown  - cram_unregister + retry-loop memory offline\n  post_teardown - kthread stop, drain all flush buffers via CCI\n\nUsage:\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n   echo $PCI_DEV > /sys/bus/pci/drivers/cxl_compression/bind\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n---\n drivers/cxl/type3_drivers/Kconfig             |    1 +\n drivers/cxl/type3_drivers/Makefile            |    1 +\n .../cxl/type3_drivers/cxl_compression/Kconfig |   20 +\n .../type3_drivers/cxl_compression/Makefile    |    4 +\n .../cxl_compression/compression.c             | 1025 +++++++++++++++++\n 5 files changed, 1051 insertions(+)\n create mode 100644 drivers/cxl/type3_drivers/cxl_compression/Kconfig\n create mode 100644 drivers/cxl/type3_drivers/cxl_compression/Makefile\n create mode 100644 drivers/cxl/type3_drivers/cxl_compression/compression.c\n\ndiff --git a/drivers/cxl/type3_drivers/Kconfig b/drivers/cxl/type3_drivers/Kconfig\nindex 369b21763856..98f73e46730e 100644\n--- a/drivers/cxl/type3_drivers/Kconfig\n+++ b/drivers/cxl/type3_drivers/Kconfig\n@@ -1,2 +1,3 @@\n # SPDX-License-Identifier: GPL-2.0\n source \"drivers/cxl/type3_drivers/cxl_mempolicy/Kconfig\"\n+source \"drivers/cxl/type3_drivers/cxl_compression/Kconfig\"\ndiff --git a/drivers/cxl/type3_drivers/Makefile b/drivers/cxl/type3_drivers/Makefile\nindex 2b82265ff118..f5b0766d92af 100644\n--- a/drivers/cxl/type3_drivers/Makefile\n+++ b/drivers/cxl/type3_drivers/Makefile\n@@ -1,2 +1,3 @@\n # SPDX-License-Identifier: GPL-2.0\n obj-$(CONFIG_CXL_MEMPOLICY) += cxl_mempolicy/\n+obj-$(CONFIG_CXL_COMPRESSION) += cxl_compression/\ndiff --git a/drivers/cxl/type3_drivers/cxl_compression/Kconfig b/drivers/cxl/type3_drivers/cxl_compression/Kconfig\nnew file mode 100644\nindex 000000000000..8c891a48b000\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_compression/Kconfig\n@@ -0,0 +1,20 @@\n+config CXL_COMPRESSION\n+\ttristate \"CXL Compression Memory Driver\"\n+\tdepends on CXL_PCI\n+\tdepends on CXL_REGION\n+\tdepends on CRAM\n+\thelp\n+\t  This driver provides an alternative PCI binding for CXL memory\n+\t  devices with compressed memory support. It converts CXL RAM\n+\t  regions to sysram for direct memory hotplug and registers with\n+\t  the CRAM subsystem for transparent compression.\n+\n+\t  Page reclamation uses the standard CXL Media Operations Zero\n+\t  command (opcode 0x4402). If the device does not support it,\n+\t  the driver falls back to inline CPU zeroing.\n+\n+\t  Usage: First unbind the device from cxl_pci, then bind to\n+\t  cxl_compression. The driver will initialize the CXL device and\n+\t  convert any RAM regions to use direct memory hotplug via sysram.\n+\n+\t  If unsure say 'n'.\ndiff --git a/drivers/cxl/type3_drivers/cxl_compression/Makefile b/drivers/cxl/type3_drivers/cxl_compression/Makefile\nnew file mode 100644\nindex 000000000000..46f34809bf74\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_compression/Makefile\n@@ -0,0 +1,4 @@\n+# SPDX-License-Identifier: GPL-2.0\n+obj-$(CONFIG_CXL_COMPRESSION) += cxl_compression.o\n+cxl_compression-y := compression.o\n+ccflags-y += -I$(srctree)/drivers/cxl\ndiff --git a/drivers/cxl/type3_drivers/cxl_compression/compression.c b/drivers/cxl/type3_drivers/cxl_compression/compression.c\nnew file mode 100644\nindex 000000000000..e4c8b62227e2\n--- /dev/null\n+++ b/drivers/cxl/type3_drivers/cxl_compression/compression.c\n@@ -0,0 +1,1025 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+/* Copyright(c) 2026 Meta Platforms, Inc. All rights reserved. */\n+/*\n+ * CXL Compression Driver\n+ *\n+ * This driver provides an alternative binding for CXL memory devices that\n+ * converts all associated RAM regions to sysram_regions for direct memory\n+ * hotplug, bypassing the standard dax region path.\n+ *\n+ * Page reclamation uses the standard CXL Media Operations Zero command\n+ * (opcode 0x4402, class 0x01, subclass 0x01).  Watermark interrupts\n+ * are delivered via separate MSI-X vectors (12 for lthresh, 13 for\n+ * hthresh), injected externally via QMP.\n+ *\n+ * Usage:\n+ *   1. Device initially binds to cxl_pci at boot\n+ *   2. Unbind from cxl_pci:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_pci/unbind\n+ *   3. Bind to cxl_compression:\n+ *        echo $PCI_DEV > /sys/bus/pci/drivers/cxl_compression/bind\n+ */\n+\n+#include <linux/unaligned.h>\n+#include <linux/io-64-nonatomic-lo-hi.h>\n+#include <linux/module.h>\n+#include <linux/delay.h>\n+#include <linux/sizes.h>\n+#include <linux/mutex.h>\n+#include <linux/list.h>\n+#include <linux/pci.h>\n+#include <linux/io.h>\n+#include <linux/interrupt.h>\n+#include <linux/bitmap.h>\n+#include <linux/highmem.h>\n+#include <linux/workqueue.h>\n+#include <linux/kthread.h>\n+#include <linux/rcupdate.h>\n+#include <linux/percpu.h>\n+#include <linux/sched.h>\n+#include <linux/cram.h>\n+#include <linux/memory_hotplug.h>\n+#include <linux/xarray.h>\n+#include <cxl/mailbox.h>\n+#include \"cxlmem.h\"\n+#include \"cxl.h\"\n+\n+/*\n+ * Per-device compression context lookup.\n+ *\n+ * pci_set_drvdata() MUST store cxlds because mbox_to_cxlds() uses\n+ * dev_get_drvdata() to recover the cxl_dev_state from the mailbox host\n+ * device.  Storing anything else in pci drvdata breaks every CXL mailbox\n+ * command.  Use an xarray keyed by pci_dev pointer so that multiple\n+ * devices can bind concurrently without colliding.\n+ */\n+static DEFINE_XARRAY(comp_ctx_xa);\n+\n+static struct cxl_compression_ctx *pdev_to_comp_ctx(struct pci_dev *pdev)\n+{\n+\treturn xa_load(&comp_ctx_xa, (unsigned long)pdev);\n+}\n+\n+#define CXL_MEDIA_OP_OPCODE\t\t0x4402\n+#define CXL_MEDIA_OP_CLASS_SANITIZE\t0x01\n+#define CXL_MEDIA_OP_SUBC_ZERO\t\t0x01\n+\n+struct cxl_dpa_range {\n+\t__le64 starting_dpa;\n+\t__le64 length;\n+} __packed;\n+\n+struct cxl_media_op_input {\n+\tu8 media_operation_class;\n+\tu8 media_operation_subclass;\n+\t__le16 reserved;\n+\t__le32 dpa_range_count;\n+\tstruct cxl_dpa_range ranges[];\n+} __packed;\n+\n+#define CXL_CT3_MSIX_LTHRESH\t\t12\n+#define CXL_CT3_MSIX_HTHRESH\t\t13\n+#define CXL_CT3_MSIX_VECTOR_NR\t\t14\n+#define CXL_FLUSH_INTERVAL_DEFAULT_MS\t1000\n+\n+static unsigned int flush_buf_size;\n+module_param(flush_buf_size, uint, 0444);\n+MODULE_PARM_DESC(flush_buf_size,\n+\t\t \"Max DPA ranges per media ops CCI command (0 = use hw max)\");\n+\n+static unsigned int flush_interval_ms = CXL_FLUSH_INTERVAL_DEFAULT_MS;\n+module_param(flush_interval_ms, uint, 0644);\n+MODULE_PARM_DESC(flush_interval_ms,\n+\t\t \"Flush worker interval in ms (default 1000)\");\n+\n+struct cxl_flush_buf {\n+\tunsigned int count;\n+\tunsigned int max;\t\t\t/* max ranges per command */\n+\tstruct cxl_media_op_input *cmd;\t\t/* pre-allocated CCI payload */\n+\tstruct folio **folios;\t\t\t/* parallel folio tracking */\n+};\n+\n+struct cxl_flush_ctx;\n+\n+struct cxl_pcpu_flush {\n+\tstruct cxl_flush_buf __rcu *active;\t/* callback writes here */\n+\tstruct cxl_flush_buf *overflow_spare;\t/* spare for overflow work */\n+\tstruct work_struct overflow_work;\t/* per-CPU overflow flush */\n+\tstruct cxl_flush_ctx *ctx;\t\t/* backpointer */\n+};\n+\n+/**\n+ * struct cxl_flush_ctx - Per-region flush context\n+ * @flush_record: two-level bitmap, 1 bit per 4KB page, tracks in-flight ops\n+ * @flush_record_pages: number of pages in the flush_record array\n+ * @nr_pages: total number of 4KB pages in the region\n+ * @base_pfn: starting PFN of the region (for DPA offset calculation)\n+ * @buf_max: max DPA ranges per CCI command\n+ * @media_ops_supported: true if device supports media operations zero\n+ * @pcpu: per-CPU flush state\n+ * @kthread_spares: array[nr_cpu_ids] of spare buffers for the kthread\n+ * @flush_thread: round-robin kthread\n+ * @mbox: pointer to CXL mailbox for sending CCI commands\n+ * @dev: device for logging\n+ * @nid: NUMA node of the private region\n+ */\n+struct cxl_flush_ctx {\n+\tunsigned long\t**flush_record;\n+\tunsigned int\t flush_record_pages;\n+\tunsigned long\t nr_pages;\n+\tunsigned long\t base_pfn;\n+\tunsigned int\t buf_max;\n+\tbool\t\t media_ops_supported;\n+\tstruct cxl_pcpu_flush __percpu *pcpu;\n+\tstruct cxl_flush_buf **kthread_spares;\n+\tstruct task_struct *flush_thread;\n+\tstruct cxl_mailbox *mbox;\n+\tstruct device\t*dev;\n+\tint\t\t nid;\n+};\n+\n+/* Bits per page-sized bitmap chunk */\n+#define FLUSH_RECORD_BITS_PER_PAGE\t(PAGE_SIZE * BITS_PER_BYTE)\n+#define FLUSH_RECORD_SHIFT\t\t(PAGE_SHIFT + 3)\n+\n+static unsigned long **flush_record_alloc(unsigned long nr_bits,\n+\t\t\t\t\t  unsigned int *nr_pages_out)\n+{\n+\tunsigned int nr_pages = DIV_ROUND_UP(nr_bits, FLUSH_RECORD_BITS_PER_PAGE);\n+\tunsigned long **pages;\n+\tunsigned int i;\n+\n+\tpages = kcalloc(nr_pages, sizeof(*pages), GFP_KERNEL);\n+\tif (!pages)\n+\t\treturn NULL;\n+\n+\tfor (i = 0; i < nr_pages; i++) {\n+\t\tpages[i] = (unsigned long *)get_zeroed_page(GFP_KERNEL);\n+\t\tif (!pages[i])\n+\t\t\tgoto err;\n+\t}\n+\n+\t*nr_pages_out = nr_pages;\n+\treturn pages;\n+\n+err:\n+\twhile (i--)\n+\t\tfree_page((unsigned long)pages[i]);\n+\tkfree(pages);\n+\treturn NULL;\n+}\n+\n+static void flush_record_free(unsigned long **pages, unsigned int nr_pages)\n+{\n+\tunsigned int i;\n+\n+\tif (!pages)\n+\t\treturn;\n+\n+\tfor (i = 0; i < nr_pages; i++)\n+\t\tfree_page((unsigned long)pages[i]);\n+\tkfree(pages);\n+}\n+\n+static inline bool flush_record_test_and_clear(unsigned long **pages,\n+\t\t\t\t\t       unsigned long idx)\n+{\n+\treturn test_and_clear_bit(idx & (FLUSH_RECORD_BITS_PER_PAGE - 1),\n+\t\t\t\t  pages[idx >> FLUSH_RECORD_SHIFT]);\n+}\n+\n+static inline void flush_record_set(unsigned long **pages, unsigned long idx)\n+{\n+\tset_bit(idx & (FLUSH_RECORD_BITS_PER_PAGE - 1),\n+\t\tpages[idx >> FLUSH_RECORD_SHIFT]);\n+}\n+\n+static struct cxl_flush_buf *cxl_flush_buf_alloc(unsigned int max, int nid)\n+{\n+\tstruct cxl_flush_buf *buf;\n+\n+\tbuf = kzalloc_node(sizeof(*buf), GFP_KERNEL, nid);\n+\tif (!buf)\n+\t\treturn NULL;\n+\n+\tbuf->max = max;\n+\tbuf->cmd = kvzalloc_node(struct_size(buf->cmd, ranges, max),\n+\t\t\t\t GFP_KERNEL, nid);\n+\tif (!buf->cmd)\n+\t\tgoto err_cmd;\n+\n+\tbuf->folios = kcalloc_node(max, sizeof(struct folio *),\n+\t\t\t\t   GFP_KERNEL, nid);\n+\tif (!buf->folios)\n+\t\tgoto err_folios;\n+\n+\treturn buf;\n+\n+err_folios:\n+\tkvfree(buf->cmd);\n+err_cmd:\n+\tkfree(buf);\n+\treturn NULL;\n+}\n+\n+static void cxl_flush_buf_free(struct cxl_flush_buf *buf)\n+{\n+\tif (!buf)\n+\t\treturn;\n+\tkvfree(buf->cmd);\n+\tkfree(buf->folios);\n+\tkfree(buf);\n+}\n+\n+static inline void cxl_flush_buf_reset(struct cxl_flush_buf *buf)\n+{\n+\tbuf->count = 0;\n+}\n+\n+static void cxl_flush_buf_send(struct cxl_flush_ctx *ctx,\n+\t\t\t       struct cxl_flush_buf *buf)\n+{\n+\tstruct cxl_mbox_cmd mbox_cmd;\n+\tunsigned int count = buf->count;\n+\tunsigned int i;\n+\tint rc;\n+\n+\tif (count == 0)\n+\t\treturn;\n+\n+\tif (!ctx->media_ops_supported) {\n+\t\t/* No device support, zero all folios inline */\n+\t\tfor (i = 0; i < count; i++)\n+\t\t\tfolio_zero_range(buf->folios[i], 0,\n+\t\t\t\t\t folio_size(buf->folios[i]));\n+\t\tgoto release;\n+\t}\n+\n+\tbuf->cmd->media_operation_class = CXL_MEDIA_OP_CLASS_SANITIZE;\n+\tbuf->cmd->media_operation_subclass = CXL_MEDIA_OP_SUBC_ZERO;\n+\tbuf->cmd->reserved = 0;\n+\tbuf->cmd->dpa_range_count = cpu_to_le32(count);\n+\n+\tmbox_cmd = (struct cxl_mbox_cmd) {\n+\t\t.opcode = CXL_MEDIA_OP_OPCODE,\n+\t\t.payload_in = buf->cmd,\n+\t\t.size_in = struct_size(buf->cmd, ranges, count),\n+\t\t.poll_interval_ms = 1000,\n+\t\t.poll_count = 30,\n+\t};\n+\n+\trc = cxl_internal_send_cmd(ctx->mbox, &mbox_cmd);\n+\tif (rc) {\n+\t\tdev_warn(ctx->dev,\n+\t\t\t \"media ops zero CCI command failed: %d\\n\", rc);\n+\n+\t\t/* Zero all folios inline on failure */\n+\t\tfor (i = 0; i < count; i++)\n+\t\t\tfolio_zero_range(buf->folios[i], 0,\n+\t\t\t\t\t folio_size(buf->folios[i]));\n+\t}\n+\n+release:\n+\tfor (i = 0; i < count; i++)\n+\t\tfolio_put(buf->folios[i]);\n+\n+\tcxl_flush_buf_reset(buf);\n+}\n+\n+static int cxl_compression_flush_cb(struct folio *folio, void *private)\n+{\n+\tstruct cxl_flush_ctx *ctx = private;\n+\tunsigned long pfn = folio_pfn(folio);\n+\tunsigned long idx = pfn - ctx->base_pfn;\n+\tunsigned long nr = folio_nr_pages(folio);\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tstruct cxl_flush_buf *buf;\n+\tunsigned long flags;\n+\tunsigned int pos;\n+\n+\t/* Case (a): flush record bit set, resolution from our media op */\n+\tif (flush_record_test_and_clear(ctx->flush_record, idx))\n+\t\treturn 0;\n+\n+\tdev_dbg_ratelimited(ctx->dev,\n+\t\t\t     \"flush_cb: folio pfn=%lx order=%u idx=%lu cpu=%d\\n\",\n+\t\t\t     pfn, folio_order(folio), idx,\n+\t\t\t     raw_smp_processor_id());\n+\n+\tlocal_irq_save(flags);\n+\trcu_read_lock();\n+\n+\tpcpu = this_cpu_ptr(ctx->pcpu);\n+\tbuf = rcu_dereference(pcpu->active);\n+\n+\tif (unlikely(!buf || buf->count >= buf->max)) {\n+\t\trcu_read_unlock();\n+\t\tlocal_irq_restore(flags);\n+\t\tif (buf)\n+\t\t\tschedule_work_on(raw_smp_processor_id(),\n+\t\t\t\t\t &pcpu->overflow_work);\n+\t\treturn 2;\n+\t}\n+\n+\t/* Case (b): write DPA range directly into pre-formatted CCI buffer */\n+\tfolio_get(folio);\n+\tflush_record_set(ctx->flush_record, idx);\n+\n+\tpos = buf->count;\n+\tbuf->folios[pos] = folio;\n+\tbuf->cmd->ranges[pos].starting_dpa = cpu_to_le64((u64)idx * PAGE_SIZE);\n+\tbuf->cmd->ranges[pos].length = cpu_to_le64((u64)nr * PAGE_SIZE);\n+\tbuf->count = pos + 1;\n+\n+\trcu_read_unlock();\n+\tlocal_irq_restore(flags);\n+\n+\treturn 1;\n+}\n+\n+static int cxl_flush_kthread_fn(void *data)\n+{\n+\tstruct cxl_flush_ctx *ctx = data;\n+\tstruct cxl_flush_buf *dirty;\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tint cpu;\n+\tbool any_dirty;\n+\n+\twhile (!kthread_should_stop()) {\n+\t\tany_dirty = false;\n+\n+\t\t/* Phase 1: Swap all per-CPU buffers */\n+\t\tfor_each_possible_cpu(cpu) {\n+\t\t\tstruct cxl_flush_buf *spare = ctx->kthread_spares[cpu];\n+\n+\t\t\tif (!spare)\n+\t\t\t\tcontinue;\n+\n+\t\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\t\t\tcxl_flush_buf_reset(spare);\n+\t\t\tdirty = rcu_replace_pointer(pcpu->active, spare, true);\n+\t\t\tctx->kthread_spares[cpu] = dirty;\n+\n+\t\t\tif (dirty && dirty->count > 0) {\n+\t\t\t\tdev_dbg(ctx->dev,\n+\t\t\t\t\t \"flush_kthread: cpu=%d has %u dirty ranges\\n\",\n+\t\t\t\t\t cpu, dirty->count);\n+\t\t\t\tany_dirty = true;\n+\t\t\t}\n+\t\t}\n+\n+\t\tif (!any_dirty)\n+\t\t\tgoto sleep;\n+\n+\t\t/* Phase 2: Single synchronize_rcu for all swaps */\n+\t\tsynchronize_rcu();\n+\n+\t\t/* Phase 3: Send CCI commands for dirty buffers */\n+\t\tfor_each_possible_cpu(cpu) {\n+\t\t\tdirty = ctx->kthread_spares[cpu];\n+\t\t\tif (dirty && dirty->count > 0)\n+\t\t\t\tcxl_flush_buf_send(ctx, dirty);\n+\t\t\t/* dirty is now clean, stays as kthread_spares[cpu] */\n+\t\t}\n+\n+sleep:\n+\t\tschedule_timeout_interruptible(\n+\t\t\tmsecs_to_jiffies(flush_interval_ms));\n+\t}\n+\n+\treturn 0;\n+}\n+\n+static void cxl_flush_overflow_work(struct work_struct *work)\n+{\n+\tstruct cxl_pcpu_flush *pcpu =\n+\t\tcontainer_of(work, struct cxl_pcpu_flush, overflow_work);\n+\tstruct cxl_flush_ctx *ctx = pcpu->ctx;\n+\tstruct cxl_flush_buf *dirty, *spare;\n+\tunsigned long flags;\n+\n+\tdev_dbg(ctx->dev, \"flush_overflow: cpu=%d buffer full, flushing\\n\",\n+\t\t raw_smp_processor_id());\n+\n+\tspare = pcpu->overflow_spare;\n+\tif (!spare)\n+\t\treturn;\n+\n+\tcxl_flush_buf_reset(spare);\n+\n+\tlocal_irq_save(flags);\n+\tdirty = rcu_replace_pointer(pcpu->active, spare, true);\n+\tlocal_irq_restore(flags);\n+\n+\tpcpu->overflow_spare = dirty;\n+\n+\tsynchronize_rcu();\n+\tcxl_flush_buf_send(ctx, dirty);\n+}\n+\n+struct cxl_teardown_ctx {\n+\tstruct cxl_flush_ctx *flush_ctx;\n+\tstruct cxl_sysram *sysram;\n+\tint nid;\n+};\n+\n+static void cxl_compression_pre_teardown(void *data)\n+{\n+\tstruct cxl_teardown_ctx *tctx = data;\n+\n+\tif (!tctx->flush_ctx)\n+\t\treturn;\n+\n+\t/*\n+\t * Unregister the CRAM node before memory goes offline.\n+\t * node_private_clear_ops requires the node_private to still\n+\t * exist, which is destroyed during memory removal.\n+\t */\n+\tcram_unregister_private_node(tctx->nid);\n+\n+\t/*\n+\t * Offline and remove CXL memory with retry.  CXL compressed\n+\t * memory may have pages pinned by in-flight flush operations;\n+\t * keep retrying until they complete.  Once done, sysram->res\n+\t * is NULL so the devm sysram_unregister action that follows\n+\t * will skip the hotplug removal.\n+\t */\n+\tif (tctx->sysram) {\n+\t\tint rc, retries = 0;\n+\n+\t\twhile (true) {\n+\t\t\trc = cxl_sysram_offline_and_remove(tctx->sysram);\n+\t\t\tif (!rc)\n+\t\t\t\tbreak;\n+\t\t\tif (++retries > 60) {\n+\t\t\t\tpr_err(\"cxl_compression: memory offline failed after %d retries, giving up\\n\",\n+\t\t\t\t       retries);\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t\tpr_info(\"cxl_compression: memory offline failed (%d), retrying...\\n\",\n+\t\t\t\trc);\n+\t\t\tmsleep(1000);\n+\t\t}\n+\t}\n+}\n+\n+static void cxl_compression_post_teardown(void *data)\n+{\n+\tstruct cxl_teardown_ctx *tctx = data;\n+\tstruct cxl_flush_ctx *ctx = tctx->flush_ctx;\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tstruct cxl_flush_buf *buf;\n+\tint cpu;\n+\n+\tif (!ctx)\n+\t\treturn;\n+\n+\t/* cram_unregister_private_node already called in pre_teardown */\n+\n+\tif (ctx->flush_thread) {\n+\t\tkthread_stop(ctx->flush_thread);\n+\t\tctx->flush_thread = NULL;\n+\t}\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\t\tcancel_work_sync(&pcpu->overflow_work);\n+\t}\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\n+\t\tbuf = rcu_dereference_raw(pcpu->active);\n+\t\tif (buf && buf->count > 0)\n+\t\t\tcxl_flush_buf_send(ctx, buf);\n+\n+\t\tif (pcpu->overflow_spare && pcpu->overflow_spare->count > 0)\n+\t\t\tcxl_flush_buf_send(ctx, pcpu->overflow_spare);\n+\n+\t\tif (ctx->kthread_spares && ctx->kthread_spares[cpu]) {\n+\t\t\tbuf = ctx->kthread_spares[cpu];\n+\t\t\tif (buf->count > 0)\n+\t\t\t\tcxl_flush_buf_send(ctx, buf);\n+\t\t}\n+\t}\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcpu = per_cpu_ptr(ctx->pcpu, cpu);\n+\n+\t\tbuf = rcu_dereference_raw(pcpu->active);\n+\t\tcxl_flush_buf_free(buf);\n+\n+\t\tcxl_flush_buf_free(pcpu->overflow_spare);\n+\n+\t\tif (ctx->kthread_spares)\n+\t\t\tcxl_flush_buf_free(ctx->kthread_spares[cpu]);\n+\t}\n+\n+\tkfree(ctx->kthread_spares);\n+\tfree_percpu(ctx->pcpu);\n+\tflush_record_free(ctx->flush_record, ctx->flush_record_pages);\n+}\n+\n+/**\n+ * struct cxl_compression_ctx - Per-device context for compression driver\n+ * @mbox: CXL mailbox for issuing CCI commands\n+ * @pdev: PCI device\n+ * @flush_ctx: Flush context for deferred page reclamation\n+ * @tctx: Teardown context for devm actions\n+ * @sysram: Sysram device for offline+remove in remove path\n+ * @nid: NUMA node ID, NUMA_NO_NODE if unset\n+ * @cxlmd: The memdev associated with this context\n+ * @cxlr: Region created by this driver (NULL if pre-existing)\n+ * @cxled: Endpoint decoder with DPA allocated by this driver\n+ * @regions_converted: Number of regions successfully converted\n+ * @media_ops_supported: Device supports media operations zero (0x4402)\n+ */\n+struct cxl_compression_ctx {\n+\tstruct cxl_mailbox *mbox;\n+\tstruct pci_dev *pdev;\n+\tstruct cxl_flush_ctx *flush_ctx;\n+\tstruct cxl_teardown_ctx *tctx;\n+\tstruct cxl_sysram *sysram;\n+\tint nid;\n+\tstruct cxl_memdev *cxlmd;\n+\tstruct cxl_region *cxlr;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tint regions_converted;\n+\tbool media_ops_supported;\n+};\n+\n+/*\n+ * Probe whether the device supports Media Operations Zero (0x4402).\n+ * Send a zero-count command, a conforming device returns SUCCESS,\n+ * a device that doesn't support it returns UNSUPPORTED (-ENXIO).\n+ */\n+static bool cxl_probe_media_ops_zero(struct cxl_mailbox *mbox,\n+\t\t\t\t     struct device *dev)\n+{\n+\tstruct cxl_media_op_input probe = {\n+\t\t.media_operation_class = CXL_MEDIA_OP_CLASS_SANITIZE,\n+\t\t.media_operation_subclass = CXL_MEDIA_OP_SUBC_ZERO,\n+\t\t.dpa_range_count = 0,\n+\t};\n+\tstruct cxl_mbox_cmd cmd = {\n+\t\t.opcode = CXL_MEDIA_OP_OPCODE,\n+\t\t.payload_in = &probe,\n+\t\t.size_in = sizeof(probe),\n+\t};\n+\tint rc;\n+\n+\trc = cxl_internal_send_cmd(mbox, &cmd);\n+\tif (rc) {\n+\t\tdev_info(dev,\n+\t\t\t \"media operations zero not supported (rc=%d), using inline zeroing\\n\",\n+\t\t\t rc);\n+\t\treturn false;\n+\t}\n+\n+\tdev_info(dev, \"media operations zero (0x4402) supported\\n\");\n+\treturn true;\n+}\n+\n+struct cxl_compression_wm_ctx {\n+\tstruct device *dev;\n+\tint nid;\n+};\n+\n+static irqreturn_t cxl_compression_lthresh_irq(int irq, void *data)\n+{\n+\tstruct cxl_compression_wm_ctx *wm = data;\n+\n+\tdev_info(wm->dev, \"lthresh watermark: pressuring node %d\\n\", wm->nid);\n+\tcram_set_pressure(wm->nid, CRAM_PRESSURE_MAX);\n+\treturn IRQ_HANDLED;\n+}\n+\n+static irqreturn_t cxl_compression_hthresh_irq(int irq, void *data)\n+{\n+\tstruct cxl_compression_wm_ctx *wm = data;\n+\n+\tdev_info(wm->dev, \"hthresh watermark: resuming node %d\\n\", wm->nid);\n+\tcram_set_pressure(wm->nid, 0);\n+\treturn IRQ_HANDLED;\n+}\n+\n+static int convert_region_to_sysram(struct cxl_region *cxlr,\n+\t\t\t\t    struct pci_dev *pdev)\n+{\n+\tstruct cxl_compression_ctx *comp_ctx = pdev_to_comp_ctx(pdev);\n+\tstruct device *dev = cxl_region_dev(cxlr);\n+\tstruct cxl_compression_wm_ctx *wm_ctx;\n+\tstruct cxl_teardown_ctx *tctx;\n+\tstruct cxl_flush_ctx *flush_ctx;\n+\tstruct cxl_pcpu_flush *pcpu;\n+\tresource_size_t region_start, region_size;\n+\tstruct range hpa_range;\n+\tint nid;\n+\tint irq;\n+\tint cpu;\n+\tint rc;\n+\n+\tif (cxl_region_mode(cxlr) != CXL_PARTMODE_RAM) {\n+\t\tdev_dbg(dev, \"skipping non-RAM region (mode=%d)\\n\",\n+\t\t\tcxl_region_mode(cxlr));\n+\t\treturn 0;\n+\t}\n+\n+\tdev_info(dev, \"converting region to sysram\\n\");\n+\n+\trc = devm_cxl_add_sysram(cxlr, true, MMOP_ONLINE_MOVABLE);\n+\tif (rc) {\n+\t\tdev_err(dev, \"failed to add sysram region: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\ttctx = devm_kzalloc(dev, sizeof(*tctx), GFP_KERNEL);\n+\tif (!tctx)\n+\t\treturn -ENOMEM;\n+\n+\trc = devm_add_action_or_reset(dev, cxl_compression_post_teardown, tctx);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\t/* Find the sysram child device for pre_teardown */\n+\tcomp_ctx->sysram = cxl_region_find_sysram(cxlr);\n+\tif (comp_ctx->sysram)\n+\t\ttctx->sysram = comp_ctx->sysram;\n+\n+\trc = cxl_get_region_range(cxlr, &hpa_range);\n+\tif (rc) {\n+\t\tdev_err(dev, \"failed to get region range: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\tnid = phys_to_target_node(hpa_range.start);\n+\tif (nid == NUMA_NO_NODE)\n+\t\tnid = memory_add_physaddr_to_nid(hpa_range.start);\n+\n+\tregion_start = hpa_range.start;\n+\tregion_size = range_len(&hpa_range);\n+\n+\tflush_ctx = devm_kzalloc(dev, sizeof(*flush_ctx), GFP_KERNEL);\n+\tif (!flush_ctx)\n+\t\treturn -ENOMEM;\n+\n+\tflush_ctx->base_pfn = PHYS_PFN(region_start);\n+\tflush_ctx->nr_pages = region_size >> PAGE_SHIFT;\n+\tflush_ctx->flush_record = flush_record_alloc(flush_ctx->nr_pages,\n+\t\t\t\t\t\t     &flush_ctx->flush_record_pages);\n+\tif (!flush_ctx->flush_record)\n+\t\treturn -ENOMEM;\n+\n+\tflush_ctx->mbox = comp_ctx->mbox;\n+\tflush_ctx->dev = dev;\n+\tflush_ctx->nid = nid;\n+\tflush_ctx->media_ops_supported = comp_ctx->media_ops_supported;\n+\n+\t/*\n+\t * Cap buffer at max DPA ranges that fit in one CCI payload.\n+\t * Header is 8 bytes (struct cxl_media_op_input), each range\n+\t * is 16 bytes (struct cxl_dpa_range).  The module parameter\n+\t * flush_buf_size can further limit this (0 = use hw max).\n+\t */\n+\tflush_ctx->buf_max = (flush_ctx->mbox->payload_size -\n+\t\t\t      sizeof(struct cxl_media_op_input)) /\n+\t\t\t     sizeof(struct cxl_dpa_range);\n+\tif (flush_buf_size && flush_buf_size < flush_ctx->buf_max)\n+\t\tflush_ctx->buf_max = flush_buf_size;\n+\tif (flush_ctx->buf_max == 0)\n+\t\tflush_ctx->buf_max = 1;\n+\n+\tdev_info(dev,\n+\t\t \"flush buffer: %u DPA ranges per command (payload %zu bytes, media_ops %s)\\n\",\n+\t\t flush_ctx->buf_max, flush_ctx->mbox->payload_size,\n+\t\t flush_ctx->media_ops_supported ? \"yes\" : \"no\");\n+\n+\tflush_ctx->pcpu = alloc_percpu(struct cxl_pcpu_flush);\n+\tif (!flush_ctx->pcpu)\n+\t\treturn -ENOMEM;\n+\n+\tflush_ctx->kthread_spares = kcalloc(nr_cpu_ids,\n+\t\t\t\t\t    sizeof(struct cxl_flush_buf *),\n+\t\t\t\t\t    GFP_KERNEL);\n+\tif (!flush_ctx->kthread_spares)\n+\t\tgoto err_pcpu_init;\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tstruct cxl_flush_buf *active_buf, *overflow_buf, *spare_buf;\n+\n+\t\tactive_buf = cxl_flush_buf_alloc(flush_ctx->buf_max, nid);\n+\t\tif (!active_buf)\n+\t\t\tgoto err_pcpu_init;\n+\n+\t\toverflow_buf = cxl_flush_buf_alloc(flush_ctx->buf_max, nid);\n+\t\tif (!overflow_buf) {\n+\t\t\tcxl_flush_buf_free(active_buf);\n+\t\t\tgoto err_pcpu_init;\n+\t\t}\n+\n+\t\tspare_buf = cxl_flush_buf_alloc(flush_ctx->buf_max, nid);\n+\t\tif (!spare_buf) {\n+\t\t\tcxl_flush_buf_free(active_buf);\n+\t\t\tcxl_flush_buf_free(overflow_buf);\n+\t\t\tgoto err_pcpu_init;\n+\t\t}\n+\n+\t\tpcpu = per_cpu_ptr(flush_ctx->pcpu, cpu);\n+\t\tpcpu->ctx = flush_ctx;\n+\t\trcu_assign_pointer(pcpu->active, active_buf);\n+\t\tpcpu->overflow_spare = overflow_buf;\n+\t\tINIT_WORK(&pcpu->overflow_work, cxl_flush_overflow_work);\n+\n+\t\tflush_ctx->kthread_spares[cpu] = spare_buf;\n+\t}\n+\n+\tflush_ctx->flush_thread = kthread_create_on_node(\n+\t\tcxl_flush_kthread_fn, flush_ctx, nid, \"cxl-flush/%d\", nid);\n+\tif (IS_ERR(flush_ctx->flush_thread)) {\n+\t\trc = PTR_ERR(flush_ctx->flush_thread);\n+\t\tflush_ctx->flush_thread = NULL;\n+\t\tgoto err_pcpu_init;\n+\t}\n+\twake_up_process(flush_ctx->flush_thread);\n+\n+\trc = cram_register_private_node(nid, cxlr,\n+\t\t\t\t\tcxl_compression_flush_cb, flush_ctx);\n+\tif (rc) {\n+\t\tdev_err(dev, \"failed to register cram node %d: %d\\n\", nid, rc);\n+\t\tgoto err_pcpu_init;\n+\t}\n+\n+\ttctx->flush_ctx = flush_ctx;\n+\ttctx->nid = nid;\n+\n+\trc = devm_add_action_or_reset(dev, cxl_compression_pre_teardown, tctx);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tcomp_ctx->flush_ctx = flush_ctx;\n+\tcomp_ctx->tctx = tctx;\n+\tcomp_ctx->nid = nid;\n+\n+\t/*\n+\t * Register watermark IRQ handlers on &pdev->dev for\n+\t * MSI-X vector 12 (lthresh) and vector 13 (hthresh).\n+\t */\n+\twm_ctx = devm_kzalloc(&pdev->dev, sizeof(*wm_ctx), GFP_KERNEL);\n+\tif (!wm_ctx)\n+\t\treturn -ENOMEM;\n+\n+\twm_ctx->dev = &pdev->dev;\n+\twm_ctx->nid = nid;\n+\n+\tirq = pci_irq_vector(pdev, CXL_CT3_MSIX_LTHRESH);\n+\tif (irq >= 0) {\n+\t\trc = devm_request_threaded_irq(&pdev->dev, irq, NULL,\n+\t\t\t\t\t       cxl_compression_lthresh_irq,\n+\t\t\t\t\t       IRQF_ONESHOT,\n+\t\t\t\t\t       \"cxl-lthresh\", wm_ctx);\n+\t\tif (rc)\n+\t\t\tdev_warn(&pdev->dev,\n+\t\t\t\t \"failed to register lthresh IRQ: %d\\n\", rc);\n+\t}\n+\n+\tirq = pci_irq_vector(pdev, CXL_CT3_MSIX_HTHRESH);\n+\tif (irq >= 0) {\n+\t\trc = devm_request_threaded_irq(&pdev->dev, irq, NULL,\n+\t\t\t\t\t       cxl_compression_hthresh_irq,\n+\t\t\t\t\t       IRQF_ONESHOT,\n+\t\t\t\t\t       \"cxl-hthresh\", wm_ctx);\n+\t\tif (rc)\n+\t\t\tdev_warn(&pdev->dev,\n+\t\t\t\t \"failed to register hthresh IRQ: %d\\n\", rc);\n+\t}\n+\n+\treturn 0;\n+\n+err_pcpu_init:\n+\tif (flush_ctx->flush_thread)\n+\t\tkthread_stop(flush_ctx->flush_thread);\n+\tfor_each_possible_cpu(cpu) {\n+\t\tstruct cxl_flush_buf *buf;\n+\n+\t\tpcpu = per_cpu_ptr(flush_ctx->pcpu, cpu);\n+\n+\t\tbuf = rcu_dereference_raw(pcpu->active);\n+\t\tcxl_flush_buf_free(buf);\n+\n+\t\tcxl_flush_buf_free(pcpu->overflow_spare);\n+\n+\t\tif (flush_ctx->kthread_spares)\n+\t\t\tcxl_flush_buf_free(flush_ctx->kthread_spares[cpu]);\n+\t}\n+\tkfree(flush_ctx->kthread_spares);\n+\tfree_percpu(flush_ctx->pcpu);\n+\tflush_record_free(flush_ctx->flush_record, flush_ctx->flush_record_pages);\n+\treturn rc ? rc : -ENOMEM;\n+}\n+\n+static struct cxl_region *create_ram_region(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_root_decoder *cxlrd;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tstruct cxl_region *cxlr;\n+\tresource_size_t ram_size, avail;\n+\n+\tram_size = cxl_ram_size(cxlmd->cxlds);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev, \"no RAM capacity available\\n\");\n+\t\treturn ERR_PTR(-ENODEV);\n+\t}\n+\n+\tram_size = ALIGN_DOWN(ram_size, SZ_256M);\n+\tif (ram_size == 0) {\n+\t\tdev_info(&cxlmd->dev, \"RAM capacity too small (< 256M)\\n\");\n+\t\treturn ERR_PTR(-ENOSPC);\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"creating RAM region for %lld MB\\n\",\n+\t\t ram_size >> 20);\n+\n+\tcxlrd = cxl_get_hpa_freespace(cxlmd, ram_size, &avail);\n+\tif (IS_ERR(cxlrd)) {\n+\t\tdev_err(&cxlmd->dev, \"no HPA freespace: %ld\\n\",\n+\t\t\tPTR_ERR(cxlrd));\n+\t\treturn ERR_CAST(cxlrd);\n+\t}\n+\n+\tcxled = cxl_request_dpa(cxlmd, CXL_PARTMODE_RAM, ram_size);\n+\tif (IS_ERR(cxled)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to request DPA: %ld\\n\",\n+\t\t\tPTR_ERR(cxled));\n+\t\tcxl_put_root_decoder(cxlrd);\n+\t\treturn ERR_CAST(cxled);\n+\t}\n+\n+\tcxlr = cxl_create_region(cxlrd, &cxled, 1);\n+\tcxl_put_root_decoder(cxlrd);\n+\tif (IS_ERR(cxlr)) {\n+\t\tdev_err(&cxlmd->dev, \"failed to create region: %ld\\n\",\n+\t\t\tPTR_ERR(cxlr));\n+\t\tcxl_dpa_free(cxled);\n+\t\treturn cxlr;\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"created region %s\\n\",\n+\t\t dev_name(cxl_region_dev(cxlr)));\n+\tpdev_to_comp_ctx(to_pci_dev(cxlmd->dev.parent))->cxled = cxled;\n+\treturn cxlr;\n+}\n+\n+static int cxl_compression_attach_probe(struct cxl_memdev *cxlmd)\n+{\n+\tstruct pci_dev *pdev = to_pci_dev(cxlmd->dev.parent);\n+\tstruct cxl_compression_ctx *comp_ctx = pdev_to_comp_ctx(pdev);\n+\tstruct cxl_region *regions[8];\n+\tstruct cxl_region *cxlr;\n+\tint nr, i, converted = 0, errors = 0;\n+\tint rc;\n+\n+\tcomp_ctx->cxlmd = cxlmd;\n+\tcomp_ctx->mbox = &cxlmd->cxlds->cxl_mbox;\n+\n+\t/* Probe device for media operations zero support */\n+\tcomp_ctx->media_ops_supported =\n+\t\tcxl_probe_media_ops_zero(comp_ctx->mbox,\n+\t\t\t\t\t &cxlmd->dev);\n+\n+\tdev_info(&cxlmd->dev, \"compression attach: looking for regions\\n\");\n+\n+\tnr = cxl_get_committed_regions(cxlmd, regions, ARRAY_SIZE(regions));\n+\tfor (i = 0; i < nr; i++) {\n+\t\tif (cxl_region_mode(regions[i]) == CXL_PARTMODE_RAM) {\n+\t\t\trc = convert_region_to_sysram(regions[i], pdev);\n+\t\t\tif (rc)\n+\t\t\t\terrors++;\n+\t\t\telse\n+\t\t\t\tconverted++;\n+\t\t}\n+\t\tput_device(cxl_region_dev(regions[i]));\n+\t}\n+\n+\tif (converted > 0) {\n+\t\tdev_info(&cxlmd->dev,\n+\t\t\t \"converted %d regions to sysram (%d errors)\\n\",\n+\t\t\t converted, errors);\n+\t\treturn errors ? -EIO : 0;\n+\t}\n+\n+\tdev_info(&cxlmd->dev, \"no existing regions, creating RAM region\\n\");\n+\n+\tcxlr = create_ram_region(cxlmd);\n+\tif (IS_ERR(cxlr)) {\n+\t\trc = PTR_ERR(cxlr);\n+\t\tif (rc == -ENODEV) {\n+\t\t\tdev_info(&cxlmd->dev,\n+\t\t\t\t \"could not create RAM region: %d\\n\", rc);\n+\t\t\treturn 0;\n+\t\t}\n+\t\treturn rc;\n+\t}\n+\n+\trc = convert_region_to_sysram(cxlr, pdev);\n+\tif (rc) {\n+\t\tdev_err(&cxlmd->dev,\n+\t\t\t\"failed to convert region to sysram: %d\\n\", rc);\n+\t\treturn rc;\n+\t}\n+\n+\tcomp_ctx->cxlr = cxlr;\n+\n+\tdev_info(&cxlmd->dev, \"created and converted region %s to sysram\\n\",\n+\t\t dev_name(cxl_region_dev(cxlr)));\n+\n+\treturn 0;\n+}\n+\n+static const struct cxl_memdev_attach cxl_compression_attach = {\n+\t.probe = cxl_compression_attach_probe,\n+};\n+\n+static int cxl_compression_probe(struct pci_dev *pdev,\n+\t\t\t\t const struct pci_device_id *id)\n+{\n+\tstruct cxl_compression_ctx *comp_ctx;\n+\tstruct cxl_memdev *cxlmd;\n+\tint rc;\n+\n+\tdev_info(&pdev->dev, \"cxl_compression: probing device\\n\");\n+\n+\tcomp_ctx = devm_kzalloc(&pdev->dev, sizeof(*comp_ctx), GFP_KERNEL);\n+\tif (!comp_ctx)\n+\t\treturn -ENOMEM;\n+\tcomp_ctx->nid = NUMA_NO_NODE;\n+\tcomp_ctx->pdev = pdev;\n+\n+\trc = xa_insert(&comp_ctx_xa, (unsigned long)pdev, comp_ctx, GFP_KERNEL);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tcxlmd = cxl_pci_type3_probe_init(pdev, &cxl_compression_attach);\n+\tif (IS_ERR(cxlmd)) {\n+\t\txa_erase(&comp_ctx_xa, (unsigned long)pdev);\n+\t\treturn PTR_ERR(cxlmd);\n+\t}\n+\n+\tcomp_ctx->cxlmd = cxlmd;\n+\tcomp_ctx->mbox = &cxlmd->cxlds->cxl_mbox;\n+\n+\tdev_info(&pdev->dev, \"cxl_compression: probe complete\\n\");\n+\treturn 0;\n+}\n+\n+static void cxl_compression_remove(struct pci_dev *pdev)\n+{\n+\tstruct cxl_compression_ctx *comp_ctx = xa_erase(&comp_ctx_xa,\n+\t\t\t\t\t\t\t(unsigned long)pdev);\n+\n+\tdev_info(&pdev->dev, \"cxl_compression: removing device\\n\");\n+\n+\tif (!comp_ctx || comp_ctx->nid == NUMA_NO_NODE)\n+\t\treturn;\n+\n+\t/*\n+\t * Destroy the region, devm actions on the region device handle teardown\n+\t * in registration-reverse order:\n+\t *   1. pre_teardown:  cram_unregister + retry-forever memory offline\n+\t *   2. sysram_unregister: device_unregister (sysram->res is NULL\n+\t *      after pre_teardown, so cxl_sysram_release skips hotplug)\n+\t *   3. post_teardown: kthread stop, flush cleanup\n+\t *\n+\t * PCI MMIO is still live so CCI commands in post_teardown work.\n+\t */\n+\tif (comp_ctx->cxlr) {\n+\t\tcxl_destroy_region(comp_ctx->cxlr);\n+\t\tcomp_ctx->cxlr = NULL;\n+\t}\n+\n+\tif (comp_ctx->cxled) {\n+\t\tcxl_dpa_free(comp_ctx->cxled);\n+\t\tcomp_ctx->cxled = NULL;\n+\t}\n+}\n+\n+static const struct pci_device_id cxl_compression_pci_tbl[] = {\n+\t{ PCI_DEVICE(PCI_VENDOR_ID_INTEL, 0x0d93) },\n+\t{ /* terminate list */ },\n+};\n+MODULE_DEVICE_TABLE(pci, cxl_compression_pci_tbl);\n+\n+static struct pci_driver cxl_compression_driver = {\n+\t.name\t\t= KBUILD_MODNAME,\n+\t.id_table\t= cxl_compression_pci_tbl,\n+\t.probe\t\t= cxl_compression_probe,\n+\t.remove\t\t= cxl_compression_remove,\n+\t.driver\t= {\n+\t\t.probe_type\t= PROBE_PREFER_ASYNCHRONOUS,\n+\t},\n+};\n+\n+module_pci_driver(cxl_compression_driver);\n+\n+MODULE_DESCRIPTION(\"CXL: Compression Memory Driver with SysRAM regions\");\n+MODULE_LICENSE(\"GPL v2\");\n+MODULE_IMPORT_NS(\"CXL\");\n-- \n2.53.0",
              "reply_to": "",
              "message_date": "2026-02-22",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "David (Arm)",
              "summary": "Reviewer David expressed concern about adding special-casing for private memory nodes, similar to ZONE_DEVICE, and suggested discussing the topic further.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "concern",
                "special-casing"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "I'm concerned about adding more special-casing (similar to what we \nalready added for ZONE_DEVICE) all over the place.\n\nLike the whole folio_managed_() stuff in mprotect.c\n\nHaving that said, sounds like a reasonable topic to discuss.\n\n-- \nCheers,\n\nDavid",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged a concern about the semantics of zone_device hooks and proposed two alternative solutions: reusing vma_wants_writenotify() or adding a new vma flag to track protected/device pages.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a concern",
                "proposed alternative solutions"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "It's a valid concern - and is why I tried to re-use as many of the\nzone_device hooks as possible.  It does not seem zone_device has quite\nthe same semantics for a case like this, so I had to make something new.\n\nDEVICE_COHERENT injects a temporary swap entry to allow the device to do\na large atomic operation - then the page table is restored and the CPU\nis free to change entries as it pleases.\n\nAnother option would be to add the hook to vma_wants_writenotify()\ninstead of the page table code - and mask MM_CP_TRY_CHANGE_WRITABLE.\n\nThis would require adding a vma flag - or maybe a count of protected /\ndevice pages.\n\nint mprotect_fixup() {\n    ...\n    if (vma_wants_manual_pte_write_upgrade(vma))\n        mm_cp_flags |= MM_CP_TRY_CHANGE_WRITABLE;\n}\n\nbool vma_wants_writenotify(struct vm_area_struct *vma, pgprot_t vm_page_prot)\n{\n    if (vma->managed_wrprotect)\n        return true;\n}\n\nThat would localize the change in folio_managed_fixup_migration_pte() :\n\nstatic inline pte_t folio_managed_fixup_migration_pte(struct page *new,\n                                                      pte_t pte,\n                                                      pte_t old_pte,\n                                                      struct vm_area_struct *vma)\n{\n    ...\n    } else if (folio_managed_wrprotect(page_folio(new))) {\n        pte = pte_wrprotect(pte);\n+       atomic_inc(&vma->managed_wrprotect);\n    }\n    return pte;\n}\n\nThis would cover both the huge_memory.c and mprotect, and maybe that's\njust generally cleaner? I can try that to see if it actually works.\n\n~Gregory",
              "reply_to": "David (Arm)",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author acknowledged that existing hooks can be used for write protection and agreed to remove redundant code from page table walks.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged existing solution",
                "agreed to simplify"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "scratch all this - existing hooks exist for exactly this purpose:\n\n\tcan_change_[pte|pmd]_writable()\n\nSurprised I missed this.\n\nI can clean this up to remove it from the page table walks.\n\nStill valid to question whether we want this, but at least the hook\nlives with other write-protect hooks now.\n\n~Gregory",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Alistair Popple",
              "summary": "Reviewer Alistair Popple expressed concerns that N_MEMORY_PRIVATE may not be the best solution for reusing the existing mm buddy allocator and suggested considering alternative approaches such as adapting DRM's standalone buddy allocator, while also noting that device memory exposure to userspace is an interesting aspect of the series.\n\nThe reviewer agrees that the patch provides a standard interface to userspace for managing device memory and suggests using the existing NUMA APIs as a reasonable approach.\n\nReviewer Alistair Popple noted that the proposed cxl_compression driver is similar to ZONE_DEVICE and questioned why it cannot be extended instead of duplicating code, pointing out a potential lock ordering issue with reclaim paths when using pgmap for ZONE_DEVICE pages. He suggested exploring alternative storage options such as page_ext or considering the future replacement of struct page with folios.\n\nReviewer suggested that the cxl_compression PCI driver is similar to existing ZONE_DEVICE methods and proposed building on those instead of introducing a new feature set.\n\nReviewer Alistair Popple noted that the implementation duplicates a lot of hooks, similar to those provided by ZONE_DEVICE, and requested further discussion on this aspect.\n\nReviewer questioned whether allocation must be handled by the mm allocator, suggesting that a device allocator library could be written or reused from drm_buddy.c\n\nThe reviewer questioned the characterization of ZONE_DEVICE pages as not being real struct pages, suggesting that perspective on this may vary depending on one's role in the mm subsystem, and asked for clarification on what limitations are actually being addressed.\n\nReviewer suggested that ZONE_DEVICE_COHERENT could be extended to support the use case, proposing a couple of extra dev_pagemap_ops and LRU access as a potential solution.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "alternative solutions",
                "agreement",
                "endorsement",
                "suggested alternatives",
                "duplicates hooks",
                "similar to ZONE_DEVICE",
                "clarification requested",
                "questioning characterization"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Having had to re-implement entire portions of mm/ in a driver I agree this isn't\nsomething anyone sane should do :-) However aspects of ZONE_DEVICE were added\nprecisely to help with that so I'm not sure N_MEMORY_PRIVATE is the only or best\nway to do that.\n\nBased on our discussion at LPC I believe one of the primary motivators here was\nto re-use the existing mm buddy allocator rather than writing your own. I remain\nto be convinced that alone is justification enough for doing all this - DRM for\nexample already has quite a nice standalone buddy allocator (drm_buddy.c) that\ncould presumably be used, or adapted for use, by any device driver.\n\nThe interesting part of this series (which I have skimmed but not read in\ndetail) is how device memory gets exposed to userspace - this is something that\nexisting ZONE_DEVICE implementations don't address, instead leaving it up to\ndrivers and associated userspace stacks to deal with allocation, migration, etc.\n\n---\n\nThis is I think is one of the key things that should be enabled - providing a\nstandard interface to userspace for managing device memory. The existing NUMA\nAPIs do seem like a reasonable way to do this.\n\n---\n\nOne does not have to squint too hard to see that the above is not so different\nfrom what ZONE_DEVICE provides today via dev_pagemap_ops(). So I think I think\nit would be worth outlining why the existing ZONE_DEVICE mechanism can't be\nextended to provide these kind of services.\n\nThis seems to add a bunch of code just to use NODE_DATA instead of page->pgmap,\nwithout really explaining why just extending dev_pagemap_ops wouldn't work. The\nobvious reason is that if you want to support things like reclaim, compaction,\netc. these pages need to be on the LRU, which is a little bit hard when that\nfield is also used by the pgmap pointer for ZONE_DEVICE pages.\n\nBut it might be good to explore other options for storing the pgmap - for\nexample page_ext could be used.  Or I hear struct page may go away in place of\nfolios any day now, so maybe that gives us space for both :-)\n\n---\n\nThe above also looks pretty similar to the existing ZONE_DEVICE methods for\ndoing this which is another reason to argue for just building up the feature set\nof the existing boondoggle rather than adding another thingymebob.\n\nIt seems the key thing we are looking for is:\n\n1) A userspace API to allocate/manage device memory (ie. move_pages(), mbind(),\netc.)\n\n2) Allowing reclaim/LRU list processing of device memory.\n\n---\n\ndiscussion (hopefully I can make it to LSFMM). Mostly I'm interested in the\nimplementation as this does on the surface seem to sprinkle around and duplicate\na lot of hooks similar to what ZONE_DEVICE already provides.\n\n---\n\nFor basic allocation I agree this is the case. But there's no reason some device\nallocator library couldn't be written. Or in fact as pointed out above reuse the\nalready existing one in drm_buddy.c.  So would be interested to hear arguments\nfor why allocation has to be done by the mm allocator and/or why an allocation\nlibrary wouldn't work here given DRM already has them.\n\n---\n\nZONE_DEVICE pages are in fact real struct pages, but I will concede that\nperspective probably depends on which bits of the mm you play in. The real\nlimitations you seem to be addressing is more around how we get these pages in\nan LRU, or are there other limitations?\n\n---\n\nWhat I'd like to explore is why ZONE_DEVICE_COHERENT couldn't just be extended\nto support your usecase? It seems a couple of extra dev_pagemap_ops and being\nable to go on the LRU would get you there.\n\n - Alistair",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "The author acknowledges that using ZONE_DEVICE is insufficient for N_MEMORY_PRIVATE, as it introduces unnecessary complexity. They propose reusing the buddy allocator instead, which would simplify the implementation and eliminate issues related to zones.\n\nThe author explains that the callback similarity between ZONE_DEVICE and private nodes is intentional, as they require the same set of hooks but with different defaults. They argue that extending ZONE_DEVICE into these areas would be cumbersome and inefficient, and that the current implementation is a more straightforward solution.\n\nAuthor addressed a concern about the per-page pgmap and device-to-node mappings, agreeing that NODE_DATA is the right direction regardless of struct page's future or zone it lives in.\n\nThe author acknowledges that implementing mempolicy support for N_MEMORY_PRIVATE is more complex than initially thought, explaining that it requires adding code to vma_alloc_folio_noprof and dealing with ZONE_DEVICE's overloaded nature. They suggest two options: putting pages in the buddy or adding pgmap->device_alloc() callbacks at every allocation site.\n\nAuthor acknowledged reviewer's concern about reusing mm/ services and explained that using the buddy underpins the rest of these services, making it a more efficient approach.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledges a fix is needed",
                "proposes an alternative solution",
                "acknowledges feedback",
                "provides explanation",
                "agreed with reviewer's suggestion",
                "provided explanation",
                "acknowledges complexity",
                "suggests two options",
                "acknowledged",
                "explained"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "I agree that buddy-access alone is insufficient justification, it\nstarted off that way - but if you want mempolicy/NUMA UAPI access,\nit turns into \"Re-use all of MM\" - and that means using the buddy.\n\nI also expected ZONE_DEVICE vs NODE_DATA to be the primary discussion,\n\nI raise replacing it as a thought experiment, but not the proposal.\n\nThe idea that drm/ is going to switch to private nodes is outside the\nrealm of reality, but part of that is because of years of infrastructure\nbuilt on the assumption that re-using mm/ is infeasible.\n\nBut, lets talk about DEVICE_COHERENT\n\n---\n\nDEVICE_COHERENT is the odd-man out among ZONE_DEVICE modes. The others\nuse softleaf entries and don't allow direct mappings.\n\n(DEVICE_PRIVATE sort of does if you squint, but you can also view that\n a bit like PROT_NONE or read-only controls to force migrations).\n\nIf you take DEVICE_COHERENT and:\n\n- Move pgmap out of the struct page (page_ext, NODE_DATA, etc) to free\n  the LRU list_head\n- Put pages in the buddy (free lists, watermarks, managed_pages) or add\n  pgmap->device_alloc() at every allocation callsite / buddy hook\n- Add LRU support (aging, reclaim, compaction)\n- Add isolated gating (new GFP flag and adjusted zonelist filtering)\n- Add new dev_pagemap_ops callbacks for the various mm/ features\n- Audit evey folio_is_zone_device() to distinguish zone device modes\n\n... you've built N_MEMORY_PRIVATE inside ZONE_DEVICE. Except now\npage_zone(page) returns ZONE_DEVICE - so you inherit the wrong\ndefaults at every existing ZONE_DEVICE check. \n\nSkip-sites become things to opt-out of instead of opting into.\n\nYou just end up with\n\nif (folio_is_zone_device(folio))\n    if (folio_is_my_special_zone_device())\n    else ....\n\nand this just generalizes to\n\nif (folio_is_private_managed(folio))\n    folio_managed_my_hooked_operation()\n\nSo you get the same code, but have added more complexity to ZONE_DEVICE.\n\nI don't think that's needed if we just recognize ZONE is the wrong\nabstraction to be operating on.\n\nHonestly, even ZONE_MOVABLE becomes pointless with N_MEMORY_PRIVATE\nif you disallow longterm pinning - because the managing service handles\nallocations (it has to inject GFP_PRIVATE to get access) or selectively\nenables the mm/ services it knows are safe (mempolicy).\n\nEven if you allow longterm pinning, if your service controls what does\nthe pinning it can still be reclaimable - just manually (killing\nprocesses) instead of letting hotplug do it via migration.\n\nIf your service only allocates movable pages - your ZONE_NORMAL is\neffectively ZONE_MOVABLE.  \n\nIn some cases we use ZONE_MOVABLE to prevent the kernel from allocating\nmemory onto devices (like CXL).  This means struct page is forced to\ntake up DRAM or use memmap_on_memory - meaning you lose high-value\ncapacity or sacrifice contiguity (less huge page support).\n\nThis entire problem can evaporate if you can just use ZONE_NORMAL.\n\nThere are a lot of benefits to just re-using the buddy like this.\n\nZones are the wrong abstraction and cause more problems.\n\n---\n\nYou don't have to squint because it was deliberate :]\n\nThe callback similarity is the feature - they're the same logical\noperations.  The difference is the direction of the defaults.\n\nExtending ZONE_DEVICE into these areas requires the same set of hooks,\nplus distinguishing \"old ZONE_DEVICE\" from \"new ZONE_DEVICE\".\n\nWhere there are new injection sites, it's because ZONE_DEVICE opts\nout of ever touching that code in some other silently implied way.\n\nFor example, reclaim/compaction doesn't run because ZONE_DEVICE doesn't\nadd to managed_pages (among other reasons).\n\nYou'd have to go figure out how to hack those things into ZONE_DEVICE \n*and then* opt every *other* ZONE_DEVICE mode *back out*.\n\nSo you still end up with something like this anyway:\n\nstatic inline bool folio_managed_handle_fault(struct folio *folio,\n                                              struct vm_fault *vmf,\n                                              enum pgtable_level level,\n                                              vm_fault_t *ret)\n{\n        /* Zone device pages use swap entries; handled in do_swap_page */\n        if (folio_is_zone_device(folio))\n                return false;\n\n        if (folio_is_private_node(folio))\n\t\t...\n        return false;\n}\n\n---\n\nIf NUMA is the interface we want, then NODE_DATA is the right direction\nregardless of struct page's future or what zone it lives in.\n\nThere's no reason to keep per-page pgmap w/ device-to-node mappings.\n\nYou can have one driver manage multiple devices with the same numa node\nif it uses the same owner context (PFN already differentiates devices).\n\nThe existing code allows for this.\n\n---\n\nOn (1): ZONE_DEVICE NUMA UAPI is harder than it looks from the surface\n\nMuch of the kernel mm/ infrastructure is written on top of the buddy and\nexpects N_MEMORY to be the sole arbiter of \"Where to Acquire Pages\".\n\nMempolicy depends on:\n   - Buddy support or a new alloc hook around the buddy\n\n   - Migration support (mbind() after allocation migrates)\n     - Migration also deeply assumes buddy and LRU support\n\n   - Changing validations on node states\n     - mempolicy checks N_MEMORY membership, so you have to hack\n       N_MEMORY onto ZONE_DEVICE\n       (or teach it about a new node state... N_MEMORY_PRIVATE)\n\n\nGetting mempolicy to work with N_MEMORY_PRIVATE amounts to adding 2\nlines of code in vma_alloc_folio_noprof:\n\nstruct folio *vma_alloc_folio_noprof(gfp_t gfp, int order,\n                                     struct vm_area_struct *vma,\n\t\t\t\t     unsigned long addr)\n{\n        if (pol->flags & MPOL_F_PRIVATE)\n                gfp |= __GFP_PRIVATE;\n\n        folio = folio_alloc_mpol_noprof(gfp, order, pol, ilx, numa_node_id());\n\t/* Woo! I faulted a DEVICE PAGE! */\n}\n\nBut this requires the pages to be managed by the buddy.\n\nThe rest of the mempolicy support is around keeping sane nodemasks when\nthings like cpuset.mems rebinds occur and validating you don't end up\nwith private nodes that don't support mempolicy in your nodemask.\n\nYou have to do all of this anyway, but with the added bonus of fighting\nwith the overloaded nature of ZONE_DEVICE at every step.\n\n==========\n\nOn (2): Assume you solve LRU. \n\nZone Device has no free lists, managed_pages, or watermarks.\n\nkswapd can't run, compaction has no targets, vmscan's pressure model\ndoesn't function.  These all come for free when the pages are\nbuddy-managed on a real zone.  Why re-invent the wheel?\n\n==========\n\nSo you really have two options here:\n\na) Put pages in the buddy, or\n\nb) Add pgmap->device_alloc() callbacks at every allocation site that\n   could target a node:\n     - vma_alloc_folio\n     - alloc_migration_target\n     - alloc_demote_folio\n     - alloc_pages_node\n     - alloc_contig_pages\n     - list goes on\n\nOr more likely - hooking get_page_from_freelist.  Which at that\npoint... just use the buddy?  You're already deep in the hot path.\n\n---\n\nUsing the buddy underpins the rest of mm/ services we want to re-use.\n\nThat's basically it.  Otherwise you have to inject hooks into every\nsurface that touches the buddy...\n\n... or in the buddy (get_page_from_freelist), at which point why not\njust use the buddy?\n\n~Gregory",
              "reply_to": "Alistair Popple",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price (author)",
              "summary": "Author considered reviewer's suggestion to simplify patch by removing N_MEMORY_PRIVATE and instead checking NODE_DATA(target_nid)->private, agreeing to explore this alternative.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "considering alternative approach",
                "agrees to look at it more"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "This gave me something to chew on\n\nI think this can be done without introducing N_MEMORY_PRIVATE and just\nchecking:   NODE_DATA(target_nid)->private\n\nmeaning these nodes can just be N_MEMORY with the same isolations.\n\nI'll look at this a bit more.\n\n~Gregory",
              "reply_to": "",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC][RFC PATCH v4 00/27] Private Memory Nodes (w/ Compressed RAM)",
          "message_id": "aaDN6ocubzGUz6zc@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aaDN6ocubzGUz6zc@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-26T22:49:18Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch introduces Private Memory Nodes (PMNs) and Compressed RAM, aiming to improve memory management on NUMA systems.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Gregory Price",
              "summary": "Raised concerns about the feasibility of a 1-to-1 mapping of node-to-pgmap for existing ZONE_DEVICE users, suggesting it may lead to performance issues on non-NUMA systems.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC][RFC PATCH v4 00/27] Private Memory Nodes (w/ Compressed RAM)",
          "message_id": "aZ_gALm7aE3d4IcP@gourry-fedora-PF4VCD3F",
          "url": "https://lore.kernel.org/all/aZ_gALm7aE3d4IcP@gourry-fedora-PF4VCD3F/",
          "date": "2026-02-26T05:54:13Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch introduces Private Memory Nodes (PMNs) and Compressed RAM, aiming to improve memory management on NUMA systems.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Gregory Price",
              "summary": "Raised concerns about the feasibility of a 1-to-1 mapping of node-to-pgmap for existing ZONE_DEVICE users, suggesting it may lead to performance issues on non-NUMA systems.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On Thu, Feb 26, 2026 at 12:54:08AM -0500, Gregory Price wrote:\n> On Thu, Feb 26, 2026 at 02:27:24PM +1100, Alistair Popple wrote:\n> \n> > > If NUMA is the interface we want, then NODE_DATA is the right direction\n> > > regardless of struct page's future or what zone it lives in.\n> > > \n> > > There's no reason to keep per-page pgmap w/ device-to-node mappings.\n> > \n> > In reality I suspect that's already the case today. I'm not sure we need\n> > per-page pgmap.\n> >\n> \n> Probably, and maybe there's a good argument for stealing 80-90% of the\n> common surface here, shunting ZONE_DEVICE to use this instead of pgmap\n> before we go all the way to private nodes.\n> \n\nOut of curiosity i went digging through existing users, and it seems\nlike the average driver has 1-8 discrete pgmaps, with Nouveau being an\noutliar that does ad-hoc registering in 256MB chunks, with the relevant\nannoyance being the percpu_ref it uses to track lifetime of the pgmap,\nand the fact that they can be non-contiguous.\n\ntl;dr here:  a 1-to-1 mapping of node-to-pgmap isn't realistic for most\nexisting ZONE_DEVICE users, meaning a 1-op lookup (page->pgmap) turns\ninto a multi-op pointer chase on and range comparison.\n\nNot sure that turns out well for anyone (only on ZONE_DEVICE / managed\nnode users, all traditional nodes still have a simple pgdat or page->flag\nlookup to check membership).\n\nThere's an argument for trying to do this just for the sake of getting\npgmap out of struct page/folio, but this only deals with the problem on\nNUMA systems.\n\nFor non-numa systems the pgmap still probably ends up in folio_ext\n(assuming we get there), but even that might not be sufficient get LRU\nback.  Might need Willy's opinion here.\n\n~Gregory\n\n\n\n",
              "reply_to": "",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Jeff Layton",
      "primary_email": "jlayton@kernel.org",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 61/61] vfs: update core format strings for u64 i_ino",
          "message_id": "20260226-iino-u64-v1-61-ccceff366db9@kernel.org",
          "url": "https://lore.kernel.org/all/20260226-iino-u64-v1-61-ccceff366db9@kernel.org/",
          "date": "2026-02-26T16:09:05Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch updates the format strings in various Linux kernel files to accommodate the change from unsigned long to u64 for inode numbers (i_ino). The changes affect the pipe, dcache, fserror, and eventpoll modules, updating print statements to use %llu/%llx instead of %lu/%lx for printing inode numbers. This ensures that inode numbers are printed correctly as 64-bit values.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Jeff Layton",
              "summary": "reviewer noted that the patch updates format strings from %lu/%lx to %llu/%llx and 0UL literal to 0ULL in various files, but did not provide any specific feedback or concerns",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Update format strings from %lu/%lx to %llu/%llx and 0UL literal to\n0ULL in pipe, dcache, fserror, and eventpoll, now that i_ino is u64\ninstead of unsigned long.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n fs/dcache.c    | 4 ++--\n fs/eventpoll.c | 2 +-\n fs/fserror.c   | 2 +-\n fs/pipe.c      | 2 +-\n 4 files changed, 5 insertions(+), 5 deletions(-)\n\ndiff --git a/fs/dcache.c b/fs/dcache.c\nindex 24f4f3acaa8cffd6f98124eec38c1a92d6c9fd8e..9e8425ecd88955c72027d21591b1d12c87e7e8aa 100644\n--- a/fs/dcache.c\n+++ b/fs/dcache.c\n@@ -1637,11 +1637,11 @@ static enum d_walk_ret umount_check(void *_data, struct dentry *dentry)\n \tif (dentry == _data && dentry->d_lockref.count == 1)\n \t\treturn D_WALK_CONTINUE;\n \n-\tWARN(1, \"BUG: Dentry %p{i=%lx,n=%pd} \"\n+\tWARN(1, \"BUG: Dentry %p{i=%llx,n=%pd} \"\n \t\t\t\" still in use (%d) [unmount of %s %s]\\n\",\n \t\t       dentry,\n \t\t       dentry->d_inode ?\n-\t\t       dentry->d_inode->i_ino : 0UL,\n+\t\t       dentry->d_inode->i_ino : 0ULL,\n \t\t       dentry,\n \t\t       dentry->d_lockref.count,\n \t\t       dentry->d_sb->s_type->name,\ndiff --git a/fs/eventpoll.c b/fs/eventpoll.c\nindex 5714e900567c499739bb205f43bb6bf73f7ebe54..4ccd4d2e31adf571f939d2e777123e40302e565f 100644\n--- a/fs/eventpoll.c\n+++ b/fs/eventpoll.c\n@@ -1080,7 +1080,7 @@ static void ep_show_fdinfo(struct seq_file *m, struct file *f)\n \t\tstruct inode *inode = file_inode(epi->ffd.file);\n \n \t\tseq_printf(m, \"tfd: %8d events: %8x data: %16llx \"\n-\t\t\t   \" pos:%lli ino:%lx sdev:%x\\n\",\n+\t\t\t   \" pos:%lli ino:%llx sdev:%x\\n\",\n \t\t\t   epi->ffd.fd, epi->event.events,\n \t\t\t   (long long)epi->event.data,\n \t\t\t   (long long)epi->ffd.file->f_pos,\ndiff --git a/fs/fserror.c b/fs/fserror.c\nindex 06ca86adab9b769dfb72ec58b9e51627abee5152..1e4d11fd9562fd158a23b64ca60e9b7e01719cb8 100644\n--- a/fs/fserror.c\n+++ b/fs/fserror.c\n@@ -176,7 +176,7 @@ void fserror_report(struct super_block *sb, struct inode *inode,\n lost:\n \tif (inode)\n \t\tpr_err_ratelimited(\n- \"%s: lost file I/O error report for ino %lu type %u pos 0x%llx len 0x%llx error %d\",\n+ \"%s: lost file I/O error report for ino %llu type %u pos 0x%llx len 0x%llx error %d\",\n \t\t       sb->s_id, inode->i_ino, type, pos, len, error);\n \telse\n \t\tpr_err_ratelimited(\ndiff --git a/fs/pipe.c b/fs/pipe.c\nindex b44a756c0b4165edc2801b2290bf35480245d7a6..9841648c9cf3e8e569cf6ba5c792624fe92396f5 100644\n--- a/fs/pipe.c\n+++ b/fs/pipe.c\n@@ -873,7 +873,7 @@ static struct vfsmount *pipe_mnt __ro_after_init;\n  */\n static char *pipefs_dname(struct dentry *dentry, char *buffer, int buflen)\n {\n-\treturn dynamic_dname(buffer, buflen, \"pipe:[%lu]\",\n+\treturn dynamic_dname(buffer, buflen, \"pipe:[%llu]\",\n \t\t\t\td_inode(dentry)->i_ino);\n }\n \n\n-- \n2.53.0\n\n\n______________________________________________________\nLinux MTD discussion mailing list\nhttp://lists.infradead.org/mailman/listinfo/linux-mtd/",
              "reply_to": "",
              "message_date": "2026-02-26",
              "message_id": "20260226-iino-u64-v1-61-ccceff366db9@kernel.org",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "20260226162222.GH13829@frogsfrogsfrogs",
              "analysis_source": "heuristic"
            },
            {
              "author": "Mathieu Desnoyers",
              "summary": "Reviewer noted that applying this patch as a fixup at the end of the series would break bisectability, making it harder to identify the exact commit causing issues.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Doing this as a fixup at the end of the series breaks bissectability.\n\nThanks,\n\nMathieu\n\n-- \nMathieu Desnoyers\nEfficiOS Inc.\nhttps://www.efficios.com",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "045af923-9067-4898-9a0a-3728e7935346@efficios.com",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer noted that the patch updates format strings from %lu/%lx to %llu/%llx, but did not update the literal 0UL to 0ULL in all affected files.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "______________________________________________________\nLinux MTD discussion mailing list\nhttp://lists.infradead.org/mailman/listinfo/linux-mtd/",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "20260226162222.GH13829@frogsfrogsfrogs",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 12/61] nfsd: update format strings for u64 i_ino",
          "message_id": "9dd2d94989c395a99576b6a8896c582dd33f6740.camel@kernel.org",
          "url": "https://lore.kernel.org/all/9dd2d94989c395a99576b6a8896c582dd33f6740.camel@kernel.org/",
          "date": "2026-02-26T20:06:54Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch updates the format strings in various Linux kernel files to accommodate the change from unsigned long to u64 for inode numbers (i_ino). The changes affect the pipe, dcache, fserror, and eventpoll modules, updating print statements to use %llu/%llx instead of %lu/%lx for printing inode numbers. This ensures that inode numbers are printed correctly as 64-bit values.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Jeff Layton",
              "summary": "reviewer noted that the patch updates format strings from %lu/%lx to %llu/%llx and 0UL literal to 0ULL in various files, but did not provide any specific feedback or concerns",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Update format strings from %lu/%lx to %llu/%llx and 0UL literal to\n0ULL in pipe, dcache, fserror, and eventpoll, now that i_ino is u64\ninstead of unsigned long.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n fs/dcache.c    | 4 ++--\n fs/eventpoll.c | 2 +-\n fs/fserror.c   | 2 +-\n fs/pipe.c      | 2 +-\n 4 files changed, 5 insertions(+), 5 deletions(-)\n\ndiff --git a/fs/dcache.c b/fs/dcache.c\nindex 24f4f3acaa8cffd6f98124eec38c1a92d6c9fd8e..9e8425ecd88955c72027d21591b1d12c87e7e8aa 100644\n--- a/fs/dcache.c\n+++ b/fs/dcache.c\n@@ -1637,11 +1637,11 @@ static enum d_walk_ret umount_check(void *_data, struct dentry *dentry)\n \tif (dentry == _data && dentry->d_lockref.count == 1)\n \t\treturn D_WALK_CONTINUE;\n \n-\tWARN(1, \"BUG: Dentry %p{i=%lx,n=%pd} \"\n+\tWARN(1, \"BUG: Dentry %p{i=%llx,n=%pd} \"\n \t\t\t\" still in use (%d) [unmount of %s %s]\\n\",\n \t\t       dentry,\n \t\t       dentry->d_inode ?\n-\t\t       dentry->d_inode->i_ino : 0UL,\n+\t\t       dentry->d_inode->i_ino : 0ULL,\n \t\t       dentry,\n \t\t       dentry->d_lockref.count,\n \t\t       dentry->d_sb->s_type->name,\ndiff --git a/fs/eventpoll.c b/fs/eventpoll.c\nindex 5714e900567c499739bb205f43bb6bf73f7ebe54..4ccd4d2e31adf571f939d2e777123e40302e565f 100644\n--- a/fs/eventpoll.c\n+++ b/fs/eventpoll.c\n@@ -1080,7 +1080,7 @@ static void ep_show_fdinfo(struct seq_file *m, struct file *f)\n \t\tstruct inode *inode = file_inode(epi->ffd.file);\n \n \t\tseq_printf(m, \"tfd: %8d events: %8x data: %16llx \"\n-\t\t\t   \" pos:%lli ino:%lx sdev:%x\\n\",\n+\t\t\t   \" pos:%lli ino:%llx sdev:%x\\n\",\n \t\t\t   epi->ffd.fd, epi->event.events,\n \t\t\t   (long long)epi->event.data,\n \t\t\t   (long long)epi->ffd.file->f_pos,\ndiff --git a/fs/fserror.c b/fs/fserror.c\nindex 06ca86adab9b769dfb72ec58b9e51627abee5152..1e4d11fd9562fd158a23b64ca60e9b7e01719cb8 100644\n--- a/fs/fserror.c\n+++ b/fs/fserror.c\n@@ -176,7 +176,7 @@ void fserror_report(struct super_block *sb, struct inode *inode,\n lost:\n \tif (inode)\n \t\tpr_err_ratelimited(\n- \"%s: lost file I/O error report for ino %lu type %u pos 0x%llx len 0x%llx error %d\",\n+ \"%s: lost file I/O error report for ino %llu type %u pos 0x%llx len 0x%llx error %d\",\n \t\t       sb->s_id, inode->i_ino, type, pos, len, error);\n \telse\n \t\tpr_err_ratelimited(\ndiff --git a/fs/pipe.c b/fs/pipe.c\nindex b44a756c0b4165edc2801b2290bf35480245d7a6..9841648c9cf3e8e569cf6ba5c792624fe92396f5 100644\n--- a/fs/pipe.c\n+++ b/fs/pipe.c\n@@ -873,7 +873,7 @@ static struct vfsmount *pipe_mnt __ro_after_init;\n  */\n static char *pipefs_dname(struct dentry *dentry, char *buffer, int buflen)\n {\n-\treturn dynamic_dname(buffer, buflen, \"pipe:[%lu]\",\n+\treturn dynamic_dname(buffer, buflen, \"pipe:[%llu]\",\n \t\t\t\td_inode(dentry)->i_ino);\n }\n \n\n-- \n2.53.0\n\n\n______________________________________________________\nLinux MTD discussion mailing list\nhttp://lists.infradead.org/mailman/listinfo/linux-mtd/",
              "reply_to": "",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Mathieu Desnoyers",
              "summary": "Reviewer noted that applying this patch as a fixup at the end of the series would break bisectability, making it harder to identify the exact commit causing issues.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Doing this as a fixup at the end of the series breaks bissectability.\n\nThanks,\n\nMathieu\n\n-- \nMathieu Desnoyers\nEfficiOS Inc.\nhttps://www.efficios.com",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer noted that the patch updates format strings from %lu/%lx to %llu/%llx, but did not update the literal 0UL to 0ULL in all affected files.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "______________________________________________________\nLinux MTD discussion mailing list\nhttp://lists.infradead.org/mailman/listinfo/linux-mtd/",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 17/61] nilfs2: update for u64 i_ino",
          "message_id": "d8d47ebf099b21bf20f7284837f8164a19590010.camel@kernel.org",
          "url": "https://lore.kernel.org/all/d8d47ebf099b21bf20f7284837f8164a19590010.camel@kernel.org/",
          "date": "2026-02-26T20:06:24Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch updates the format strings in various Linux kernel files to accommodate the change from unsigned long to u64 for inode numbers (i_ino). The changes affect the pipe, dcache, fserror, and eventpoll modules, updating print statements to use %llu/%llx instead of %lu/%lx for printing inode numbers. This ensures that inode numbers are printed correctly as 64-bit values.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Jeff Layton",
              "summary": "reviewer noted that the patch updates format strings from %lu/%lx to %llu/%llx and 0UL literal to 0ULL in various files, but did not provide any specific feedback or concerns",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Update format strings from %lu/%lx to %llu/%llx and 0UL literal to\n0ULL in pipe, dcache, fserror, and eventpoll, now that i_ino is u64\ninstead of unsigned long.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n fs/dcache.c    | 4 ++--\n fs/eventpoll.c | 2 +-\n fs/fserror.c   | 2 +-\n fs/pipe.c      | 2 +-\n 4 files changed, 5 insertions(+), 5 deletions(-)\n\ndiff --git a/fs/dcache.c b/fs/dcache.c\nindex 24f4f3acaa8cffd6f98124eec38c1a92d6c9fd8e..9e8425ecd88955c72027d21591b1d12c87e7e8aa 100644\n--- a/fs/dcache.c\n+++ b/fs/dcache.c\n@@ -1637,11 +1637,11 @@ static enum d_walk_ret umount_check(void *_data, struct dentry *dentry)\n \tif (dentry == _data && dentry->d_lockref.count == 1)\n \t\treturn D_WALK_CONTINUE;\n \n-\tWARN(1, \"BUG: Dentry %p{i=%lx,n=%pd} \"\n+\tWARN(1, \"BUG: Dentry %p{i=%llx,n=%pd} \"\n \t\t\t\" still in use (%d) [unmount of %s %s]\\n\",\n \t\t       dentry,\n \t\t       dentry->d_inode ?\n-\t\t       dentry->d_inode->i_ino : 0UL,\n+\t\t       dentry->d_inode->i_ino : 0ULL,\n \t\t       dentry,\n \t\t       dentry->d_lockref.count,\n \t\t       dentry->d_sb->s_type->name,\ndiff --git a/fs/eventpoll.c b/fs/eventpoll.c\nindex 5714e900567c499739bb205f43bb6bf73f7ebe54..4ccd4d2e31adf571f939d2e777123e40302e565f 100644\n--- a/fs/eventpoll.c\n+++ b/fs/eventpoll.c\n@@ -1080,7 +1080,7 @@ static void ep_show_fdinfo(struct seq_file *m, struct file *f)\n \t\tstruct inode *inode = file_inode(epi->ffd.file);\n \n \t\tseq_printf(m, \"tfd: %8d events: %8x data: %16llx \"\n-\t\t\t   \" pos:%lli ino:%lx sdev:%x\\n\",\n+\t\t\t   \" pos:%lli ino:%llx sdev:%x\\n\",\n \t\t\t   epi->ffd.fd, epi->event.events,\n \t\t\t   (long long)epi->event.data,\n \t\t\t   (long long)epi->ffd.file->f_pos,\ndiff --git a/fs/fserror.c b/fs/fserror.c\nindex 06ca86adab9b769dfb72ec58b9e51627abee5152..1e4d11fd9562fd158a23b64ca60e9b7e01719cb8 100644\n--- a/fs/fserror.c\n+++ b/fs/fserror.c\n@@ -176,7 +176,7 @@ void fserror_report(struct super_block *sb, struct inode *inode,\n lost:\n \tif (inode)\n \t\tpr_err_ratelimited(\n- \"%s: lost file I/O error report for ino %lu type %u pos 0x%llx len 0x%llx error %d\",\n+ \"%s: lost file I/O error report for ino %llu type %u pos 0x%llx len 0x%llx error %d\",\n \t\t       sb->s_id, inode->i_ino, type, pos, len, error);\n \telse\n \t\tpr_err_ratelimited(\ndiff --git a/fs/pipe.c b/fs/pipe.c\nindex b44a756c0b4165edc2801b2290bf35480245d7a6..9841648c9cf3e8e569cf6ba5c792624fe92396f5 100644\n--- a/fs/pipe.c\n+++ b/fs/pipe.c\n@@ -873,7 +873,7 @@ static struct vfsmount *pipe_mnt __ro_after_init;\n  */\n static char *pipefs_dname(struct dentry *dentry, char *buffer, int buflen)\n {\n-\treturn dynamic_dname(buffer, buflen, \"pipe:[%lu]\",\n+\treturn dynamic_dname(buffer, buflen, \"pipe:[%llu]\",\n \t\t\t\td_inode(dentry)->i_ino);\n }\n \n\n-- \n2.53.0\n\n\n______________________________________________________\nLinux MTD discussion mailing list\nhttp://lists.infradead.org/mailman/listinfo/linux-mtd/",
              "reply_to": "",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Mathieu Desnoyers",
              "summary": "Reviewer noted that applying this patch as a fixup at the end of the series would break bisectability, making it harder to identify the exact commit causing issues.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Doing this as a fixup at the end of the series breaks bissectability.\n\nThanks,\n\nMathieu\n\n-- \nMathieu Desnoyers\nEfficiOS Inc.\nhttps://www.efficios.com",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer noted that the patch updates format strings from %lu/%lx to %llu/%llx, but did not update the literal 0UL to 0ULL in all affected files.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "______________________________________________________\nLinux MTD discussion mailing list\nhttp://lists.infradead.org/mailman/listinfo/linux-mtd/",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 02/61] vfs: change i_ino from unsigned long to u64",
          "message_id": "b8cbab6fecc6a2743538d139eb1b054965b74841.camel@kernel.org",
          "url": "https://lore.kernel.org/all/b8cbab6fecc6a2743538d139eb1b054965b74841.camel@kernel.org/",
          "date": "2026-02-26T17:59:34Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch updates the format strings in various Linux kernel files to accommodate the change from unsigned long to u64 for inode numbers (i_ino). The changes affect the pipe, dcache, fserror, and eventpoll modules, updating print statements to use %llu/%llx instead of %lu/%lx for printing inode numbers. This ensures that inode numbers are printed correctly as 64-bit values.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Jeff Layton",
              "summary": "reviewer noted that the patch updates format strings from %lu/%lx to %llu/%llx and 0UL literal to 0ULL in various files, but did not provide any specific feedback or concerns",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Update format strings from %lu/%lx to %llu/%llx and 0UL literal to\n0ULL in pipe, dcache, fserror, and eventpoll, now that i_ino is u64\ninstead of unsigned long.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n fs/dcache.c    | 4 ++--\n fs/eventpoll.c | 2 +-\n fs/fserror.c   | 2 +-\n fs/pipe.c      | 2 +-\n 4 files changed, 5 insertions(+), 5 deletions(-)\n\ndiff --git a/fs/dcache.c b/fs/dcache.c\nindex 24f4f3acaa8cffd6f98124eec38c1a92d6c9fd8e..9e8425ecd88955c72027d21591b1d12c87e7e8aa 100644\n--- a/fs/dcache.c\n+++ b/fs/dcache.c\n@@ -1637,11 +1637,11 @@ static enum d_walk_ret umount_check(void *_data, struct dentry *dentry)\n \tif (dentry == _data && dentry->d_lockref.count == 1)\n \t\treturn D_WALK_CONTINUE;\n \n-\tWARN(1, \"BUG: Dentry %p{i=%lx,n=%pd} \"\n+\tWARN(1, \"BUG: Dentry %p{i=%llx,n=%pd} \"\n \t\t\t\" still in use (%d) [unmount of %s %s]\\n\",\n \t\t       dentry,\n \t\t       dentry->d_inode ?\n-\t\t       dentry->d_inode->i_ino : 0UL,\n+\t\t       dentry->d_inode->i_ino : 0ULL,\n \t\t       dentry,\n \t\t       dentry->d_lockref.count,\n \t\t       dentry->d_sb->s_type->name,\ndiff --git a/fs/eventpoll.c b/fs/eventpoll.c\nindex 5714e900567c499739bb205f43bb6bf73f7ebe54..4ccd4d2e31adf571f939d2e777123e40302e565f 100644\n--- a/fs/eventpoll.c\n+++ b/fs/eventpoll.c\n@@ -1080,7 +1080,7 @@ static void ep_show_fdinfo(struct seq_file *m, struct file *f)\n \t\tstruct inode *inode = file_inode(epi->ffd.file);\n \n \t\tseq_printf(m, \"tfd: %8d events: %8x data: %16llx \"\n-\t\t\t   \" pos:%lli ino:%lx sdev:%x\\n\",\n+\t\t\t   \" pos:%lli ino:%llx sdev:%x\\n\",\n \t\t\t   epi->ffd.fd, epi->event.events,\n \t\t\t   (long long)epi->event.data,\n \t\t\t   (long long)epi->ffd.file->f_pos,\ndiff --git a/fs/fserror.c b/fs/fserror.c\nindex 06ca86adab9b769dfb72ec58b9e51627abee5152..1e4d11fd9562fd158a23b64ca60e9b7e01719cb8 100644\n--- a/fs/fserror.c\n+++ b/fs/fserror.c\n@@ -176,7 +176,7 @@ void fserror_report(struct super_block *sb, struct inode *inode,\n lost:\n \tif (inode)\n \t\tpr_err_ratelimited(\n- \"%s: lost file I/O error report for ino %lu type %u pos 0x%llx len 0x%llx error %d\",\n+ \"%s: lost file I/O error report for ino %llu type %u pos 0x%llx len 0x%llx error %d\",\n \t\t       sb->s_id, inode->i_ino, type, pos, len, error);\n \telse\n \t\tpr_err_ratelimited(\ndiff --git a/fs/pipe.c b/fs/pipe.c\nindex b44a756c0b4165edc2801b2290bf35480245d7a6..9841648c9cf3e8e569cf6ba5c792624fe92396f5 100644\n--- a/fs/pipe.c\n+++ b/fs/pipe.c\n@@ -873,7 +873,7 @@ static struct vfsmount *pipe_mnt __ro_after_init;\n  */\n static char *pipefs_dname(struct dentry *dentry, char *buffer, int buflen)\n {\n-\treturn dynamic_dname(buffer, buflen, \"pipe:[%lu]\",\n+\treturn dynamic_dname(buffer, buflen, \"pipe:[%llu]\",\n \t\t\t\td_inode(dentry)->i_ino);\n }\n \n\n-- \n2.53.0\n\n\n______________________________________________________\nLinux MTD discussion mailing list\nhttp://lists.infradead.org/mailman/listinfo/linux-mtd/",
              "reply_to": "",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Mathieu Desnoyers",
              "summary": "Reviewer noted that applying this patch as a fixup at the end of the series would break bisectability, making it harder to identify the exact commit causing issues.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Doing this as a fixup at the end of the series breaks bissectability.\n\nThanks,\n\nMathieu\n\n-- \nMathieu Desnoyers\nEfficiOS Inc.\nhttps://www.efficios.com",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer noted that the patch updates format strings from %lu/%lx to %llu/%llx, but did not update the literal 0UL to 0ULL in all affected files.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "______________________________________________________\nLinux MTD discussion mailing list\nhttp://lists.infradead.org/mailman/listinfo/linux-mtd/",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 03/61] trace: update VFS-layer trace events for u64 i_ino",
          "message_id": "353dfa5494f8d83eff01b0c7dc8451c81463ce57.camel@kernel.org",
          "url": "https://lore.kernel.org/all/353dfa5494f8d83eff01b0c7dc8451c81463ce57.camel@kernel.org/",
          "date": "2026-02-26T17:54:01Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch updates the format strings in various Linux kernel files to accommodate the change from unsigned long to u64 for inode numbers (i_ino). The changes affect the pipe, dcache, fserror, and eventpoll modules, updating print statements to use %llu/%llx instead of %lu/%lx for printing inode numbers. This ensures that inode numbers are printed correctly as 64-bit values.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Jeff Layton",
              "summary": "reviewer noted that the patch updates format strings from %lu/%lx to %llu/%llx and 0UL literal to 0ULL in various files, but did not provide any specific feedback or concerns",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Update format strings from %lu/%lx to %llu/%llx and 0UL literal to\n0ULL in pipe, dcache, fserror, and eventpoll, now that i_ino is u64\ninstead of unsigned long.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n fs/dcache.c    | 4 ++--\n fs/eventpoll.c | 2 +-\n fs/fserror.c   | 2 +-\n fs/pipe.c      | 2 +-\n 4 files changed, 5 insertions(+), 5 deletions(-)\n\ndiff --git a/fs/dcache.c b/fs/dcache.c\nindex 24f4f3acaa8cffd6f98124eec38c1a92d6c9fd8e..9e8425ecd88955c72027d21591b1d12c87e7e8aa 100644\n--- a/fs/dcache.c\n+++ b/fs/dcache.c\n@@ -1637,11 +1637,11 @@ static enum d_walk_ret umount_check(void *_data, struct dentry *dentry)\n \tif (dentry == _data && dentry->d_lockref.count == 1)\n \t\treturn D_WALK_CONTINUE;\n \n-\tWARN(1, \"BUG: Dentry %p{i=%lx,n=%pd} \"\n+\tWARN(1, \"BUG: Dentry %p{i=%llx,n=%pd} \"\n \t\t\t\" still in use (%d) [unmount of %s %s]\\n\",\n \t\t       dentry,\n \t\t       dentry->d_inode ?\n-\t\t       dentry->d_inode->i_ino : 0UL,\n+\t\t       dentry->d_inode->i_ino : 0ULL,\n \t\t       dentry,\n \t\t       dentry->d_lockref.count,\n \t\t       dentry->d_sb->s_type->name,\ndiff --git a/fs/eventpoll.c b/fs/eventpoll.c\nindex 5714e900567c499739bb205f43bb6bf73f7ebe54..4ccd4d2e31adf571f939d2e777123e40302e565f 100644\n--- a/fs/eventpoll.c\n+++ b/fs/eventpoll.c\n@@ -1080,7 +1080,7 @@ static void ep_show_fdinfo(struct seq_file *m, struct file *f)\n \t\tstruct inode *inode = file_inode(epi->ffd.file);\n \n \t\tseq_printf(m, \"tfd: %8d events: %8x data: %16llx \"\n-\t\t\t   \" pos:%lli ino:%lx sdev:%x\\n\",\n+\t\t\t   \" pos:%lli ino:%llx sdev:%x\\n\",\n \t\t\t   epi->ffd.fd, epi->event.events,\n \t\t\t   (long long)epi->event.data,\n \t\t\t   (long long)epi->ffd.file->f_pos,\ndiff --git a/fs/fserror.c b/fs/fserror.c\nindex 06ca86adab9b769dfb72ec58b9e51627abee5152..1e4d11fd9562fd158a23b64ca60e9b7e01719cb8 100644\n--- a/fs/fserror.c\n+++ b/fs/fserror.c\n@@ -176,7 +176,7 @@ void fserror_report(struct super_block *sb, struct inode *inode,\n lost:\n \tif (inode)\n \t\tpr_err_ratelimited(\n- \"%s: lost file I/O error report for ino %lu type %u pos 0x%llx len 0x%llx error %d\",\n+ \"%s: lost file I/O error report for ino %llu type %u pos 0x%llx len 0x%llx error %d\",\n \t\t       sb->s_id, inode->i_ino, type, pos, len, error);\n \telse\n \t\tpr_err_ratelimited(\ndiff --git a/fs/pipe.c b/fs/pipe.c\nindex b44a756c0b4165edc2801b2290bf35480245d7a6..9841648c9cf3e8e569cf6ba5c792624fe92396f5 100644\n--- a/fs/pipe.c\n+++ b/fs/pipe.c\n@@ -873,7 +873,7 @@ static struct vfsmount *pipe_mnt __ro_after_init;\n  */\n static char *pipefs_dname(struct dentry *dentry, char *buffer, int buflen)\n {\n-\treturn dynamic_dname(buffer, buflen, \"pipe:[%lu]\",\n+\treturn dynamic_dname(buffer, buflen, \"pipe:[%llu]\",\n \t\t\t\td_inode(dentry)->i_ino);\n }\n \n\n-- \n2.53.0\n\n\n______________________________________________________\nLinux MTD discussion mailing list\nhttp://lists.infradead.org/mailman/listinfo/linux-mtd/",
              "reply_to": "",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Mathieu Desnoyers",
              "summary": "Reviewer noted that applying this patch as a fixup at the end of the series would break bisectability, making it harder to identify the exact commit causing issues.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Doing this as a fixup at the end of the series breaks bissectability.\n\nThanks,\n\nMathieu\n\n-- \nMathieu Desnoyers\nEfficiOS Inc.\nhttps://www.efficios.com",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer noted that the patch updates format strings from %lu/%lx to %llu/%llx, but did not update the literal 0UL to 0ULL in all affected files.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "______________________________________________________\nLinux MTD discussion mailing list\nhttp://lists.infradead.org/mailman/listinfo/linux-mtd/",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 00/61] vfs: change inode->i_ino from unsigned long to u64",
          "message_id": "5826915dea6b149707f68847f0f3330a25615eae.camel@kernel.org",
          "url": "https://lore.kernel.org/all/5826915dea6b149707f68847f0f3330a25615eae.camel@kernel.org/",
          "date": "2026-02-26T17:02:03Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch updates the format strings in various Linux kernel files to accommodate the change from unsigned long to u64 for inode numbers (i_ino). The changes affect the pipe, dcache, fserror, and eventpoll modules, updating print statements to use %llu/%llx instead of %lu/%lx for printing inode numbers. This ensures that inode numbers are printed correctly as 64-bit values.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Jeff Layton",
              "summary": "reviewer noted that the patch updates format strings from %lu/%lx to %llu/%llx and 0UL literal to 0ULL in various files, but did not provide any specific feedback or concerns",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Update format strings from %lu/%lx to %llu/%llx and 0UL literal to\n0ULL in pipe, dcache, fserror, and eventpoll, now that i_ino is u64\ninstead of unsigned long.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n fs/dcache.c    | 4 ++--\n fs/eventpoll.c | 2 +-\n fs/fserror.c   | 2 +-\n fs/pipe.c      | 2 +-\n 4 files changed, 5 insertions(+), 5 deletions(-)\n\ndiff --git a/fs/dcache.c b/fs/dcache.c\nindex 24f4f3acaa8cffd6f98124eec38c1a92d6c9fd8e..9e8425ecd88955c72027d21591b1d12c87e7e8aa 100644\n--- a/fs/dcache.c\n+++ b/fs/dcache.c\n@@ -1637,11 +1637,11 @@ static enum d_walk_ret umount_check(void *_data, struct dentry *dentry)\n \tif (dentry == _data && dentry->d_lockref.count == 1)\n \t\treturn D_WALK_CONTINUE;\n \n-\tWARN(1, \"BUG: Dentry %p{i=%lx,n=%pd} \"\n+\tWARN(1, \"BUG: Dentry %p{i=%llx,n=%pd} \"\n \t\t\t\" still in use (%d) [unmount of %s %s]\\n\",\n \t\t       dentry,\n \t\t       dentry->d_inode ?\n-\t\t       dentry->d_inode->i_ino : 0UL,\n+\t\t       dentry->d_inode->i_ino : 0ULL,\n \t\t       dentry,\n \t\t       dentry->d_lockref.count,\n \t\t       dentry->d_sb->s_type->name,\ndiff --git a/fs/eventpoll.c b/fs/eventpoll.c\nindex 5714e900567c499739bb205f43bb6bf73f7ebe54..4ccd4d2e31adf571f939d2e777123e40302e565f 100644\n--- a/fs/eventpoll.c\n+++ b/fs/eventpoll.c\n@@ -1080,7 +1080,7 @@ static void ep_show_fdinfo(struct seq_file *m, struct file *f)\n \t\tstruct inode *inode = file_inode(epi->ffd.file);\n \n \t\tseq_printf(m, \"tfd: %8d events: %8x data: %16llx \"\n-\t\t\t   \" pos:%lli ino:%lx sdev:%x\\n\",\n+\t\t\t   \" pos:%lli ino:%llx sdev:%x\\n\",\n \t\t\t   epi->ffd.fd, epi->event.events,\n \t\t\t   (long long)epi->event.data,\n \t\t\t   (long long)epi->ffd.file->f_pos,\ndiff --git a/fs/fserror.c b/fs/fserror.c\nindex 06ca86adab9b769dfb72ec58b9e51627abee5152..1e4d11fd9562fd158a23b64ca60e9b7e01719cb8 100644\n--- a/fs/fserror.c\n+++ b/fs/fserror.c\n@@ -176,7 +176,7 @@ void fserror_report(struct super_block *sb, struct inode *inode,\n lost:\n \tif (inode)\n \t\tpr_err_ratelimited(\n- \"%s: lost file I/O error report for ino %lu type %u pos 0x%llx len 0x%llx error %d\",\n+ \"%s: lost file I/O error report for ino %llu type %u pos 0x%llx len 0x%llx error %d\",\n \t\t       sb->s_id, inode->i_ino, type, pos, len, error);\n \telse\n \t\tpr_err_ratelimited(\ndiff --git a/fs/pipe.c b/fs/pipe.c\nindex b44a756c0b4165edc2801b2290bf35480245d7a6..9841648c9cf3e8e569cf6ba5c792624fe92396f5 100644\n--- a/fs/pipe.c\n+++ b/fs/pipe.c\n@@ -873,7 +873,7 @@ static struct vfsmount *pipe_mnt __ro_after_init;\n  */\n static char *pipefs_dname(struct dentry *dentry, char *buffer, int buflen)\n {\n-\treturn dynamic_dname(buffer, buflen, \"pipe:[%lu]\",\n+\treturn dynamic_dname(buffer, buflen, \"pipe:[%llu]\",\n \t\t\t\td_inode(dentry)->i_ino);\n }\n \n\n-- \n2.53.0\n\n\n______________________________________________________\nLinux MTD discussion mailing list\nhttp://lists.infradead.org/mailman/listinfo/linux-mtd/",
              "reply_to": "",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Mathieu Desnoyers",
              "summary": "Reviewer noted that applying this patch as a fixup at the end of the series would break bisectability, making it harder to identify the exact commit causing issues.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Doing this as a fixup at the end of the series breaks bissectability.\n\nThanks,\n\nMathieu\n\n-- \nMathieu Desnoyers\nEfficiOS Inc.\nhttps://www.efficios.com",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer noted that the patch updates format strings from %lu/%lx to %llu/%llx, but did not update the literal 0UL to 0ULL in all affected files.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "______________________________________________________\nLinux MTD discussion mailing list\nhttp://lists.infradead.org/mailman/listinfo/linux-mtd/",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 02/61] vfs: change i_ino from unsigned long to u64",
          "message_id": "e0759209c02d81b877b136f1e2b9500ba69b4f35.camel@kernel.org",
          "url": "https://lore.kernel.org/all/e0759209c02d81b877b136f1e2b9500ba69b4f35.camel@kernel.org/",
          "date": "2026-02-26T16:45:19Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch updates the format strings in various Linux kernel files to accommodate the change from unsigned long to u64 for inode numbers (i_ino). The changes affect the pipe, dcache, fserror, and eventpoll modules, updating print statements to use %llu/%llx instead of %lu/%lx for printing inode numbers. This ensures that inode numbers are printed correctly as 64-bit values.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Jeff Layton",
              "summary": "reviewer noted that the patch updates format strings from %lu/%lx to %llu/%llx and 0UL literal to 0ULL in various files, but did not provide any specific feedback or concerns",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Update format strings from %lu/%lx to %llu/%llx and 0UL literal to\n0ULL in pipe, dcache, fserror, and eventpoll, now that i_ino is u64\ninstead of unsigned long.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n fs/dcache.c    | 4 ++--\n fs/eventpoll.c | 2 +-\n fs/fserror.c   | 2 +-\n fs/pipe.c      | 2 +-\n 4 files changed, 5 insertions(+), 5 deletions(-)\n\ndiff --git a/fs/dcache.c b/fs/dcache.c\nindex 24f4f3acaa8cffd6f98124eec38c1a92d6c9fd8e..9e8425ecd88955c72027d21591b1d12c87e7e8aa 100644\n--- a/fs/dcache.c\n+++ b/fs/dcache.c\n@@ -1637,11 +1637,11 @@ static enum d_walk_ret umount_check(void *_data, struct dentry *dentry)\n \tif (dentry == _data && dentry->d_lockref.count == 1)\n \t\treturn D_WALK_CONTINUE;\n \n-\tWARN(1, \"BUG: Dentry %p{i=%lx,n=%pd} \"\n+\tWARN(1, \"BUG: Dentry %p{i=%llx,n=%pd} \"\n \t\t\t\" still in use (%d) [unmount of %s %s]\\n\",\n \t\t       dentry,\n \t\t       dentry->d_inode ?\n-\t\t       dentry->d_inode->i_ino : 0UL,\n+\t\t       dentry->d_inode->i_ino : 0ULL,\n \t\t       dentry,\n \t\t       dentry->d_lockref.count,\n \t\t       dentry->d_sb->s_type->name,\ndiff --git a/fs/eventpoll.c b/fs/eventpoll.c\nindex 5714e900567c499739bb205f43bb6bf73f7ebe54..4ccd4d2e31adf571f939d2e777123e40302e565f 100644\n--- a/fs/eventpoll.c\n+++ b/fs/eventpoll.c\n@@ -1080,7 +1080,7 @@ static void ep_show_fdinfo(struct seq_file *m, struct file *f)\n \t\tstruct inode *inode = file_inode(epi->ffd.file);\n \n \t\tseq_printf(m, \"tfd: %8d events: %8x data: %16llx \"\n-\t\t\t   \" pos:%lli ino:%lx sdev:%x\\n\",\n+\t\t\t   \" pos:%lli ino:%llx sdev:%x\\n\",\n \t\t\t   epi->ffd.fd, epi->event.events,\n \t\t\t   (long long)epi->event.data,\n \t\t\t   (long long)epi->ffd.file->f_pos,\ndiff --git a/fs/fserror.c b/fs/fserror.c\nindex 06ca86adab9b769dfb72ec58b9e51627abee5152..1e4d11fd9562fd158a23b64ca60e9b7e01719cb8 100644\n--- a/fs/fserror.c\n+++ b/fs/fserror.c\n@@ -176,7 +176,7 @@ void fserror_report(struct super_block *sb, struct inode *inode,\n lost:\n \tif (inode)\n \t\tpr_err_ratelimited(\n- \"%s: lost file I/O error report for ino %lu type %u pos 0x%llx len 0x%llx error %d\",\n+ \"%s: lost file I/O error report for ino %llu type %u pos 0x%llx len 0x%llx error %d\",\n \t\t       sb->s_id, inode->i_ino, type, pos, len, error);\n \telse\n \t\tpr_err_ratelimited(\ndiff --git a/fs/pipe.c b/fs/pipe.c\nindex b44a756c0b4165edc2801b2290bf35480245d7a6..9841648c9cf3e8e569cf6ba5c792624fe92396f5 100644\n--- a/fs/pipe.c\n+++ b/fs/pipe.c\n@@ -873,7 +873,7 @@ static struct vfsmount *pipe_mnt __ro_after_init;\n  */\n static char *pipefs_dname(struct dentry *dentry, char *buffer, int buflen)\n {\n-\treturn dynamic_dname(buffer, buflen, \"pipe:[%lu]\",\n+\treturn dynamic_dname(buffer, buflen, \"pipe:[%llu]\",\n \t\t\t\td_inode(dentry)->i_ino);\n }\n \n\n-- \n2.53.0\n\n\n______________________________________________________\nLinux MTD discussion mailing list\nhttp://lists.infradead.org/mailman/listinfo/linux-mtd/",
              "reply_to": "",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Mathieu Desnoyers",
              "summary": "Reviewer noted that applying this patch as a fixup at the end of the series would break bisectability, making it harder to identify the exact commit causing issues.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Doing this as a fixup at the end of the series breaks bissectability.\n\nThanks,\n\nMathieu\n\n-- \nMathieu Desnoyers\nEfficiOS Inc.\nhttps://www.efficios.com",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer noted that the patch updates format strings from %lu/%lx to %llu/%llx, but did not update the literal 0UL to 0ULL in all affected files.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "______________________________________________________\nLinux MTD discussion mailing list\nhttp://lists.infradead.org/mailman/listinfo/linux-mtd/",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 02/61] vfs: change i_ino from unsigned long to u64",
          "message_id": "df0b9e26fca0dc56a10e2f6792892c7b5f23c84b.camel@kernel.org",
          "url": "https://lore.kernel.org/all/df0b9e26fca0dc56a10e2f6792892c7b5f23c84b.camel@kernel.org/",
          "date": "2026-02-26T16:35:44Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch updates the format strings in various Linux kernel files to accommodate the change from unsigned long to u64 for inode numbers (i_ino). The changes affect the pipe, dcache, fserror, and eventpoll modules, updating print statements to use %llu/%llx instead of %lu/%lx for printing inode numbers. This ensures that inode numbers are printed correctly as 64-bit values.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Jeff Layton",
              "summary": "reviewer noted that the patch updates format strings from %lu/%lx to %llu/%llx and 0UL literal to 0ULL in various files, but did not provide any specific feedback or concerns",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Update format strings from %lu/%lx to %llu/%llx and 0UL literal to\n0ULL in pipe, dcache, fserror, and eventpoll, now that i_ino is u64\ninstead of unsigned long.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n fs/dcache.c    | 4 ++--\n fs/eventpoll.c | 2 +-\n fs/fserror.c   | 2 +-\n fs/pipe.c      | 2 +-\n 4 files changed, 5 insertions(+), 5 deletions(-)\n\ndiff --git a/fs/dcache.c b/fs/dcache.c\nindex 24f4f3acaa8cffd6f98124eec38c1a92d6c9fd8e..9e8425ecd88955c72027d21591b1d12c87e7e8aa 100644\n--- a/fs/dcache.c\n+++ b/fs/dcache.c\n@@ -1637,11 +1637,11 @@ static enum d_walk_ret umount_check(void *_data, struct dentry *dentry)\n \tif (dentry == _data && dentry->d_lockref.count == 1)\n \t\treturn D_WALK_CONTINUE;\n \n-\tWARN(1, \"BUG: Dentry %p{i=%lx,n=%pd} \"\n+\tWARN(1, \"BUG: Dentry %p{i=%llx,n=%pd} \"\n \t\t\t\" still in use (%d) [unmount of %s %s]\\n\",\n \t\t       dentry,\n \t\t       dentry->d_inode ?\n-\t\t       dentry->d_inode->i_ino : 0UL,\n+\t\t       dentry->d_inode->i_ino : 0ULL,\n \t\t       dentry,\n \t\t       dentry->d_lockref.count,\n \t\t       dentry->d_sb->s_type->name,\ndiff --git a/fs/eventpoll.c b/fs/eventpoll.c\nindex 5714e900567c499739bb205f43bb6bf73f7ebe54..4ccd4d2e31adf571f939d2e777123e40302e565f 100644\n--- a/fs/eventpoll.c\n+++ b/fs/eventpoll.c\n@@ -1080,7 +1080,7 @@ static void ep_show_fdinfo(struct seq_file *m, struct file *f)\n \t\tstruct inode *inode = file_inode(epi->ffd.file);\n \n \t\tseq_printf(m, \"tfd: %8d events: %8x data: %16llx \"\n-\t\t\t   \" pos:%lli ino:%lx sdev:%x\\n\",\n+\t\t\t   \" pos:%lli ino:%llx sdev:%x\\n\",\n \t\t\t   epi->ffd.fd, epi->event.events,\n \t\t\t   (long long)epi->event.data,\n \t\t\t   (long long)epi->ffd.file->f_pos,\ndiff --git a/fs/fserror.c b/fs/fserror.c\nindex 06ca86adab9b769dfb72ec58b9e51627abee5152..1e4d11fd9562fd158a23b64ca60e9b7e01719cb8 100644\n--- a/fs/fserror.c\n+++ b/fs/fserror.c\n@@ -176,7 +176,7 @@ void fserror_report(struct super_block *sb, struct inode *inode,\n lost:\n \tif (inode)\n \t\tpr_err_ratelimited(\n- \"%s: lost file I/O error report for ino %lu type %u pos 0x%llx len 0x%llx error %d\",\n+ \"%s: lost file I/O error report for ino %llu type %u pos 0x%llx len 0x%llx error %d\",\n \t\t       sb->s_id, inode->i_ino, type, pos, len, error);\n \telse\n \t\tpr_err_ratelimited(\ndiff --git a/fs/pipe.c b/fs/pipe.c\nindex b44a756c0b4165edc2801b2290bf35480245d7a6..9841648c9cf3e8e569cf6ba5c792624fe92396f5 100644\n--- a/fs/pipe.c\n+++ b/fs/pipe.c\n@@ -873,7 +873,7 @@ static struct vfsmount *pipe_mnt __ro_after_init;\n  */\n static char *pipefs_dname(struct dentry *dentry, char *buffer, int buflen)\n {\n-\treturn dynamic_dname(buffer, buflen, \"pipe:[%lu]\",\n+\treturn dynamic_dname(buffer, buflen, \"pipe:[%llu]\",\n \t\t\t\td_inode(dentry)->i_ino);\n }\n \n\n-- \n2.53.0\n\n\n______________________________________________________\nLinux MTD discussion mailing list\nhttp://lists.infradead.org/mailman/listinfo/linux-mtd/",
              "reply_to": "",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Gave Acked-by",
              "sentiment": "POSITIVE",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Mathieu Desnoyers",
              "summary": "Reviewer noted that applying this patch as a fixup at the end of the series would break bisectability, making it harder to identify the exact commit causing issues.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Doing this as a fixup at the end of the series breaks bissectability.\n\nThanks,\n\nMathieu\n\n-- \nMathieu Desnoyers\nEfficiOS Inc.\nhttps://www.efficios.com",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Darrick Wong",
              "summary": "Reviewer noted that the patch updates format strings from %lu/%lx to %llu/%llx, but did not update the literal 0UL to 0ULL in all affected files.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "______________________________________________________\nLinux MTD discussion mailing list\nhttp://lists.infradead.org/mailman/listinfo/linux-mtd/",
              "reply_to": "Jeff Layton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: xfstest failures depending on delegated_attributes on/off",
          "message_id": "ba2e136d81fd9f4b822a1ae6ba98baba699078dd.camel@kernel.org",
          "url": "https://lore.kernel.org/all/ba2e136d81fd9f4b822a1ae6ba98baba699078dd.camel@kernel.org/",
          "date": "2026-02-26T14:36:33Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Jeff Layton",
              "summary": "Identified a potential issue with file timestamps not being updated correctly when using delegated attributes. Provided a network trace to support the analysis and suggested further investigation.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "WAITING_FOR_REVIEW"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH 0/4] tighten nstree visibility checks",
          "message_id": "cec0047f9c5ab7fc5bce625ff68458c54cb935c1.camel@kernel.org",
          "url": "https://lore.kernel.org/all/cec0047f9c5ab7fc5bce625ff68458c54cb935c1.camel@kernel.org/",
          "date": "2026-02-26T15:18:44Z",
          "in_reply_to": null,
          "ack_type": "Reviewed-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": []
        }
      ],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Joanne Koong",
      "primary_email": "joannelkoong@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v6 1/3] fuse: add compound command to combine multiple requests",
          "message_id": "CAJnrk1ZjhgGtZY556C3wptdw7uJoo8kuakfTvkQ-D3LzV4BkHg@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1ZjhgGtZY556C3wptdw7uJoo8kuakfTvkQ-D3LzV4BkHg@mail.gmail.com/",
          "date": "2026-02-26T23:05:12Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch proposes adding a compound command to the fuse kernel module, allowing multiple requests to be combined into a single operation.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Joanne Koong",
              "summary": "Raised concerns about the necessity of the compound command and suggested that libfuse should handle everything correctly. Proposed removing the FUSE_COMPOUND_CONTINUE flag as it is confusing.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "CONTENTIOUS"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v4 19/25] fuse: add io-uring kernel-managed buffer ring",
          "message_id": "CAJnrk1ZQC=tE8z+D1rSnNijkMhXDPEZaB_9WL9NPZyKmG2=NSQ@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1ZQC=tE8z+D1rSnNijkMhXDPEZaB_9WL9NPZyKmG2=NSQ@mail.gmail.com/",
          "date": "2026-02-26T20:58:33Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch adds kernel-managed buffer ring support for io-uring in the FUSE filesystem.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Joanne Koong",
              "summary": "Made a minor suggestion to add a comment for clarity and agreed to update it. Also pointed out that no ring entry is used when there's no payload, but this was already handled in fuse_uring_prep_buffer().",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "AGREEMENT"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [LSF/MM/BPF TOPIC] Where is fuse going? API cleanup, restructuring and more",
          "message_id": "CAJnrk1ZJksW=uz1itdh+zoaQBo_XQ4ZSF13BSnZXMie5pBCvYA@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1ZJksW=uz1itdh+zoaQBo_XQ4ZSF13BSnZXMie5pBCvYA@mail.gmail.com/",
          "date": "2026-02-26T20:21:56Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "",
          "analysis_source": "llm",
          "review_comments": []
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v6 3/3] fuse: add an implementation of open+getattr",
          "message_id": "CAJnrk1ZsvtZh9vZoN=ca_wrs5enTfAQeNBYppOzZH=c+ARaP3Q@mail.gmail.com",
          "url": "https://lore.kernel.org/all/CAJnrk1ZsvtZh9vZoN=ca_wrs5enTfAQeNBYppOzZH=c+ARaP3Q@mail.gmail.com/",
          "date": "2026-02-26T19:12:12Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch proposes adding a compound command to the fuse kernel module, allowing multiple requests to be combined into a single operation.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Joanne Koong",
              "summary": "Raised concerns about the necessity of the compound command and suggested that libfuse should handle everything correctly. Proposed removing the FUSE_COMPOUND_CONTINUE flag as it is confusing.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "CONTENTIOUS"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Johannes Weiner",
      "primary_email": "hannes@cmpxchg.org",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v4] mm: introduce a new page type for page pool in page type",
          "message_id": "aaCVzc2lexW0TiPf@cmpxchg.org",
          "url": "https://lore.kernel.org/all/aaCVzc2lexW0TiPf@cmpxchg.org/",
          "date": "2026-02-26T18:49:55Z",
          "in_reply_to": null,
          "ack_type": "Acked-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Introduction of a new page type for the page pool in the page type system.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Johannes Weiner",
              "summary": "Approved the MM bits of the patch with minor suggestions for optimization.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "LGTM"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Joshua Hahn",
      "primary_email": "joshua.hahnjy@gmail.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH 0/8] mm/zswap, zsmalloc: Per-memcg-lruvec zswap accounting",
          "message_id": "20260226192936.3190275-1-joshua.hahnjy@gmail.com",
          "url": "https://lore.kernel.org/all/20260226192936.3190275-1-joshua.hahnjy@gmail.com/",
          "date": "2026-02-26T19:29:40Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch series introduces per-memcg-lruvec zswap accounting by adding a new array of objcg pointers to the zpdesc structure, allowing for accurate tracking of memory usage by zswap and reducing NR_ZSWAP. The changes also move memcg charges to the zsmalloc layer, which is the only user of zswap at this time.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "David Hildenbrand",
              "summary": "Raised concerns about the increased size of struct zpdesc and the potential impact on performance. Suggested exploring alternative solutions to achieve the desired accounting functionality without incurring significant overhead.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "PERFORMANCE_CONCERN"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[RFC PATCH 0/6] mm/memcontrol: Make memcg limits tier-aware",
          "message_id": "20260223223830.586018-1-joshua.hahnjy@gmail.com",
          "url": "https://lore.kernel.org/all/20260223223830.586018-1-joshua.hahnjy@gmail.com/",
          "date": "2026-02-24T00:19:15Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-24",
          "patch_summary": "This patch introduces a new sysfs entry to toggle between memory cgroup (memcg) limits that are proportional to the system's top-tier capacity ratio. The goal is to make memcg limits tier-aware, allowing for more efficient resource allocation and utilization. This is achieved by adding a boolean flag `tier_aware_memcg_limits` and two new sysfs attributes: `tier_aware_memcg_show` and `tier_aware_memcg_store`. When enabled, the memcg limits will be adjusted based on the system's top-tier capacity ratio, allowing for more efficient resource allocation. The patch is part of a larger series that aims to make memory management more efficient and scalable.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Joshua Hahn (author)",
              "summary": "The author addressed a concern about the performance impact of relying on per-memcg-lruvec statistics for limit checking, explaining that introducing a new cacheline in struct page_counter to track tiered memory limits and usage would reduce latency. The author confirmed that this approach is being taken instead of using lruvec stats.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged a concern",
                "confirmed an alternative solution"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On systems with tiered memory, there is currently no tracking of memory\nat the tier-memcg granularity. While per-memcg-lruvec serves at a finer\ngranularity that can be accumulated to give us the desired\nper-tier-memcg accounting, relying on these lruvec stats for limit\nchecking can prove touch too many hot paths too frequently and can\nintroduce increased latency for other memcg users.\n\nInstead, add a new cacheline in struct page_counter to track toptier\nmemcg limits and usage, as well as cached capacity values. This\ncacheline is only used by the mem_cgroup->memory page_counter.\n\nAlso, introduce helpers that use these new fields to calculate\nproportional toptier high and low values, based on the system's\ntoptier:total capacity ratio.\n\nSigned-off-by: Joshua Hahn <joshua.hahnjy@gmail.com>\n---\n include/linux/page_counter.h | 22 +++++++++++++++++++++-\n mm/page_counter.c            | 34 ++++++++++++++++++++++++++++++++++\n 2 files changed, 55 insertions(+), 1 deletion(-)\n\ndiff --git a/include/linux/page_counter.h b/include/linux/page_counter.h\nindex d649b6bbbc87..128c1272c88c 100644\n--- a/include/linux/page_counter.h\n+++ b/include/linux/page_counter.h\n@@ -5,6 +5,7 @@\n #include <linux/atomic.h>\n #include <linux/cache.h>\n #include <linux/limits.h>\n+#include <linux/nodemask.h>\n #include <asm/page.h>\n \n struct page_counter {\n@@ -31,9 +32,23 @@ struct page_counter {\n \t/* Latest cg2 reset watermark */\n \tunsigned long local_watermark;\n \n-\t/* Keep all the read most fields in a separete cacheline. */\n+\t/* Keep all the tiered memory fields in a separate cacheline. */\n \tCACHELINE_PADDING(_pad2_);\n \n+\tatomic_long_t toptier_usage;\n+\n+\t/* effective toptier-proportional low protection */\n+\tunsigned long etoptier_low;\n+\tatomic_long_t toptier_low_usage;\n+\tatomic_long_t children_toptier_low_usage;\n+\n+\t/* Cached toptier capacity for proportional limit calculations */\n+\tunsigned long toptier_capacity;\n+\tunsigned long total_capacity;\n+\n+\t/* Keep all the read most fields in a separate cacheline. */\n+\tCACHELINE_PADDING(_pad3_);\n+\n \tbool protection_support;\n \tbool track_failcnt;\n \tunsigned long min;\n@@ -61,6 +76,9 @@ static inline void page_counter_init(struct page_counter *counter,\n \tcounter->parent = parent;\n \tcounter->protection_support = protection_support;\n \tcounter->track_failcnt = false;\n+\tcounter->toptier_usage = (atomic_long_t)ATOMIC_LONG_INIT(0);\n+\tcounter->toptier_capacity = 0;\n+\tcounter->total_capacity = 0;\n }\n \n static inline unsigned long page_counter_read(struct page_counter *counter)\n@@ -103,6 +121,8 @@ static inline void page_counter_reset_watermark(struct page_counter *counter)\n void page_counter_calculate_protection(struct page_counter *root,\n \t\t\t\t       struct page_counter *counter,\n \t\t\t\t       bool recursive_protection);\n+unsigned long page_counter_toptier_high(struct page_counter *counter);\n+unsigned long page_counter_toptier_low(struct page_counter *counter);\n #else\n static inline void page_counter_calculate_protection(struct page_counter *root,\n \t\t\t\t\t\t     struct page_counter *counter,\ndiff --git a/mm/page_counter.c b/mm/page_counter.c\nindex 661e0f2a5127..5ec97811c418 100644\n--- a/mm/page_counter.c\n+++ b/mm/page_counter.c\n@@ -462,4 +462,38 @@ void page_counter_calculate_protection(struct page_counter *root,\n \t\t\tatomic_long_read(&parent->children_low_usage),\n \t\t\trecursive_protection));\n }\n+\n+unsigned long page_counter_toptier_high(struct page_counter *counter)\n+{\n+\tunsigned long high = READ_ONCE(counter->high);\n+\tunsigned long toptier_cap, total_cap;\n+\n+\tif (high == PAGE_COUNTER_MAX)\n+\t\treturn PAGE_COUNTER_MAX;\n+\n+\ttoptier_cap = counter->toptier_capacity;\n+\ttotal_cap = counter->total_capacity;\n+\n+\tif (!total_cap)\n+\t\treturn PAGE_COUNTER_MAX;\n+\n+\treturn mult_frac(high, toptier_cap, total_cap);\n+}\n+\n+unsigned long page_counter_toptier_low(struct page_counter *counter)\n+{\n+\tunsigned long low = READ_ONCE(counter->low);\n+\tunsigned long toptier_cap, total_cap;\n+\n+\tif (!low)\n+\t\treturn 0;\n+\n+\ttoptier_cap = counter->toptier_capacity;\n+\ttotal_cap = counter->total_capacity;\n+\n+\tif (!total_cap)\n+\t\treturn 0;\n+\n+\treturn mult_frac(low, toptier_cap, total_cap);\n+}\n #endif /* CONFIG_MEMCG || CONFIG_CGROUP_DMEM */\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "The author addressed a concern about updating toptier statistics after charging or uncharging memory control groups (memcgs). They modified the `charge_memcg` function to update toptier fields only when charging succeeds, and added new functions `memcg_charge_toptier` and `memcg_uncharge_toptier` to handle this. The author also updated other functions like `uncharge_batch`, `uncharge_folio`, and `mem_cgroup_replace_folio` to account for toptier usage.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "acknowledged a fix is needed",
                "modified code"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Modify memcg charging and uncharging sites to also update toptier\nstatistics.\n\nUnfortunately, try_charge_memcg is unaware of the physical folio being\ncharged; it only deals with nr_pages. Instead of modifying\ntry_charge_memcg, instead adjust the toptier fields once\ntry_charge_memcg succeeds, inside charge_memcg.\n\nSigned-off-by: Joshua Hahn <joshua.hahnjy@gmail.com>\n---\n mm/memcontrol.c | 39 +++++++++++++++++++++++++++++++++++++++\n 1 file changed, 39 insertions(+)\n\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex f3e4a6ce7181..07464f02c529 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -1948,6 +1948,24 @@ static void memcg_uncharge(struct mem_cgroup *memcg, unsigned int nr_pages)\n \t\tpage_counter_uncharge(&memcg->memsw, nr_pages);\n }\n \n+static void memcg_charge_toptier(struct mem_cgroup *memcg,\n+\t\t\t\t unsigned long nr_pages)\n+{\n+\tstruct page_counter *c;\n+\n+\tfor (c = &memcg->memory; c; c = c->parent)\n+\t\tatomic_long_add(nr_pages, &c->toptier_usage);\n+}\n+\n+static void memcg_uncharge_toptier(struct mem_cgroup *memcg,\n+\t\t\t\t   unsigned long nr_pages)\n+{\n+\tstruct page_counter *c;\n+\n+\tfor (c = &memcg->memory; c; c = c->parent)\n+\t\tatomic_long_sub(nr_pages, &c->toptier_usage);\n+}\n+\n /*\n  * Returns stocks cached in percpu and reset cached information.\n  */\n@@ -4830,6 +4848,9 @@ static int charge_memcg(struct folio *folio, struct mem_cgroup *memcg,\n \tif (ret)\n \t\tgoto out;\n \n+\tif (node_is_toptier(folio_nid(folio)))\n+\t\tmemcg_charge_toptier(memcg, folio_nr_pages(folio));\n+\n \tcss_get(&memcg->css);\n \tcommit_charge(folio, memcg);\n \tmemcg1_commit_charge(folio, memcg);\n@@ -4921,6 +4942,7 @@ int mem_cgroup_swapin_charge_folio(struct folio *folio, struct mm_struct *mm,\n struct uncharge_gather {\n \tstruct mem_cgroup *memcg;\n \tunsigned long nr_memory;\n+\tunsigned long nr_toptier;\n \tunsigned long pgpgout;\n \tunsigned long nr_kmem;\n \tint nid;\n@@ -4941,6 +4963,8 @@ static void uncharge_batch(const struct uncharge_gather *ug)\n \t\t}\n \t\tmemcg1_oom_recover(ug->memcg);\n \t}\n+\tif (ug->nr_toptier)\n+\t\tmemcg_uncharge_toptier(ug->memcg, ug->nr_toptier);\n \n \tmemcg1_uncharge_batch(ug->memcg, ug->pgpgout, ug->nr_memory, ug->nid);\n \n@@ -4989,6 +5013,9 @@ static void uncharge_folio(struct folio *folio, struct uncharge_gather *ug)\n \n \tnr_pages = folio_nr_pages(folio);\n \n+\tif (node_is_toptier(folio_nid(folio)))\n+\t\tug->nr_toptier += nr_pages;\n+\n \tif (folio_memcg_kmem(folio)) {\n \t\tug->nr_memory += nr_pages;\n \t\tug->nr_kmem += nr_pages;\n@@ -5072,6 +5099,10 @@ void mem_cgroup_replace_folio(struct folio *old, struct folio *new)\n \t\t\tpage_counter_charge(&memcg->memsw, nr_pages);\n \t}\n \n+\t/* The old folio's toptier_usage will be decremented when it is freed */\n+\tif (node_is_toptier(folio_nid(new)))\n+\t\tmemcg_charge_toptier(memcg, nr_pages);\n+\n \tcss_get(&memcg->css);\n \tcommit_charge(new, memcg);\n \tmemcg1_commit_charge(new, memcg);\n@@ -5091,6 +5122,7 @@ void mem_cgroup_replace_folio(struct folio *old, struct folio *new)\n void mem_cgroup_migrate(struct folio *old, struct folio *new)\n {\n \tstruct mem_cgroup *memcg;\n+\tint old_toptier, new_toptier;\n \n \tVM_BUG_ON_FOLIO(!folio_test_locked(old), old);\n \tVM_BUG_ON_FOLIO(!folio_test_locked(new), new);\n@@ -5111,6 +5143,13 @@ void mem_cgroup_migrate(struct folio *old, struct folio *new)\n \tif (!memcg)\n \t\treturn;\n \n+\told_toptier = node_is_toptier(folio_nid(old));\n+\tnew_toptier = node_is_toptier(folio_nid(new));\n+\tif (old_toptier && !new_toptier)\n+\t\tmemcg_uncharge_toptier(memcg, folio_nr_pages(old));\n+\telse if (!old_toptier && new_toptier)\n+\t\tmemcg_charge_toptier(memcg, folio_nr_pages(old));\n+\n \t/* Transfer the charge and the css ref */\n \tcommit_charge(new, memcg);\n \n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "Author addressed a concern about the dynamic nature of toptier nodes and introduced functions to calculate and update toptier capacity during cpuset.mems changes and memory hotplug events.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "acknowledged a need for dynamic updates",
                "introduced new functions"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "What a memcg considers to be a valid toptier node is defined by three\ncriteria: (1) The node has CPUs, (2) The node has online memory,\nand (3) The node is within the cgroup's cpuset.mems.\n\nOf the three, the second and third criteria are the only ones that can\nchange dynamically during runtime, via memory hotplug events and\ncpuset.mems changes, respectively.\n\nIntroduce functions to calculate and update toptier capacity, and call\nthem during cpuset.mems changes and memory hotplug events.\n\nSigned-off-by: Joshua Hahn <joshua.hahnjy@gmail.com>\n---\n include/linux/memcontrol.h   |  6 ++++++\n include/linux/memory-tiers.h | 29 +++++++++++++++++++++++++\n include/linux/page_counter.h |  2 ++\n kernel/cgroup/cpuset.c       |  2 +-\n mm/memcontrol.c              | 17 +++++++++++++++\n mm/memory-tiers.c            | 41 ++++++++++++++++++++++++++++++++++++\n mm/page_counter.c            |  8 +++++++\n 7 files changed, 104 insertions(+), 1 deletion(-)\n\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex 5173a9f16721..900a36112b62 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -608,6 +608,8 @@ static inline void mem_cgroup_protection(struct mem_cgroup *root,\n void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n \t\t\t\t     struct mem_cgroup *memcg);\n \n+void update_memcg_toptier_capacity(void);\n+\n static inline bool mem_cgroup_unprotected(struct mem_cgroup *target,\n \t\t\t\t\t  struct mem_cgroup *memcg)\n {\n@@ -1116,6 +1118,10 @@ static inline void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n {\n }\n \n+static inline void update_memcg_toptier_capacity(void)\n+{\n+}\n+\n static inline bool mem_cgroup_unprotected(struct mem_cgroup *target,\n \t\t\t\t\t  struct mem_cgroup *memcg)\n {\ndiff --git a/include/linux/memory-tiers.h b/include/linux/memory-tiers.h\nindex 85440473effb..cf616885e0db 100644\n--- a/include/linux/memory-tiers.h\n+++ b/include/linux/memory-tiers.h\n@@ -53,6 +53,9 @@ int mt_perf_to_adistance(struct access_coordinate *perf, int *adist);\n struct memory_dev_type *mt_find_alloc_memory_type(int adist,\n \t\t\t\t\t\t  struct list_head *memory_types);\n void mt_put_memory_types(struct list_head *memory_types);\n+void mt_get_toptier_nodemask(nodemask_t *mask, const nodemask_t *allowed);\n+unsigned long mt_get_toptier_capacity(const nodemask_t *allowed);\n+unsigned long mt_get_total_capacity(const nodemask_t *allowed);\n #ifdef CONFIG_MIGRATION\n int next_demotion_node(int node, const nodemask_t *allowed_mask);\n void node_get_allowed_targets(pg_data_t *pgdat, nodemask_t *targets);\n@@ -152,5 +155,31 @@ static inline struct memory_dev_type *mt_find_alloc_memory_type(int adist,\n static inline void mt_put_memory_types(struct list_head *memory_types)\n {\n }\n+\n+static inline void mt_get_toptier_nodemask(nodemask_t *mask,\n+\t\t\t\t\t   const nodemask_t *allowed)\n+{\n+\t*mask = node_states[N_MEMORY];\n+\tif (allowed)\n+\t\tnodes_and(*mask, *mask, *allowed);\n+}\n+\n+static inline unsigned long mt_get_toptier_capacity(const nodemask_t *allowed)\n+{\n+\tint nid;\n+\tunsigned long capacity = 0;\n+\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tif (allowed && !node_isset(nid, *allowed))\n+\t\t\tcontinue;\n+\t\tcapacity += NODE_DATA(nid)->node_present_pages;\n+\t}\n+\treturn capacity;\n+}\n+\n+static inline unsigned long mt_get_total_capacity(const nodemask_t *allowed)\n+{\n+\treturn mt_get_toptier_capacity(allowed);\n+}\n #endif\t/* CONFIG_NUMA */\n #endif  /* _LINUX_MEMORY_TIERS_H */\ndiff --git a/include/linux/page_counter.h b/include/linux/page_counter.h\nindex 128c1272c88c..ada5f1dd75d4 100644\n--- a/include/linux/page_counter.h\n+++ b/include/linux/page_counter.h\n@@ -121,6 +121,8 @@ static inline void page_counter_reset_watermark(struct page_counter *counter)\n void page_counter_calculate_protection(struct page_counter *root,\n \t\t\t\t       struct page_counter *counter,\n \t\t\t\t       bool recursive_protection);\n+void page_counter_update_toptier_capacity(struct page_counter *counter,\n+\t\t\t\t\t  const nodemask_t *allowed);\n unsigned long page_counter_toptier_high(struct page_counter *counter);\n unsigned long page_counter_toptier_low(struct page_counter *counter);\n #else\ndiff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c\nindex 7607dfe516e6..e5641dc1af88 100644\n--- a/kernel/cgroup/cpuset.c\n+++ b/kernel/cgroup/cpuset.c\n@@ -2620,7 +2620,6 @@ static void update_nodemasks_hier(struct cpuset *cs, nodemask_t *new_mems)\n \trcu_read_lock();\n \tcpuset_for_each_descendant_pre(cp, pos_css, cs) {\n \t\tstruct cpuset *parent = parent_cs(cp);\n-\n \t\tbool has_mems = nodes_and(*new_mems, cp->mems_allowed, parent->effective_mems);\n \n \t\t/*\n@@ -2701,6 +2700,7 @@ static int update_nodemask(struct cpuset *cs, struct cpuset *trialcs,\n \n \t/* use trialcs->mems_allowed as a temp variable */\n \tupdate_nodemasks_hier(cs, &trialcs->mems_allowed);\n+\tupdate_memcg_toptier_capacity();\n \treturn 0;\n }\n \ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 0be1e823d813..f3e4a6ce7181 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -54,6 +54,7 @@\n #include <linux/seq_file.h>\n #include <linux/vmpressure.h>\n #include <linux/memremap.h>\n+#include <linux/memory-tiers.h>\n #include <linux/mm_inline.h>\n #include <linux/swap_cgroup.h>\n #include <linux/cpu.h>\n@@ -3906,6 +3907,7 @@ mem_cgroup_css_alloc(struct cgroup_subsys_state *parent_css)\n \n \t\tpage_counter_init(&memcg->memory, &parent->memory, memcg_on_dfl);\n \t\tpage_counter_init(&memcg->swap, &parent->swap, false);\n+\t\tpage_counter_update_toptier_capacity(&memcg->memory, NULL);\n #ifdef CONFIG_MEMCG_V1\n \t\tmemcg->memory.track_failcnt = !memcg_on_dfl;\n \t\tWRITE_ONCE(memcg->oom_kill_disable, READ_ONCE(parent->oom_kill_disable));\n@@ -3917,6 +3919,7 @@ mem_cgroup_css_alloc(struct cgroup_subsys_state *parent_css)\n \t\tinit_memcg_events();\n \t\tpage_counter_init(&memcg->memory, NULL, true);\n \t\tpage_counter_init(&memcg->swap, NULL, false);\n+\t\tpage_counter_update_toptier_capacity(&memcg->memory, NULL);\n #ifdef CONFIG_MEMCG_V1\n \t\tpage_counter_init(&memcg->kmem, NULL, false);\n \t\tpage_counter_init(&memcg->tcpmem, NULL, false);\n@@ -4804,6 +4807,20 @@ void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n \tpage_counter_calculate_protection(&root->memory, &memcg->memory, recursive_protection);\n }\n \n+void update_memcg_toptier_capacity(void)\n+{\n+\tstruct mem_cgroup *memcg;\n+\tnodemask_t allowed;\n+\n+\tfor_each_mem_cgroup(memcg) {\n+\t\tif (memcg == root_mem_cgroup)\n+\t\t\tcontinue;\n+\n+\t\tcpuset_nodes_allowed(memcg->css.cgroup, &allowed);\n+\t\tpage_counter_update_toptier_capacity(&memcg->memory, &allowed);\n+\t}\n+}\n+\n static int charge_memcg(struct folio *folio, struct mem_cgroup *memcg,\n \t\t\tgfp_t gfp)\n {\ndiff --git a/mm/memory-tiers.c b/mm/memory-tiers.c\nindex a88256381519..259caaf4be8f 100644\n--- a/mm/memory-tiers.c\n+++ b/mm/memory-tiers.c\n@@ -889,6 +889,7 @@ static int __meminit memtier_hotplug_callback(struct notifier_block *self,\n \t\tmutex_lock(&memory_tier_lock);\n \t\tif (clear_node_memory_tier(nn->nid))\n \t\t\testablish_demotion_targets();\n+\t\tupdate_memcg_toptier_capacity();\n \t\tmutex_unlock(&memory_tier_lock);\n \t\tbreak;\n \tcase NODE_ADDED_FIRST_MEMORY:\n@@ -896,6 +897,7 @@ static int __meminit memtier_hotplug_callback(struct notifier_block *self,\n \t\tmemtier = set_node_memory_tier(nn->nid);\n \t\tif (!IS_ERR(memtier))\n \t\t\testablish_demotion_targets();\n+\t\tupdate_memcg_toptier_capacity();\n \t\tmutex_unlock(&memory_tier_lock);\n \t\tbreak;\n \t}\n@@ -941,6 +943,45 @@ bool numa_demotion_enabled = false;\n \n bool tier_aware_memcg_limits;\n \n+void mt_get_toptier_nodemask(nodemask_t *mask, const nodemask_t *allowed)\n+{\n+\tint nid;\n+\n+\t*mask = NODE_MASK_NONE;\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tif (node_is_toptier(nid))\n+\t\t\tnode_set(nid, *mask);\n+\t}\n+\tif (allowed)\n+\t\tnodes_and(*mask, *mask, *allowed);\n+}\n+\n+unsigned long mt_get_toptier_capacity(const nodemask_t *allowed)\n+{\n+\tint nid;\n+\tunsigned long capacity = 0;\n+\tnodemask_t mask;\n+\n+\tmt_get_toptier_nodemask(&mask, allowed);\n+\tfor_each_node_mask(nid, mask)\n+\t\tcapacity += NODE_DATA(nid)->node_present_pages;\n+\n+\treturn capacity;\n+}\n+\n+unsigned long mt_get_total_capacity(const nodemask_t *allowed)\n+{\n+\tint nid;\n+\tunsigned long capacity = 0;\n+\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tif (allowed && !node_isset(nid, *allowed))\n+\t\t\tcontinue;\n+\t\tcapacity += NODE_DATA(nid)->node_present_pages;\n+\t}\n+\treturn capacity;\n+}\n+\n #ifdef CONFIG_MIGRATION\n #ifdef CONFIG_SYSFS\n static ssize_t demotion_enabled_show(struct kobject *kobj,\ndiff --git a/mm/page_counter.c b/mm/page_counter.c\nindex 5ec97811c418..cf21c72bfd4e 100644\n--- a/mm/page_counter.c\n+++ b/mm/page_counter.c\n@@ -11,6 +11,7 @@\n #include <linux/string.h>\n #include <linux/sched.h>\n #include <linux/bug.h>\n+#include <linux/memory-tiers.h>\n #include <asm/page.h>\n \n static bool track_protection(struct page_counter *c)\n@@ -463,6 +464,13 @@ void page_counter_calculate_protection(struct page_counter *root,\n \t\t\trecursive_protection));\n }\n \n+void page_counter_update_toptier_capacity(struct page_counter *counter,\n+\t\t\t\t\t  const nodemask_t *allowed)\n+{\n+\tcounter->toptier_capacity = mt_get_toptier_capacity(allowed);\n+\tcounter->total_capacity = mt_get_total_capacity(allowed);\n+}\n+\n unsigned long page_counter_toptier_high(struct page_counter *counter)\n {\n \tunsigned long high = READ_ONCE(counter->high);\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "The author is addressing a concern about fairness in distributing toptier memory among workloads, which is currently impossible due to the lack of tier-aware memcg limits. The author explains that their patch extends the existing memory.low protection to be tier-aware and provides best-effort attempts at protecting a fair proportion of toptier memory. The enforcement of tier-aware memcg limits is gated behind a sysctl.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "clarification",
                "explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On machines serving multiple workloads whose memory is isolated via\nthe memory cgroup controller, it is currently impossible to enforce a\nfair distribution of toptier memory among the worloads, as the only\nenforcable limits have to do with total memory footprint, but not where\nthat memory resides.\n\nThis makes ensuring a consistent and baseline performance difficult, as\neach workload's performance is heavily impacted by workload-external\nfactors such as which other workloads are co-located in the same host,\nand the order at which different workloads are started.\n\nExtend the existing memory.low protection to be tier-aware in the\ncharging, enforcement, and protection calculation to provide\nbest-effort attempts at protecting a fair proportion of toptier memory.\n\nUpdates to protection and charging are performed in the same path as\nthe standard memcontrol equivalents. Enforcing tier-aware memcg limits\nhowever, are gated behind the sysctl tier_aware_memcg. This is so that\nruntime-enabling of tier aware limits can account for memory already\npresent in the system.\n\nSigned-off-by: Joshua Hahn <joshua.hahnjy@gmail.com>\n---\n include/linux/memcontrol.h   | 15 +++++++++++----\n include/linux/page_counter.h |  7 ++++---\n kernel/cgroup/dmem.c         |  2 +-\n mm/memcontrol.c              | 14 ++++++++++++--\n mm/page_counter.c            | 35 ++++++++++++++++++++++++++++++++++-\n mm/vmscan.c                  | 13 +++++++++----\n 6 files changed, 71 insertions(+), 15 deletions(-)\n\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex 900a36112b62..a998a1e3b8b0 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -606,7 +606,9 @@ static inline void mem_cgroup_protection(struct mem_cgroup *root,\n }\n \n void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n-\t\t\t\t     struct mem_cgroup *memcg);\n+\t\t\t\t     struct mem_cgroup *memcg, bool toptier);\n+\n+unsigned long mem_cgroup_toptier_usage(struct mem_cgroup *memcg);\n \n void update_memcg_toptier_capacity(void);\n \n@@ -623,11 +625,15 @@ static inline bool mem_cgroup_unprotected(struct mem_cgroup *target,\n }\n \n static inline bool mem_cgroup_below_low(struct mem_cgroup *target,\n-\t\t\t\t\tstruct mem_cgroup *memcg)\n+\t\t\t\t\tstruct mem_cgroup *memcg, bool toptier)\n {\n \tif (mem_cgroup_unprotected(target, memcg))\n \t\treturn false;\n \n+\tif (toptier)\n+\t\treturn READ_ONCE(memcg->memory.etoptier_low) >=\n+\t\t\t\t mem_cgroup_toptier_usage(memcg);\n+\n \treturn READ_ONCE(memcg->memory.elow) >=\n \t\tpage_counter_read(&memcg->memory);\n }\n@@ -1114,7 +1120,8 @@ static inline void mem_cgroup_protection(struct mem_cgroup *root,\n }\n \n static inline void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n-\t\t\t\t\t\t   struct mem_cgroup *memcg)\n+\t\t\t\t\t\t   struct mem_cgroup *memcg,\n+\t\t\t\t\t\t   bool toptier)\n {\n }\n \n@@ -1128,7 +1135,7 @@ static inline bool mem_cgroup_unprotected(struct mem_cgroup *target,\n \treturn true;\n }\n static inline bool mem_cgroup_below_low(struct mem_cgroup *target,\n-\t\t\t\t\tstruct mem_cgroup *memcg)\n+\t\t\t\t\tstruct mem_cgroup *memcg, bool toptier)\n {\n \treturn false;\n }\ndiff --git a/include/linux/page_counter.h b/include/linux/page_counter.h\nindex ada5f1dd75d4..6635ee7b9575 100644\n--- a/include/linux/page_counter.h\n+++ b/include/linux/page_counter.h\n@@ -120,15 +120,16 @@ static inline void page_counter_reset_watermark(struct page_counter *counter)\n #if IS_ENABLED(CONFIG_MEMCG) || IS_ENABLED(CONFIG_CGROUP_DMEM)\n void page_counter_calculate_protection(struct page_counter *root,\n \t\t\t\t       struct page_counter *counter,\n-\t\t\t\t       bool recursive_protection);\n+\t\t\t\t       bool recursive_protection, bool toptier);\n void page_counter_update_toptier_capacity(struct page_counter *counter,\n \t\t\t\t\t  const nodemask_t *allowed);\n unsigned long page_counter_toptier_high(struct page_counter *counter);\n unsigned long page_counter_toptier_low(struct page_counter *counter);\n #else\n static inline void page_counter_calculate_protection(struct page_counter *root,\n-\t\t\t\t\t\t     struct page_counter *counter,\n-\t\t\t\t\t\t     bool recursive_protection) {}\n+\t\t\t\t\t\tstruct page_counter *counter,\n+\t\t\t\t\t\tbool recursive_protection,\n+\t\t\t\t\t\tbool toptier) {}\n #endif\n \n #endif /* _LINUX_PAGE_COUNTER_H */\ndiff --git a/kernel/cgroup/dmem.c b/kernel/cgroup/dmem.c\nindex 1ea6afffa985..536d43c42de8 100644\n--- a/kernel/cgroup/dmem.c\n+++ b/kernel/cgroup/dmem.c\n@@ -277,7 +277,7 @@ dmem_cgroup_calculate_protection(struct dmem_cgroup_pool_state *limit_pool,\n \t\t\tcontinue;\n \n \t\tpage_counter_calculate_protection(\n-\t\t\tclimit, &found_pool->cnt, true);\n+\t\t\tclimit, &found_pool->cnt, true, false);\n \n \t\tif (found_pool == test_pool)\n \t\t\tbreak;\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 07464f02c529..8aa7ae361a73 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -4806,12 +4806,13 @@ struct cgroup_subsys memory_cgrp_subsys = {\n  * mem_cgroup_calculate_protection - check if memory consumption is in the normal range\n  * @root: the top ancestor of the sub-tree being checked\n  * @memcg: the memory cgroup to check\n+ * @toptier: whether the caller is in a toptier node\n  *\n  * WARNING: This function is not stateless! It can only be used as part\n  *          of a top-down tree iteration, not for isolated queries.\n  */\n void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n-\t\t\t\t     struct mem_cgroup *memcg)\n+\t\t\t\t     struct mem_cgroup *memcg, bool toptier)\n {\n \tbool recursive_protection =\n \t\tcgrp_dfl_root.flags & CGRP_ROOT_MEMORY_RECURSIVE_PROT;\n@@ -4822,7 +4823,16 @@ void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n \tif (!root)\n \t\troot = root_mem_cgroup;\n \n-\tpage_counter_calculate_protection(&root->memory, &memcg->memory, recursive_protection);\n+\tpage_counter_calculate_protection(&root->memory, &memcg->memory,\n+\t\t\t\t\t  recursive_protection, toptier);\n+}\n+\n+unsigned long mem_cgroup_toptier_usage(struct mem_cgroup *memcg)\n+{\n+\tif (mem_cgroup_disabled() || !memcg)\n+\t\treturn 0;\n+\n+\treturn atomic_long_read(&memcg->memory.toptier_usage);\n }\n \n void update_memcg_toptier_capacity(void)\ndiff --git a/mm/page_counter.c b/mm/page_counter.c\nindex cf21c72bfd4e..79d46a1c4c0c 100644\n--- a/mm/page_counter.c\n+++ b/mm/page_counter.c\n@@ -410,12 +410,39 @@ static unsigned long effective_protection(unsigned long usage,\n \treturn ep;\n }\n \n+static void calculate_protection_toptier(struct page_counter *counter,\n+\t\t\t\t\t bool recursive_protection)\n+{\n+\tstruct page_counter *parent = counter->parent;\n+\tunsigned long toptier_low;\n+\tunsigned long toptier_usage, parent_toptier_usage;\n+\tunsigned long toptier_protected, old_toptier_protected;\n+\tlong delta;\n+\n+\ttoptier_low = page_counter_toptier_low(counter);\n+\ttoptier_usage = atomic_long_read(&counter->toptier_usage);\n+\tparent_toptier_usage = atomic_long_read(&parent->toptier_usage);\n+\n+\t/* Propagate toptier low usage to parent for sibling distribution */\n+\ttoptier_protected = min(toptier_usage, toptier_low);\n+\told_toptier_protected = atomic_long_xchg(&counter->toptier_low_usage,\n+\t\t\t\t\t\t toptier_protected);\n+\tdelta = toptier_protected - old_toptier_protected;\n+\tatomic_long_add(delta, &parent->children_toptier_low_usage);\n+\n+\tWRITE_ONCE(counter->etoptier_low,\n+\t\t   effective_protection(toptier_usage, parent_toptier_usage,\n+\t\t   toptier_low, READ_ONCE(parent->etoptier_low),\n+\t\t   atomic_long_read(&parent->children_toptier_low_usage),\n+\t\t   recursive_protection));\n+}\n \n /**\n  * page_counter_calculate_protection - check if memory consumption is in the normal range\n  * @root: the top ancestor of the sub-tree being checked\n  * @counter: the page_counter the counter to update\n  * @recursive_protection: Whether to use memory_recursiveprot behavior.\n+ * @toptier: Whether to calculate toptier-proportional protection\n  *\n  * Calculates elow/emin thresholds for given page_counter.\n  *\n@@ -424,7 +451,7 @@ static unsigned long effective_protection(unsigned long usage,\n  */\n void page_counter_calculate_protection(struct page_counter *root,\n \t\t\t\t       struct page_counter *counter,\n-\t\t\t\t       bool recursive_protection)\n+\t\t\t\t       bool recursive_protection, bool toptier)\n {\n \tunsigned long usage, parent_usage;\n \tstruct page_counter *parent = counter->parent;\n@@ -446,6 +473,9 @@ void page_counter_calculate_protection(struct page_counter *root,\n \tif (parent == root) {\n \t\tcounter->emin = READ_ONCE(counter->min);\n \t\tcounter->elow = READ_ONCE(counter->low);\n+\t\tif (toptier)\n+\t\t\tWRITE_ONCE(counter->etoptier_low,\n+\t\t\t\t   page_counter_toptier_low(counter));\n \t\treturn;\n \t}\n \n@@ -462,6 +492,9 @@ void page_counter_calculate_protection(struct page_counter *root,\n \t\t\tREAD_ONCE(parent->elow),\n \t\t\tatomic_long_read(&parent->children_low_usage),\n \t\t\trecursive_protection));\n+\n+\tif (toptier)\n+\t\tcalculate_protection_toptier(counter, recursive_protection);\n }\n \n void page_counter_update_toptier_capacity(struct page_counter *counter,\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 6a87ac7be43c..5b4cb030a477 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -4144,6 +4144,7 @@ static void lru_gen_age_node(struct pglist_data *pgdat, struct scan_control *sc)\n \tstruct mem_cgroup *memcg;\n \tunsigned long min_ttl = READ_ONCE(lru_gen_min_ttl);\n \tbool reclaimable = !min_ttl;\n+\tbool toptier = node_is_toptier(pgdat->node_id);\n \n \tVM_WARN_ON_ONCE(!current_is_kswapd());\n \n@@ -4153,7 +4154,7 @@ static void lru_gen_age_node(struct pglist_data *pgdat, struct scan_control *sc)\n \tdo {\n \t\tstruct lruvec *lruvec = mem_cgroup_lruvec(memcg, pgdat);\n \n-\t\tmem_cgroup_calculate_protection(NULL, memcg);\n+\t\tmem_cgroup_calculate_protection(NULL, memcg, toptier);\n \n \t\tif (!reclaimable)\n \t\t\treclaimable = lruvec_is_reclaimable(lruvec, sc, min_ttl);\n@@ -4905,12 +4906,14 @@ static int shrink_one(struct lruvec *lruvec, struct scan_control *sc)\n \tunsigned long reclaimed = sc->nr_reclaimed;\n \tstruct mem_cgroup *memcg = lruvec_memcg(lruvec);\n \tstruct pglist_data *pgdat = lruvec_pgdat(lruvec);\n+\tbool toptier = tier_aware_memcg_limits &&\n+\t\t       node_is_toptier(pgdat->node_id);\n \n \t/* lru_gen_age_node() called mem_cgroup_calculate_protection() */\n \tif (mem_cgroup_below_min(NULL, memcg))\n \t\treturn MEMCG_LRU_YOUNG;\n \n-\tif (mem_cgroup_below_low(NULL, memcg)) {\n+\tif (mem_cgroup_below_low(NULL, memcg, toptier)) {\n \t\t/* see the comment on MEMCG_NR_GENS */\n \t\tif (READ_ONCE(lruvec->lrugen.seg) != MEMCG_LRU_TAIL)\n \t\t\treturn MEMCG_LRU_TAIL;\n@@ -5960,6 +5963,7 @@ static void shrink_node_memcgs(pg_data_t *pgdat, struct scan_control *sc)\n \t};\n \tstruct mem_cgroup_reclaim_cookie *partial = &reclaim;\n \tstruct mem_cgroup *memcg;\n+\tbool toptier = node_is_toptier(pgdat->node_id);\n \n \t/*\n \t * In most cases, direct reclaimers can do partial walks\n@@ -5987,7 +5991,7 @@ static void shrink_node_memcgs(pg_data_t *pgdat, struct scan_control *sc)\n \t\t */\n \t\tcond_resched();\n \n-\t\tmem_cgroup_calculate_protection(target_memcg, memcg);\n+\t\tmem_cgroup_calculate_protection(target_memcg, memcg, toptier);\n \n \t\tif (mem_cgroup_below_min(target_memcg, memcg)) {\n \t\t\t/*\n@@ -5995,7 +5999,8 @@ static void shrink_node_memcgs(pg_data_t *pgdat, struct scan_control *sc)\n \t\t\t * If there is no reclaimable memory, OOM.\n \t\t\t */\n \t\t\tcontinue;\n-\t\t} else if (mem_cgroup_below_low(target_memcg, memcg)) {\n+\t\t} else if (mem_cgroup_below_low(target_memcg, memcg,\n+\t\t\t\t\ttier_aware_memcg_limits && toptier)) {\n \t\t\t/*\n \t\t\t * Soft protection.\n \t\t\t * Respect the protection only as long as\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "The author addressed a concern about the interaction between memory cgroups and toptier usage limits by explaining how they plan to modify the existing high protection mechanism to be tier-aware, adding a new nodemask parameter to try_to_free_mem_cgroup_pages, and introducing a new function to calculate overage for toptier usage. The author confirmed that these changes will address the issue of workload-external factors impacting performance.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledged concern",
                "planned fix"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On machines serving multiple workloads whose memory is isolated via the\nmemory cgroup controller, it is currently impossible to enforce a fair\ndistribution of toptier memory among the workloads, as the only\nenforcable limits have to do with total memory footprint, but not where\nthat memory resides.\n\nThis makes ensuring a consistent and baseline performance difficult, as\neach workload's performance is heavily impacted by workload-external\nfactors wuch as which other workloads are co-located in the same host,\nand the order at which different workloads are started.\n\nExtend the existing memory.high protection to be tier-aware in the\ncharging and enforcement to limit toptier-hogging for workloads.\n\nAlso, add a new nodemask parameter to try_to_free_mem_cgroup_pages,\nwhich can be used to selectively reclaim from memory at the\nmemcg-tier interection of a cgroup.\n\nSigned-off-by: Joshua Hahn <joshua.hahnjy@gmail.com>\n---\n include/linux/swap.h |  3 +-\n mm/memcontrol-v1.c   |  6 ++--\n mm/memcontrol.c      | 85 +++++++++++++++++++++++++++++++++++++-------\n mm/vmscan.c          | 11 +++---\n 4 files changed, 84 insertions(+), 21 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 0effe3cc50f5..c6037ac7bf6e 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -368,7 +368,8 @@ extern unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,\n \t\t\t\t\t\t  unsigned long nr_pages,\n \t\t\t\t\t\t  gfp_t gfp_mask,\n \t\t\t\t\t\t  unsigned int reclaim_options,\n-\t\t\t\t\t\t  int *swappiness);\n+\t\t\t\t\t\t  int *swappiness,\n+\t\t\t\t\t\t  nodemask_t *allowed);\n extern unsigned long mem_cgroup_shrink_node(struct mem_cgroup *mem,\n \t\t\t\t\t\tgfp_t gfp_mask, bool noswap,\n \t\t\t\t\t\tpg_data_t *pgdat,\ndiff --git a/mm/memcontrol-v1.c b/mm/memcontrol-v1.c\nindex 0b39ba608109..29630c7f3567 100644\n--- a/mm/memcontrol-v1.c\n+++ b/mm/memcontrol-v1.c\n@@ -1497,7 +1497,8 @@ static int mem_cgroup_resize_max(struct mem_cgroup *memcg,\n \t\t}\n \n \t\tif (!try_to_free_mem_cgroup_pages(memcg, 1, GFP_KERNEL,\n-\t\t\t\tmemsw ? 0 : MEMCG_RECLAIM_MAY_SWAP, NULL)) {\n+\t\t\t\tmemsw ? 0 : MEMCG_RECLAIM_MAY_SWAP,\n+\t\t\t\tNULL, NULL)) {\n \t\t\tret = -EBUSY;\n \t\t\tbreak;\n \t\t}\n@@ -1529,7 +1530,8 @@ static int mem_cgroup_force_empty(struct mem_cgroup *memcg)\n \t\t\treturn -EINTR;\n \n \t\tif (!try_to_free_mem_cgroup_pages(memcg, 1, GFP_KERNEL,\n-\t\t\t\t\t\t  MEMCG_RECLAIM_MAY_SWAP, NULL))\n+\t\t\t\t\t\t  MEMCG_RECLAIM_MAY_SWAP,\n+\t\t\t\t\t\t  NULL, NULL))\n \t\t\tnr_retries--;\n \t}\n \ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 8aa7ae361a73..ebd4a1b73c51 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -2184,18 +2184,30 @@ static unsigned long reclaim_high(struct mem_cgroup *memcg,\n \n \tdo {\n \t\tunsigned long pflags;\n-\n-\t\tif (page_counter_read(&memcg->memory) <=\n-\t\t    READ_ONCE(memcg->memory.high))\n+\t\tnodemask_t toptier_nodes, *reclaim_nodes;\n+\t\tbool mem_high_ok, toptier_high_ok;\n+\n+\t\tmt_get_toptier_nodemask(&toptier_nodes, NULL);\n+\t\tmem_high_ok = page_counter_read(&memcg->memory) <=\n+\t\t\t      READ_ONCE(memcg->memory.high);\n+\t\ttoptier_high_ok = !(tier_aware_memcg_limits &&\n+\t\t\t\t    mem_cgroup_toptier_usage(memcg) >\n+\t\t\t\t    page_counter_toptier_high(&memcg->memory));\n+\t\tif (mem_high_ok && toptier_high_ok)\n \t\t\tcontinue;\n \n+\t\tif (mem_high_ok && !toptier_high_ok)\n+\t\t\treclaim_nodes = &toptier_nodes;\n+\t\telse\n+\t\t\treclaim_nodes = NULL;\n+\n \t\tmemcg_memory_event(memcg, MEMCG_HIGH);\n \n \t\tpsi_memstall_enter(&pflags);\n \t\tnr_reclaimed += try_to_free_mem_cgroup_pages(memcg, nr_pages,\n \t\t\t\t\t\t\tgfp_mask,\n \t\t\t\t\t\t\tMEMCG_RECLAIM_MAY_SWAP,\n-\t\t\t\t\t\t\tNULL);\n+\t\t\t\t\t\t\tNULL, reclaim_nodes);\n \t\tpsi_memstall_leave(&pflags);\n \t} while ((memcg = parent_mem_cgroup(memcg)) &&\n \t\t !mem_cgroup_is_root(memcg));\n@@ -2296,6 +2308,24 @@ static u64 mem_find_max_overage(struct mem_cgroup *memcg)\n \treturn max_overage;\n }\n \n+static u64 toptier_find_max_overage(struct mem_cgroup *memcg)\n+{\n+\tu64 overage, max_overage = 0;\n+\n+\tif (!tier_aware_memcg_limits)\n+\t\treturn 0;\n+\n+\tdo {\n+\t\tunsigned long usage = mem_cgroup_toptier_usage(memcg);\n+\t\tunsigned long high = page_counter_toptier_high(&memcg->memory);\n+\n+\t\toverage = calculate_overage(usage, high);\n+\t\tmax_overage = max(overage, max_overage);\n+\t} while ((memcg = parent_mem_cgroup(memcg)) &&\n+\t\t  !mem_cgroup_is_root(memcg));\n+\n+\treturn max_overage;\n+}\n static u64 swap_find_max_overage(struct mem_cgroup *memcg)\n {\n \tu64 overage, max_overage = 0;\n@@ -2401,6 +2431,14 @@ void __mem_cgroup_handle_over_high(gfp_t gfp_mask)\n \tpenalty_jiffies += calculate_high_delay(memcg, nr_pages,\n \t\t\t\t\t\tswap_find_max_overage(memcg));\n \n+\t/*\n+\t * Don't double-penalize for toptier high overage if system-wide\n+\t * memory.high has already been breached.\n+\t */\n+\tif (!penalty_jiffies)\n+\t\tpenalty_jiffies += calculate_high_delay(memcg, nr_pages,\n+\t\t\t\t\ttoptier_find_max_overage(memcg));\n+\n \t/*\n \t * Clamp the max delay per usermode return so as to still keep the\n \t * application moving forwards and also permit diagnostics, albeit\n@@ -2503,7 +2541,8 @@ static int try_charge_memcg(struct mem_cgroup *memcg, gfp_t gfp_mask,\n \n \tpsi_memstall_enter(&pflags);\n \tnr_reclaimed = try_to_free_mem_cgroup_pages(mem_over_limit, nr_pages,\n-\t\t\t\t\t\t    gfp_mask, reclaim_options, NULL);\n+\t\t\t\t\t\t    gfp_mask, reclaim_options,\n+\t\t\t\t\t\t    NULL, NULL);\n \tpsi_memstall_leave(&pflags);\n \n \tif (mem_cgroup_margin(mem_over_limit) >= nr_pages)\n@@ -2592,23 +2631,26 @@ static int try_charge_memcg(struct mem_cgroup *memcg, gfp_t gfp_mask,\n \t * reclaim, the cost of mismatch is negligible.\n \t */\n \tdo {\n-\t\tbool mem_high, swap_high;\n+\t\tbool mem_high, swap_high, toptier_high = false;\n \n \t\tmem_high = page_counter_read(&memcg->memory) >\n \t\t\tREAD_ONCE(memcg->memory.high);\n \t\tswap_high = page_counter_read(&memcg->swap) >\n \t\t\tREAD_ONCE(memcg->swap.high);\n+\t\ttoptier_high = tier_aware_memcg_limits &&\n+\t\t\t       (mem_cgroup_toptier_usage(memcg) >\n+\t\t\t\tpage_counter_toptier_high(&memcg->memory));\n \n \t\t/* Don't bother a random interrupted task */\n \t\tif (!in_task()) {\n-\t\t\tif (mem_high) {\n+\t\t\tif (mem_high || toptier_high) {\n \t\t\t\tschedule_work(&memcg->high_work);\n \t\t\t\tbreak;\n \t\t\t}\n \t\t\tcontinue;\n \t\t}\n \n-\t\tif (mem_high || swap_high) {\n+\t\tif (mem_high || swap_high || toptier_high) {\n \t\t\t/*\n \t\t\t * The allocating tasks in this cgroup will need to do\n \t\t\t * reclaim or be throttled to prevent further growth\n@@ -4476,7 +4518,7 @@ static ssize_t memory_high_write(struct kernfs_open_file *of,\n \tstruct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));\n \tunsigned int nr_retries = MAX_RECLAIM_RETRIES;\n \tbool drained = false;\n-\tunsigned long high;\n+\tunsigned long high, toptier_high;\n \tint err;\n \n \tbuf = strstrip(buf);\n@@ -4485,15 +4527,22 @@ static ssize_t memory_high_write(struct kernfs_open_file *of,\n \t\treturn err;\n \n \tpage_counter_set_high(&memcg->memory, high);\n+\ttoptier_high = page_counter_toptier_high(&memcg->memory);\n \n \tif (of->file->f_flags & O_NONBLOCK)\n \t\tgoto out;\n \n \tfor (;;) {\n \t\tunsigned long nr_pages = page_counter_read(&memcg->memory);\n+\t\tunsigned long toptier_pages = mem_cgroup_toptier_usage(memcg);\n \t\tunsigned long reclaimed;\n+\t\tunsigned long to_free;\n+\t\tnodemask_t toptier_nodes, *reclaim_nodes;\n+\t\tbool mem_high_ok = nr_pages <= high;\n+\t\tbool toptier_high_ok = !(tier_aware_memcg_limits &&\n+\t\t\t\t\t toptier_pages > toptier_high);\n \n-\t\tif (nr_pages <= high)\n+\t\tif (mem_high_ok && toptier_high_ok)\n \t\t\tbreak;\n \n \t\tif (signal_pending(current))\n@@ -4505,8 +4554,17 @@ static ssize_t memory_high_write(struct kernfs_open_file *of,\n \t\t\tcontinue;\n \t\t}\n \n-\t\treclaimed = try_to_free_mem_cgroup_pages(memcg, nr_pages - high,\n-\t\t\t\t\tGFP_KERNEL, MEMCG_RECLAIM_MAY_SWAP, NULL);\n+\t\tmt_get_toptier_nodemask(&toptier_nodes, NULL);\n+\t\tif (mem_high_ok && !toptier_high_ok) {\n+\t\t\treclaim_nodes = &toptier_nodes;\n+\t\t\tto_free = toptier_pages - toptier_high;\n+\t\t} else {\n+\t\t\treclaim_nodes = NULL;\n+\t\t\tto_free = nr_pages - high;\n+\t\t}\n+\t\treclaimed = try_to_free_mem_cgroup_pages(memcg, to_free,\n+\t\t\t\t\tGFP_KERNEL, MEMCG_RECLAIM_MAY_SWAP,\n+\t\t\t\t\tNULL, reclaim_nodes);\n \n \t\tif (!reclaimed && !nr_retries--)\n \t\t\tbreak;\n@@ -4558,7 +4616,8 @@ static ssize_t memory_max_write(struct kernfs_open_file *of,\n \n \t\tif (nr_reclaims) {\n \t\t\tif (!try_to_free_mem_cgroup_pages(memcg, nr_pages - max,\n-\t\t\t\t\tGFP_KERNEL, MEMCG_RECLAIM_MAY_SWAP, NULL))\n+\t\t\t\t\tGFP_KERNEL, MEMCG_RECLAIM_MAY_SWAP,\n+\t\t\t\t\tNULL, NULL))\n \t\t\t\tnr_reclaims--;\n \t\t\tcontinue;\n \t\t}\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 5b4cb030a477..94498734b4f5 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -6652,7 +6652,7 @@ unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,\n \t\t\t\t\t   unsigned long nr_pages,\n \t\t\t\t\t   gfp_t gfp_mask,\n \t\t\t\t\t   unsigned int reclaim_options,\n-\t\t\t\t\t   int *swappiness)\n+\t\t\t\t\t   int *swappiness, nodemask_t *allowed)\n {\n \tunsigned long nr_reclaimed;\n \tunsigned int noreclaim_flag;\n@@ -6668,6 +6668,7 @@ unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,\n \t\t.may_unmap = 1,\n \t\t.may_swap = !!(reclaim_options & MEMCG_RECLAIM_MAY_SWAP),\n \t\t.proactive = !!(reclaim_options & MEMCG_RECLAIM_PROACTIVE),\n+\t\t.nodemask = allowed,\n \t};\n \t/*\n \t * Traverse the ZONELIST_FALLBACK zonelist of the current node to put\n@@ -6693,7 +6694,7 @@ unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,\n \t\t\t\t\t   unsigned long nr_pages,\n \t\t\t\t\t   gfp_t gfp_mask,\n \t\t\t\t\t   unsigned int reclaim_options,\n-\t\t\t\t\t   int *swappiness)\n+\t\t\t\t\t   int *swappiness, nodemask_t *allowed)\n {\n \treturn 0;\n }\n@@ -7806,9 +7807,9 @@ int user_proactive_reclaim(char *buf,\n \t\t\treclaim_options = MEMCG_RECLAIM_MAY_SWAP |\n \t\t\t\t\t  MEMCG_RECLAIM_PROACTIVE;\n \t\t\treclaimed = try_to_free_mem_cgroup_pages(memcg,\n-\t\t\t\t\t\t batch_size, gfp_mask,\n-\t\t\t\t\t\t reclaim_options,\n-\t\t\t\t\t\t swappiness == -1 ? NULL : &swappiness);\n+\t\t\t\t\tbatch_size, gfp_mask, reclaim_options,\n+\t\t\t\t\tswappiness == -1 ? NULL : &swappiness,\n+\t\t\t\t\tNULL);\n \t\t} else {\n \t\t\tstruct scan_control sc = {\n \t\t\t\t.gfp_mask = current_gfp_context(gfp_mask),\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "The author is addressing a concern that the patch does not account for systems where all memory is equal, and explaining how tier-aware memcg limits can provide better quality of service guarantees in systems with tiered memory.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "addressing_concern",
                "providing_explanation"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Memory cgroups provide an interface that allow multiple workloads on a\nhost to co-exist, and establish both weak and strong memory isolation\nguarantees. For large servers and small embedded systems alike, memcgs\nprovide an effective way to provide a baseline quality of service for\nprotected workloads.\n\nThis works, because for the most part, all memory is equal (except for\nzram / zswap). Restricting a cgroup's memory footprint restricts how\nmuch it can hurt other workloads competing for memory. Likewise, setting\nmemory.low or memory.min limits can provide weak and strong guarantees\nto the performance of a cgroup.\n\nHowever, on systems with tiered memory (e.g. CXL / compressed memory),\nthe quality of service guarantees that memcg limits enforced become less\neffective, as memcg has no awareness of the physical location of its\ncharged memory. In other words, a workload that is well-behaved within\nits memcg limits may still be hurting the performance of other\nwell-behaving workloads on the system by hogging more than its\n\"fair share\" of toptier memory.\n\nIntroduce tier-aware memcg limits, which scale memory.low/high to\nreflect the ratio of toptier:total memory the cgroup has access.\n\nTake the following scenario as an example:\nOn a host with 3:1 toptier:lowtier, say 150G toptier, and 50Glowtier,\nsetting a cgroup's limits to:\n\tmemory.min:  15G\n\tmemory.low:  20G\n\tmemory.high: 40G\n\tmemory.max:  50G\n\nWill be enforced at the toptier as:\n\tmemory.min:          15G\n\tmemory.toptier_low:  15G (20 * 150/200)\n\tmemory.toptier_high: 30G (40 * 150/200)\n\tmemory.max:          50G\n\nLet's say that there are 4 such cgroups on the host. Previously, it would\nbe possible for 3 hosts to completely take over all of DRAM, while one\ncgroup could only access the lowtier memory. In the perspective of a\ntier-agnostic memcg limit enforcement, the three cgroups are all\nwell-behaved, consuming within their memory limits.\n\nThis is not to say that the scenario above is incorrect. In fact, for\nletting the hottest cgroups run in DRAM while pushing out colder cgroups\nto lowtier memory lets the system perform the most aggregate work total.\n\nBut for other scenarios, the target might not be maximizing aggregate\nwork, but maximizing the minimum performance guarantee for each\nindividual workload (think hosts shared across different users, such as\nVM hosting services).\n\nTo reflect these two scenarios, introduce a sysctl tier_aware_memcg,\nwhich allows the host to toggle between enforcing and overlooking\ntoptier memcg limit breaches.\n\nThis work is inspired & based off of Kaiyang Zhao's work from 2024 [1],\nwhere he referred to this concept as \"memory tiering fairness\".\nThe biggest difference in the implementations lie in how toptier memory\nis tracked; in his implementation, an lruvec stat aggregation is done on\neach usage check, while in this implementation, a new cacheline is\nintroduced in page_coutner to keep track of toptier usage (Kaiyang also\nintroduces a new cachline in page_counter, but only uses it to cache\ncapacity and thresholds). This implementation also extends the memory\nlimit enforcement to memory.high as well.\n\n[1] https://lore.kernel.org/linux-mm/20240920221202.1734227-1-kaiyang2@cs.cmu.edu/\n\n---\nJoshua Hahn (6):\n  mm/memory-tiers: Introduce tier-aware memcg limit sysfs\n  mm/page_counter: Introduce tiered memory awareness to page_counter\n  mm/memory-tiers, memcontrol: Introduce toptier capacity updates\n  mm/memcontrol: Charge and uncharge from toptier\n  mm/memcontrol, page_counter: Make memory.low tier-aware\n  mm/memcontrol: Make memory.high tier-aware\n\n include/linux/memcontrol.h   |  21 ++++-\n include/linux/memory-tiers.h |  30 +++++++\n include/linux/page_counter.h |  31 ++++++-\n include/linux/swap.h         |   3 +-\n kernel/cgroup/cpuset.c       |   2 +-\n kernel/cgroup/dmem.c         |   2 +-\n mm/memcontrol-v1.c           |   6 +-\n mm/memcontrol.c              | 155 +++++++++++++++++++++++++++++++----\n mm/memory-tiers.c            |  63 ++++++++++++++\n mm/page_counter.c            |  77 ++++++++++++++++-\n mm/vmscan.c                  |  24 ++++--\n 11 files changed, 376 insertions(+), 38 deletions(-)\n\n-- \n2.47.3",
              "reply_to": "",
              "message_date": "2026-02-23",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Michal Hocko",
              "summary": "Reviewer Michal Hocko questioned whether it's typical for active workingset sizes of all workloads to not fit into the top tier, suggesting that promotions would otherwise ensure most active memory is in the top tier.\n\nReviewer Michal Hocko expressed concerns that the current implementation focuses only on the top tier, potentially overlooking similar issues in other tiers, and questioned the need to duplicate limits for each/top tier.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "questioning typical behavior",
                "requesting clarification",
                "focusing only on top tier",
                "potential oversight of other tiers"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "This assumes that the active workingset size of all workloads doesn't\nfit into the top tier right? Otherwise promotions would make sure to\nthat we have the most active memory in the top tier. Is this typical in\nreal life configurations?\n\nOr do you intend to limit memory consumption on particular tier even\nwithout an external pressure?\n\n---\n\nLet's spend some more time with the interface first. You seem to be\nfocusing only on the top tier with this interface, right? Is this really the\nright way to go long term? What makes you believe that we do not really\nhit the same issue with other tiers as well? Also do we want/need to\nduplicate all the limits for each/top tier? What is the reasoning for\nthe switch to be runtime sysctl rather than boot-time or cgroup mount\noption?\n\nI will likely have more questions but these are immediate ones after\nreading the cover. Please note I haven't really looked at the\nimplementation yet. I really want to understand usecases and interface\nfirst.\n-- \nMichal Hocko\nSUSE Labs",
              "reply_to": "Joshua Hahn",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joshua Hahn (author)",
              "summary": "Author is addressing Michal's feedback by explaining their intention to discuss the project scope and scalability in a separate forum, LSFMMBPF, rather than revising the current patch series.\n\nAuthor acknowledged a concern about the impact of a workload violating its fair share of toptier memory, explaining that it mostly hurts other workloads when the aggregate working set size exceeds toptier memory capacity.\n\nAuthor acknowledged that traditional throughput-maximizing approach may not be suitable for all use cases and explained how tier-aware memcg limits can provide a more optimal solution for workloads prioritizing performance guarantees or low latency.\n\nAuthor agrees that the patch's tier-aware memcg limit feature is relevant to cloud providers and hyperscalers, providing two examples of realistic scenarios.\n\nThe author is addressing a concern about the interface for tier-aware memcg limits, specifically whether it should allow workloads to use more memory than their fair share in opportunistic mode. The author proposes two modes: fixed (limiting usage when a workload exceeds its fair share) and opportunistic (allowing workloads to use more memory but restricting them only when the top-tier is pressured). They ask for feedback on these options, suggesting that implementing opportunistic mode would require additional sysctl changes.\n\nAuthor acknowledges that the patch series was sent out prematurely and agrees it would have been better to send a proposal for LSFMMBPF first, indicating no immediate need for further revision.\n\nAuthor acknowledged that the current implementation only supports two-tiered systems and is open to revising it in the future when more complex systems become common.\n\nAuthor asked for clarification on whether reviewer's concern was about multiple nodes within a tier or multiple tiers within a tier, indicating uncertainty and need for further discussion.\n\nAuthor addressed Michal Hocko's concern that allowing cgroups to set their own mount options for tier-aware memcg limits could lead to inconsistent behavior and undermine the purpose of having a performance guarantee. Author agrees that this is not a good idea, but no specific fix or restructuring plan was mentioned.\n\nAuthor acknowledged that the feedback was positive and thanked the reviewer, indicating no further revision is needed.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "clarification",
                "separate discussion",
                "clarifying explanation",
                "no clear resolution signal",
                "acknowledged a different perspective",
                "explained reasoning",
                "agreement",
                "support",
                "asking for clarification",
                "proposing alternative solutions",
                "acknowledges feedback",
                "agrees with alternative approach",
                "open to revision",
                "acknowledged limitation",
                "uncertainty",
                "need_for_further_discussion",
                "acknowledged a problem",
                "agreed with feedback",
                "acknowledged",
                "thanked"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "Hello Michal,\n\nI hope that you are doing well! Thank you for taking the time to review my\nwork and leaving your thoughts.\n\nI wanted to note that I hope to bring this discussion to LSFMMBPF as well,\nto discuss what the scope of the project should be, what usecases there\nare (as I will note below), how to make this scalable and sustainable\nfor the future, etc. I'll send out a topic proposal later today. I had\nseparated the series from the proposal because I imagined that this\nseries would go through many versions, so it would be helpful to have\nthe topic as a unified place for pre-conference discussions.\n\n---\n\nYes, for the scenario above, a workload that is violating its fair share\nof toptier memory mostly hurts other workloads if the aggregate working\nset size of all workloads exceeds the size of toptier memory.\n\n---\n\nThis is true. And for a lot of usecases, this is 100% the right thing to do.\nHowever, with this patch I want to encourage a different perspective,\nwhich is to think about things in a per-workload perspective, and not a\nper-system perspective.\n\nHaving hot memory in high tiers and cold memory in low tiers is only\nlogical, since we increase the system's throughput and make the most\noptimal choices for latency. However, what about systems that care about\nobjectives other than simply maximizing throughput?\n\nIn the original cover letter I offered an example of VM hosting services\nthat care less about maximizing host-wide throughput, but more on ensuring\na bottomline performance guarantee for all workloads running on the system.\nFor the users on these services, they don't care that the host their VM is\nrunning on is maximizing throughput; rather, they care that their VM meets\nthe performance guarantees that their provider promised. If there is no\nway to know or enforce which tier of memory their workload lands on, either\nthe bottomline guarantee becomes very underestimated, or users must deal\nwith a high variance in performance.\n\nHere's another example: Let's say there is a host with multiple workloads,\neach serving queries for a database. The host would like to guarantee the\nlowest maximum latency possible, while maximizing the total throughput\nof the system. Once again in this situation, without tier-aware memcg\nlimits the host can maximize throughput, but can only make severely\nunderestimated promises on the bottom line.\n\n---\n\nI would say so. I think that the two examples above are realistic\nscenarios that cloud providers and hyperscalers might face on tiered systems.\n\n---\n\nThis is a great question, and one that I hope to discuss at LSFMMBPF\nto see how people expect an interface like this to work.\n\nOver the past few weeks, I have been discussing this idea during the\nLinux Memory Hotness and Promotion biweekly calls with Gregory Price [1].\nOne of the proposals that we made there (but did not include in this\nseries) is the idea of \"fixed\" vs. \"opportunistic\" reclaim.\n\nFixed mode is what we have here -- start limiting toptier usage whenever\na workload goes above its fair slice of toptier.\nOpportunistic mode would allow workloads to use more toptier memory than\nits fair share, but only be restricted when toptier is pressured.\n\nWhat do you think about these two options? For the stated goal of this\nseries, which is to help maximize the bottom line for workloads, fair\nshare seemed to make sense. Implementing opportunistic mode changes\non top of this work would most likely just be another sysctl.\n\n---\n\nThat sounds good with me, my goal was to bring this out as an RFC patchset\nso folks could look at the code and understand the motivation, and then send\nout the LSFMMBPF topic proposal. In retrospect I think I should have done\nit in the opposite order. I'm sorry if this caused any confusion.\n\n---\n\nYes, that's right. I'm not sure if this is the right way to go long-term\n(say, past the next 5 years). My thinking was that I can stick with doing\nthis for toptier vs. non-toptier memory for now, and deal with having\n3+ tiers in the future, when we start to have systems with that many tiers.\nAFAICT two-tiered systems are still ~relatively new, and I don't think\nthere are a lot of genuine usecases for enforcing mid-tier memory limits\nas of now. Of course, I would be excited to learn about these usecases\nand work this patchset to support them as well if anybody has them.\n\n---\n\nSorry, I'm not sure that I completely understood this question. Are you\nreferring to the case where we have multiple nodes in the toptier?\nIf so, then all of those nodes are treated the same, and don't have\nunique limits. Or are you referring to the case where we have multiple\ntiers in the toptier? If so, I hope the answer above can answer this too.\n\n---\n\nGood point : -) I don't think cgroup mount options are a good idea,\nsince this would mean that we can have a set of cgroups self-policing\ntheir toptier usage, while another cgroup allocates memory unrestricted.\nThis would punish the self-policing cgroup and we would lose the benefit\nof having a bottomline performance guarantee.\n\n---\n\nThat sounds good to me, thank you again for reviewing this work!\nI hope you have a great day : -)\nJoshua\n\n[1] https://lore.kernel.org/linux-mm/c8bc2dce-d4ec-c16e-8df4-2624c48cfc06@google.com/",
              "reply_to": "Michal Hocko",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Gregory Price",
              "summary": "Reviewer Gregory Price noted that the patch does not handle the case where tier-aware memcg limits are enabled, but the system has only one memory tier, which could lead to incorrect behavior and requested a fix for this edge case.\n\nReviewer Gregory Price expressed concern that the patch reduces the usefulness of secondary tiers of memory by introducing tier-aware memcg limits, which may lead to performance variance and discourage deployment on such machines.\n\nReviewer Gregory Price noted that tier-awareness is a significant blocker for deploying mixed workloads on large, dense memory systems with multiple tiers (2+), and suggested using the existing nobs (max/high/low/min) to proportionally control coherent memory tiers.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "requested changes",
                "blocker",
                "significant"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Just injecting a few points here\n(disclosure: I have been in the development loop for this feature)\n\n---\n\nYes / No.  This makes the assumption that you always want this.\n\nBarring a minimum Quality of Service mechanism (as Joshua explains)\nthis reduces the usefulness of a secondary tier of memory.\n\nServices will just prefer not to be deployed to these kinds of\nmachines because the performance variance is too high.\n\n---\n\nThe answer is unequivocally yes.\n\nLacking tier-awareness is actually a huge blocker for deploying mixed\nworkloads on large, dense memory systems with multiple tiers (2+).\n\nTechnically we're already at 4-ish tiers: DDR, CXL, ZSWAP, SWAP.\n\nWe have zswap/swap controls in cgroups already, we just lack that same\ncontrol for coherent memory tiers.  This tries to use the existing nobs\n(max/high/low/min) to do what they already do - just proportionally.\n\n~Gregory",
              "reply_to": "Joshua Hahn",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Kaiyang Zhao",
              "summary": "Reviewer Kaiyang Zhao noted that the results of a preprint paper on arXiv confirmed that co-colocated workloads can have working set sizes exceeding top-tier memory capacity, causing contention and significant variations in tail latency and throughput.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEUTRAL"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Hello! I'm the author of the RFC in 2024. Just want to add that we've\nrecently released a preprint paper on arXiv that includes case studies\nwith a few of Meta's production workloads using a prototype version of\nthe patches.\n\nThe results confirmed that co-colocated workloads can have working set\nsizes exceeding the limited top-tier memory capacity given today's\nserver memory shapes and workload stacking settings, causing contention\nof top-tier memory. Workloads see significant variations in tail\nlatency and throughput depending on the share of top-tier tier memory\nthey get, which this patch set will alleviate.\n\nBest,\nKaiyang\n\n[1] https://arxiv.org/pdf/2602.08800",
              "reply_to": "Gregory Price",
              "message_date": "2026-02-24",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 3/8] mm/zsmalloc: Introduce objcgs pointer in struct zpdesc",
          "message_id": "20260226214338.1062367-1-joshua.hahnjy@gmail.com",
          "url": "https://lore.kernel.org/all/20260226214338.1062367-1-joshua.hahnjy@gmail.com/",
          "date": "2026-02-26T21:43:41Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch series introduces per-memcg-lruvec zswap accounting by adding a new array of objcg pointers to the zpdesc structure, allowing for accurate tracking of memory usage by zswap and reducing NR_ZSWAP. The changes also move memcg charges to the zsmalloc layer, which is the only user of zswap at this time.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "David Hildenbrand",
              "summary": "Raised concerns about the increased size of struct zpdesc and the potential impact on performance. Suggested exploring alternative solutions to achieve the desired accounting functionality without incurring significant overhead.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "PERFORMANCE_CONCERN"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [RFC PATCH v5 02/10] migrate: Add migrate_misplaced_folios_batch()",
          "message_id": "20260226204059.481964-1-joshua.hahnjy@gmail.com",
          "url": "https://lore.kernel.org/all/20260226204059.481964-1-joshua.hahnjy@gmail.com/",
          "date": "2026-02-26T20:41:03Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch adds a new function to migrate misplaced folios in batches, but reviewer Joshua Hahn raises concerns about the single-folio case and promotion checks.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Joshua Hahn",
              "summary": "Raised concerns about single-folio case and promotion checks, suggesting additional checks for tier promotion and skipping count_memcg_events due to complexity.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [RFC PATCH v1 0/7] Open HugeTLB allocation routine for more generic use",
          "message_id": "20260226180821.2218448-1-joshua.hahnjy@gmail.com",
          "url": "https://lore.kernel.org/all/20260226180821.2218448-1-joshua.hahnjy@gmail.com/",
          "date": "2026-02-26T18:08:24Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch introduces a generic hugeTLB allocation routine for more flexible use, but raises questions about its applicability to guest_memfd.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Joshua Hahn",
              "summary": "Raised questions about the applicability of the patch to guest_memfd, suggesting clarification on whether the allocation failure case optimization is also relevant for guest_memfd.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [RFC PATCH 0/6] mm/memcontrol: Make memcg limits tier-aware",
          "message_id": "20260226160840.1220006-1-joshua.hahnjy@gmail.com",
          "url": "https://lore.kernel.org/all/20260226160840.1220006-1-joshua.hahnjy@gmail.com/",
          "date": "2026-02-26T17:52:36Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch aims to make memcg limits tier-aware, addressing concerns around thrashing and reclaim activity.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Michal Hocko",
              "summary": "Raised concerns about thrashing and reclaim activity, suggesting that the effective toptier memory limits should be exposed as a new sysfs file. Also mentioned scenarios where reclaim activity occurs even when there is available memory.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "thrashing",
                "reclaim activity"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Joshua Hahn",
              "summary": "Acknowledged Michal's concerns and suggested two potential components to address thrashing: throttling promotions when toptier is facing cgroup-local pressure, and background balancing between nodes. Also mentioned that enforcing limits based on capacity could be a feasible solution.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "acknowledgment",
                "suggestion"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "Michal Hocko",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "JP Kobryn",
      "primary_email": "inwardvessel@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Kiryl Shutsemau",
      "primary_email": "kas@kernel.org",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Leo Martins",
      "primary_email": "loemra.dev@gmail.com",
      "patches_submitted": [
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v4 3/3] btrfs: add tracepoint for search slot restart tracking",
          "message_id": "29af2660c8c3946d422f57910ec50916463a345b.1772097864.git.loemra.dev@gmail.com",
          "url": "https://lore.kernel.org/all/29af2660c8c3946d422f57910ec50916463a345b.1772097864.git.loemra.dev@gmail.com/",
          "date": "2026-02-26T09:51:17Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch adds a new tracepoint to the Btrfs file system, specifically for tracking search slot restarts during btrfs_search_slot() operations. The tracepoint records the root, tree level, and reason for each restart, enabling more detailed analysis of COW amplification under memory pressure.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Filipe Manana",
              "summary": "Provided a Reviewed-by tag, indicating they have reviewed the code and found it acceptable for inclusion in the kernel.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "LGTM"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v4 2/3] btrfs: inhibit extent buffer writeback to prevent COW amplification",
          "message_id": "9e49ee6ee946e6cabb6b691693a955dbd201055c.1772097864.git.loemra.dev@gmail.com",
          "url": "https://lore.kernel.org/all/9e49ee6ee946e6cabb6b691693a955dbd201055c.1772097864.git.loemra.dev@gmail.com/",
          "date": "2026-02-26T09:51:16Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch adds a new tracepoint to the Btrfs file system, specifically for tracking search slot restarts during btrfs_search_slot() operations. The tracepoint records the root, tree level, and reason for each restart, enabling more detailed analysis of COW amplification under memory pressure.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Filipe Manana",
              "summary": "Provided a Reviewed-by tag, indicating they have reviewed the code and found it acceptable for inclusion in the kernel.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "LGTM"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "On Thu, Feb 26, 2026 at 9:58AM Leo Martins <loemra.dev@gmail.com> wrote:\n>\n> Inhibit writeback on COW'd extent buffers for the lifetime of the\n> transaction handle, preventing background writeback from setting\n> BTRFS_HEADER_FLAG_WRITTEN and causing unnecessary re-COW.\n>\n> COW amplification occurs when background writeback flushes an extent\n> buffer that a transaction handle is still actively modifying. When\n> lock_extent_buffer_for_io() transitions a buffer from dirty to\n> writeback, it sets BTRFS_HEADER_FLAG_WRITTEN, marking the block as\n> having been persisted to disk at its current bytenr. Once WRITTEN is\n> set, should_cow_block() must either COW the block again or overwrite\n> it in place, both of which are unnecessary overhead when the buffer\n> is still being modified by the same handle that allocated it. By\n> inhibiting background writeback on actively-used buffers, WRITTEN is\n> never set while a transaction handle holds a reference to the buffer,\n> avoiding this overhead entirely.\n>\n> Add an atomic_t writeback_inhibitors counter to struct extent_buffer,\n> which fits in an existing 6-byte hole without increasing struct size.\n> When a buffer is COW'd in btrfs_force_cow_block(), call\n> btrfs_inhibit_eb_writeback() to store the eb in the transaction\n> handle's writeback_inhibited_ebs xarray (keyed by eb->start), take a\n> reference, and increment writeback_inhibitors. The function handles\n> dedup (same eb inhibited twice by the same handle) and replacement\n> (different eb at the same logical address). Allocation failure is\n> graceful: the buffer simply falls back to the pre-existing behavior\n> where it may be written back and re-COW'd.\n>\n> Also inhibit writeback in should_cow_block() when COW is skipped,\n> so that every transaction handle that reuses an already-COW'd buffer\n> also inhibits its writeback. Without this, if handle A COWs a block\n> and inhibits it, and handle B later reuses the same block without\n> inhibiting, handle A's uninhibit on end_transaction leaves the buffer\n> unprotected while handle B is still using it. This ensures all handles\n> that access a COW'd buffer contribute to the inhibitor count, and the\n> buffer remains protected until the last handle releases it.\n>\n> In lock_extent_buffer_for_io(), when writeback_inhibitors is non-zero\n> and the writeback mode is WB_SYNC_NONE, skip the buffer. WB_SYNC_NONE\n> is used by the VM flusher threads for background and periodic\n> writeback, which are the only paths that cause COW amplification by\n> opportunistically writing out dirty extent buffers mid-transaction.\n> Skipping these is safe because the buffers remain dirty in the page\n> cache and will be written out at transaction commit time.\n>\n> WB_SYNC_ALL must always proceed regardless of writeback_inhibitors.\n> This is required for correctness in the fsync path: btrfs_sync_log()\n> writes log tree blocks via filemap_fdatawrite_range() (WB_SYNC_ALL)\n> while the transaction handle that inhibited those same blocks is still\n> active. Without the WB_SYNC_ALL bypass, those inhibited log tree\n> blocks would be silently skipped, resulting in an incomplete log on\n> disk and corruption on replay. btrfs_write_and_wait_transaction()\n> also uses WB_SYNC_ALL via filemap_fdatawrite_range(); for that path,\n> inhibitors are already cleared beforehand, but the bypass ensures\n> correctness regardless.\n>\n> Uninhibit in __btrfs_end_transaction() before atomic_dec(num_writers)\n> to prevent a race where the committer proceeds while buffers are still\n> inhibited. Also uninhibit in btrfs_commit_transaction() before writing\n> and in cleanup_transaction() for the error path.\n>\n> Signed-off-by: Leo Martins <loemra.dev@gmail.com>\n> ---\n>  fs/btrfs/ctree.c       |  9 ++++++\n>  fs/btrfs/extent_io.c   | 63 +++++++++++++++++++++++++++++++++++++++++-\n>  fs/btrfs/extent_io.h   |  6 ++++\n>  fs/btrfs/transaction.c | 19 +++++++++++++\n>  fs/btrfs/transaction.h |  3 ++\n>  5 files changed, 99 insertions(+), 1 deletion(-)\n>\n> diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c\n> index ea7cfc3a9e89..46a715c95bc8 100644\n> --- a/fs/btrfs/ctree.c\n> +++ b/fs/btrfs/ctree.c\n> @@ -21,6 +21,7 @@\n>  #include \"fs.h\"\n>  #include \"accessors.h\"\n>  #include \"extent-tree.h\"\n> +#include \"extent_io.h\"\n>  #include \"relocation.h\"\n>  #include \"file-item.h\"\n>\n> @@ -590,6 +591,10 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,\n>                 btrfs_tree_unlock(buf);\n>         free_extent_buffer_stale(buf);\n>         btrfs_mark_buffer_dirty(trans, cow);\n> +\n> +       /* Inhibit writeback on the COW'd buffer for this transaction handle. */\n> +       btrfs_inhibit_eb_writeback(trans, cow);\n\nBtw, that comment is redundant. It's clear what we are doing, since\nthe function's name is clear about what it does and the eb is named\n\"cow\".\nUsually we add comments for things that are not obvious.\n\n> +\n>         *cow_ret = cow;\n>         return 0;\n>\n> @@ -617,6 +622,9 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,\n>   * When returning false for a WRITTEN buffer allocated in the current\n>   * transaction, re-dirties the buffer for in-place overwrite instead\n>   * of requesting a new COW.\n> + *\n> + * When returning false, inhibits background writeback on the buffer\n> + * for the lifetime of the transaction handle.\n>   */\n>  static inline bool should_cow_block(struct btrfs_trans_handle *trans,\n>                                     const struct btrfs_root *root,\n> @@ -684,6 +692,7 @@ static inline bool should_cow_block(struct btrfs_trans_handle *trans,\n>                 btrfs_mark_buffer_dirty(trans, buf);\n>         }\n>\n> +       btrfs_inhibit_eb_writeback(trans, buf);\n>         return false;\n>  }\n>\n> diff --git a/fs/btrfs/extent_io.c b/fs/btrfs/extent_io.c\n> index ff1fc699a6ca..e04e42a81978 100644\n> --- a/fs/btrfs/extent_io.c\n> +++ b/fs/btrfs/extent_io.c\n> @@ -1940,7 +1940,9 @@ static noinline_for_stack bool lock_extent_buffer_for_io(struct extent_buffer *e\n>          * of time.\n>          */\n>         spin_lock(&eb->refs_lock);\n> -       if (test_and_clear_bit(EXTENT_BUFFER_DIRTY, &eb->bflags)) {\n> +       if ((wbc->sync_mode == WB_SYNC_ALL ||\n> +            atomic_read(&eb->writeback_inhibitors) == 0) &&\n> +           test_and_clear_bit(EXTENT_BUFFER_DIRTY, &eb->bflags)) {\n>                 XA_STATE(xas, &fs_info->buffer_tree, eb->start >> fs_info->nodesize_bits);\n>                 unsigned long flags;\n>\n> @@ -2999,6 +3001,64 @@ static inline void btrfs_release_extent_buffer(struct extent_buffer *eb)\n>         kmem_cache_free(extent_buffer_cache, eb);\n>  }\n>\n> +/*\n> + * btrfs_inhibit_eb_writeback - Inhibit writeback on buffer during transaction.\n> + * @trans: transaction handle that will own the inhibitor\n> + * @eb: extent buffer to inhibit writeback on\n> + *\n> + * Attempts to track this extent buffer in the transaction's inhibited set.\n> + * If memory allocation fails, the buffer is simply not tracked. It may\n> + * be written back and need re-COW, which is the original behavior.\n> + * This is acceptable since inhibiting writeback is an optimization.\n> + */\n> +void btrfs_inhibit_eb_writeback(struct btrfs_trans_handle *trans,\n> +                               struct extent_buffer *eb)\n> +{\n> +       unsigned long index = eb->start >> trans->fs_info->nodesize_bits;\n> +       void *old;\n> +\n> +       /* Check if already inhibited by this handle. */\n> +       old = xa_load(&trans->writeback_inhibited_ebs, index);\n> +       if (old == eb)\n> +               return;\n> +\n> +       /* Take reference for the xarray entry. */\n> +       refcount_inc(&eb->refs);\n> +\n> +       old = xa_store(&trans->writeback_inhibited_ebs, index, eb, GFP_NOFS);\n> +       if (xa_is_err(old)) {\n> +               /* Allocation failed, just skip inhibiting this buffer. */\n> +               free_extent_buffer(eb);\n> +               return;\n> +       }\n> +\n> +       /* Handle replacement of different eb at same index. */\n> +       if (old && old != eb) {\n> +               struct extent_buffer *old_eb = old;\n> +\n> +               atomic_dec(&old_eb->writeback_inhibitors);\n> +               free_extent_buffer(old_eb);\n> +       }\n> +\n> +       atomic_inc(&eb->writeback_inhibitors);\n\nBtw, at the top of this function we should assert the eb is locked.\n\nOtherwise,\n\nReviewed-by: Filipe Manana <fdmanana@suse.com>\n\nThanks.\n\n> +}\n> +\n> +/*\n> + * btrfs_uninhibit_all_eb_writeback - Uninhibit writeback on all buffers.\n> + * @trans: transaction handle to clean up\n> + */\n> +void btrfs_uninhibit_all_eb_writeback(struct btrfs_trans_handle *trans)\n> +{\n> +       struct extent_buffer *eb;\n> +       unsigned long index;\n> +\n> +       xa_for_each(&trans->writeback_inhibited_ebs, index, eb) {\n> +               atomic_dec(&eb->writeback_inhibitors);\n> +               free_extent_buffer(eb);\n> +       }\n> +       xa_destroy(&trans->writeback_inhibited_ebs);\n> +}\n> +\n>  static struct extent_buffer *__alloc_extent_buffer(struct btrfs_fs_info *fs_info,\n>                                                    u64 start)\n>  {\n> @@ -3009,6 +3069,7 @@ static struct extent_buffer *__alloc_extent_buffer(struct btrfs_fs_info *fs_info\n>         eb->len = fs_info->nodesize;\n>         eb->fs_info = fs_info;\n>         init_rwsem(&eb->lock);\n> +       atomic_set(&eb->writeback_inhibitors, 0);\n>\n>         btrfs_leak_debug_add_eb(eb);\n>\n> diff --git a/fs/btrfs/extent_io.h b/fs/btrfs/extent_io.h\n> index 73571d5d3d5a..fb68fbd4866c 100644\n> --- a/fs/btrfs/extent_io.h\n> +++ b/fs/btrfs/extent_io.h\n> @@ -102,6 +102,8 @@ struct extent_buffer {\n>         /* >= 0 if eb belongs to a log tree, -1 otherwise */\n>         s8 log_index;\n>         u8 folio_shift;\n> +       /* Inhibits WB_SYNC_NONE writeback when > 0. */\n> +       atomic_t writeback_inhibitors;\n>         struct rcu_head rcu_head;\n>\n>         struct rw_semaphore lock;\n> @@ -381,4 +383,8 @@ void btrfs_extent_buffer_leak_debug_check(struct btrfs_fs_info *fs_info);\n>  #define btrfs_extent_buffer_leak_debug_check(fs_info)  do {} while (0)\n>  #endif\n>\n> +void btrfs_inhibit_eb_writeback(struct btrfs_trans_handle *trans,\n> +                              struct extent_buffer *eb);\n> +void btrfs_uninhibit_all_eb_writeback(struct btrfs_trans_handle *trans);\n> +\n>  #endif\n> diff --git a/fs/btrfs/transaction.c b/fs/btrfs/transaction.c\n> index f4cc9e1a1b93..a9a22629b49d 100644\n> --- a/fs/btrfs/transaction.c\n> +++ b/fs/btrfs/transaction.c\n> @@ -15,6 +15,7 @@\n>  #include \"misc.h\"\n>  #include \"ctree.h\"\n>  #include \"disk-io.h\"\n> +#include \"extent_io.h\"\n>  #include \"transaction.h\"\n>  #include \"locking.h\"\n>  #include \"tree-log.h\"\n> @@ -688,6 +689,8 @@ start_transaction(struct btrfs_root *root, unsigned int num_items,\n>                 goto alloc_fail;\n>         }\n>\n> +       xa_init(&h->writeback_inhibited_ebs);\n> +\n>         /*\n>          * If we are JOIN_NOLOCK we're already committing a transaction and\n>          * waiting on this guy, so we don't need to do the sb_start_intwrite\n> @@ -1083,6 +1086,13 @@ static int __btrfs_end_transaction(struct btrfs_trans_handle *trans,\n>         if (trans->type & __TRANS_FREEZABLE)\n>                 sb_end_intwrite(info->sb);\n>\n> +       /*\n> +        * Uninhibit extent buffer writeback before decrementing num_writers,\n> +        * since the decrement wakes the committing thread which needs all\n> +        * buffers uninhibited to write them to disk.\n> +        */\n> +       btrfs_uninhibit_all_eb_writeback(trans);\n> +\n>         WARN_ON(cur_trans != info->running_transaction);\n>         WARN_ON(atomic_read(&cur_trans->num_writers) < 1);\n>         atomic_dec(&cur_trans->num_writers);\n> @@ -2110,6 +2120,7 @@ static void cleanup_transaction(struct btrfs_trans_handle *trans, int err)\n>         if (!test_bit(BTRFS_FS_RELOC_RUNNING, &fs_info->flags))\n>                 btrfs_scrub_cancel(fs_info);\n>\n> +       btrfs_uninhibit_all_eb_writeback(trans);\n>         kmem_cache_free(btrfs_trans_handle_cachep, trans);\n>  }\n>\n> @@ -2556,6 +2567,14 @@ int btrfs_commit_transaction(struct btrfs_trans_handle *trans)\n>             fs_info->cleaner_kthread)\n>                 wake_up_process(fs_info->cleaner_kthread);\n>\n> +       /*\n> +        * Uninhibit writeback on all extent buffers inhibited during this\n> +        * transaction before writing them to disk. Inhibiting prevented\n> +        * writeback while the transaction was building, but now we need\n> +        * them written.\n> +        */\n> +       btrfs_uninhibit_all_eb_writeback(trans);\n> +\n>         ret = btrfs_write_and_wait_transaction(trans);\n>         if (unlikely(ret)) {\n>                 btrfs_err(fs_info, \"error while writing out transaction: %d\", ret);\n> diff --git a/fs/btrfs/transaction.h b/fs/btrfs/transaction.h\n> index 18ef069197e5..7d70fe486758 100644\n> --- a/fs/btrfs/transaction.h\n> +++ b/fs/btrfs/transaction.h\n> @@ -12,6 +12,7 @@\n>  #include <linux/time64.h>\n>  #include <linux/mutex.h>\n>  #include <linux/wait.h>\n> +#include <linux/xarray.h>\n>  #include \"btrfs_inode.h\"\n>  #include \"delayed-ref.h\"\n>\n> @@ -162,6 +163,8 @@ struct btrfs_trans_handle {\n>         struct btrfs_fs_info *fs_info;\n>         struct list_head new_bgs;\n>         struct btrfs_block_rsv delayed_rsv;\n> +       /* Extent buffers with writeback inhibited by this handle. */\n> +       struct xarray writeback_inhibited_ebs;\n>  };\n>\n>  /*\n> --\n> 2.47.3\n>\n>\n",
              "reply_to": "",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v4 1/3] btrfs: skip COW for written extent buffers allocated in current transaction",
          "message_id": "e8e4f5e396d821ba9ed6a4eee073ae8628d52aeb.1772097864.git.loemra.dev@gmail.com",
          "url": "https://lore.kernel.org/all/e8e4f5e396d821ba9ed6a4eee073ae8628d52aeb.1772097864.git.loemra.dev@gmail.com/",
          "date": "2026-02-26T09:51:14Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch adds a new tracepoint to the Btrfs file system, specifically for tracking search slot restarts during btrfs_search_slot() operations. The tracepoint records the root, tree level, and reason for each restart, enabling more detailed analysis of COW amplification under memory pressure.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Filipe Manana",
              "summary": "Provided a Reviewed-by tag, indicating they have reviewed the code and found it acceptable for inclusion in the kernel.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "LGTM"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v4 0/3] btrfs: fix COW amplification under memory pressure",
          "message_id": "cover.1772097864.git.loemra.dev@gmail.com",
          "url": "https://lore.kernel.org/all/cover.1772097864.git.loemra.dev@gmail.com/",
          "date": "2026-02-26T09:51:12Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch adds a new tracepoint to the Btrfs file system, specifically for tracking search slot restarts during btrfs_search_slot() operations. The tracepoint records the root, tree level, and reason for each restart, enabling more detailed analysis of COW amplification under memory pressure.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Filipe Manana",
              "summary": "Provided a Reviewed-by tag, indicating they have reviewed the code and found it acceptable for inclusion in the kernel.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "LGTM"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 3/3] btrfs: add tracepoint for search slot restart tracking",
          "message_id": "18c04d9a68f64fa5e36dde196306170d0fb437d9.1771884128.git.loemra.dev@gmail.com",
          "url": "https://lore.kernel.org/all/18c04d9a68f64fa5e36dde196306170d0fb437d9.1771884128.git.loemra.dev@gmail.com/",
          "date": "2026-02-24T19:22:55Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-24",
          "patch_summary": "This patch adds a new tracepoint to the Btrfs kernel module, allowing for tracking of search slot restarts in btrfs_search_slot(). The tracepoint records the root, tree level, and reason for each restart, enabling more detailed analysis of COW amplification under memory pressure.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "David Hildenbrand",
              "summary": "The patch looks good, but the author should consider adding a comment to explain why the per-restart-site tracepoint is necessary and how it improves over the existing counter-based approach.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 2/3] btrfs: inhibit extent buffer writeback to prevent COW amplification",
          "message_id": "cc847a35e26cc4dfad18c59e3c525cea507ff440.1771884128.git.loemra.dev@gmail.com",
          "url": "https://lore.kernel.org/all/cc847a35e26cc4dfad18c59e3c525cea507ff440.1771884128.git.loemra.dev@gmail.com/",
          "date": "2026-02-24T19:22:54Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-24",
          "patch_summary": "This patch adds a new tracepoint to the Btrfs kernel module, allowing for tracking of search slot restarts in btrfs_search_slot(). The tracepoint records the root, tree level, and reason for each restart, enabling more detailed analysis of COW amplification under memory pressure.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "David Hildenbrand",
              "summary": "The patch looks good, but the author should consider adding a comment to explain why the per-restart-site tracepoint is necessary and how it improves over the existing counter-based approach.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 1/3] btrfs: skip COW for written extent buffers allocated in current transaction",
          "message_id": "4ce911a475b998ddf76951629ad203e6440ab0ca.1771884128.git.loemra.dev@gmail.com",
          "url": "https://lore.kernel.org/all/4ce911a475b998ddf76951629ad203e6440ab0ca.1771884128.git.loemra.dev@gmail.com/",
          "date": "2026-02-24T19:22:52Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-24",
          "patch_summary": "This patch adds a new tracepoint to the Btrfs kernel module, allowing for tracking of search slot restarts in btrfs_search_slot(). The tracepoint records the root, tree level, and reason for each restart, enabling more detailed analysis of COW amplification under memory pressure.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "David Hildenbrand",
              "summary": "The patch looks good, but the author should consider adding a comment to explain why the per-restart-site tracepoint is necessary and how it improves over the existing counter-based approach.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_submitted",
          "subject": "[PATCH v3 0/3] btrfs: fix COW amplification under memory pressure",
          "message_id": "cover.1771884128.git.loemra.dev@gmail.com",
          "url": "https://lore.kernel.org/all/cover.1771884128.git.loemra.dev@gmail.com/",
          "date": "2026-02-24T19:22:51Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": true,
          "submitted_date": "2026-02-24",
          "patch_summary": "This patch adds a new tracepoint to the Btrfs kernel module, allowing for tracking of search slot restarts in btrfs_search_slot(). The tracepoint records the root, tree level, and reason for each restart, enabling more detailed analysis of COW amplification under memory pressure.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "David Hildenbrand",
              "summary": "The patch looks good, but the author should consider adding a comment to explain why the per-restart-site tracepoint is necessary and how it improves over the existing counter-based approach.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v3 2/3] btrfs: inhibit extent buffer writeback to prevent COW amplification",
          "message_id": "20260226023050.2138594-1-loemra.dev@gmail.com",
          "url": "https://lore.kernel.org/all/20260226023050.2138594-1-loemra.dev@gmail.com/",
          "date": "2026-02-26T02:30:54Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch addresses a potential issue in the Btrfs file system where COW (Copy On Write) amplification could occur due to incorrect handling of extent buffer writeback. The problem arises when multiple transactions modify the same extent buffer, and one transaction inhibits writeback after COWing the buffer, while another transaction reuses the buffer without re-COWing it. To fix this, the patch modifies the code to inhibit writeback not only when a buffer is COWed but also whenever a handle reuses a COWed buffer without re-COWing it.",
          "analysis_source": "llm-per-reviewer",
          "review_comments": [
            {
              "author": "Sun YangKai",
              "summary": "Reviewer noted that the current implementation only inhibits writeback when an extent buffer (eb) is COW'd, but does not account for cases where a transaction handle reuses a previously COW'd eb without re-COWing it, potentially leading to COW amplification.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential bug",
                "requested change"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Oh, I've just got the idea. It's inhibited per trans handle. Then it \nmakes a lot of sense.\n\nThere might be some ebs that were inhibited due to tree rebalancing or \ntree walking and are no longer used, but I don't think this is a blocker \nfor this patch.\n\nLooking forward to your next patch version :)",
              "reply_to": "Leo Martins",
              "message_date": "2026-02-26",
              "message_id": "f5788abc-f4de-4f3a-9ab1-7aaa579c89c9@gmail.com",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Mark Harmstone",
      "primary_email": "mark@harmstone.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Nhat Pham",
      "primary_email": "nphamcs@gmail.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Rik van Riel",
      "primary_email": "riel@surriel.com",
      "patches_submitted": [],
      "patches_reviewed": [],
      "patches_acked": [],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Shakeel Butt",
      "primary_email": "shakeel.butt@linux.dev",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 3/8] mm/zsmalloc: Introduce objcgs pointer in struct zpdesc",
          "message_id": "aaC80TsqmKpwFzQf@linux.dev",
          "url": "https://lore.kernel.org/all/aaC80TsqmKpwFzQf@linux.dev/",
          "date": "2026-02-26T21:37:51Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch series introduces per-memcg-lruvec zswap accounting by adding a new array of objcg pointers to the zpdesc structure, allowing for accurate tracking of memory usage by zswap and reducing NR_ZSWAP. The changes also move memcg charges to the zsmalloc layer, which is the only user of zswap at this time.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "David Hildenbrand",
              "summary": "Raised concerns about the increased size of struct zpdesc and the potential impact on performance. Suggested exploring alternative solutions to achieve the desired accounting functionality without incurring significant overhead.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "PERFORMANCE_CONCERN"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v3 5/5] mm: add tracepoints for zone lock",
          "message_id": "aaC7S2ensAGTyujc@linux.dev",
          "url": "https://lore.kernel.org/all/aaC7S2ensAGTyujc@linux.dev/",
          "date": "2026-02-26T21:32:06Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch adds tracepoint instrumentation to zone lock acquire and release operations in the Linux kernel. The implementation uses a lightweight inline helper that checks whether tracing is enabled, and calls an out-of-line helper when tracing is active. When CONFIG_TRACING is disabled, the helpers compile to empty inline stubs, leaving the fast path unaffected. This allows for easier debugging and analysis of zone lock behavior.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted a minor issue with the zone lock tracing implementation, specifically mentioning an empty inline stub in the !CONFIG_TRACING branch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "minor issue"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "One nit below other than that:\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n\n[...]",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Andrew Morton",
              "summary": "Reviewer Andrew Morton questioned the necessity of exporting several files, specifically mentioning include/linux/mmzone.h and mm/compaction.c among others.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "export",
                "necessity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Do we need the exports at all?\n\ninclude/linux/mmzone.h\ninclude/linux/zone_lock.h\ninclude/trace/events/zone_lock.h\nMAINTAINERS\nmm/compaction.c\nmm/internal.h\nmm/Makefile\nmm/memory_hotplug.c\nmm/mm_init.c\nmm/page_alloc.c\nmm/page_isolation.c\nmm/page_owner.c\nmm/page_reporting.c\nmm/show_mem.c\nmm/vmscan.c\nmm/vmstat.c\nmm/zone_lock.c",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that zone lock tracepoints may require exports like mmap_lock wrappers to prevent drivers from taking the lock directly",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential locking issue",
                "export requirements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Very good point and we don't. I think this might just be copying the mmap_lock\ntracepoint wrappers which might need the exports as some drivers might be taking\nthe mmap_lock.\n\nDmitry, please confirm (test) and let us know.",
              "reply_to": "Andrew Morton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v5 update 30/32] mm: memcontrol: convert objcg to be per-memcg per-node type",
          "message_id": "aaCmpSNRZU1wIYxq@linux.dev",
          "url": "https://lore.kernel.org/all/aaCmpSNRZU1wIYxq@linux.dev/",
          "date": "2026-02-26T20:06:17Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Conversion of objcg to per-memcg per-node type in mm: memcontrol",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Raised a question about whether page/folio/slab points to the same node's objcg for a given memcg, but didn't express an opinion on the patch itself.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Qi Zheng",
              "summary": "Addressed Shakeel's concern by stating that maybe not, and suggested changing the comparison to objcg->memcg equality.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "\n\nOn 2/27/26 4:05 AM, Shakeel Butt wrote:\n> On Wed, Feb 25, 2026 at 05:44:56PM +0800, Qi Zheng wrote:\n>> From: Qi Zheng <zhengqi.arch@bytedance.com>\n>>\n>> Convert objcg to be per-memcg per-node type, so that when reparent LRU\n>> folios later, we can hold the lru lock at the node level, thus avoiding\n>> holding too many lru locks at once.\n>>\n>> Signed-off-by: Qi Zheng <zhengqi.arch@bytedance.com>\n>> ---\n>> changlog:\n>>   - fix a missing root_obj_cgroup conversion and completely delete\n>>     root_obj_cgroup.\n>>\n> \n> After this patch, do we care that page/folio/slab points to the objcg of the\n> same node as them for a given memcg?\n\nMaybe not. My only concern is whether the kernel has a way of\ndetermining whether two folios belong to the same memcg by checking if\nthe objcg pointers are equal. If so, it needs to be changed to check if\nobjcg->memcg are equal.\n\n> \n\n\n",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-27",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v5 29/32] mm: memcontrol: prepare for reparenting non-hierarchical stats",
          "message_id": "aaB7yYSpAaC5uInq@linux.dev",
          "url": "https://lore.kernel.org/all/aaB7yYSpAaC5uInq@linux.dev/",
          "date": "2026-02-26T17:03:19Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Conversion of objcg to per-memcg per-node type in mm: memcontrol",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Raised a question about whether page/folio/slab points to the same node's objcg for a given memcg, but didn't express an opinion on the patch itself.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Qi Zheng",
              "summary": "Addressed Shakeel's concern by stating that maybe not, and suggested changing the comparison to objcg->memcg equality.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "\n\nOn 2/27/26 4:05 AM, Shakeel Butt wrote:\n> On Wed, Feb 25, 2026 at 05:44:56PM +0800, Qi Zheng wrote:\n>> From: Qi Zheng <zhengqi.arch@bytedance.com>\n>>\n>> Convert objcg to be per-memcg per-node type, so that when reparent LRU\n>> folios later, we can hold the lru lock at the node level, thus avoiding\n>> holding too many lru locks at once.\n>>\n>> Signed-off-by: Qi Zheng <zhengqi.arch@bytedance.com>\n>> ---\n>> changlog:\n>>   - fix a missing root_obj_cgroup conversion and completely delete\n>>     root_obj_cgroup.\n>>\n> \n> After this patch, do we care that page/folio/slab points to the objcg of the\n> same node as them for a given memcg?\n\nMaybe not. My only concern is whether the kernel has a way of\ndetermining whether two folios belong to the same memcg by checking if\nthe objcg pointers are equal. If so, it needs to be changed to check if\nobjcg->memcg are equal.\n\n> \n\n\n",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-27",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v2 0/3] dma-buf: heaps: cma: enable dmem cgroup accounting",
          "message_id": "aaBNYqv2xJ4QEyh4@linux.dev",
          "url": "https://lore.kernel.org/all/aaBNYqv2xJ4QEyh4@linux.dev/",
          "date": "2026-02-26T13:45:52Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch aims to enable dmem cgroup accounting for dma-buf heaps, specifically for the cma heap.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Asked for clarification on what 'memcg doesn't limit swap' means in the context of dmem cgroup accounting.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "WAITING_FOR_REVIEW"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v5 29/32] mm: memcontrol: prepare for reparenting non-hierarchical stats",
          "message_id": "aZ-R87JfacQ2gGq1@linux.dev",
          "url": "https://lore.kernel.org/all/aZ-R87JfacQ2gGq1@linux.dev/",
          "date": "2026-02-26T00:25:49Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Conversion of objcg to per-memcg per-node type in mm: memcontrol",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Raised a question about whether page/folio/slab points to the same node's objcg for a given memcg, but didn't express an opinion on the patch itself.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On Thu, Feb 26, 2026 at 07:16:50AM -0800, Yosry Ahmed wrote:\n> > > Did you measure the impact of making state_local atomic on the flush\n> > > path? It's a slow path but we've seen pain from it being too slow\n> > > before, because it extends the critical section of the rstat flush\n> > > lock.\n> >\n> > Qi, please measure the impact on flushing and if no impact then no need to do\n> > anything as I don't want anymore churn in this series.\n> >\n> > >\n> > > Can we keep this non-atomic and use mod_memcg_lruvec_state() here? It\n> > > will update the stat on the local counter and it will be added to\n> > > state_local in the flush path when needed. We can even force another\n> > > flush in reparent_state_local () after reparenting is completed, if we\n> > > want to avoid leaving a potentially large stat update pending, as it\n> > > can be missed by mem_cgroup_flush_stats_ratelimited().\n> > >\n> > > Same for reparent_memcg_state_local(), we can probably use mod_memcg_state()?\n> >\n> > Yosry, do you mind sending the patch you are thinking about over this series?\n> \n> Honestly, I'd rather squash it into this patch if possible. It avoids\n> churn in the history (switch to atomics and back), and is arguably\n> simpler than checking for regressions in the flush path.\n\nYup, let's squash it into the original patch. Please add your sign-off tag.\n\n> \n> What I have in mind is the diff below (build tested only). Qi, would\n> you be able to test this? It applies directly on this patch in mm-new:\n\nQi, please squash this diff into the patch and test. You might need to change\nthe subsequent patches. Once you are done with testing, you can post the diffs\nfor those in reply to those patches and we will ask Andrew to squash into\norinigal ones.\n\nThe diff looks good to me though.\n> \n> diff --git a/mm/memcontrol.c b/mm/memcontrol.c\n> index d82dbfcc28057..404565e80cbf3 100644\n> --- a/mm/memcontrol.c\n> +++ b/mm/memcontrol.c\n> @@ -234,11 +234,18 @@ static inline void reparent_state_local(struct\n> mem_cgroup *memcg, struct mem_cgr\n>         if (cgroup_subsys_on_dfl(memory_cgrp_subsys))\n>                 return;\n> \n> +       /*\n> +        * Reparent stats exposed non-hierarchically. Flush @memcg's\n> stats first to\n> +        * read its stats accurately , and conservatively flush @parent's stats\n> +        * after reparenting to avoid hiding a potentially large stat update\n> +        * (e.g. from callers of mem_cgroup_flush_stats_ratelimited()).\n> +        */\n>         __mem_cgroup_flush_stats(memcg, true);\n> \n> -       /* The following counts are all non-hierarchical and need to\n> be reparented. */\n>         reparent_memcg1_state_local(memcg, parent);\n>         reparent_memcg1_lruvec_state_local(memcg, parent);\n> +\n> +       __mem_cgroup_flush_stats(parent, true);\n>  }\n>  #else\n>  static inline void reparent_state_local(struct mem_cgroup *memcg,\n> struct mem_cgroup *parent)\n> @@ -442,7 +449,7 @@ struct lruvec_stats {\n>         long state[NR_MEMCG_NODE_STAT_ITEMS];\n> \n>         /* Non-hierarchical (CPU aggregated) state */\n> -       atomic_long_t state_local[NR_MEMCG_NODE_STAT_ITEMS];\n> +       long state_local[NR_MEMCG_NODE_STAT_ITEMS];\n> \n>         /* Pending child counts during tree propagation */\n>         long state_pending[NR_MEMCG_NODE_STAT_ITEMS];\n> @@ -485,7 +492,7 @@ unsigned long lruvec_page_state_local(struct lruvec *lruvec,\n>                 return 0;\n> \n>         pn = container_of(lruvec, struct mem_cgroup_per_node, lruvec);\n> -       x = atomic_long_read(&(pn->lruvec_stats->state_local[i]));\n> +       x = READ_ONCE(pn->lruvec_stats->state_local[i]);\n>  #ifdef CONFIG_SMP\n>         if (x < 0)\n>                 x = 0;\n> @@ -493,6 +500,10 @@ unsigned long lruvec_page_state_local(struct\n> lruvec *lruvec,\n>         return x;\n>  }\n> \n> +static void mod_memcg_lruvec_state(struct lruvec *lruvec,\n> +                                  enum node_stat_item idx,\n> +                                  int val);\n> +\n>  #ifdef CONFIG_MEMCG_V1\n>  void reparent_memcg_lruvec_state_local(struct mem_cgroup *memcg,\n>                                        struct mem_cgroup *parent, int idx)\n> @@ -506,12 +517,10 @@ void reparent_memcg_lruvec_state_local(struct\n> mem_cgroup *memcg,\n>         for_each_node(nid) {\n>                 struct lruvec *child_lruvec = mem_cgroup_lruvec(memcg,\n> NODE_DATA(nid));\n>                 struct lruvec *parent_lruvec =\n> mem_cgroup_lruvec(parent, NODE_DATA(nid));\n> -               struct mem_cgroup_per_node *parent_pn;\n>                 unsigned long value =\n> lruvec_page_state_local(child_lruvec, idx);\n> \n> -               parent_pn = container_of(parent_lruvec, struct\n> mem_cgroup_per_node, lruvec);\n> -\n> -               atomic_long_add(value,\n> &(parent_pn->lruvec_stats->state_local[i]));\n> +               mod_memcg_lruvec_state(child_lruvec, idx, -value);\n> +               mod_memcg_lruvec_state(parent_lruvec, idx, value);\n>         }\n>  }\n>  #endif\n> @@ -598,7 +607,7 @@ struct memcg_vmstats {\n>         unsigned long           events[NR_MEMCG_EVENTS];\n> \n>         /* Non-hierarchical (CPU aggregated) page state & events */\n> -       atomic_long_t           state_local[MEMCG_VMSTAT_SIZE];\n> +       long                    state_local[MEMCG_VMSTAT_SIZE];\n>         unsigned long           events_local[NR_MEMCG_EVENTS];\n> \n>         /* Pending child counts during tree propagation */\n> @@ -835,7 +844,7 @@ unsigned long memcg_page_state_local(struct\n> mem_cgroup *memcg, int idx)\n>         if (WARN_ONCE(BAD_STAT_IDX(i), \"%s: missing stat item %d\\n\",\n> __func__, idx))\n>                 return 0;\n> \n> -       x = atomic_long_read(&(memcg->vmstats->state_local[i]));\n> +       x = READ_ONCE(memcg->vmstats->state_local[i]);\n>  #ifdef CONFIG_SMP\n>         if (x < 0)\n>                 x = 0;\n> @@ -852,7 +861,8 @@ void reparent_memcg_state_local(struct mem_cgroup *memcg,\n>         if (WARN_ONCE(BAD_STAT_IDX(i), \"%s: missing stat item %d\\n\",\n> __func__, idx))\n>                 return;\n> \n> -       atomic_long_add(value, &(parent->vmstats->state_local[i]));\n> +       mod_memcg_state(memcg, idx, -value);\n> +       mod_memcg_state(parent, idx, value);\n>  }\n>  #endif\n> \n> @@ -4174,8 +4184,6 @@ struct aggregate_control {\n>         long *aggregate;\n>         /* pointer to the non-hierarchichal (CPU aggregated) counters */\n>         long *local;\n> -       /* pointer to the atomic non-hierarchichal (CPU aggregated) counters */\n> -       atomic_long_t *alocal;\n>         /* pointer to the pending child counters during tree propagation */\n>         long *pending;\n>         /* pointer to the parent's pending counters, could be NULL */\n> @@ -4213,12 +4221,8 @@ static void mem_cgroup_stat_aggregate(struct\n> aggregate_control *ac)\n>                 }\n> \n>                 /* Aggregate counts on this level and propagate upwards */\n> -               if (delta_cpu) {\n> -                       if (ac->local)\n> -                               ac->local[i] += delta_cpu;\n> -                       else if (ac->alocal)\n> -                               atomic_long_add(delta_cpu, &(ac->alocal[i]));\n> -               }\n> +               if (delta_cpu)\n> +                       ac->local[i] += delta_cpu;\n> \n>                 if (delta) {\n>                         ac->aggregate[i] += delta;\n> @@ -4289,8 +4293,7 @@ static void mem_cgroup_css_rstat_flush(struct\n> cgroup_subsys_state *css, int cpu)\n> \n>         ac = (struct aggregate_control) {\n>                 .aggregate = memcg->vmstats->state,\n> -               .local = NULL,\n> -               .alocal = memcg->vmstats->state_local,\n> +               .local = memcg->vmstats->state_local,\n>                 .pending = memcg->vmstats->state_pending,\n>                 .ppending = parent ? parent->vmstats->state_pending : NULL,\n>                 .cstat = statc->state,\n> @@ -4323,8 +4326,7 @@ static void mem_cgroup_css_rstat_flush(struct\n> cgroup_subsys_state *css, int cpu)\n> \n>                 ac = (struct aggregate_control) {\n>                         .aggregate = lstats->state,\n> -                       .local = NULL,\n> -                       .alocal = lstats->state_local,\n> +                       .local = lstats->state_local,\n>                         .pending = lstats->state_pending,\n>                         .ppending = plstats ? plstats->state_pending : NULL,\n>                         .cstat = lstatc->state,\n",
              "reply_to": "",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Qi Zheng",
              "summary": "Addressed Shakeel's concern by stating that maybe not, and suggested changing the comparison to objcg->memcg equality.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "\n\nOn 2/27/26 4:05 AM, Shakeel Butt wrote:\n> On Wed, Feb 25, 2026 at 05:44:56PM +0800, Qi Zheng wrote:\n>> From: Qi Zheng <zhengqi.arch@bytedance.com>\n>>\n>> Convert objcg to be per-memcg per-node type, so that when reparent LRU\n>> folios later, we can hold the lru lock at the node level, thus avoiding\n>> holding too many lru locks at once.\n>>\n>> Signed-off-by: Qi Zheng <zhengqi.arch@bytedance.com>\n>> ---\n>> changlog:\n>>   - fix a missing root_obj_cgroup conversion and completely delete\n>>     root_obj_cgroup.\n>>\n> \n> After this patch, do we care that page/folio/slab points to the objcg of the\n> same node as them for a given memcg?\n\nMaybe not. My only concern is whether the kernel has a way of\ndetermining whether two folios belong to the same memcg by checking if\nthe objcg pointers are equal. If so, it needs to be changed to check if\nobjcg->memcg are equal.\n\n> \n\n\n",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-27",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH 3/3] ptdesc: Account page tables to memcgs again",
          "message_id": "aZ-Innu9a3ND6Pdq@linux.dev",
          "url": "https://lore.kernel.org/all/aZ-Innu9a3ND6Pdq@linux.dev/",
          "date": "2026-02-26T00:00:06Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "The patch introduces a new function, memcg_stat_mod(), which updates memory cgroup counters. A reviewer suggests replacing this function with an existing one, mod_memcg_state(), to maintain consistency in core stat accounting functions.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Johannes Weiner",
              "summary": "Suggested replacing memcg_stat_mod() with mod_memcg_state() to maintain consistency in core stat accounting functions.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "reviewer requested changes"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v3 5/5] mm: add tracepoints for zone lock",
          "message_id": "aaCbMwGNuUO1SbUb@linux.dev",
          "url": "https://lore.kernel.org/all/aaCbMwGNuUO1SbUb@linux.dev/",
          "date": "2026-02-26T19:15:00Z",
          "in_reply_to": null,
          "ack_type": "Acked-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch adds tracepoint instrumentation to zone lock acquire and release operations in the Linux kernel. The implementation uses a lightweight inline helper that checks whether tracing is enabled, and calls an out-of-line helper when tracing is active. When CONFIG_TRACING is disabled, the helpers compile to empty inline stubs, leaving the fast path unaffected. This allows for easier debugging and analysis of zone lock behavior.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted a minor issue with the zone lock tracing implementation, specifically mentioning an empty inline stub in the !CONFIG_TRACING branch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "minor issue"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "One nit below other than that:\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n\n[...]",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Andrew Morton",
              "summary": "Reviewer Andrew Morton questioned the necessity of exporting several files, specifically mentioning include/linux/mmzone.h and mm/compaction.c among others.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "export",
                "necessity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Do we need the exports at all?\n\ninclude/linux/mmzone.h\ninclude/linux/zone_lock.h\ninclude/trace/events/zone_lock.h\nMAINTAINERS\nmm/compaction.c\nmm/internal.h\nmm/Makefile\nmm/memory_hotplug.c\nmm/mm_init.c\nmm/page_alloc.c\nmm/page_isolation.c\nmm/page_owner.c\nmm/page_reporting.c\nmm/show_mem.c\nmm/vmscan.c\nmm/vmstat.c\nmm/zone_lock.c",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that zone lock tracepoints may require exports like mmap_lock wrappers to prevent drivers from taking the lock directly",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential locking issue",
                "export requirements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Very good point and we don't. I think this might just be copying the mmap_lock\ntracepoint wrappers which might need the exports as some drivers might be taking\nthe mmap_lock.\n\nDmitry, please confirm (test) and let us know.",
              "reply_to": "Andrew Morton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v3 4/5] mm: rename zone->lock to zone->_lock",
          "message_id": "aaCacJZDka_cp9KO@linux.dev",
          "url": "https://lore.kernel.org/all/aaCacJZDka_cp9KO@linux.dev/",
          "date": "2026-02-26T19:10:07Z",
          "in_reply_to": null,
          "ack_type": "Acked-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch adds tracepoint instrumentation to zone lock acquire and release operations in the Linux kernel. The implementation uses a lightweight inline helper that checks whether tracing is enabled, and calls an out-of-line helper when tracing is active. When CONFIG_TRACING is disabled, the helpers compile to empty inline stubs, leaving the fast path unaffected. This allows for easier debugging and analysis of zone lock behavior.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted a minor issue with the zone lock tracing implementation, specifically mentioning an empty inline stub in the !CONFIG_TRACING branch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "minor issue"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "One nit below other than that:\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n\n[...]",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Andrew Morton",
              "summary": "Reviewer Andrew Morton questioned the necessity of exporting several files, specifically mentioning include/linux/mmzone.h and mm/compaction.c among others.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "export",
                "necessity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Do we need the exports at all?\n\ninclude/linux/mmzone.h\ninclude/linux/zone_lock.h\ninclude/trace/events/zone_lock.h\nMAINTAINERS\nmm/compaction.c\nmm/internal.h\nmm/Makefile\nmm/memory_hotplug.c\nmm/mm_init.c\nmm/page_alloc.c\nmm/page_isolation.c\nmm/page_owner.c\nmm/page_reporting.c\nmm/show_mem.c\nmm/vmscan.c\nmm/vmstat.c\nmm/zone_lock.c",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that zone lock tracepoints may require exports like mmap_lock wrappers to prevent drivers from taking the lock directly",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential locking issue",
                "export requirements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Very good point and we don't. I think this might just be copying the mmap_lock\ntracepoint wrappers which might need the exports as some drivers might be taking\nthe mmap_lock.\n\nDmitry, please confirm (test) and let us know.",
              "reply_to": "Andrew Morton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v3 3/5] mm: convert compaction to zone lock wrappers",
          "message_id": "aaCZ6Kmi71liz7K5@linux.dev",
          "url": "https://lore.kernel.org/all/aaCZ6Kmi71liz7K5@linux.dev/",
          "date": "2026-02-26T19:08:07Z",
          "in_reply_to": null,
          "ack_type": "Acked-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "This patch adds tracepoint instrumentation to zone lock acquire and release operations in the Linux kernel. The implementation uses a lightweight inline helper that checks whether tracing is enabled, and calls an out-of-line helper when tracing is active. When CONFIG_TRACING is disabled, the helpers compile to empty inline stubs, leaving the fast path unaffected. This allows for easier debugging and analysis of zone lock behavior.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted a minor issue with the zone lock tracing implementation, specifically mentioning an empty inline stub in the !CONFIG_TRACING branch.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "minor issue"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "One nit below other than that:\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n\n[...]",
              "reply_to": "Dmitry Ilvokhin",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Andrew Morton",
              "summary": "Reviewer Andrew Morton questioned the necessity of exporting several files, specifically mentioning include/linux/mmzone.h and mm/compaction.c among others.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "export",
                "necessity"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Do we need the exports at all?\n\ninclude/linux/mmzone.h\ninclude/linux/zone_lock.h\ninclude/trace/events/zone_lock.h\nMAINTAINERS\nmm/compaction.c\nmm/internal.h\nmm/Makefile\nmm/memory_hotplug.c\nmm/mm_init.c\nmm/page_alloc.c\nmm/page_isolation.c\nmm/page_owner.c\nmm/page_reporting.c\nmm/show_mem.c\nmm/vmscan.c\nmm/vmstat.c\nmm/zone_lock.c",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Reviewer noted that zone lock tracepoints may require exports like mmap_lock wrappers to prevent drivers from taking the lock directly",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "potential locking issue",
                "export requirements"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Reviewed-by"
              ],
              "raw_body": "Very good point and we don't. I think this might just be copying the mmap_lock\ntracepoint wrappers which might need the exports as some drivers might be taking\nthe mmap_lock.\n\nDmitry, please confirm (test) and let us know.",
              "reply_to": "Andrew Morton",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH] memcg: fix slab accounting in refill_obj_stock() trylock path",
          "message_id": "aaBM0fN8fqER7Avf@linux.dev",
          "url": "https://lore.kernel.org/all/aaBM0fN8fqER7Avf@linux.dev/",
          "date": "2026-02-26T13:39:08Z",
          "in_reply_to": null,
          "ack_type": "Acked-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Fixes a slab accounting bug in refill_obj_stock() trylock path, ensuring correct stats for NR_SLAB_RECLAIMABLE_B and NR_SLAB_UNRECLAIMABLE_B.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Vlastimil Babka",
              "summary": "Raised questions about the user-visible impact of the bug, seeking clarification on how it affects system behavior.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "WAITING_FOR_REVIEW"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On 2/26/26 14:39, Shakeel Butt wrote:\n> On Thu, Feb 26, 2026 at 07:51:37PM +0800, Hao Li wrote:\n>> In the trylock path of refill_obj_stock(), mod_objcg_mlstate() should\n>> use the real alloc/free bytes (i.e., nr_acct) for accounting, rather\n>> than nr_bytes.\n>> \n>> Fixes: 200577f69f29 (\"memcg: objcg stock trylock without irq disabling\")\n>> Cc: stable@vger.kernel.org\n>> Signed-off-by: Hao Li <hao.li@linux.dev>\n> \n> Thanks for the fix.\n> \n> Acked-by: Shakeel Butt <shakeel.butt@linux.dev>\n\nWhat are the user-visible effects of the bug?\n",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Hao Li",
              "summary": "Provided additional context and explanation of the bug's impact, clarifying that it affects slab accounting stats.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "WAITING_FOR_REVIEW"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On Thu, Feb 26, 2026 at 02:44:02PM +0100, Vlastimil Babka wrote:\n> On 2/26/26 14:39, Shakeel Butt wrote:\n> > On Thu, Feb 26, 2026 at 07:51:37PM +0800, Hao Li wrote:\n> >> In the trylock path of refill_obj_stock(), mod_objcg_mlstate() should\n> >> use the real alloc/free bytes (i.e., nr_acct) for accounting, rather\n> >> than nr_bytes.\n> >> \n> >> Fixes: 200577f69f29 (\"memcg: objcg stock trylock without irq disabling\")\n> >> Cc: stable@vger.kernel.org\n> >> Signed-off-by: Hao Li <hao.li@linux.dev>\n> > \n> > Thanks for the fix.\n> > \n> > Acked-by: Shakeel Butt <shakeel.butt@linux.dev>\n> \n> What are the user-visible effects of the bug?\n\nThe user-visible impact is that the NR_SLAB_RECLAIMABLE_B and\nNR_SLAB_UNRECLAIMABLE_B stats can end up being incorrect.\n\nFor example, if a user allocates a 6144-byte object, then before this fix\nrefill_obj_stock() calls mod_objcg_mlstate(..., nr_bytes=2048), even though it\nshould account for 6144 bytes (i.e., nr_acct).\n\nWhen the user later frees the same object with kfree(), refill_obj_stock() calls\nmod_objcg_mlstate(..., nr_bytes=6144). This ends up adding 6144 to the stats,\nbut it should be applying -6144 (i.e., nr_acct) since the object is being\nfreed.\n\n-- \nThanks,\nHao\n\n",
              "reply_to": "",
              "message_date": "2026-02-27",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Shakeel Butt",
              "summary": "Acknowledged the fix and applied it, indicating a positive sentiment towards the patch.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "LGTM"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "Hao Li",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v5 31/32] mm: memcontrol: eliminate the problem of dying memory cgroup for LRU folios",
          "message_id": "aZ-yctMtmVYVawK-@linux.dev",
          "url": "https://lore.kernel.org/all/aZ-yctMtmVYVawK-@linux.dev/",
          "date": "2026-02-26T02:40:28Z",
          "in_reply_to": null,
          "ack_type": "Acked-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Conversion of objcg to per-memcg per-node type in mm: memcontrol",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Raised a question about whether page/folio/slab points to the same node's objcg for a given memcg, but didn't express an opinion on the patch itself.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Qi Zheng",
              "summary": "Addressed Shakeel's concern by stating that maybe not, and suggested changing the comparison to objcg->memcg equality.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "\n\nOn 2/27/26 4:05 AM, Shakeel Butt wrote:\n> On Wed, Feb 25, 2026 at 05:44:56PM +0800, Qi Zheng wrote:\n>> From: Qi Zheng <zhengqi.arch@bytedance.com>\n>>\n>> Convert objcg to be per-memcg per-node type, so that when reparent LRU\n>> folios later, we can hold the lru lock at the node level, thus avoiding\n>> holding too many lru locks at once.\n>>\n>> Signed-off-by: Qi Zheng <zhengqi.arch@bytedance.com>\n>> ---\n>> changlog:\n>>   - fix a missing root_obj_cgroup conversion and completely delete\n>>     root_obj_cgroup.\n>>\n> \n> After this patch, do we care that page/folio/slab points to the objcg of the\n> same node as them for a given memcg?\n\nMaybe not. My only concern is whether the kernel has a way of\ndetermining whether two folios belong to the same memcg by checking if\nthe objcg pointers are equal. If so, it needs to be changed to check if\nobjcg->memcg are equal.\n\n> \n\n\n",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-27",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v5 update 30/32] mm: memcontrol: convert objcg to be per-memcg per-node type",
          "message_id": "aZ-uNV1biPYLhJ48@linux.dev",
          "url": "https://lore.kernel.org/all/aZ-uNV1biPYLhJ48@linux.dev/",
          "date": "2026-02-26T02:28:13Z",
          "in_reply_to": null,
          "ack_type": "Acked-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Conversion of objcg to per-memcg per-node type in mm: memcontrol",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Raised a question about whether page/folio/slab points to the same node's objcg for a given memcg, but didn't express an opinion on the patch itself.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Qi Zheng",
              "summary": "Addressed Shakeel's concern by stating that maybe not, and suggested changing the comparison to objcg->memcg equality.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "\n\nOn 2/27/26 4:05 AM, Shakeel Butt wrote:\n> On Wed, Feb 25, 2026 at 05:44:56PM +0800, Qi Zheng wrote:\n>> From: Qi Zheng <zhengqi.arch@bytedance.com>\n>>\n>> Convert objcg to be per-memcg per-node type, so that when reparent LRU\n>> folios later, we can hold the lru lock at the node level, thus avoiding\n>> holding too many lru locks at once.\n>>\n>> Signed-off-by: Qi Zheng <zhengqi.arch@bytedance.com>\n>> ---\n>> changlog:\n>>   - fix a missing root_obj_cgroup conversion and completely delete\n>>     root_obj_cgroup.\n>>\n> \n> After this patch, do we care that page/folio/slab points to the objcg of the\n> same node as them for a given memcg?\n\nMaybe not. My only concern is whether the kernel has a way of\ndetermining whether two folios belong to the same memcg by checking if\nthe objcg pointers are equal. If so, it needs to be changed to check if\nobjcg->memcg are equal.\n\n> \n\n\n",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-27",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH v5 29/32] mm: memcontrol: prepare for reparenting non-hierarchical stats",
          "message_id": "aZ-kefGBeT-RzGcG@linux.dev",
          "url": "https://lore.kernel.org/all/aZ-kefGBeT-RzGcG@linux.dev/",
          "date": "2026-02-26T01:41:58Z",
          "in_reply_to": null,
          "ack_type": "Acked-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Conversion of objcg to per-memcg per-node type in mm: memcontrol",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Shakeel Butt",
              "summary": "Raised a question about whether page/folio/slab points to the same node's objcg for a given memcg, but didn't express an opinion on the patch itself.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Qi Zheng",
              "summary": "Addressed Shakeel's concern by stating that maybe not, and suggested changing the comparison to objcg->memcg equality.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "\n\nOn 2/27/26 4:05 AM, Shakeel Butt wrote:\n> On Wed, Feb 25, 2026 at 05:44:56PM +0800, Qi Zheng wrote:\n>> From: Qi Zheng <zhengqi.arch@bytedance.com>\n>>\n>> Convert objcg to be per-memcg per-node type, so that when reparent LRU\n>> folios later, we can hold the lru lock at the node level, thus avoiding\n>> holding too many lru locks at once.\n>>\n>> Signed-off-by: Qi Zheng <zhengqi.arch@bytedance.com>\n>> ---\n>> changlog:\n>>   - fix a missing root_obj_cgroup conversion and completely delete\n>>     root_obj_cgroup.\n>>\n> \n> After this patch, do we care that page/folio/slab points to the objcg of the\n> same node as them for a given memcg?\n\nMaybe not. My only concern is whether the kernel has a way of\ndetermining whether two folios belong to the same memcg by checking if\nthe objcg pointers are equal. If so, it needs to be changed to check if\nobjcg->memcg are equal.\n\n> \n\n\n",
              "reply_to": "Shakeel Butt",
              "message_date": "2026-02-27",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "discussions_posted": [],
      "errors": []
    },
    {
      "name": "Usama Arif",
      "primary_email": "usama.arif@linux.dev",
      "patches_submitted": [],
      "patches_reviewed": [
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [RFC v2 16/21] mm: thp: add THP_SPLIT_PMD_FAILED counter",
          "message_id": "a108bab5-9b53-4b90-a102-cc6fc22bb389@linux.dev",
          "url": "https://lore.kernel.org/all/a108bab5-9b53-4b90-a102-cc6fc22bb389@linux.dev/",
          "date": "2026-02-26T14:22:41Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "THP_SPLIT_PMD_FAILED counter patch requires CONFIG_TRANSPARENT_HUGEPAGE guard and additional diff in a future series.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Usama Arif",
              "summary": "Identified a need for CONFIG_TRANSPARENT_HUGEPAGE guard and additional diff in a future series. Requested author to address these concerns.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "CONFIG_TRANSPARENT_HUGEPAGE",
                "additional diff needed"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [RFC V1 16/16] arm64/mm: Add initial support for FEAT_D128 page tables",
          "message_id": "20260226141024.1869713-1-usama.arif@linux.dev",
          "url": "https://lore.kernel.org/all/20260226141024.1869713-1-usama.arif@linux.dev/",
          "date": "2026-02-26T14:10:34Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Initial support for FEAT_D128 page tables on arm64",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Usama Arif",
              "summary": "Raised concerns about the patch's readiness, suggesting that if unsure, it's better to say N. Also questioned whether a certain value represents PAGE_SHIFT and its implications for page sizes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "unsure, say N",
                "PAGE_SHIFT question"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [RFC V1 10/16] arm64/mm: Route all pgtable writes via ptdesc_set()",
          "message_id": "20260226123747.801496-1-usama.arif@linux.dev",
          "url": "https://lore.kernel.org/all/20260226123747.801496-1-usama.arif@linux.dev/",
          "date": "2026-02-26T12:37:55Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Initial support for FEAT_D128 page tables on arm64",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Usama Arif",
              "summary": "Raised concerns about the patch's readiness, suggesting that if unsure, it's better to say N. Also questioned whether a certain value represents PAGE_SHIFT and its implications for page sizes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "unsure, say N",
                "PAGE_SHIFT question"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [RFC V1 01/16] mm: Abstract printing of pxd_val()",
          "message_id": "20260226123442.759447-1-usama.arif@linux.dev",
          "url": "https://lore.kernel.org/all/20260226123442.759447-1-usama.arif@linux.dev/",
          "date": "2026-02-26T12:34:58Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Initial support for FEAT_D128 page tables on arm64",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Usama Arif",
              "summary": "Raised concerns about the patch's readiness, suggesting that if unsure, it's better to say N. Also questioned whether a certain value represents PAGE_SHIFT and its implications for page sizes.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "unsure, say N",
                "PAGE_SHIFT question"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_reviewed",
          "subject": "Re: [PATCH v2] smaps: Report correct page sizes with THP",
          "message_id": "20260226120818.408966-1-usama.arif@linux.dev",
          "url": "https://lore.kernel.org/all/20260226120818.408966-1-usama.arif@linux.dev/",
          "date": "2026-02-26T12:08:29Z",
          "in_reply_to": null,
          "ack_type": null,
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Usama Arif questions whether the MMUPageSize field should always be present in smaps output, even for VMAs without resident pages.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Usama Arif",
              "summary": "Raised the issue of MMUPageSize field presence in smaps output for VMAs without resident pages, suggesting a potential fix to always add it.",
              "sentiment": "NEUTRAL",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "patches_acked": [
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH mm-unstable v15 11/13] mm/khugepaged: avoid unnecessary mTHP collapse attempts",
          "message_id": "20260226162653.3802758-1-usama.arif@linux.dev",
          "url": "https://lore.kernel.org/all/20260226162653.3802758-1-usama.arif@linux.dev/",
          "date": "2026-02-26T16:27:05Z",
          "in_reply_to": null,
          "ack_type": "Acked-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Patch aims to avoid unnecessary mTHP collapse attempts by considering specific error codes.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Usama Arif",
              "summary": "Raised concern that SCAN_CGROUP_CHARGE_FAIL might not be a reliable indicator of resource constraint, and suggested considering other error codes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Nico Pache",
              "summary": "Provided additional context and insight into the decisions made in the patch, highlighting potential scenarios where collapsing or charging large order pages might be unproductive.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "POSITIVE"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On Thu, Feb 26, 2026 at 9:27AM Usama Arif <usama.arif@linux.dev> wrote:\n>\n> On Wed, 25 Feb 2026 20:26:31 -0700 Nico Pache <npache@redhat.com> wrote:\n>\n> > There are cases where, if an attempted collapse fails, all subsequent\n> > orders are guaranteed to also fail. Avoid these collapse attempts by\n> > bailing out early.\n> >\n> > Signed-off-by: Nico Pache <npache@redhat.com>\n> > ---\n> >  mm/khugepaged.c | 35 ++++++++++++++++++++++++++++++++++-\n> >  1 file changed, 34 insertions(+), 1 deletion(-)\n> >\n> > diff --git a/mm/khugepaged.c b/mm/khugepaged.c\n> > index 1c3711ed4513..388d3f2537e2 100644\n> > --- a/mm/khugepaged.c\n> > +++ b/mm/khugepaged.c\n> > @@ -1492,9 +1492,42 @@ static int mthp_collapse(struct mm_struct *mm, unsigned long address,\n> >                       ret = collapse_huge_page(mm, collapse_address, referenced,\n> >                                                unmapped, cc, mmap_locked,\n> >                                                order);\n> > -                     if (ret == SCAN_SUCCEED) {\n> > +\n> > +                     switch (ret) {\n> > +                     /* Cases were we continue to next collapse candidate */\n> > +                     case SCAN_SUCCEED:\n> >                               collapsed += nr_pte_entries;\n> > +                             fallthrough;\n> > +                     case SCAN_PTE_MAPPED_HUGEPAGE:\n> >                               continue;\n> > +                     /* Cases were lower orders might still succeed */\n> > +                     case SCAN_LACK_REFERENCED_PAGE:\n> > +                     case SCAN_EXCEED_NONE_PTE:\n> > +                     case SCAN_EXCEED_SWAP_PTE:\n> > +                     case SCAN_EXCEED_SHARED_PTE:\n> > +                     case SCAN_PAGE_LOCK:\n> > +                     case SCAN_PAGE_COUNT:\n> > +                     case SCAN_PAGE_LRU:\n> > +                     case SCAN_PAGE_NULL:\n> > +                     case SCAN_DEL_PAGE_LRU:\n> > +                     case SCAN_PTE_NON_PRESENT:\n> > +                     case SCAN_PTE_UFFD_WP:\n> > +                     case SCAN_ALLOC_HUGE_PAGE_FAIL:\n> > +                             goto next_order;\n> > +                     /* Cases were no further collapse is possible */\n> > +                     case SCAN_CGROUP_CHARGE_FAIL:\n>\n> The only one that stands out to me is SCAN_CGROUP_CHARGE_FAIL. memcg charging\n> of higher order folio might fail, but a lower order folio might pass?\n> That said, if the cgroup is that tight, continuing collapse work may not\n> be productive.\n>\n> Acked-by: Usama Arif <usama.arif@linux.dev>\n\nThanks! IIRC, David and I discussed all of these off chain to confirm\ntheir placement. I had this in the 'next_order' case at some point and\nDavid recommended it to \"fail\" for the same reason you state here:\ncollapsing or charging large order pages in such a tight cgroup is\nlikely unproductive and not worth the effort.\n\nIn contrast, SCAN_ALLOC_HUGE_PAGE_FAIL does not necessarily indicate a\nresource constraint, but it could. We might fail to allocate an N-page\nsize due to fragmentation, but we could easily find an (N-1) size. We\ncould also have a scenario where a lack of memory causes the failure,\niterating all the way down, which would be unproductive. However, at\nthat point the OOM reaper should be active and the system will already\nbe cornered in multiple ways, so it should be ok.\n\nHopefully that gives some insight into the decisions made here :)\n\nCheers,\n-- Nico\n\n>\n> > +                     case SCAN_COPY_MC:\n> > +                     case SCAN_ADDRESS_RANGE:\n> > +                     case SCAN_NO_PTE_TABLE:\n> > +                     case SCAN_ANY_PROCESS:\n> > +                     case SCAN_VMA_NULL:\n> > +                     case SCAN_VMA_CHECK:\n> > +                     case SCAN_SCAN_ABORT:\n> > +                     case SCAN_PAGE_ANON:\n> > +                     case SCAN_PMD_MAPPED:\n> > +                     case SCAN_FAIL:\n> > +                     default:\n> > +                             return collapsed;\n> >                       }\n> >               }\n> >\n> > --\n> > 2.53.0\n> >\n> >\n>\n\n\n",
              "reply_to": "Usama Arif",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        },
        {
          "activity_type": "patch_acked",
          "subject": "Re: [PATCH mm-unstable v15 12/13] mm/khugepaged: run khugepaged for all orders",
          "message_id": "20260226155334.3373953-1-usama.arif@linux.dev",
          "url": "https://lore.kernel.org/all/20260226155334.3373953-1-usama.arif@linux.dev/",
          "date": "2026-02-26T15:53:49Z",
          "in_reply_to": null,
          "ack_type": "Acked-by",
          "is_ongoing": false,
          "submitted_date": null,
          "patch_summary": "Patch aims to avoid unnecessary mTHP collapse attempts by considering specific error codes.",
          "analysis_source": "llm",
          "review_comments": [
            {
              "author": "Usama Arif",
              "summary": "Raised concern that SCAN_CGROUP_CHARGE_FAIL might not be a reliable indicator of resource constraint, and suggested considering other error codes.",
              "sentiment": "NEEDS_WORK",
              "sentiment_signals": [
                "NEEDS_WORK"
              ],
              "has_inline_review": false,
              "tags_given": [
                "Acked-by"
              ],
              "raw_body": "",
              "reply_to": "",
              "message_date": "",
              "message_id": "",
              "analysis_source": "llm"
            },
            {
              "author": "Nico Pache",
              "summary": "Provided additional context and insight into the decisions made in the patch, highlighting potential scenarios where collapsing or charging large order pages might be unproductive.",
              "sentiment": "POSITIVE",
              "sentiment_signals": [
                "POSITIVE"
              ],
              "has_inline_review": false,
              "tags_given": [],
              "raw_body": "On Thu, Feb 26, 2026 at 9:27AM Usama Arif <usama.arif@linux.dev> wrote:\n>\n> On Wed, 25 Feb 2026 20:26:31 -0700 Nico Pache <npache@redhat.com> wrote:\n>\n> > There are cases where, if an attempted collapse fails, all subsequent\n> > orders are guaranteed to also fail. Avoid these collapse attempts by\n> > bailing out early.\n> >\n> > Signed-off-by: Nico Pache <npache@redhat.com>\n> > ---\n> >  mm/khugepaged.c | 35 ++++++++++++++++++++++++++++++++++-\n> >  1 file changed, 34 insertions(+), 1 deletion(-)\n> >\n> > diff --git a/mm/khugepaged.c b/mm/khugepaged.c\n> > index 1c3711ed4513..388d3f2537e2 100644\n> > --- a/mm/khugepaged.c\n> > +++ b/mm/khugepaged.c\n> > @@ -1492,9 +1492,42 @@ static int mthp_collapse(struct mm_struct *mm, unsigned long address,\n> >                       ret = collapse_huge_page(mm, collapse_address, referenced,\n> >                                                unmapped, cc, mmap_locked,\n> >                                                order);\n> > -                     if (ret == SCAN_SUCCEED) {\n> > +\n> > +                     switch (ret) {\n> > +                     /* Cases were we continue to next collapse candidate */\n> > +                     case SCAN_SUCCEED:\n> >                               collapsed += nr_pte_entries;\n> > +                             fallthrough;\n> > +                     case SCAN_PTE_MAPPED_HUGEPAGE:\n> >                               continue;\n> > +                     /* Cases were lower orders might still succeed */\n> > +                     case SCAN_LACK_REFERENCED_PAGE:\n> > +                     case SCAN_EXCEED_NONE_PTE:\n> > +                     case SCAN_EXCEED_SWAP_PTE:\n> > +                     case SCAN_EXCEED_SHARED_PTE:\n> > +                     case SCAN_PAGE_LOCK:\n> > +                     case SCAN_PAGE_COUNT:\n> > +                     case SCAN_PAGE_LRU:\n> > +                     case SCAN_PAGE_NULL:\n> > +                     case SCAN_DEL_PAGE_LRU:\n> > +                     case SCAN_PTE_NON_PRESENT:\n> > +                     case SCAN_PTE_UFFD_WP:\n> > +                     case SCAN_ALLOC_HUGE_PAGE_FAIL:\n> > +                             goto next_order;\n> > +                     /* Cases were no further collapse is possible */\n> > +                     case SCAN_CGROUP_CHARGE_FAIL:\n>\n> The only one that stands out to me is SCAN_CGROUP_CHARGE_FAIL. memcg charging\n> of higher order folio might fail, but a lower order folio might pass?\n> That said, if the cgroup is that tight, continuing collapse work may not\n> be productive.\n>\n> Acked-by: Usama Arif <usama.arif@linux.dev>\n\nThanks! IIRC, David and I discussed all of these off chain to confirm\ntheir placement. I had this in the 'next_order' case at some point and\nDavid recommended it to \"fail\" for the same reason you state here:\ncollapsing or charging large order pages in such a tight cgroup is\nlikely unproductive and not worth the effort.\n\nIn contrast, SCAN_ALLOC_HUGE_PAGE_FAIL does not necessarily indicate a\nresource constraint, but it could. We might fail to allocate an N-page\nsize due to fragmentation, but we could easily find an (N-1) size. We\ncould also have a scenario where a lack of memory causes the failure,\niterating all the way down, which would be unproductive. However, at\nthat point the OOM reaper should be active and the system will already\nbe cornered in multiple ways, so it should be ok.\n\nHopefully that gives some insight into the decisions made here :)\n\nCheers,\n-- Nico\n\n>\n> > +                     case SCAN_COPY_MC:\n> > +                     case SCAN_ADDRESS_RANGE:\n> > +                     case SCAN_NO_PTE_TABLE:\n> > +                     case SCAN_ANY_PROCESS:\n> > +                     case SCAN_VMA_NULL:\n> > +                     case SCAN_VMA_CHECK:\n> > +                     case SCAN_SCAN_ABORT:\n> > +                     case SCAN_PAGE_ANON:\n> > +                     case SCAN_PMD_MAPPED:\n> > +                     case SCAN_FAIL:\n> > +                     default:\n> > +                             return collapsed;\n> >                       }\n> >               }\n> >\n> > --\n> > 2.53.0\n> >\n> >\n>\n\n\n",
              "reply_to": "Usama Arif",
              "message_date": "2026-02-26",
              "message_id": "",
              "analysis_source": "llm"
            }
          ]
        }
      ],
      "discussions_posted": [],
      "errors": []
    }
  ]
}