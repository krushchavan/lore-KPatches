<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Review Comments: [PATCH 3/4] mm: convert compaction to zone lock wrappers</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto,
                         "Helvetica Neue", Arial, sans-serif;
            background: #f5f5f5;
            color: #333;
            line-height: 1.6;
            padding: 20px;
            max-width: 900px;
            margin: 0 auto;
        }
        .home-link { margin-bottom: 12px; display: block; }
        .home-link a { color: #0366d6; text-decoration: none; font-size: 0.9em; }
        .home-link a:hover { text-decoration: underline; }

        h1 { font-size: 1.3em; margin-bottom: 2px; color: #1a1a1a; line-height: 1.3; }

        .lore-link { font-size: 0.85em; margin: 4px 0 6px; display: block; }
        .lore-link a { color: #0366d6; text-decoration: none; }
        .lore-link a:hover { text-decoration: underline; }

        .date-range {
            font-size: 0.8em;
            color: #888;
            margin-bottom: 16px;
        }
        .date-range a { color: #0366d6; text-decoration: none; }
        .date-range a:hover { text-decoration: underline; }

        /* thread-node scroll margin so the card isn't clipped at the top */
        .thread-node { scroll-margin-top: 8px; }

        /* ── Patch summary ──────────────────────────────────────────── */
        .patch-summary-block {
            background: #fff;
            border-radius: 8px;
            padding: 12px 16px;
            margin-bottom: 20px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            border-left: 3px solid #4a90d9;
        }
        .patch-summary-label {
            font-size: 0.72em;
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 0.06em;
            color: #4a90d9;
            margin-bottom: 4px;
        }
        .patch-summary-text {
            font-size: 0.88em;
            color: #444;
            line-height: 1.55;
        }

        /* ── Thread tree ────────────────────────────────────────────── */
        .thread-tree {
            display: flex;
            flex-direction: column;
            gap: 6px;
        }

        /* Depth indentation via left border */
        .thread-node { position: relative; }
        .thread-children {
            margin-left: 20px;
            padding-left: 12px;
            border-left: 2px solid #e0e0e0;
            margin-top: 6px;
            display: flex;
            flex-direction: column;
            gap: 6px;
        }

        /* ── Review comment card ────────────────────────────────────── */
        .review-comment {
            background: #fff;
            border-radius: 6px;
            padding: 10px 14px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            font-size: 0.88em;
        }
        .review-comment-header {
            display: flex;
            flex-wrap: wrap;
            align-items: center;
            gap: 6px;
            margin-bottom: 5px;
        }
        .review-author {
            font-weight: 700;
            color: #1a1a1a;
            font-size: 0.95em;
        }

        /* Date chip — links back to the daily report */
        .date-chip {
            font-size: 0.75em;
            color: #777;
            background: #f0f0f0;
            border-radius: 10px;
            padding: 1px 7px;
            text-decoration: none;
            white-space: nowrap;
        }
        a.date-chip:hover { background: #e0e8f5; color: #0366d6; }

        .badge {
            display: inline-block;
            padding: 1px 8px;
            border-radius: 10px;
            font-size: 0.75em;
            font-weight: 600;
        }
        .inline-review-badge {
            display: inline-block;
            padding: 0 6px;
            border-radius: 8px;
            font-size: 0.78em;
            font-weight: 500;
            background: #e3f2fd;
            color: #1565c0;
        }
        .review-tag-badge {
            display: inline-block;
            padding: 0 6px;
            border-radius: 8px;
            font-size: 0.78em;
            font-weight: 500;
            background: #e8f5e9;
            color: #2e7d32;
        }
        .analysis-source-badge {
            display: inline-block;
            padding: 1px 7px;
            border-radius: 10px;
            font-size: 0.72em;
            font-weight: 600;
            border: 1px solid rgba(0,0,0,0.1);
        }

        .review-comment-text {
            color: #444;
            line-height: 1.55;
            margin-bottom: 4px;
        }
        .review-comment-signals {
            margin-top: 3px;
            font-size: 0.85em;
            color: #aaa;
            font-style: italic;
        }

        /* ── Collapsible raw body ───────────────────────────────────── */
        .raw-body-toggle {
            margin-top: 5px;
            font-size: 0.85em;
        }
        .raw-body-toggle summary {
            cursor: pointer;
            color: #888;
            padding: 2px 0;
            font-weight: 500;
            font-size: 0.9em;
            list-style: none;
        }
        .raw-body-toggle summary::-webkit-details-marker { display: none; }
        .raw-body-toggle summary::before { content: "▶ "; font-size: 0.7em; }
        .raw-body-toggle[open] summary::before { content: "▼ "; }
        .raw-body-toggle summary:hover { color: #555; }
        .raw-body-text {
            white-space: pre-wrap;
            font-size: 0.95em;
            background: #f8f8f8;
            padding: 8px 10px;
            border-radius: 4px;
            max-height: 360px;
            overflow-y: auto;
            margin-top: 4px;
            line-height: 1.5;
            color: #444;
            border: 1px solid #e8e8e8;
        }
        .reply-to-label {
            font-size: 0.8em;
            color: #999;
            font-style: italic;
            margin-top: 3px;
        }
        .lore-link {
            display: inline-block;
            margin-top: 4px;
            font-size: 0.82em;
            color: #0366d6;
            text-decoration: none;
            font-weight: 500;
            white-space: nowrap;
        }
        .lore-link:hover { text-decoration: underline; color: #0056b3; }

        .no-reviews {
            color: #aaa;
            font-size: 0.85em;
            font-style: italic;
            padding: 8px 0;
        }

        footer {
            text-align: center;
            color: #bbb;
            font-size: 0.78em;
            margin-top: 36px;
            padding: 16px;
        }
    </style>
</head>
<body>
    <div class="home-link"><a href="../index.html">&larr; Back to reports</a></div>
    <h1>[PATCH 3/4] mm: convert compaction to zone lock wrappers</h1>
    <div class="lore-link"><a href="https://lore.kernel.org/all/3462b7fd26123c69ccdd121a894da14bbfafdd9d.1770821420.git.d@ilvokhin.com/" target="_blank">View on lore.kernel.org &rarr;</a></div>
    <div class="date-range">Active on: <a href="#2026-02-24">2026-02-24</a> &bull; <a href="#2026-02-23">2026-02-23</a> &bull; <a href="#2026-02-20">2026-02-20</a> &bull; <a href="#2026-02-11">2026-02-11</a></div>
    
    <div class="thread-tree">
<div class="thread-node depth-0" id="2026-02-11">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Dmitry Ilvokhin (author)</span>
<a class="date-chip" href="../2026-02-20_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-11">2026-02-11</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author is addressing a concern about direct zone lock acquire/release operations being replaced with the newly introduced wrappers. The author confirms that the changes are purely mechanical substitutions and no functional change is intended, but notes that the compaction path will be handled separately in a following patch due to additional non-trivial modifications.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Replace direct zone lock acquire/release operations with the
newly introduced wrappers.

The changes are purely mechanical substitutions. No functional change
intended. Locking semantics and ordering remain unchanged.

The compaction path is left unchanged for now and will be
handled separately in the following patch due to additional
non-trivial modifications.

Signed-off-by: Dmitry Ilvokhin &lt;d@ilvokhin.com&gt;
---
 mm/memory_hotplug.c |  9 +++---
 mm/mm_init.c        |  3 +-
 mm/page_alloc.c     | 73 +++++++++++++++++++++++----------------------
 mm/page_isolation.c | 19 ++++++------
 mm/page_reporting.c | 13 ++++----
 mm/show_mem.c       |  5 ++--
 mm/vmscan.c         |  5 ++--
 mm/vmstat.c         |  9 +++---
 8 files changed, 72 insertions(+), 64 deletions(-)

diff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c
index bc805029da51..cfc0103fa50e 100644
--- a/mm/memory_hotplug.c
+++ b/mm/memory_hotplug.c
@@ -36,6 +36,7 @@
 #include &lt;linux/rmap.h&gt;
 #include &lt;linux/module.h&gt;
 #include &lt;linux/node.h&gt;
+#include &lt;linux/zone_lock.h&gt;
 
 #include &lt;asm/tlbflush.h&gt;
 
@@ -1190,9 +1191,9 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,
 	 * Fixup the number of isolated pageblocks before marking the sections
 	 * onlining, such that undo_isolate_page_range() works correctly.
 	 */
-	spin_lock_irqsave(&amp;zone-&gt;lock, flags);
+	zone_lock_irqsave(zone, flags);
 	zone-&gt;nr_isolate_pageblock += nr_pages / pageblock_nr_pages;
-	spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+	zone_unlock_irqrestore(zone, flags);
 
 	/*
 	 * If this zone is not populated, then it is not in zonelist.
@@ -2041,9 +2042,9 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,
 	 * effectively stale; nobody should be touching them. Fixup the number
 	 * of isolated pageblocks, memory onlining will properly revert this.
 	 */
-	spin_lock_irqsave(&amp;zone-&gt;lock, flags);
+	zone_lock_irqsave(zone, flags);
 	zone-&gt;nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;
-	spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+	zone_unlock_irqrestore(zone, flags);
 
 	lru_cache_enable();
 	zone_pcp_enable(zone);
diff --git a/mm/mm_init.c b/mm/mm_init.c
index 1a29a719af58..426e5a0256f9 100644
--- a/mm/mm_init.c
+++ b/mm/mm_init.c
@@ -32,6 +32,7 @@
 #include &lt;linux/vmstat.h&gt;
 #include &lt;linux/kexec_handover.h&gt;
 #include &lt;linux/hugetlb.h&gt;
+#include &lt;linux/zone_lock.h&gt;
 #include &quot;internal.h&quot;
 #include &quot;slab.h&quot;
 #include &quot;shuffle.h&quot;
@@ -1425,7 +1426,7 @@ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx,
 	zone_set_nid(zone, nid);
 	zone-&gt;name = zone_names[idx];
 	zone-&gt;zone_pgdat = NODE_DATA(nid);
-	spin_lock_init(&amp;zone-&gt;lock);
+	zone_lock_init(zone);
 	zone_seqlock_init(zone);
 	zone_pcp_init(zone);
 }
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index e4104973e22f..2c9fe30da7a1 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -54,6 +54,7 @@
 #include &lt;linux/delayacct.h&gt;
 #include &lt;linux/cacheinfo.h&gt;
 #include &lt;linux/pgalloc_tag.h&gt;
+#include &lt;linux/zone_lock.h&gt;
 #include &lt;asm/div64.h&gt;
 #include &quot;internal.h&quot;
 #include &quot;shuffle.h&quot;
@@ -1494,7 +1495,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,
 	/* Ensure requested pindex is drained first. */
 	pindex = pindex - 1;
 
-	spin_lock_irqsave(&amp;zone-&gt;lock, flags);
+	zone_lock_irqsave(zone, flags);
 
 	while (count &gt; 0) {
 		struct list_head *list;
@@ -1527,7 +1528,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,
 		} while (count &gt; 0 &amp;&amp; !list_empty(list));
 	}
 
-	spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+	zone_unlock_irqrestore(zone, flags);
 }
 
 /* Split a multi-block free page into its individual pageblocks. */
@@ -1571,12 +1572,12 @@ static void free_one_page(struct zone *zone, struct page *page,
 	unsigned long flags;
 
 	if (unlikely(fpi_flags &amp; FPI_TRYLOCK)) {
-		if (!spin_trylock_irqsave(&amp;zone-&gt;lock, flags)) {
+		if (!zone_trylock_irqsave(zone, flags)) {
 			add_page_to_zone_llist(zone, page, order);
 			return;
 		}
 	} else {
-		spin_lock_irqsave(&amp;zone-&gt;lock, flags);
+		zone_lock_irqsave(zone, flags);
 	}
 
 	/* The lock succeeded. Process deferred pages. */
@@ -1594,7 +1595,7 @@ static void free_one_page(struct zone *zone, struct page *page,
 		}
 	}
 	split_large_buddy(zone, page, pfn, order, fpi_flags);
-	spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+	zone_unlock_irqrestore(zone, flags);
 
 	__count_vm_events(PGFREE, 1 &lt;&lt; order);
 }
@@ -2547,10 +2548,10 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,
 	int i;
 
 	if (unlikely(alloc_flags &amp; ALLOC_TRYLOCK)) {
-		if (!spin_trylock_irqsave(&amp;zone-&gt;lock, flags))
+		if (!zone_trylock_irqsave(zone, flags))
 			return 0;
 	} else {
-		spin_lock_irqsave(&amp;zone-&gt;lock, flags);
+		zone_lock_irqsave(zone, flags);
 	}
 	for (i = 0; i &lt; count; ++i) {
 		struct page *page = __rmqueue(zone, order, migratetype,
@@ -2570,7 +2571,7 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,
 		 */
 		list_add_tail(&amp;page-&gt;pcp_list, list);
 	}
-	spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+	zone_unlock_irqrestore(zone, flags);
 
 	return i;
 }
@@ -3235,10 +3236,10 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,
 	do {
 		page = NULL;
 		if (unlikely(alloc_flags &amp; ALLOC_TRYLOCK)) {
-			if (!spin_trylock_irqsave(&amp;zone-&gt;lock, flags))
+			if (!zone_trylock_irqsave(zone, flags))
 				return NULL;
 		} else {
-			spin_lock_irqsave(&amp;zone-&gt;lock, flags);
+			zone_lock_irqsave(zone, flags);
 		}
 		if (alloc_flags &amp; ALLOC_HIGHATOMIC)
 			page = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);
@@ -3257,11 +3258,11 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,
 				page = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);
 
 			if (!page) {
-				spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+				zone_unlock_irqrestore(zone, flags);
 				return NULL;
 			}
 		}
-		spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+		zone_unlock_irqrestore(zone, flags);
 	} while (check_new_pages(page, order));
 
 	__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 &lt;&lt; order);
@@ -3448,7 +3449,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,
 	if (zone-&gt;nr_reserved_highatomic &gt;= max_managed)
 		return;
 
-	spin_lock_irqsave(&amp;zone-&gt;lock, flags);
+	zone_lock_irqsave(zone, flags);
 
 	/* Recheck the nr_reserved_highatomic limit under the lock */
 	if (zone-&gt;nr_reserved_highatomic &gt;= max_managed)
@@ -3470,7 +3471,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,
 	}
 
 out_unlock:
-	spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+	zone_unlock_irqrestore(zone, flags);
 }
 
 /*
@@ -3503,7 +3504,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,
 					pageblock_nr_pages)
 			continue;
 
-		spin_lock_irqsave(&amp;zone-&gt;lock, flags);
+		zone_lock_irqsave(zone, flags);
 		for (order = 0; order &lt; NR_PAGE_ORDERS; order++) {
 			struct free_area *area = &amp;(zone-&gt;free_area[order]);
 			unsigned long size;
@@ -3551,11 +3552,11 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,
 			 */
 			WARN_ON_ONCE(ret == -1);
 			if (ret &gt; 0) {
-				spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+				zone_unlock_irqrestore(zone, flags);
 				return ret;
 			}
 		}
-		spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+		zone_unlock_irqrestore(zone, flags);
 	}
 
 	return false;
@@ -6435,7 +6436,7 @@ static void __setup_per_zone_wmarks(void)
 	for_each_zone(zone) {
 		u64 tmp;
 
-		spin_lock_irqsave(&amp;zone-&gt;lock, flags);
+		zone_lock_irqsave(zone, flags);
 		tmp = (u64)pages_min * zone_managed_pages(zone);
 		tmp = div64_ul(tmp, lowmem_pages);
 		if (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {
@@ -6476,7 +6477,7 @@ static void __setup_per_zone_wmarks(void)
 		zone-&gt;_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;
 		trace_mm_setup_per_zone_wmarks(zone);
 
-		spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+		zone_unlock_irqrestore(zone, flags);
 	}
 
 	/* update totalreserve_pages */
@@ -7246,7 +7247,7 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,
 	zonelist = node_zonelist(nid, gfp_mask);
 	for_each_zone_zonelist_nodemask(zone, z, zonelist,
 					gfp_zone(gfp_mask), nodemask) {
-		spin_lock_irqsave(&amp;zone-&gt;lock, flags);
+		zone_lock_irqsave(zone, flags);
 
 		pfn = ALIGN(zone-&gt;zone_start_pfn, nr_pages);
 		while (zone_spans_last_pfn(zone, pfn, nr_pages)) {
@@ -7260,18 +7261,18 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,
 				 * allocation spinning on this lock, it may
 				 * win the race and cause allocation to fail.
 				 */
-				spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+				zone_unlock_irqrestore(zone, flags);
 				ret = alloc_contig_frozen_range_noprof(pfn,
 							pfn + nr_pages,
 							ACR_FLAGS_NONE,
 							gfp_mask);
 				if (!ret)
 					return pfn_to_page(pfn);
-				spin_lock_irqsave(&amp;zone-&gt;lock, flags);
+				zone_lock_irqsave(zone, flags);
 			}
 			pfn += nr_pages;
 		}
-		spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+		zone_unlock_irqrestore(zone, flags);
 	}
 	/*
 	 * If we failed, retry the search, but treat regions with HugeTLB pages
@@ -7425,7 +7426,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,
 
 	offline_mem_sections(pfn, end_pfn);
 	zone = page_zone(pfn_to_page(pfn));
-	spin_lock_irqsave(&amp;zone-&gt;lock, flags);
+	zone_lock_irqsave(zone, flags);
 	while (pfn &lt; end_pfn) {
 		page = pfn_to_page(pfn);
 		/*
@@ -7455,7 +7456,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,
 		del_page_from_free_list(page, zone, order, MIGRATE_ISOLATE);
 		pfn += (1 &lt;&lt; order);
 	}
-	spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+	zone_unlock_irqrestore(zone, flags);
 
 	return end_pfn - start_pfn - already_offline;
 }
@@ -7531,7 +7532,7 @@ bool take_page_off_buddy(struct page *page)
 	unsigned int order;
 	bool ret = false;
 
-	spin_lock_irqsave(&amp;zone-&gt;lock, flags);
+	zone_lock_irqsave(zone, flags);
 	for (order = 0; order &lt; NR_PAGE_ORDERS; order++) {
 		struct page *page_head = page - (pfn &amp; ((1 &lt;&lt; order) - 1));
 		int page_order = buddy_order(page_head);
@@ -7552,7 +7553,7 @@ bool take_page_off_buddy(struct page *page)
 		if (page_count(page_head) &gt; 0)
 			break;
 	}
-	spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+	zone_unlock_irqrestore(zone, flags);
 	return ret;
 }
 
@@ -7565,7 +7566,7 @@ bool put_page_back_buddy(struct page *page)
 	unsigned long flags;
 	bool ret = false;
 
-	spin_lock_irqsave(&amp;zone-&gt;lock, flags);
+	zone_lock_irqsave(zone, flags);
 	if (put_page_testzero(page)) {
 		unsigned long pfn = page_to_pfn(page);
 		int migratetype = get_pfnblock_migratetype(page, pfn);
@@ -7576,7 +7577,7 @@ bool put_page_back_buddy(struct page *page)
 			ret = true;
 		}
 	}
-	spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+	zone_unlock_irqrestore(zone, flags);
 
 	return ret;
 }
@@ -7625,7 +7626,7 @@ static void __accept_page(struct zone *zone, unsigned long *flags,
 	account_freepages(zone, -MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);
 	__mod_zone_page_state(zone, NR_UNACCEPTED, -MAX_ORDER_NR_PAGES);
 	__ClearPageUnaccepted(page);
-	spin_unlock_irqrestore(&amp;zone-&gt;lock, *flags);
+	zone_unlock_irqrestore(zone, *flags);
 
 	accept_memory(page_to_phys(page), PAGE_SIZE &lt;&lt; MAX_PAGE_ORDER);
 
@@ -7637,9 +7638,9 @@ void accept_page(struct page *page)
 	struct zone *zone = page_zone(page);
 	unsigned long flags;
 
-	spin_lock_irqsave(&amp;zone-&gt;lock, flags);
+	zone_lock_irqsave(zone, flags);
 	if (!PageUnaccepted(page)) {
-		spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+		zone_unlock_irqrestore(zone, flags);
 		return;
 	}
 
@@ -7652,11 +7653,11 @@ static bool try_to_accept_memory_one(struct zone *zone)
 	unsigned long flags;
 	struct page *page;
 
-	spin_lock_irqsave(&amp;zone-&gt;lock, flags);
+	zone_lock_irqsave(zone, flags);
 	page = list_first_entry_or_null(&amp;zone-&gt;unaccepted_pages,
 					struct page, lru);
 	if (!page) {
-		spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+		zone_unlock_irqrestore(zone, flags);
 		return false;
 	}
 
@@ -7713,12 +7714,12 @@ static bool __free_unaccepted(struct page *page)
 	if (!lazy_accept)
 		return false;
 
-	spin_lock_irqsave(&amp;zone-&gt;lock, flags);
+	zone_lock_irqsave(zone, flags);
 	list_add_tail(&amp;page-&gt;lru, &amp;zone-&gt;unaccepted_pages);
 	account_freepages(zone, MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);
 	__mod_zone_page_state(zone, NR_UNACCEPTED, MAX_ORDER_NR_PAGES);
 	__SetPageUnaccepted(page);
-	spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+	zone_unlock_irqrestore(zone, flags);
 
 	return true;
 }
diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index c48ff5c00244..56a272f38b66 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -10,6 +10,7 @@
 #include &lt;linux/hugetlb.h&gt;
 #include &lt;linux/page_owner.h&gt;
 #include &lt;linux/migrate.h&gt;
+#include &lt;linux/zone_lock.h&gt;
 #include &quot;internal.h&quot;
 
 #define CREATE_TRACE_POINTS
@@ -173,7 +174,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,
 	if (PageUnaccepted(page))
 		accept_page(page);
 
-	spin_lock_irqsave(&amp;zone-&gt;lock, flags);
+	zone_lock_irqsave(zone, flags);
 
 	/*
 	 * We assume the caller intended to SET migrate type to isolate.
@@ -181,7 +182,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,
 	 * set it before us.
 	 */
 	if (is_migrate_isolate_page(page)) {
-		spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+		zone_unlock_irqrestore(zone, flags);
 		return -EBUSY;
 	}
 
@@ -200,15 +201,15 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,
 			mode);
 	if (!unmovable) {
 		if (!pageblock_isolate_and_move_free_pages(zone, page)) {
-			spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+			zone_unlock_irqrestore(zone, flags);
 			return -EBUSY;
 		}
 		zone-&gt;nr_isolate_pageblock++;
-		spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+		zone_unlock_irqrestore(zone, flags);
 		return 0;
 	}
 
-	spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+	zone_unlock_irqrestore(zone, flags);
 	if (mode == PB_ISOLATE_MODE_MEM_OFFLINE) {
 		/*
 		 * printk() with zone-&gt;lock held will likely trigger a
@@ -229,7 +230,7 @@ static void unset_migratetype_isolate(struct page *page)
 	struct page *buddy;
 
 	zone = page_zone(page);
-	spin_lock_irqsave(&amp;zone-&gt;lock, flags);
+	zone_lock_irqsave(zone, flags);
 	if (!is_migrate_isolate_page(page))
 		goto out;
 
@@ -280,7 +281,7 @@ static void unset_migratetype_isolate(struct page *page)
 	}
 	zone-&gt;nr_isolate_pageblock--;
 out:
-	spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+	zone_unlock_irqrestore(zone, flags);
 }
 
 static inline struct page *
@@ -641,9 +642,9 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,
 
 	/* Check all pages are free or marked as ISOLATED */
 	zone = page_zone(page);
-	spin_lock_irqsave(&amp;zone-&gt;lock, flags);
+	zone_lock_irqsave(zone, flags);
 	pfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn, mode);
-	spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+	zone_unlock_irqrestore(zone, flags);
 
 	ret = pfn &lt; end_pfn ? -EBUSY : 0;
 
diff --git a/mm/page_reporting.c b/mm/page_reporting.c
index 8a03effda749..ac2ac8fd0487 100644
--- a/mm/page_reporting.c
+++ b/mm/page_reporting.c
@@ -7,6 +7,7 @@
 #include &lt;linux/module.h&gt;
 #include &lt;linux/delay.h&gt;
 #include &lt;linux/scatterlist.h&gt;
+#include &lt;linux/zone_lock.h&gt;
 
 #include &quot;page_reporting.h&quot;
 #include &quot;internal.h&quot;
@@ -161,7 +162,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,
 	if (list_empty(list))
 		return err;
 
-	spin_lock_irq(&amp;zone-&gt;lock);
+	zone_lock_irq(zone);
 
 	/*
 	 * Limit how many calls we will be making to the page reporting
@@ -219,7 +220,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,
 			list_rotate_to_front(&amp;page-&gt;lru, list);
 
 		/* release lock before waiting on report processing */
-		spin_unlock_irq(&amp;zone-&gt;lock);
+		zone_unlock_irq(zone);
 
 		/* begin processing pages in local list */
 		err = prdev-&gt;report(prdev, sgl, PAGE_REPORTING_CAPACITY);
@@ -231,7 +232,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,
 		budget--;
 
 		/* reacquire zone lock and resume processing */
-		spin_lock_irq(&amp;zone-&gt;lock);
+		zone_lock_irq(zone);
 
 		/* flush reported pages from the sg list */
 		page_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);
@@ -251,7 +252,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,
 	if (!list_entry_is_head(next, list, lru) &amp;&amp; !list_is_first(&amp;next-&gt;lru, list))
 		list_rotate_to_front(&amp;next-&gt;lru, list);
 
-	spin_unlock_irq(&amp;zone-&gt;lock);
+	zone_unlock_irq(zone);
 
 	return err;
 }
@@ -296,9 +297,9 @@ page_reporting_process_zone(struct page_reporting_dev_info *prdev,
 		err = prdev-&gt;report(prdev, sgl, leftover);
 
 		/* flush any remaining pages out from the last report */
-		spin_lock_irq(&amp;zone-&gt;lock);
+		zone_lock_irq(zone);
 		page_reporting_drain(prdev, sgl, leftover, !err);
-		spin_unlock_irq(&amp;zone-&gt;lock);
+		zone_unlock_irq(zone);
 	}
 
 	return err;
diff --git a/mm/show_mem.c b/mm/show_mem.c
index 24078ac3e6bc..245beca127af 100644
--- a/mm/show_mem.c
+++ b/mm/show_mem.c
@@ -14,6 +14,7 @@
 #include &lt;linux/mmzone.h&gt;
 #include &lt;linux/swap.h&gt;
 #include &lt;linux/vmstat.h&gt;
+#include &lt;linux/zone_lock.h&gt;
 
 #include &quot;internal.h&quot;
 #include &quot;swap.h&quot;
@@ -363,7 +364,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z
 		show_node(zone);
 		printk(KERN_CONT &quot;%s: &quot;, zone-&gt;name);
 
-		spin_lock_irqsave(&amp;zone-&gt;lock, flags);
+		zone_lock_irqsave(zone, flags);
 		for (order = 0; order &lt; NR_PAGE_ORDERS; order++) {
 			struct free_area *area = &amp;zone-&gt;free_area[order];
 			int type;
@@ -377,7 +378,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z
 					types[order] |= 1 &lt;&lt; type;
 			}
 		}
-		spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+		zone_unlock_irqrestore(zone, flags);
 		for (order = 0; order &lt; NR_PAGE_ORDERS; order++) {
 			printk(KERN_CONT &quot;%lu*%lukB &quot;,
 			       nr[order], K(1UL) &lt;&lt; order);
diff --git a/mm/vmscan.c b/mm/vmscan.c
index 973ffb9813ea..9fe5c41e0e0a 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -58,6 +58,7 @@
 #include &lt;linux/random.h&gt;
 #include &lt;linux/mmu_notifier.h&gt;
 #include &lt;linux/parser.h&gt;
+#include &lt;linux/zone_lock.h&gt;
 
 #include &lt;asm/tlbflush.h&gt;
 #include &lt;asm/div64.h&gt;
@@ -7129,9 +7130,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)
 
 			/* Increments are under the zone lock */
 			zone = pgdat-&gt;node_zones + i;
-			spin_lock_irqsave(&amp;zone-&gt;lock, flags);
+			zone_lock_irqsave(zone, flags);
 			zone-&gt;watermark_boost -= min(zone-&gt;watermark_boost, zone_boosts[i]);
-			spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+			zone_unlock_irqrestore(zone, flags);
 		}
 
 		/*
diff --git a/mm/vmstat.c b/mm/vmstat.c
index 99270713e0c1..06b27255a626 100644
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@ -28,6 +28,7 @@
 #include &lt;linux/mm_inline.h&gt;
 #include &lt;linux/page_owner.h&gt;
 #include &lt;linux/sched/isolation.h&gt;
+#include &lt;linux/zone_lock.h&gt;
 
 #include &quot;internal.h&quot;
 
@@ -1535,10 +1536,10 @@ static void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,
 			continue;
 
 		if (!nolock)
-			spin_lock_irqsave(&amp;zone-&gt;lock, flags);
+			zone_lock_irqsave(zone, flags);
 		print(m, pgdat, zone);
 		if (!nolock)
-			spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
+			zone_unlock_irqrestore(zone, flags);
 	}
 }
 #endif
@@ -1603,9 +1604,9 @@ static void pagetypeinfo_showfree_print(struct seq_file *m,
 				}
 			}
 			seq_printf(m, &quot;%s%6lu &quot;, overflow ? &quot;&gt;&quot; : &quot;&quot;, freecount);
-			spin_unlock_irq(&amp;zone-&gt;lock);
+			zone_unlock_irq(zone);
 			cond_resched();
-			spin_lock_irq(&amp;zone-&gt;lock);
+			zone_lock_irq(zone);
 		}
 		seq_putc(m, &#x27;\n&#x27;);
 	}
-- 
2.47.3</pre>
</details>
<div class="review-comment-signals">Signals: no functional change, additional modifications</div>
</div>
<div class="thread-children">
<div class="thread-node depth-1" id="2026-02-20">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Cheatham, Benjamin</span>
<a class="date-chip" href="../2026-02-20_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-20">2026-02-20</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer suggested reordering the patch series to improve flow, recommending introducing zone lock wrappers and tracepoints together before mechanically converting users and then compaction.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">I think you can improve the flow of this series if reorder as follows:
	1. Introduce zone lock wrappers
	4. Add zone lock tracepoints
	2. Mechanically convert zone lock users to the wrappers
	3. Convert compaction to use the wrappers...

and possibly squash 1 &amp; 4 (though that might be too big of a patch). It&#x27;s better to introduce the
wrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in
patch 1 by the time they get to patch 4.

Thanks,
Ben</pre>
</details>
<div class="reply-to-label">&#8627; replying to Dmitry Ilvokhin</div>
<div class="review-comment-signals">Signals: requested changes</div>
</div>
<div class="thread-children">
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Shakeel Butt</span>
<a class="date-chip" href="../2026-02-20_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-20">2026-02-20</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Shakeel Butt expressed disagreement with the suggestion to separate zone lock wrapper patches, stating it&#x27;s &#x27;just different taste&#x27; and suggesting squashing them together instead.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">I don&#x27;t think this suggestion will make anything better. This just seems like a
different taste. If I make a suggestion, I would request to squash (1) and (2)
i.e. patch containing wrappers and their use together but that is just my taste
and would be a nit. The series ordering is good as is.</pre>
</details>
<div class="reply-to-label">&#8627; replying to Cheatham, Benjamin</div>
<div class="review-comment-signals">Signals: disagreement, personal preference</div>
</div>
<div class="thread-children">
<div class="thread-node depth-3" id="2026-02-24">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Dmitry Ilvokhin (author)</span>
<a class="date-chip" href="../2026-02-24_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-24">2026-02-24</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed Shakeel Butt&#x27;s concern about using macros for zone lock operations, explaining that it is necessary to modify the flags variable passed by the caller and maintain consistency with existing locking patterns.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">The reason for using macros in those two cases is that they need to
modify the flags variable passed by the caller, just like
spin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same
convention here.

If we used normal inline functions instead, we would need to pass a
pointer to flags, which would change the call sites and diverge from the
existing *_irqsave() locking pattern.

There is also a difference between zone_lock_irqsave() and
zone_trylock_irqsave() implementations: the former is implemented as a
do { } while (0) macro since it does not return a value, while the
latter uses a GCC extension in order to return the trylock result. This
matches spin_lock_* convention as well.</pre>
</details>
<div class="reply-to-label">&#8627; replying to Shakeel Butt</div>
<div class="review-comment-signals">Signals: explained reasoning, maintained consistency</div>
</div>
</div>
</div>
</div>
<div class="thread-node depth-2" id="2026-02-23">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Dmitry Ilvokhin (author)</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-23">2026-02-23</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Author Dmitry Ilvokhin is addressing a suggestion from reviewer Benjamin Cheatham to reorder the patches in his series. The author explains that he intentionally structured the series to separate refactoring from instrumentation changes, and reordering as suggested would mix these two concerns. The author does not agree with the suggestion and provides reasoning for their original design choice.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Hi Ben,

Thanks for the suggestion.

I structured the series intentionally to keep all behavior-preserving
refactoring separate from the actual instrumentation change.

In particular, I had to split the conversion into two patches to
separate the purely mechanical changes from the compaction
restructuring. With the current order, tracepoints addition remains a
single, atomic functional change on top of a fully converted tree. This
keeps the instrumentation isolated from the refactoring and with an
intention to make bisection and review of the behavioral change easier.

Reordering as suggested would mix instrumentation with intermediate
refactoring states, which I&#x27;d prefer to avoid.

I hope this reasoning makes sense, but I&#x27;m happy to discuss if there are
strong objections.</pre>
</details>
<div class="reply-to-label">&#8627; replying to Cheatham, Benjamin</div>
<div class="review-comment-signals">Signals: author does not agree with feedback, provides explanation</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Dmitry Ilvokhin (author)</span>
<a class="date-chip" href="../2026-02-24_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-24">2026-02-24</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Author acknowledged that the zone lock wrappers are not valuable and agreed to remove them.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Yes, I agree, there is no much value in this wrappers, will remove them,
thanks!</pre>
</details>
<div class="reply-to-label">&#8627; replying to Cheatham, Benjamin</div>
<div class="review-comment-signals">Signals: acknowledged a fix is needed, agreed to restructure</div>
</div>
</div>
</div>
</div>
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Cheatham, Benjamin</span>
<a class="date-chip" href="../2026-02-20_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-20">2026-02-20</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer suggested removing zone lock wrappers and making direct calls in this function, citing parity with compact helpers as a reason.

Reviewer noted that the function should not return a value and suggested replacing it with an if-else statement for clarity.

Reviewer suggested moving zone lock wrapper changes that don&#x27;t use compact_* to a later patch, arguing they are not directly relevant to the current patch&#x27;s additions and would fit better in the final patch.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Nit: You could remove the helpers above and just do the calls directly in this function, though
it would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay
since they have the __acquires() annotations.

---

You don&#x27;t need the return statement here (and you shouldn&#x27;t be returning a value at all).

It may be cleaner to just do an if-else statement here instead.

---

I would move this (and other wrapper changes below that don&#x27;t use compact_*) to the last patch. I understand you
didn&#x27;t change it due to location but I would argue it isn&#x27;t really relevant to what&#x27;s being added in this patch
and fits better in the last.</pre>
</details>
<div class="reply-to-label">&#8627; replying to Dmitry Ilvokhin</div>
<div class="review-comment-signals">Signals: requested changes, requested change, suggested reordering</div>
</div>
</div>
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Cheatham, Benjamin</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-23">2026-02-23</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer noted that he was unsure if the patch&#x27;s priority should be higher than his suggested reordering of the patch series, but ultimately accepted it.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">No that&#x27;s fine, I figured as much. I just wasn&#x27;t sure that was more important
to you than what (I thought) was a better reading order for the series.

Thanks,
Ben</pre>
</details>
<div class="reply-to-label">&#8627; replying to Dmitry Ilvokhin</div>
<div class="review-comment-signals">Signals: uncertainty, acceptance</div>
</div>
</div>
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Shakeel Butt</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-23">2026-02-23</a>
<span class="review-tag-badge">Acked-by</span>
<span class="badge" style="color:#155724;background:#d4edda">Positive</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Gave Acked-by</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Fri, Feb 20, 2026 at 01:09:59PM -0600, Cheatham, Benjamin wrote:
&gt; On 2/11/2026 9:22 AM, Dmitry Ilvokhin wrote:
&gt; &gt; Zone lock contention can significantly impact allocation and
&gt; &gt; reclaim latency, as it is a central synchronization point in
&gt; &gt; the page allocator and reclaim paths. Improved visibility into
&gt; &gt; its behavior is therefore important for diagnosing performance
&gt; &gt; issues in memory-intensive workloads.
&gt; &gt; 
&gt; &gt; On some production workloads at Meta, we have observed noticeable
&gt; &gt; zone lock contention. Deeper analysis of lock holders and waiters
&gt; &gt; is currently difficult with existing instrumentation.
&gt; &gt; 
&gt; &gt; While generic lock contention_begin/contention_end tracepoints
&gt; &gt; cover the slow path, they do not provide sufficient visibility
&gt; &gt; into lock hold times. In particular, the lack of a release-side
&gt; &gt; event makes it difficult to identify long lock holders and
&gt; &gt; correlate them with waiters. As a result, distinguishing between
&gt; &gt; short bursts of contention and pathological long hold times
&gt; &gt; requires additional instrumentation.
&gt; &gt; 
&gt; &gt; This patch series adds dedicated tracepoint instrumentation to
&gt; &gt; zone lock, following the existing mmap_lock tracing model.
&gt; &gt; 
&gt; &gt; The goal is to enable detailed holder/waiter analysis and lock
&gt; &gt; hold time measurements without affecting the fast path when
&gt; &gt; tracing is disabled.
&gt; &gt; 
&gt; &gt; The series is structured as follows:
&gt; &gt; 
&gt; &gt;   1. Introduce zone lock wrappers.
&gt; &gt;   2. Mechanically convert zone lock users to the wrappers.
&gt; &gt;   3. Convert compaction to use the wrappers (requires minor
&gt; &gt;      restructuring of compact_lock_irqsave()).
&gt; &gt;   4. Add zone lock tracepoints.
&gt; 
&gt; I think you can improve the flow of this series if reorder as follows:
&gt; 	1. Introduce zone lock wrappers
&gt; 	4. Add zone lock tracepoints
&gt; 	2. Mechanically convert zone lock users to the wrappers
&gt; 	3. Convert compaction to use the wrappers...
&gt; 
&gt; and possibly squash 1 &amp; 4 (though that might be too big of a patch). It&#x27;s better to introduce the
&gt; wrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in
&gt; patch 1 by the time they get to patch 4.

I don&#x27;t think this suggestion will make anything better. This just seems like a
different taste. If I make a suggestion, I would request to squash (1) and (2)
i.e. patch containing wrappers and their use together but that is just my taste
and would be a nit. The series ordering is good as is.



---

On Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:
&gt; Add thin wrappers around zone lock acquire/release operations. This
&gt; prepares the code for future tracepoint instrumentation without
&gt; modifying individual call sites.
&gt; 
&gt; Centralizing zone lock operations behind wrappers allows future
&gt; instrumentation or debugging hooks to be added without touching
&gt; all users.
&gt; 
&gt; No functional change intended. The wrappers are introduced in
&gt; preparation for subsequent patches and are not yet used.
&gt; 
&gt; Signed-off-by: Dmitry Ilvokhin &lt;d@ilvokhin.com&gt;
&gt; ---
&gt;  MAINTAINERS               |  1 +
&gt;  include/linux/zone_lock.h | 38 ++++++++++++++++++++++++++++++++++++++
&gt;  2 files changed, 39 insertions(+)
&gt;  create mode 100644 include/linux/zone_lock.h
&gt; 
&gt; diff --git a/MAINTAINERS b/MAINTAINERS
&gt; index b4088f7290be..680c9ae02d7e 100644
&gt; --- a/MAINTAINERS
&gt; +++ b/MAINTAINERS
&gt; @@ -16498,6 +16498,7 @@ F:	include/linux/pgtable.h
&gt;  F:	include/linux/ptdump.h
&gt;  F:	include/linux/vmpressure.h
&gt;  F:	include/linux/vmstat.h
&gt; +F:	include/linux/zone_lock.h
&gt;  F:	kernel/fork.c
&gt;  F:	mm/Kconfig
&gt;  F:	mm/debug.c
&gt; diff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h
&gt; new file mode 100644
&gt; index 000000000000..c531e26280e6
&gt; --- /dev/null
&gt; +++ b/include/linux/zone_lock.h
&gt; @@ -0,0 +1,38 @@
&gt; +/* SPDX-License-Identifier: GPL-2.0 */
&gt; +#ifndef _LINUX_ZONE_LOCK_H
&gt; +#define _LINUX_ZONE_LOCK_H
&gt; +
&gt; +#include &lt;linux/mmzone.h&gt;
&gt; +#include &lt;linux/spinlock.h&gt;
&gt; +
&gt; +static inline void zone_lock_init(struct zone *zone)
&gt; +{
&gt; +	spin_lock_init(&amp;zone-&gt;lock);
&gt; +}
&gt; +
&gt; +#define zone_lock_irqsave(zone, flags)				\
&gt; +do {								\
&gt; +	spin_lock_irqsave(&amp;(zone)-&gt;lock, flags);		\
&gt; +} while (0)
&gt; +
&gt; +#define zone_trylock_irqsave(zone, flags)			\
&gt; +({								\
&gt; +	spin_trylock_irqsave(&amp;(zone)-&gt;lock, flags);		\
&gt; +})

Any reason you used macros for above two and inlined functions for remaining?

&gt; +
&gt; +static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)
&gt; +{
&gt; +	spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
&gt; +}
&gt; +
&gt; +static inline void zone_lock_irq(struct zone *zone)
&gt; +{
&gt; +	spin_lock_irq(&amp;zone-&gt;lock);
&gt; +}
&gt; +
&gt; +static inline void zone_unlock_irq(struct zone *zone)
&gt; +{
&gt; +	spin_unlock_irq(&amp;zone-&gt;lock);
&gt; +}
&gt; +
&gt; +#endif /* _LINUX_ZONE_LOCK_H */
&gt; -- 
&gt; 2.47.3
&gt; 


---

On Wed, Feb 11, 2026 at 03:22:14PM +0000, Dmitry Ilvokhin wrote:
&gt; Replace direct zone lock acquire/release operations with the
&gt; newly introduced wrappers.
&gt; 
&gt; The changes are purely mechanical substitutions. No functional change
&gt; intended. Locking semantics and ordering remain unchanged.
&gt; 
&gt; The compaction path is left unchanged for now and will be
&gt; handled separately in the following patch due to additional
&gt; non-trivial modifications.
&gt; 
&gt; Signed-off-by: Dmitry Ilvokhin &lt;d@ilvokhin.com&gt;

Acked-by: Shakeel Butt &lt;shakeel.butt@linux.dev&gt;


---

On Tue, Feb 24, 2026 at 03:18:04PM +0000, Dmitry Ilvokhin wrote:
&gt; On Mon, Feb 23, 2026 at 02:36:01PM -0800, Shakeel Butt wrote:
&gt; &gt; On Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:
&gt; &gt; &gt; Add thin wrappers around zone lock acquire/release operations. This
&gt; &gt; &gt; prepares the code for future tracepoint instrumentation without
&gt; &gt; &gt; modifying individual call sites.
&gt; &gt; &gt; 
&gt; &gt; &gt; Centralizing zone lock operations behind wrappers allows future
&gt; &gt; &gt; instrumentation or debugging hooks to be added without touching
&gt; &gt; &gt; all users.
&gt; &gt; &gt; 
&gt; &gt; &gt; No functional change intended. The wrappers are introduced in
&gt; &gt; &gt; preparation for subsequent patches and are not yet used.
&gt; &gt; &gt; 
&gt; &gt; &gt; Signed-off-by: Dmitry Ilvokhin &lt;d@ilvokhin.com&gt;
&gt; &gt; &gt; ---
&gt; &gt; &gt;  MAINTAINERS               |  1 +
&gt; &gt; &gt;  include/linux/zone_lock.h | 38 ++++++++++++++++++++++++++++++++++++++
&gt; &gt; &gt;  2 files changed, 39 insertions(+)
&gt; &gt; &gt;  create mode 100644 include/linux/zone_lock.h
&gt; &gt; &gt; 
&gt; &gt; &gt; diff --git a/MAINTAINERS b/MAINTAINERS
&gt; &gt; &gt; index b4088f7290be..680c9ae02d7e 100644
&gt; &gt; &gt; --- a/MAINTAINERS
&gt; &gt; &gt; +++ b/MAINTAINERS
&gt; &gt; &gt; @@ -16498,6 +16498,7 @@ F:	include/linux/pgtable.h
&gt; &gt; &gt;  F:	include/linux/ptdump.h
&gt; &gt; &gt;  F:	include/linux/vmpressure.h
&gt; &gt; &gt;  F:	include/linux/vmstat.h
&gt; &gt; &gt; +F:	include/linux/zone_lock.h
&gt; &gt; &gt;  F:	kernel/fork.c
&gt; &gt; &gt;  F:	mm/Kconfig
&gt; &gt; &gt;  F:	mm/debug.c
&gt; &gt; &gt; diff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h
&gt; &gt; &gt; new file mode 100644
&gt; &gt; &gt; index 000000000000..c531e26280e6
&gt; &gt; &gt; --- /dev/null
&gt; &gt; &gt; +++ b/include/linux/zone_lock.h
&gt; &gt; &gt; @@ -0,0 +1,38 @@
&gt; &gt; &gt; +/* SPDX-License-Identifier: GPL-2.0 */
&gt; &gt; &gt; +#ifndef _LINUX_ZONE_LOCK_H
&gt; &gt; &gt; +#define _LINUX_ZONE_LOCK_H
&gt; &gt; &gt; +
&gt; &gt; &gt; +#include &lt;linux/mmzone.h&gt;
&gt; &gt; &gt; +#include &lt;linux/spinlock.h&gt;
&gt; &gt; &gt; +
&gt; &gt; &gt; +static inline void zone_lock_init(struct zone *zone)
&gt; &gt; &gt; +{
&gt; &gt; &gt; +	spin_lock_init(&amp;zone-&gt;lock);
&gt; &gt; &gt; +}
&gt; &gt; &gt; +
&gt; &gt; &gt; +#define zone_lock_irqsave(zone, flags)				\
&gt; &gt; &gt; +do {								\
&gt; &gt; &gt; +	spin_lock_irqsave(&amp;(zone)-&gt;lock, flags);		\
&gt; &gt; &gt; +} while (0)
&gt; &gt; &gt; +
&gt; &gt; &gt; +#define zone_trylock_irqsave(zone, flags)			\
&gt; &gt; &gt; +({								\
&gt; &gt; &gt; +	spin_trylock_irqsave(&amp;(zone)-&gt;lock, flags);		\
&gt; &gt; &gt; +})
&gt; &gt; 
&gt; &gt; Any reason you used macros for above two and inlined functions for remaining?
&gt; &gt;
&gt; 
&gt; The reason for using macros in those two cases is that they need to
&gt; modify the flags variable passed by the caller, just like
&gt; spin_lock_irqsave() and spin_trylock_irqsave() do. I followed the same
&gt; convention here.
&gt; 
&gt; If we used normal inline functions instead, we would need to pass a
&gt; pointer to flags, which would change the call sites and diverge from the
&gt; existing *_irqsave() locking pattern.
&gt; 
&gt; There is also a difference between zone_lock_irqsave() and
&gt; zone_trylock_irqsave() implementations: the former is implemented as a
&gt; do { } while (0) macro since it does not return a value, while the
&gt; latter uses a GCC extension in order to return the trylock result. This
&gt; matches spin_lock_* convention as well.
&gt; 

Cool, thanks for the explanation.


---

On Wed, Feb 11, 2026 at 03:22:13PM +0000, Dmitry Ilvokhin wrote:
&gt; Add thin wrappers around zone lock acquire/release operations. This
&gt; prepares the code for future tracepoint instrumentation without
&gt; modifying individual call sites.
&gt; 
&gt; Centralizing zone lock operations behind wrappers allows future
&gt; instrumentation or debugging hooks to be added without touching
&gt; all users.
&gt; 
&gt; No functional change intended. The wrappers are introduced in
&gt; preparation for subsequent patches and are not yet used.
&gt; 
&gt; Signed-off-by: Dmitry Ilvokhin &lt;d@ilvokhin.com&gt;

Acked-by: Shakeel Butt &lt;shakeel.butt@linux.dev&gt;
</pre>
</details>
<div class="reply-to-label">&#8627; replying to Dmitry Ilvokhin</div>
</div>
</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Dmitry Ilvokhin (author)</span>
<a class="date-chip" href="../2026-02-20_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-11">2026-02-11</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author is addressing a concern about the need to abstract compact_lock_irqsave() away from raw spinlock_t, and they are open to feedback on their chosen approach of using a small tagged struct.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Zone lock contention can significantly impact allocation and
reclaim latency, as it is a central synchronization point in
the page allocator and reclaim paths. Improved visibility into
its behavior is therefore important for diagnosing performance
issues in memory-intensive workloads.

On some production workloads at Meta, we have observed noticeable
zone lock contention. Deeper analysis of lock holders and waiters
is currently difficult with existing instrumentation.

While generic lock contention_begin/contention_end tracepoints
cover the slow path, they do not provide sufficient visibility
into lock hold times. In particular, the lack of a release-side
event makes it difficult to identify long lock holders and
correlate them with waiters. As a result, distinguishing between
short bursts of contention and pathological long hold times
requires additional instrumentation.

This patch series adds dedicated tracepoint instrumentation to
zone lock, following the existing mmap_lock tracing model.

The goal is to enable detailed holder/waiter analysis and lock
hold time measurements without affecting the fast path when
tracing is disabled.

The series is structured as follows:

  1. Introduce zone lock wrappers.
  2. Mechanically convert zone lock users to the wrappers.
  3. Convert compaction to use the wrappers (requires minor
     restructuring of compact_lock_irqsave()).
  4. Add zone lock tracepoints.

The tracepoints are added via lightweight inline helpers in the
wrappers. When tracing is disabled, the fast path remains
unchanged.

The compaction changes required abstracting compact_lock_irqsave() away from
raw spinlock_t. I chose a small tagged struct to handle both zone and LRU
locks uniformly. If there is a preferred alternative (e.g. splitting helpers
or using a different abstraction), I would appreciate feedback.

Dmitry Ilvokhin (4):
  mm: introduce zone lock wrappers
  mm: convert zone lock users to wrappers
  mm: convert compaction to zone lock wrappers
  mm: add tracepoints for zone lock

 MAINTAINERS                      |   3 +
 include/linux/zone_lock.h        | 100 ++++++++++++++++++++++++++++
 include/trace/events/zone_lock.h |  64 ++++++++++++++++++
 mm/Makefile                      |   2 +-
 mm/compaction.c                  | 108 +++++++++++++++++++++++++------
 mm/memory_hotplug.c              |   9 +--
 mm/mm_init.c                     |   3 +-
 mm/page_alloc.c                  |  73 ++++++++++-----------
 mm/page_isolation.c              |  19 +++---
 mm/page_reporting.c              |  13 ++--
 mm/show_mem.c                    |   5 +-
 mm/vmscan.c                      |   5 +-
 mm/vmstat.c                      |   9 +--
 mm/zone_lock.c                   |  31 +++++++++
 14 files changed, 360 insertions(+), 84 deletions(-)
 create mode 100644 include/linux/zone_lock.h
 create mode 100644 include/trace/events/zone_lock.h
 create mode 100644 mm/zone_lock.c

-- 
2.47.3</pre>
</details>
<div class="review-comment-signals">Signals: open_to_feedback, explaining_approach</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Dmitry Ilvokhin (author)</span>
<a class="date-chip" href="../2026-02-20_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-11">2026-02-11</a>
<span class="badge" style="color:#155724;background:#d4edda">Positive</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed the reviewer&#x27;s concern that compact_lock_irqsave() can no longer operate directly on a spinlock_t when the lock belongs to a zone, by introducing struct compact_lock to abstract the underlying lock type and dispatching to the appropriate lock/unlock helper. The author confirmed that this change is intended to prepare for future tracepoint instrumentation without modifying individual call sites.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Compaction uses compact_lock_irqsave(), which currently operates
on a raw spinlock_t pointer so that it can be used for both
zone-&gt;lock and lru_lock. Since zone lock operations are now wrapped,
compact_lock_irqsave() can no longer operate directly on a spinlock_t
when the lock belongs to a zone.

Introduce struct compact_lock to abstract the underlying lock type. The
structure carries a lock type enum and a union holding either a zone
pointer or a raw spinlock_t pointer, and dispatches to the appropriate
lock/unlock helper.

No functional change intended.

Signed-off-by: Dmitry Ilvokhin &lt;d@ilvokhin.com&gt;
---
 mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------
 1 file changed, 89 insertions(+), 19 deletions(-)

diff --git a/mm/compaction.c b/mm/compaction.c
index 1e8f8eca318c..1b000d2b95b2 100644
--- a/mm/compaction.c
+++ b/mm/compaction.c
@@ -24,6 +24,7 @@
 #include &lt;linux/page_owner.h&gt;
 #include &lt;linux/psi.h&gt;
 #include &lt;linux/cpuset.h&gt;
+#include &lt;linux/zone_lock.h&gt;
 #include &quot;internal.h&quot;
 
 #ifdef CONFIG_COMPACTION
@@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)
 }
 #endif /* CONFIG_COMPACTION */
 
+enum compact_lock_type {
+	COMPACT_LOCK_ZONE,
+	COMPACT_LOCK_RAW_SPINLOCK,
+};
+
+struct compact_lock {
+	enum compact_lock_type type;
+	union {
+		struct zone *zone;
+		spinlock_t *lock; /* Reference to lru lock */
+	};
+};
+
+static bool compact_do_zone_trylock_irqsave(struct zone *zone,
+					    unsigned long *flags)
+{
+	return zone_trylock_irqsave(zone, *flags);
+}
+
+static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,
+					   unsigned long *flags)
+{
+	return spin_trylock_irqsave(lock, *flags);
+}
+
+static bool compact_do_trylock_irqsave(struct compact_lock lock,
+				       unsigned long *flags)
+{
+	if (lock.type == COMPACT_LOCK_ZONE)
+		return compact_do_zone_trylock_irqsave(lock.zone, flags);
+
+	return compact_do_raw_trylock_irqsave(lock.lock, flags);
+}
+
+static void compact_do_zone_lock_irqsave(struct zone *zone,
+					 unsigned long *flags)
+__acquires(zone-&gt;lock)
+{
+	zone_lock_irqsave(zone, *flags);
+}
+
+static void compact_do_raw_lock_irqsave(spinlock_t *lock,
+					unsigned long *flags)
+__acquires(lock)
+{
+	spin_lock_irqsave(lock, *flags);
+}
+
+static void compact_do_lock_irqsave(struct compact_lock lock,
+				    unsigned long *flags)
+{
+	if (lock.type == COMPACT_LOCK_ZONE) {
+		compact_do_zone_lock_irqsave(lock.zone, flags);
+		return;
+	}
+
+	return compact_do_raw_lock_irqsave(lock.lock, flags);
+}
+
 /*
  * Compaction requires the taking of some coarse locks that are potentially
  * very heavily contended. For async compaction, trylock and record if the
@@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)
  *
  * Always returns true which makes it easier to track lock state in callers.
  */
-static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,
-						struct compact_control *cc)
-	__acquires(lock)
+static bool compact_lock_irqsave(struct compact_lock lock,
+				 unsigned long *flags,
+				 struct compact_control *cc)
 {
 	/* Track if the lock is contended in async mode */
 	if (cc-&gt;mode == MIGRATE_ASYNC &amp;&amp; !cc-&gt;contended) {
-		if (spin_trylock_irqsave(lock, *flags))
+		if (compact_do_trylock_irqsave(lock, flags))
 			return true;
 
 		cc-&gt;contended = true;
 	}
 
-	spin_lock_irqsave(lock, *flags);
+	compact_do_lock_irqsave(lock, flags);
 	return true;
 }
 
@@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,
  * Returns true if compaction should abort due to fatal signal pending.
  * Returns false when compaction can continue.
  */
-static bool compact_unlock_should_abort(spinlock_t *lock,
-		unsigned long flags, bool *locked, struct compact_control *cc)
+static bool compact_unlock_should_abort(struct zone *zone,
+					unsigned long flags,
+					bool *locked,
+					struct compact_control *cc)
 {
 	if (*locked) {
-		spin_unlock_irqrestore(lock, flags);
+		zone_unlock_irqrestore(zone, flags);
 		*locked = false;
 	}
 
@@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,
 		 * contention, to give chance to IRQs. Abort if fatal signal
 		 * pending.
 		 */
-		if (!(blockpfn % COMPACT_CLUSTER_MAX)
-		    &amp;&amp; compact_unlock_should_abort(&amp;cc-&gt;zone-&gt;lock, flags,
-								&amp;locked, cc))
+		if (!(blockpfn % COMPACT_CLUSTER_MAX) &amp;&amp;
+		    compact_unlock_should_abort(cc-&gt;zone, flags, &amp;locked, cc))
 			break;
 
 		nr_scanned++;
@@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,
 
 		/* If we already hold the lock, we can skip some rechecking. */
 		if (!locked) {
-			locked = compact_lock_irqsave(&amp;cc-&gt;zone-&gt;lock,
-								&amp;flags, cc);
+			struct compact_lock zol = {
+				.type = COMPACT_LOCK_ZONE,
+				.zone = cc-&gt;zone,
+			};
+
+			locked = compact_lock_irqsave(zol, &amp;flags, cc);
 
 			/* Recheck this is a buddy page under lock */
 			if (!PageBuddy(page))
@@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,
 	}
 
 	if (locked)
-		spin_unlock_irqrestore(&amp;cc-&gt;zone-&gt;lock, flags);
+		zone_unlock_irqrestore(cc-&gt;zone, flags);
 
 	/*
 	 * Be careful to not go outside of the pageblock.
@@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,
 
 		/* If we already hold the lock, we can skip some rechecking */
 		if (lruvec != locked) {
+			struct compact_lock zol = {
+				.type = COMPACT_LOCK_RAW_SPINLOCK,
+				.lock = &amp;lruvec-&gt;lru_lock,
+			};
+
 			if (locked)
 				unlock_page_lruvec_irqrestore(locked, flags);
 
-			compact_lock_irqsave(&amp;lruvec-&gt;lru_lock, &amp;flags, cc);
+			compact_lock_irqsave(zol, &amp;flags, cc);
 			locked = lruvec;
 
 			lruvec_memcg_debug(lruvec, folio);
@@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)
 		if (!area-&gt;nr_free)
 			continue;
 
-		spin_lock_irqsave(&amp;cc-&gt;zone-&gt;lock, flags);
+		zone_lock_irqsave(cc-&gt;zone, flags);
 		freelist = &amp;area-&gt;free_list[MIGRATE_MOVABLE];
 		list_for_each_entry_reverse(freepage, freelist, buddy_list) {
 			unsigned long pfn;
@@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)
 			}
 		}
 
-		spin_unlock_irqrestore(&amp;cc-&gt;zone-&gt;lock, flags);
+		zone_unlock_irqrestore(cc-&gt;zone, flags);
 
 		/* Skip fast search if enough freepages isolated */
 		if (cc-&gt;nr_freepages &gt;= cc-&gt;nr_migratepages)
@@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)
 		if (!area-&gt;nr_free)
 			continue;
 
-		spin_lock_irqsave(&amp;cc-&gt;zone-&gt;lock, flags);
+		zone_lock_irqsave(cc-&gt;zone, flags);
 		freelist = &amp;area-&gt;free_list[MIGRATE_MOVABLE];
 		list_for_each_entry(freepage, freelist, buddy_list) {
 			unsigned long free_pfn;
@@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)
 				break;
 			}
 		}
-		spin_unlock_irqrestore(&amp;cc-&gt;zone-&gt;lock, flags);
+		zone_unlock_irqrestore(cc-&gt;zone, flags);
 	}
 
 	cc-&gt;total_migrate_scanned += nr_scanned;
-- 
2.47.3</pre>
</details>
<div class="review-comment-signals">Signals: acknowledged a fix, confirmed the approach</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Dmitry Ilvokhin (author)</span>
<a class="date-chip" href="../2026-02-20_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-11">2026-02-11</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed a concern about the performance impact of adding tracepoint instrumentation to zone lock acquire/release operations, explaining that the implementation follows the mmap_lock tracepoint pattern and that the fast path is unaffected when tracing is disabled.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Add tracepoint instrumentation to zone lock acquire/release operations
via the previously introduced wrappers.

The implementation follows the mmap_lock tracepoint pattern: a
lightweight inline helper checks whether the tracepoint is enabled and
calls into an out-of-line helper when tracing is active. When
CONFIG_TRACING is disabled, helpers compile to empty inline stubs.

The fast path is unaffected when tracing is disabled.

Signed-off-by: Dmitry Ilvokhin &lt;d@ilvokhin.com&gt;
---
 MAINTAINERS                      |  2 +
 include/linux/zone_lock.h        | 64 +++++++++++++++++++++++++++++++-
 include/trace/events/zone_lock.h | 64 ++++++++++++++++++++++++++++++++
 mm/Makefile                      |  2 +-
 mm/zone_lock.c                   | 31 ++++++++++++++++
 5 files changed, 161 insertions(+), 2 deletions(-)
 create mode 100644 include/trace/events/zone_lock.h
 create mode 100644 mm/zone_lock.c

diff --git a/MAINTAINERS b/MAINTAINERS
index 680c9ae02d7e..711ffa15f4c3 100644
--- a/MAINTAINERS
+++ b/MAINTAINERS
@@ -16499,6 +16499,7 @@ F:	include/linux/ptdump.h
 F:	include/linux/vmpressure.h
 F:	include/linux/vmstat.h
 F:	include/linux/zone_lock.h
+F:	include/trace/events/zone_lock.h
 F:	kernel/fork.c
 F:	mm/Kconfig
 F:	mm/debug.c
@@ -16518,6 +16519,7 @@ F:	mm/sparse.c
 F:	mm/util.c
 F:	mm/vmpressure.c
 F:	mm/vmstat.c
+F:	mm/zone_lock.c
 N:	include/linux/page[-_]*
 
 MEMORY MANAGEMENT - EXECMEM
diff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h
index c531e26280e6..cea41dd56324 100644
--- a/include/linux/zone_lock.h
+++ b/include/linux/zone_lock.h
@@ -4,6 +4,53 @@
 
 #include &lt;linux/mmzone.h&gt;
 #include &lt;linux/spinlock.h&gt;
+#include &lt;linux/tracepoint-defs.h&gt;
+
+DECLARE_TRACEPOINT(zone_lock_start_locking);
+DECLARE_TRACEPOINT(zone_lock_acquire_returned);
+DECLARE_TRACEPOINT(zone_lock_released);
+
+#ifdef CONFIG_TRACING
+
+void __zone_lock_do_trace_start_locking(struct zone *zone);
+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success);
+void __zone_lock_do_trace_released(struct zone *zone);
+
+static inline void __zone_lock_trace_start_locking(struct zone *zone)
+{
+	if (tracepoint_enabled(zone_lock_start_locking))
+		__zone_lock_do_trace_start_locking(zone);
+}
+
+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,
+						      bool success)
+{
+	if (tracepoint_enabled(zone_lock_acquire_returned))
+		__zone_lock_do_trace_acquire_returned(zone, success);
+}
+
+static inline void __zone_lock_trace_released(struct zone *zone)
+{
+	if (tracepoint_enabled(zone_lock_released))
+		__zone_lock_do_trace_released(zone);
+}
+
+#else /* !CONFIG_TRACING */
+
+static inline void __zone_lock_trace_start_locking(struct zone *zone)
+{
+}
+
+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,
+						      bool success)
+{
+}
+
+static inline void __zone_lock_trace_released(struct zone *zone)
+{
+}
+
+#endif /* CONFIG_TRACING */
 
 static inline void zone_lock_init(struct zone *zone)
 {
@@ -12,26 +59,41 @@ static inline void zone_lock_init(struct zone *zone)
 
 #define zone_lock_irqsave(zone, flags)				\
 do {								\
+	bool success = true;					\
+								\
+	__zone_lock_trace_start_locking(zone);			\
 	spin_lock_irqsave(&amp;(zone)-&gt;lock, flags);		\
+	__zone_lock_trace_acquire_returned(zone, success);	\
 } while (0)
 
 #define zone_trylock_irqsave(zone, flags)			\
 ({								\
-	spin_trylock_irqsave(&amp;(zone)-&gt;lock, flags);		\
+	bool success;						\
+								\
+	__zone_lock_trace_start_locking(zone);			\
+	success = spin_trylock_irqsave(&amp;(zone)-&gt;lock, flags);	\
+	__zone_lock_trace_acquire_returned(zone, success);	\
+	success;						\
 })
 
 static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)
 {
+	__zone_lock_trace_released(zone);
 	spin_unlock_irqrestore(&amp;zone-&gt;lock, flags);
 }
 
 static inline void zone_lock_irq(struct zone *zone)
 {
+	bool success = true;
+
+	__zone_lock_trace_start_locking(zone);
 	spin_lock_irq(&amp;zone-&gt;lock);
+	__zone_lock_trace_acquire_returned(zone, success);
 }
 
 static inline void zone_unlock_irq(struct zone *zone)
 {
+	__zone_lock_trace_released(zone);
 	spin_unlock_irq(&amp;zone-&gt;lock);
 }
 
diff --git a/include/trace/events/zone_lock.h b/include/trace/events/zone_lock.h
new file mode 100644
index 000000000000..3df82a8c0160
--- /dev/null
+++ b/include/trace/events/zone_lock.h
@@ -0,0 +1,64 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM zone_lock
+
+#if !defined(_TRACE_ZONE_LOCK_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_ZONE_LOCK_H
+
+#include &lt;linux/tracepoint.h&gt;
+#include &lt;linux/types.h&gt;
+
+struct zone;
+
+DECLARE_EVENT_CLASS(zone_lock,
+
+	TP_PROTO(struct zone *zone),
+
+	TP_ARGS(zone),
+
+	TP_STRUCT__entry(
+		__field(struct zone *, zone)
+	),
+
+	TP_fast_assign(
+		__entry-&gt;zone = zone;
+	),
+
+	TP_printk(&quot;zone=%p&quot;, __entry-&gt;zone)
+);
+
+#define DEFINE_ZONE_LOCK_EVENT(name)			\
+	DEFINE_EVENT(zone_lock, name,			\
+		TP_PROTO(struct zone *zone),		\
+		TP_ARGS(zone))
+
+DEFINE_ZONE_LOCK_EVENT(zone_lock_start_locking);
+DEFINE_ZONE_LOCK_EVENT(zone_lock_released);
+
+TRACE_EVENT(zone_lock_acquire_returned,
+
+	TP_PROTO(struct zone *zone, bool success),
+
+	TP_ARGS(zone, success),
+
+	TP_STRUCT__entry(
+		__field(struct zone *, zone)
+		__field(bool, success)
+	),
+
+	TP_fast_assign(
+		__entry-&gt;zone = zone;
+		__entry-&gt;success = success;
+	),
+
+	TP_printk(
+		&quot;zone=%p success=%s&quot;,
+		__entry-&gt;zone,
+		__entry-&gt;success ? &quot;true&quot; : &quot;false&quot;
+	)
+);
+
+#endif /* _TRACE_ZONE_LOCK_H */
+
+/* This part must be outside protection */
+#include &lt;trace/define_trace.h&gt;
diff --git a/mm/Makefile b/mm/Makefile
index 0d85b10dbdde..fd891710c696 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -55,7 +55,7 @@ obj-y			:= filemap.o mempool.o oom_kill.o fadvise.o \
 			   mm_init.o percpu.o slab_common.o \
 			   compaction.o show_mem.o \
 			   interval_tree.o list_lru.o workingset.o \
-			   debug.o gup.o mmap_lock.o vma_init.o $(mmu-y)
+			   debug.o gup.o mmap_lock.o zone_lock.o vma_init.o $(mmu-y)
 
 # Give &#x27;page_alloc&#x27; its own module-parameter namespace
 page-alloc-y := page_alloc.o
diff --git a/mm/zone_lock.c b/mm/zone_lock.c
new file mode 100644
index 000000000000..f647fd2aca48
--- /dev/null
+++ b/mm/zone_lock.c
@@ -0,0 +1,31 @@
+// SPDX-License-Identifier: GPL-2.0
+#define CREATE_TRACE_POINTS
+#include &lt;trace/events/zone_lock.h&gt;
+
+#include &lt;linux/zone_lock.h&gt;
+
+EXPORT_TRACEPOINT_SYMBOL(zone_lock_start_locking);
+EXPORT_TRACEPOINT_SYMBOL(zone_lock_acquire_returned);
+EXPORT_TRACEPOINT_SYMBOL(zone_lock_released);
+
+#ifdef CONFIG_TRACING
+
+void __zone_lock_do_trace_start_locking(struct zone *zone)
+{
+	trace_zone_lock_start_locking(zone);
+}
+EXPORT_SYMBOL(__zone_lock_do_trace_start_locking);
+
+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success)
+{
+	trace_zone_lock_acquire_returned(zone, success);
+}
+EXPORT_SYMBOL(__zone_lock_do_trace_acquire_returned);
+
+void __zone_lock_do_trace_released(struct zone *zone)
+{
+	trace_zone_lock_released(zone);
+}
+EXPORT_SYMBOL(__zone_lock_do_trace_released);
+
+#endif /* CONFIG_TRACING */
-- 
2.47.3</pre>
</details>
<div class="review-comment-signals">Signals: clarification, explanation</div>
</div>
</div>
</div>

    <footer>LKML Daily Activity Tracker</footer>
    <script>
    // When arriving via a date anchor (e.g. #2026-02-15 from a daily report),
    // scroll the anchor into view after a brief delay so layout is complete.
    (function () {
        var hash = window.location.hash;
        if (!hash) return;
        var target = document.getElementById(hash.slice(1));
        if (!target) return;
        setTimeout(function () {
            target.scrollIntoView({behavior: 'smooth', block: 'start'});
        }, 80);
    })();
    </script>
</body>
</html>