{
  "thread_id": "cover.1770821420.git.d@ilvokhin.com",
  "subject": "[PATCH 0/4] mm: zone lock tracepoint instrumentation",
  "url": "https://lore.kernel.org/all/cover.1770821420.git.d@ilvokhin.com/",
  "dates": {
    "2026-02-20": {
      "report_file": "2026-02-20_ollama_llama3.1-8b.html",
      "developer": "Dmitry Ilvokhin",
      "reviews": [
        {
          "author": "Cheatham, Benjamin",
          "summary": "Reviewer suggested reordering the series to improve flow, recommending that zone lock wrappers and tracepoints be introduced together in a single patch before converting users to the new wrappers.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "I think you can improve the flow of this series if reorder as follows:\n\t1. Introduce zone lock wrappers\n\t4. Add zone lock tracepoints\n\t2. Mechanically convert zone lock users to the wrappers\n\t3. Convert compaction to use the wrappers...\n\nand possibly squash 1 & 4 (though that might be too big of a patch). It's better to introduce the\nwrappers and their tracepoints together before the reviewer (i.e. me) forgets what was added in\npatch 1 by the time they get to patch 4.\n\nThanks,\nBen",
          "reply_to": "Dmitry Ilvokhin",
          "message_date": "2026-02-20"
        },
        {
          "author": "Cheatham, Benjamin",
          "summary": "Reviewer suggested removing zone lock wrappers and making direct calls to spinlock functions, citing parity with compact helpers as a reason\n\nReviewer pointed out that the function should not return a value and suggested replacing it with an if-else statement.\n\nReviewer Cheatham suggested moving wrapper changes not using compact_* to the last patch, arguing they are not essential to the current patch and fit better in the final one.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "requested change",
            "minor suggestion"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Nit: You could remove the helpers above and just do the calls directly in this function, though\nit would remove the parity with the compact helpers. compact_do_lock_irqsave() helpers can stay\nsince they have the __acquires() annotations.\n\n---\n\nYou don't need the return statement here (and you shouldn't be returning a value at all).\n\nIt may be cleaner to just do an if-else statement here instead.\n\n---\n\nI would move this (and other wrapper changes below that don't use compact_*) to the last patch. I understand you\ndidn't change it due to location but I would argue it isn't really relevant to what's being added in this patch\nand fits better in the last.",
          "reply_to": "Dmitry Ilvokhin",
          "message_date": "2026-02-20"
        },
        {
          "author": "Shakeel Butt",
          "summary": "Reviewer Shakeel Butt expressed disagreement with the suggestion to squash patches (1) and (2), stating that it's just a matter of personal taste, but suggested an alternative ordering where the wrappers and their use are combined in a single patch.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "disagreement",
            "personal opinion"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "I don't think this suggestion will make anything better. This just seems like a\ndifferent taste. If I make a suggestion, I would request to squash (1) and (2)\ni.e. patch containing wrappers and their use together but that is just my taste\nand would be a nit. The series ordering is good as is.",
          "reply_to": "Cheatham, Benjamin",
          "message_date": "2026-02-20"
        }
      ],
      "analysis_source": "llm-per-reviewer",
      "patch_summary": "This patch introduces thin wrappers around zone lock acquire and release operations, allowing for future tracepoint instrumentation without modifying individual call sites. The wrappers are added to prepare the code for subsequent patches that will add tracing functionality. This change is a preparatory step and does not introduce any functional changes at this time."
    },
    "2026-02-11": {
      "report_file": "2026-02-20_ollama_llama3.1-8b.html",
      "developer": "Dmitry Ilvokhin",
      "reviews": [
        {
          "author": "Dmitry Ilvokhin (author)",
          "summary": "The author is addressing a concern about replacing direct zone lock acquire/release operations with the newly introduced wrappers. The author confirmed that the changes are purely mechanical substitutions and no functional change is intended, but noted that the compaction path will be handled separately in a following patch due to additional non-trivial modifications.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "acknowledged",
            "no functional change"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Replace direct zone lock acquire/release operations with the\nnewly introduced wrappers.\n\nThe changes are purely mechanical substitutions. No functional change\nintended. Locking semantics and ordering remain unchanged.\n\nThe compaction path is left unchanged for now and will be\nhandled separately in the following patch due to additional\nnon-trivial modifications.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/memory_hotplug.c |  9 +++---\n mm/mm_init.c        |  3 +-\n mm/page_alloc.c     | 73 +++++++++++++++++++++++----------------------\n mm/page_isolation.c | 19 ++++++------\n mm/page_reporting.c | 13 ++++----\n mm/show_mem.c       |  5 ++--\n mm/vmscan.c         |  5 ++--\n mm/vmstat.c         |  9 +++---\n 8 files changed, 72 insertions(+), 64 deletions(-)\n\ndiff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c\nindex bc805029da51..cfc0103fa50e 100644\n--- a/mm/memory_hotplug.c\n+++ b/mm/memory_hotplug.c\n@@ -36,6 +36,7 @@\n #include <linux/rmap.h>\n #include <linux/module.h>\n #include <linux/node.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n \n@@ -1190,9 +1191,9 @@ int online_pages(unsigned long pfn, unsigned long nr_pages,\n \t * Fixup the number of isolated pageblocks before marking the sections\n \t * onlining, such that undo_isolate_page_range() works correctly.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock += nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t/*\n \t * If this zone is not populated, then it is not in zonelist.\n@@ -2041,9 +2042,9 @@ int offline_pages(unsigned long start_pfn, unsigned long nr_pages,\n \t * effectively stale; nobody should be touching them. Fixup the number\n \t * of isolated pageblocks, memory onlining will properly revert this.\n \t */\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tzone->nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tlru_cache_enable();\n \tzone_pcp_enable(zone);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..426e5a0256f9 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -32,6 +32,7 @@\n #include <linux/vmstat.h>\n #include <linux/kexec_handover.h>\n #include <linux/hugetlb.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n #include \"slab.h\"\n #include \"shuffle.h\"\n@@ -1425,7 +1426,7 @@ static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx,\n \tzone_set_nid(zone, nid);\n \tzone->name = zone_names[idx];\n \tzone->zone_pgdat = NODE_DATA(nid);\n-\tspin_lock_init(&zone->lock);\n+\tzone_lock_init(zone);\n \tzone_seqlock_init(zone);\n \tzone_pcp_init(zone);\n }\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..2c9fe30da7a1 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -54,6 +54,7 @@\n #include <linux/delayacct.h>\n #include <linux/cacheinfo.h>\n #include <linux/pgalloc_tag.h>\n+#include <linux/zone_lock.h>\n #include <asm/div64.h>\n #include \"internal.h\"\n #include \"shuffle.h\"\n@@ -1494,7 +1495,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t/* Ensure requested pindex is drained first. */\n \tpindex = pindex - 1;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \twhile (count > 0) {\n \t\tstruct list_head *list;\n@@ -1527,7 +1528,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,\n \t\t} while (count > 0 && !list_empty(list));\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /* Split a multi-block free page into its individual pageblocks. */\n@@ -1571,12 +1572,12 @@ static void free_one_page(struct zone *zone, struct page *page,\n \tunsigned long flags;\n \n \tif (unlikely(fpi_flags & FPI_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags)) {\n+\t\tif (!zone_trylock_irqsave(zone, flags)) {\n \t\t\tadd_page_to_zone_llist(zone, page, order);\n \t\t\treturn;\n \t\t}\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \n \t/* The lock succeeded. Process deferred pages. */\n@@ -1594,7 +1595,7 @@ static void free_one_page(struct zone *zone, struct page *page,\n \t\t}\n \t}\n \tsplit_large_buddy(zone, page, pfn, order, fpi_flags);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \t__count_vm_events(PGFREE, 1 << order);\n }\n@@ -2547,10 +2548,10 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \tint i;\n \n \tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\treturn 0;\n \t} else {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t}\n \tfor (i = 0; i < count; ++i) {\n \t\tstruct page *page = __rmqueue(zone, order, migratetype,\n@@ -2570,7 +2571,7 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,\n \t\t */\n \t\tlist_add_tail(&page->pcp_list, list);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn i;\n }\n@@ -3235,10 +3236,10 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \tdo {\n \t\tpage = NULL;\n \t\tif (unlikely(alloc_flags & ALLOC_TRYLOCK)) {\n-\t\t\tif (!spin_trylock_irqsave(&zone->lock, flags))\n+\t\t\tif (!zone_trylock_irqsave(zone, flags))\n \t\t\t\treturn NULL;\n \t\t} else {\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t}\n \t\tif (alloc_flags & ALLOC_HIGHATOMIC)\n \t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n@@ -3257,11 +3258,11 @@ struct page *rmqueue_buddy(struct zone *preferred_zone, struct zone *zone,\n \t\t\t\tpage = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);\n \n \t\t\tif (!page) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn NULL;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t} while (check_new_pages(page, order));\n \n \t__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);\n@@ -3448,7 +3449,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \tif (zone->nr_reserved_highatomic >= max_managed)\n \t\treturn;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/* Recheck the nr_reserved_highatomic limit under the lock */\n \tif (zone->nr_reserved_highatomic >= max_managed)\n@@ -3470,7 +3471,7 @@ static void reserve_highatomic_pageblock(struct page *page, int order,\n \t}\n \n out_unlock:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n /*\n@@ -3503,7 +3504,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t\t\tpageblock_nr_pages)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &(zone->free_area[order]);\n \t\t\tunsigned long size;\n@@ -3551,11 +3552,11 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,\n \t\t\t */\n \t\t\tWARN_ON_ONCE(ret == -1);\n \t\t\tif (ret > 0) {\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\treturn ret;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \treturn false;\n@@ -6435,7 +6436,7 @@ static void __setup_per_zone_wmarks(void)\n \tfor_each_zone(zone) {\n \t\tu64 tmp;\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\ttmp = (u64)pages_min * zone_managed_pages(zone);\n \t\ttmp = div64_ul(tmp, lowmem_pages);\n \t\tif (is_highmem(zone) || zone_idx(zone) == ZONE_MOVABLE) {\n@@ -6476,7 +6477,7 @@ static void __setup_per_zone_wmarks(void)\n \t\tzone->_watermark[WMARK_PROMO] = high_wmark_pages(zone) + tmp;\n \t\ttrace_mm_setup_per_zone_wmarks(zone);\n \n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \n \t/* update totalreserve_pages */\n@@ -7246,7 +7247,7 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \tzonelist = node_zonelist(nid, gfp_mask);\n \tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n \t\t\t\t\tgfp_zone(gfp_mask), nodemask) {\n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \n \t\tpfn = ALIGN(zone->zone_start_pfn, nr_pages);\n \t\twhile (zone_spans_last_pfn(zone, pfn, nr_pages)) {\n@@ -7260,18 +7261,18 @@ struct page *alloc_contig_frozen_pages_noprof(unsigned long nr_pages,\n \t\t\t\t * allocation spinning on this lock, it may\n \t\t\t\t * win the race and cause allocation to fail.\n \t\t\t\t */\n-\t\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\t\tret = alloc_contig_frozen_range_noprof(pfn,\n \t\t\t\t\t\t\tpfn + nr_pages,\n \t\t\t\t\t\t\tACR_FLAGS_NONE,\n \t\t\t\t\t\t\tgfp_mask);\n \t\t\t\tif (!ret)\n \t\t\t\t\treturn pfn_to_page(pfn);\n-\t\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\t}\n \t\t\tpfn += nr_pages;\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n \t/*\n \t * If we failed, retry the search, but treat regions with HugeTLB pages\n@@ -7425,7 +7426,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \n \toffline_mem_sections(pfn, end_pfn);\n \tzone = page_zone(pfn_to_page(pfn));\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \twhile (pfn < end_pfn) {\n \t\tpage = pfn_to_page(pfn);\n \t\t/*\n@@ -7455,7 +7456,7 @@ unsigned long __offline_isolated_pages(unsigned long start_pfn,\n \t\tdel_page_from_free_list(page, zone, order, MIGRATE_ISOLATE);\n \t\tpfn += (1 << order);\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn end_pfn - start_pfn - already_offline;\n }\n@@ -7531,7 +7532,7 @@ bool take_page_off_buddy(struct page *page)\n \tunsigned int order;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\tstruct page *page_head = page - (pfn & ((1 << order) - 1));\n \t\tint page_order = buddy_order(page_head);\n@@ -7552,7 +7553,7 @@ bool take_page_off_buddy(struct page *page)\n \t\tif (page_count(page_head) > 0)\n \t\t\tbreak;\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \treturn ret;\n }\n \n@@ -7565,7 +7566,7 @@ bool put_page_back_buddy(struct page *page)\n \tunsigned long flags;\n \tbool ret = false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (put_page_testzero(page)) {\n \t\tunsigned long pfn = page_to_pfn(page);\n \t\tint migratetype = get_pfnblock_migratetype(page, pfn);\n@@ -7576,7 +7577,7 @@ bool put_page_back_buddy(struct page *page)\n \t\t\tret = true;\n \t\t}\n \t}\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn ret;\n }\n@@ -7625,7 +7626,7 @@ static void __accept_page(struct zone *zone, unsigned long *flags,\n \taccount_freepages(zone, -MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, -MAX_ORDER_NR_PAGES);\n \t__ClearPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, *flags);\n+\tzone_unlock_irqrestore(zone, *flags);\n \n \taccept_memory(page_to_phys(page), PAGE_SIZE << MAX_PAGE_ORDER);\n \n@@ -7637,9 +7638,9 @@ void accept_page(struct page *page)\n \tstruct zone *zone = page_zone(page);\n \tunsigned long flags;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!PageUnaccepted(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn;\n \t}\n \n@@ -7652,11 +7653,11 @@ static bool try_to_accept_memory_one(struct zone *zone)\n \tunsigned long flags;\n \tstruct page *page;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpage = list_first_entry_or_null(&zone->unaccepted_pages,\n \t\t\t\t\tstruct page, lru);\n \tif (!page) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn false;\n \t}\n \n@@ -7713,12 +7714,12 @@ static bool __free_unaccepted(struct page *page)\n \tif (!lazy_accept)\n \t\treturn false;\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tlist_add_tail(&page->lru, &zone->unaccepted_pages);\n \taccount_freepages(zone, MAX_ORDER_NR_PAGES, MIGRATE_MOVABLE);\n \t__mod_zone_page_state(zone, NR_UNACCEPTED, MAX_ORDER_NR_PAGES);\n \t__SetPageUnaccepted(page);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \treturn true;\n }\ndiff --git a/mm/page_isolation.c b/mm/page_isolation.c\nindex c48ff5c00244..56a272f38b66 100644\n--- a/mm/page_isolation.c\n+++ b/mm/page_isolation.c\n@@ -10,6 +10,7 @@\n #include <linux/hugetlb.h>\n #include <linux/page_owner.h>\n #include <linux/migrate.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #define CREATE_TRACE_POINTS\n@@ -173,7 +174,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \tif (PageUnaccepted(page))\n \t\taccept_page(page);\n \n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \n \t/*\n \t * We assume the caller intended to SET migrate type to isolate.\n@@ -181,7 +182,7 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t * set it before us.\n \t */\n \tif (is_migrate_isolate_page(page)) {\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn -EBUSY;\n \t}\n \n@@ -200,15 +201,15 @@ static int set_migratetype_isolate(struct page *page, enum pb_isolate_mode mode,\n \t\t\tmode);\n \tif (!unmovable) {\n \t\tif (!pageblock_isolate_and_move_free_pages(zone, page)) {\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t\treturn -EBUSY;\n \t\t}\n \t\tzone->nr_isolate_pageblock++;\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\treturn 0;\n \t}\n \n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \tif (mode == PB_ISOLATE_MODE_MEM_OFFLINE) {\n \t\t/*\n \t\t * printk() with zone->lock held will likely trigger a\n@@ -229,7 +230,7 @@ static void unset_migratetype_isolate(struct page *page)\n \tstruct page *buddy;\n \n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tif (!is_migrate_isolate_page(page))\n \t\tgoto out;\n \n@@ -280,7 +281,7 @@ static void unset_migratetype_isolate(struct page *page)\n \t}\n \tzone->nr_isolate_pageblock--;\n out:\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n }\n \n static inline struct page *\n@@ -641,9 +642,9 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,\n \n \t/* Check all pages are free or marked as ISOLATED */\n \tzone = page_zone(page);\n-\tspin_lock_irqsave(&zone->lock, flags);\n+\tzone_lock_irqsave(zone, flags);\n \tpfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn, mode);\n-\tspin_unlock_irqrestore(&zone->lock, flags);\n+\tzone_unlock_irqrestore(zone, flags);\n \n \tret = pfn < end_pfn ? -EBUSY : 0;\n \ndiff --git a/mm/page_reporting.c b/mm/page_reporting.c\nindex 8a03effda749..ac2ac8fd0487 100644\n--- a/mm/page_reporting.c\n+++ b/mm/page_reporting.c\n@@ -7,6 +7,7 @@\n #include <linux/module.h>\n #include <linux/delay.h>\n #include <linux/scatterlist.h>\n+#include <linux/zone_lock.h>\n \n #include \"page_reporting.h\"\n #include \"internal.h\"\n@@ -161,7 +162,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (list_empty(list))\n \t\treturn err;\n \n-\tspin_lock_irq(&zone->lock);\n+\tzone_lock_irq(zone);\n \n \t/*\n \t * Limit how many calls we will be making to the page reporting\n@@ -219,7 +220,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\t\tlist_rotate_to_front(&page->lru, list);\n \n \t\t/* release lock before waiting on report processing */\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \n \t\t/* begin processing pages in local list */\n \t\terr = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);\n@@ -231,7 +232,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \t\tbudget--;\n \n \t\t/* reacquire zone lock and resume processing */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \n \t\t/* flush reported pages from the sg list */\n \t\tpage_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);\n@@ -251,7 +252,7 @@ page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,\n \tif (!list_entry_is_head(next, list, lru) && !list_is_first(&next->lru, list))\n \t\tlist_rotate_to_front(&next->lru, list);\n \n-\tspin_unlock_irq(&zone->lock);\n+\tzone_unlock_irq(zone);\n \n \treturn err;\n }\n@@ -296,9 +297,9 @@ page_reporting_process_zone(struct page_reporting_dev_info *prdev,\n \t\terr = prdev->report(prdev, sgl, leftover);\n \n \t\t/* flush any remaining pages out from the last report */\n-\t\tspin_lock_irq(&zone->lock);\n+\t\tzone_lock_irq(zone);\n \t\tpage_reporting_drain(prdev, sgl, leftover, !err);\n-\t\tspin_unlock_irq(&zone->lock);\n+\t\tzone_unlock_irq(zone);\n \t}\n \n \treturn err;\ndiff --git a/mm/show_mem.c b/mm/show_mem.c\nindex 24078ac3e6bc..245beca127af 100644\n--- a/mm/show_mem.c\n+++ b/mm/show_mem.c\n@@ -14,6 +14,7 @@\n #include <linux/mmzone.h>\n #include <linux/swap.h>\n #include <linux/vmstat.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n #include \"swap.h\"\n@@ -363,7 +364,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\tshow_node(zone);\n \t\tprintk(KERN_CONT \"%s: \", zone->name);\n \n-\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\tzone_lock_irqsave(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tstruct free_area *area = &zone->free_area[order];\n \t\t\tint type;\n@@ -377,7 +378,7 @@ static void show_free_areas(unsigned int filter, nodemask_t *nodemask, int max_z\n \t\t\t\t\ttypes[order] |= 1 << type;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {\n \t\t\tprintk(KERN_CONT \"%lu*%lukB \",\n \t\t\t       nr[order], K(1UL) << order);\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 973ffb9813ea..9fe5c41e0e0a 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -58,6 +58,7 @@\n #include <linux/random.h>\n #include <linux/mmu_notifier.h>\n #include <linux/parser.h>\n+#include <linux/zone_lock.h>\n \n #include <asm/tlbflush.h>\n #include <asm/div64.h>\n@@ -7129,9 +7130,9 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int highest_zoneidx)\n \n \t\t\t/* Increments are under the zone lock */\n \t\t\tzone = pgdat->node_zones + i;\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\t\tzone->watermark_boost -= min(zone->watermark_boost, zone_boosts[i]);\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t\t}\n \n \t\t/*\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 99270713e0c1..06b27255a626 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -28,6 +28,7 @@\n #include <linux/mm_inline.h>\n #include <linux/page_owner.h>\n #include <linux/sched/isolation.h>\n+#include <linux/zone_lock.h>\n \n #include \"internal.h\"\n \n@@ -1535,10 +1536,10 @@ static void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,\n \t\t\tcontinue;\n \n \t\tif (!nolock)\n-\t\t\tspin_lock_irqsave(&zone->lock, flags);\n+\t\t\tzone_lock_irqsave(zone, flags);\n \t\tprint(m, pgdat, zone);\n \t\tif (!nolock)\n-\t\t\tspin_unlock_irqrestore(&zone->lock, flags);\n+\t\t\tzone_unlock_irqrestore(zone, flags);\n \t}\n }\n #endif\n@@ -1603,9 +1604,9 @@ static void pagetypeinfo_showfree_print(struct seq_file *m,\n \t\t\t\t}\n \t\t\t}\n \t\t\tseq_printf(m, \"%s%6lu \", overflow ? \">\" : \"\", freecount);\n-\t\t\tspin_unlock_irq(&zone->lock);\n+\t\t\tzone_unlock_irq(zone);\n \t\t\tcond_resched();\n-\t\t\tspin_lock_irq(&zone->lock);\n+\t\t\tzone_lock_irq(zone);\n \t\t}\n \t\tseq_putc(m, '\\n');\n \t}\n-- \n2.47.3",
          "reply_to": "",
          "message_date": "2026-02-11"
        },
        {
          "author": "Dmitry Ilvokhin (author)",
          "summary": "The author is addressing a concern about the need for additional instrumentation to measure lock hold times and distinguish between short bursts of contention and pathological long hold times. They explain that generic lock tracepoints do not provide sufficient visibility into lock hold times, particularly on the release side, making it difficult to identify long lock holders and correlate them with waiters. The author confirms that their patch series adds dedicated tracepoint instrumentation to zone lock, following the existing mmap_lock tracing model, and that the goal is to enable detailed holder/waiter analysis and lock hold time measurements without affecting the fast path when tracing is disabled.",
          "sentiment": "positive",
          "sentiment_signals": [
            "acknowledged need for additional instrumentation",
            "explained limitations of generic lock tracepoints"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Zone lock contention can significantly impact allocation and\nreclaim latency, as it is a central synchronization point in\nthe page allocator and reclaim paths. Improved visibility into\nits behavior is therefore important for diagnosing performance\nissues in memory-intensive workloads.\n\nOn some production workloads at Meta, we have observed noticeable\nzone lock contention. Deeper analysis of lock holders and waiters\nis currently difficult with existing instrumentation.\n\nWhile generic lock contention_begin/contention_end tracepoints\ncover the slow path, they do not provide sufficient visibility\ninto lock hold times. In particular, the lack of a release-side\nevent makes it difficult to identify long lock holders and\ncorrelate them with waiters. As a result, distinguishing between\nshort bursts of contention and pathological long hold times\nrequires additional instrumentation.\n\nThis patch series adds dedicated tracepoint instrumentation to\nzone lock, following the existing mmap_lock tracing model.\n\nThe goal is to enable detailed holder/waiter analysis and lock\nhold time measurements without affecting the fast path when\ntracing is disabled.\n\nThe series is structured as follows:\n\n  1. Introduce zone lock wrappers.\n  2. Mechanically convert zone lock users to the wrappers.\n  3. Convert compaction to use the wrappers (requires minor\n     restructuring of compact_lock_irqsave()).\n  4. Add zone lock tracepoints.\n\nThe tracepoints are added via lightweight inline helpers in the\nwrappers. When tracing is disabled, the fast path remains\nunchanged.\n\nThe compaction changes required abstracting compact_lock_irqsave() away from\nraw spinlock_t. I chose a small tagged struct to handle both zone and LRU\nlocks uniformly. If there is a preferred alternative (e.g. splitting helpers\nor using a different abstraction), I would appreciate feedback.\n\nDmitry Ilvokhin (4):\n  mm: introduce zone lock wrappers\n  mm: convert zone lock users to wrappers\n  mm: convert compaction to zone lock wrappers\n  mm: add tracepoints for zone lock\n\n MAINTAINERS                      |   3 +\n include/linux/zone_lock.h        | 100 ++++++++++++++++++++++++++++\n include/trace/events/zone_lock.h |  64 ++++++++++++++++++\n mm/Makefile                      |   2 +-\n mm/compaction.c                  | 108 +++++++++++++++++++++++++------\n mm/memory_hotplug.c              |   9 +--\n mm/mm_init.c                     |   3 +-\n mm/page_alloc.c                  |  73 ++++++++++-----------\n mm/page_isolation.c              |  19 +++---\n mm/page_reporting.c              |  13 ++--\n mm/show_mem.c                    |   5 +-\n mm/vmscan.c                      |   5 +-\n mm/vmstat.c                      |   9 +--\n mm/zone_lock.c                   |  31 +++++++++\n 14 files changed, 360 insertions(+), 84 deletions(-)\n create mode 100644 include/linux/zone_lock.h\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\n-- \n2.47.3",
          "reply_to": "",
          "message_date": "2026-02-11"
        },
        {
          "author": "Dmitry Ilvokhin (author)",
          "summary": "The author addressed a concern about the compaction code needing to handle both zone locks and raw spinlocks, introduced an enum and struct to abstract the underlying lock type, and confirmed that no functional change is intended.",
          "sentiment": "positive",
          "sentiment_signals": [
            "acknowledged fix needed",
            "confirmed no functional change"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Compaction uses compact_lock_irqsave(), which currently operates\non a raw spinlock_t pointer so that it can be used for both\nzone->lock and lru_lock. Since zone lock operations are now wrapped,\ncompact_lock_irqsave() can no longer operate directly on a spinlock_t\nwhen the lock belongs to a zone.\n\nIntroduce struct compact_lock to abstract the underlying lock type. The\nstructure carries a lock type enum and a union holding either a zone\npointer or a raw spinlock_t pointer, and dispatches to the appropriate\nlock/unlock helper.\n\nNo functional change intended.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n mm/compaction.c | 108 +++++++++++++++++++++++++++++++++++++++---------\n 1 file changed, 89 insertions(+), 19 deletions(-)\n\ndiff --git a/mm/compaction.c b/mm/compaction.c\nindex 1e8f8eca318c..1b000d2b95b2 100644\n--- a/mm/compaction.c\n+++ b/mm/compaction.c\n@@ -24,6 +24,7 @@\n #include <linux/page_owner.h>\n #include <linux/psi.h>\n #include <linux/cpuset.h>\n+#include <linux/zone_lock.h>\n #include \"internal.h\"\n \n #ifdef CONFIG_COMPACTION\n@@ -493,6 +494,65 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n }\n #endif /* CONFIG_COMPACTION */\n \n+enum compact_lock_type {\n+\tCOMPACT_LOCK_ZONE,\n+\tCOMPACT_LOCK_RAW_SPINLOCK,\n+};\n+\n+struct compact_lock {\n+\tenum compact_lock_type type;\n+\tunion {\n+\t\tstruct zone *zone;\n+\t\tspinlock_t *lock; /* Reference to lru lock */\n+\t};\n+};\n+\n+static bool compact_do_zone_trylock_irqsave(struct zone *zone,\n+\t\t\t\t\t    unsigned long *flags)\n+{\n+\treturn zone_trylock_irqsave(zone, *flags);\n+}\n+\n+static bool compact_do_raw_trylock_irqsave(spinlock_t *lock,\n+\t\t\t\t\t   unsigned long *flags)\n+{\n+\treturn spin_trylock_irqsave(lock, *flags);\n+}\n+\n+static bool compact_do_trylock_irqsave(struct compact_lock lock,\n+\t\t\t\t       unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE)\n+\t\treturn compact_do_zone_trylock_irqsave(lock.zone, flags);\n+\n+\treturn compact_do_raw_trylock_irqsave(lock.lock, flags);\n+}\n+\n+static void compact_do_zone_lock_irqsave(struct zone *zone,\n+\t\t\t\t\t unsigned long *flags)\n+__acquires(zone->lock)\n+{\n+\tzone_lock_irqsave(zone, *flags);\n+}\n+\n+static void compact_do_raw_lock_irqsave(spinlock_t *lock,\n+\t\t\t\t\tunsigned long *flags)\n+__acquires(lock)\n+{\n+\tspin_lock_irqsave(lock, *flags);\n+}\n+\n+static void compact_do_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t    unsigned long *flags)\n+{\n+\tif (lock.type == COMPACT_LOCK_ZONE) {\n+\t\tcompact_do_zone_lock_irqsave(lock.zone, flags);\n+\t\treturn;\n+\t}\n+\n+\treturn compact_do_raw_lock_irqsave(lock.lock, flags);\n+}\n+\n /*\n  * Compaction requires the taking of some coarse locks that are potentially\n  * very heavily contended. For async compaction, trylock and record if the\n@@ -502,19 +562,19 @@ static bool test_and_set_skip(struct compact_control *cc, struct page *page)\n  *\n  * Always returns true which makes it easier to track lock state in callers.\n  */\n-static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n-\t\t\t\t\t\tstruct compact_control *cc)\n-\t__acquires(lock)\n+static bool compact_lock_irqsave(struct compact_lock lock,\n+\t\t\t\t unsigned long *flags,\n+\t\t\t\t struct compact_control *cc)\n {\n \t/* Track if the lock is contended in async mode */\n \tif (cc->mode == MIGRATE_ASYNC && !cc->contended) {\n-\t\tif (spin_trylock_irqsave(lock, *flags))\n+\t\tif (compact_do_trylock_irqsave(lock, flags))\n \t\t\treturn true;\n \n \t\tcc->contended = true;\n \t}\n \n-\tspin_lock_irqsave(lock, *flags);\n+\tcompact_do_lock_irqsave(lock, flags);\n \treturn true;\n }\n \n@@ -530,11 +590,13 @@ static bool compact_lock_irqsave(spinlock_t *lock, unsigned long *flags,\n  * Returns true if compaction should abort due to fatal signal pending.\n  * Returns false when compaction can continue.\n  */\n-static bool compact_unlock_should_abort(spinlock_t *lock,\n-\t\tunsigned long flags, bool *locked, struct compact_control *cc)\n+static bool compact_unlock_should_abort(struct zone *zone,\n+\t\t\t\t\tunsigned long flags,\n+\t\t\t\t\tbool *locked,\n+\t\t\t\t\tstruct compact_control *cc)\n {\n \tif (*locked) {\n-\t\tspin_unlock_irqrestore(lock, flags);\n+\t\tzone_unlock_irqrestore(zone, flags);\n \t\t*locked = false;\n \t}\n \n@@ -582,9 +644,8 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t\t * contention, to give chance to IRQs. Abort if fatal signal\n \t\t * pending.\n \t\t */\n-\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX)\n-\t\t    && compact_unlock_should_abort(&cc->zone->lock, flags,\n-\t\t\t\t\t\t\t\t&locked, cc))\n+\t\tif (!(blockpfn % COMPACT_CLUSTER_MAX) &&\n+\t\t    compact_unlock_should_abort(cc->zone, flags, &locked, cc))\n \t\t\tbreak;\n \n \t\tnr_scanned++;\n@@ -613,8 +674,12 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \n \t\t/* If we already hold the lock, we can skip some rechecking. */\n \t\tif (!locked) {\n-\t\t\tlocked = compact_lock_irqsave(&cc->zone->lock,\n-\t\t\t\t\t\t\t\t&flags, cc);\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_ZONE,\n+\t\t\t\t.zone = cc->zone,\n+\t\t\t};\n+\n+\t\t\tlocked = compact_lock_irqsave(zol, &flags, cc);\n \n \t\t\t/* Recheck this is a buddy page under lock */\n \t\t\tif (!PageBuddy(page))\n@@ -649,7 +714,7 @@ static unsigned long isolate_freepages_block(struct compact_control *cc,\n \t}\n \n \tif (locked)\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t/*\n \t * Be careful to not go outside of the pageblock.\n@@ -1157,10 +1222,15 @@ isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,\n \n \t\t/* If we already hold the lock, we can skip some rechecking */\n \t\tif (lruvec != locked) {\n+\t\t\tstruct compact_lock zol = {\n+\t\t\t\t.type = COMPACT_LOCK_RAW_SPINLOCK,\n+\t\t\t\t.lock = &lruvec->lru_lock,\n+\t\t\t};\n+\n \t\t\tif (locked)\n \t\t\t\tunlock_page_lruvec_irqrestore(locked, flags);\n \n-\t\t\tcompact_lock_irqsave(&lruvec->lru_lock, &flags, cc);\n+\t\t\tcompact_lock_irqsave(zol, &flags, cc);\n \t\t\tlocked = lruvec;\n \n \t\t\tlruvec_memcg_debug(lruvec, folio);\n@@ -1555,7 +1625,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry_reverse(freepage, freelist, buddy_list) {\n \t\t\tunsigned long pfn;\n@@ -1614,7 +1684,7 @@ static void fast_isolate_freepages(struct compact_control *cc)\n \t\t\t}\n \t\t}\n \n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \n \t\t/* Skip fast search if enough freepages isolated */\n \t\tif (cc->nr_freepages >= cc->nr_migratepages)\n@@ -1988,7 +2058,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\tif (!area->nr_free)\n \t\t\tcontinue;\n \n-\t\tspin_lock_irqsave(&cc->zone->lock, flags);\n+\t\tzone_lock_irqsave(cc->zone, flags);\n \t\tfreelist = &area->free_list[MIGRATE_MOVABLE];\n \t\tlist_for_each_entry(freepage, freelist, buddy_list) {\n \t\t\tunsigned long free_pfn;\n@@ -2021,7 +2091,7 @@ static unsigned long fast_find_migrateblock(struct compact_control *cc)\n \t\t\t\tbreak;\n \t\t\t}\n \t\t}\n-\t\tspin_unlock_irqrestore(&cc->zone->lock, flags);\n+\t\tzone_unlock_irqrestore(cc->zone, flags);\n \t}\n \n \tcc->total_migrate_scanned += nr_scanned;\n-- \n2.47.3",
          "reply_to": "",
          "message_date": "2026-02-11"
        },
        {
          "author": "Dmitry Ilvokhin (author)",
          "summary": "The author addressed a concern about the performance impact of tracepoint instrumentation on the fast path, explaining that the implementation follows the mmap_lock tracepoint pattern and that helpers compile to empty inline stubs when tracing is disabled.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Add tracepoint instrumentation to zone lock acquire/release operations\nvia the previously introduced wrappers.\n\nThe implementation follows the mmap_lock tracepoint pattern: a\nlightweight inline helper checks whether the tracepoint is enabled and\ncalls into an out-of-line helper when tracing is active. When\nCONFIG_TRACING is disabled, helpers compile to empty inline stubs.\n\nThe fast path is unaffected when tracing is disabled.\n\nSigned-off-by: Dmitry Ilvokhin <d@ilvokhin.com>\n---\n MAINTAINERS                      |  2 +\n include/linux/zone_lock.h        | 64 +++++++++++++++++++++++++++++++-\n include/trace/events/zone_lock.h | 64 ++++++++++++++++++++++++++++++++\n mm/Makefile                      |  2 +-\n mm/zone_lock.c                   | 31 ++++++++++++++++\n 5 files changed, 161 insertions(+), 2 deletions(-)\n create mode 100644 include/trace/events/zone_lock.h\n create mode 100644 mm/zone_lock.c\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 680c9ae02d7e..711ffa15f4c3 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16499,6 +16499,7 @@ F:\tinclude/linux/ptdump.h\n F:\tinclude/linux/vmpressure.h\n F:\tinclude/linux/vmstat.h\n F:\tinclude/linux/zone_lock.h\n+F:\tinclude/trace/events/zone_lock.h\n F:\tkernel/fork.c\n F:\tmm/Kconfig\n F:\tmm/debug.c\n@@ -16518,6 +16519,7 @@ F:\tmm/sparse.c\n F:\tmm/util.c\n F:\tmm/vmpressure.c\n F:\tmm/vmstat.c\n+F:\tmm/zone_lock.c\n N:\tinclude/linux/page[-_]*\n \n MEMORY MANAGEMENT - EXECMEM\ndiff --git a/include/linux/zone_lock.h b/include/linux/zone_lock.h\nindex c531e26280e6..cea41dd56324 100644\n--- a/include/linux/zone_lock.h\n+++ b/include/linux/zone_lock.h\n@@ -4,6 +4,53 @@\n \n #include <linux/mmzone.h>\n #include <linux/spinlock.h>\n+#include <linux/tracepoint-defs.h>\n+\n+DECLARE_TRACEPOINT(zone_lock_start_locking);\n+DECLARE_TRACEPOINT(zone_lock_acquire_returned);\n+DECLARE_TRACEPOINT(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone);\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success);\n+void __zone_lock_do_trace_released(struct zone *zone);\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_start_locking))\n+\t\t__zone_lock_do_trace_start_locking(zone);\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+\tif (tracepoint_enabled(zone_lock_acquire_returned))\n+\t\t__zone_lock_do_trace_acquire_returned(zone, success);\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+\tif (tracepoint_enabled(zone_lock_released))\n+\t\t__zone_lock_do_trace_released(zone);\n+}\n+\n+#else /* !CONFIG_TRACING */\n+\n+static inline void __zone_lock_trace_start_locking(struct zone *zone)\n+{\n+}\n+\n+static inline void __zone_lock_trace_acquire_returned(struct zone *zone,\n+\t\t\t\t\t\t      bool success)\n+{\n+}\n+\n+static inline void __zone_lock_trace_released(struct zone *zone)\n+{\n+}\n+\n+#endif /* CONFIG_TRACING */\n \n static inline void zone_lock_init(struct zone *zone)\n {\n@@ -12,26 +59,41 @@ static inline void zone_lock_init(struct zone *zone)\n \n #define zone_lock_irqsave(zone, flags)\t\t\t\t\\\n do {\t\t\t\t\t\t\t\t\\\n+\tbool success = true;\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n \tspin_lock_irqsave(&(zone)->lock, flags);\t\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n } while (0)\n \n #define zone_trylock_irqsave(zone, flags)\t\t\t\\\n ({\t\t\t\t\t\t\t\t\\\n-\tspin_trylock_irqsave(&(zone)->lock, flags);\t\t\\\n+\tbool success;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t__zone_lock_trace_start_locking(zone);\t\t\t\\\n+\tsuccess = spin_trylock_irqsave(&(zone)->lock, flags);\t\\\n+\t__zone_lock_trace_acquire_returned(zone, success);\t\\\n+\tsuccess;\t\t\t\t\t\t\\\n })\n \n static inline void zone_unlock_irqrestore(struct zone *zone, unsigned long flags)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irqrestore(&zone->lock, flags);\n }\n \n static inline void zone_lock_irq(struct zone *zone)\n {\n+\tbool success = true;\n+\n+\t__zone_lock_trace_start_locking(zone);\n \tspin_lock_irq(&zone->lock);\n+\t__zone_lock_trace_acquire_returned(zone, success);\n }\n \n static inline void zone_unlock_irq(struct zone *zone)\n {\n+\t__zone_lock_trace_released(zone);\n \tspin_unlock_irq(&zone->lock);\n }\n \ndiff --git a/include/trace/events/zone_lock.h b/include/trace/events/zone_lock.h\nnew file mode 100644\nindex 000000000000..3df82a8c0160\n--- /dev/null\n+++ b/include/trace/events/zone_lock.h\n@@ -0,0 +1,64 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#undef TRACE_SYSTEM\n+#define TRACE_SYSTEM zone_lock\n+\n+#if !defined(_TRACE_ZONE_LOCK_H) || defined(TRACE_HEADER_MULTI_READ)\n+#define _TRACE_ZONE_LOCK_H\n+\n+#include <linux/tracepoint.h>\n+#include <linux/types.h>\n+\n+struct zone;\n+\n+DECLARE_EVENT_CLASS(zone_lock,\n+\n+\tTP_PROTO(struct zone *zone),\n+\n+\tTP_ARGS(zone),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t),\n+\n+\tTP_printk(\"zone=%p\", __entry->zone)\n+);\n+\n+#define DEFINE_ZONE_LOCK_EVENT(name)\t\t\t\\\n+\tDEFINE_EVENT(zone_lock, name,\t\t\t\\\n+\t\tTP_PROTO(struct zone *zone),\t\t\\\n+\t\tTP_ARGS(zone))\n+\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_start_locking);\n+DEFINE_ZONE_LOCK_EVENT(zone_lock_released);\n+\n+TRACE_EVENT(zone_lock_acquire_returned,\n+\n+\tTP_PROTO(struct zone *zone, bool success),\n+\n+\tTP_ARGS(zone, success),\n+\n+\tTP_STRUCT__entry(\n+\t\t__field(struct zone *, zone)\n+\t\t__field(bool, success)\n+\t),\n+\n+\tTP_fast_assign(\n+\t\t__entry->zone = zone;\n+\t\t__entry->success = success;\n+\t),\n+\n+\tTP_printk(\n+\t\t\"zone=%p success=%s\",\n+\t\t__entry->zone,\n+\t\t__entry->success ? \"true\" : \"false\"\n+\t)\n+);\n+\n+#endif /* _TRACE_ZONE_LOCK_H */\n+\n+/* This part must be outside protection */\n+#include <trace/define_trace.h>\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 0d85b10dbdde..fd891710c696 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -55,7 +55,7 @@ obj-y\t\t\t:= filemap.o mempool.o oom_kill.o fadvise.o \\\n \t\t\t   mm_init.o percpu.o slab_common.o \\\n \t\t\t   compaction.o show_mem.o \\\n \t\t\t   interval_tree.o list_lru.o workingset.o \\\n-\t\t\t   debug.o gup.o mmap_lock.o vma_init.o $(mmu-y)\n+\t\t\t   debug.o gup.o mmap_lock.o zone_lock.o vma_init.o $(mmu-y)\n \n # Give 'page_alloc' its own module-parameter namespace\n page-alloc-y := page_alloc.o\ndiff --git a/mm/zone_lock.c b/mm/zone_lock.c\nnew file mode 100644\nindex 000000000000..f647fd2aca48\n--- /dev/null\n+++ b/mm/zone_lock.c\n@@ -0,0 +1,31 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#define CREATE_TRACE_POINTS\n+#include <trace/events/zone_lock.h>\n+\n+#include <linux/zone_lock.h>\n+\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_start_locking);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_acquire_returned);\n+EXPORT_TRACEPOINT_SYMBOL(zone_lock_released);\n+\n+#ifdef CONFIG_TRACING\n+\n+void __zone_lock_do_trace_start_locking(struct zone *zone)\n+{\n+\ttrace_zone_lock_start_locking(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_start_locking);\n+\n+void __zone_lock_do_trace_acquire_returned(struct zone *zone, bool success)\n+{\n+\ttrace_zone_lock_acquire_returned(zone, success);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_acquire_returned);\n+\n+void __zone_lock_do_trace_released(struct zone *zone)\n+{\n+\ttrace_zone_lock_released(zone);\n+}\n+EXPORT_SYMBOL(__zone_lock_do_trace_released);\n+\n+#endif /* CONFIG_TRACING */\n-- \n2.47.3",
          "reply_to": "",
          "message_date": "2026-02-11"
        }
      ],
      "analysis_source": "llm-per-reviewer",
      "patch_summary": "This patch introduces thin wrappers around zone lock acquire and release operations, allowing for future tracepoint instrumentation without modifying individual call sites. The wrappers are added to prepare the code for subsequent patches that will add tracing functionality. This change is a preparatory step and does not introduce any functional changes at this time."
    },
    "2026-02-23": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Dmitry Ilvokhin",
      "reviews": [
        {
          "author": "Dmitry Ilvokhin (author)",
          "summary": "Author addressed a concern about the patch series structure and refactoring order, explaining that he intentionally split the conversion into two patches to keep instrumentation isolated from intermediate refactoring states.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Hi Ben,\n\nThanks for the suggestion.\n\nI structured the series intentionally to keep all behavior-preserving\nrefactoring separate from the actual instrumentation change.\n\nIn particular, I had to split the conversion into two patches to\nseparate the purely mechanical changes from the compaction\nrestructuring. With the current order, tracepoints addition remains a\nsingle, atomic functional change on top of a fully converted tree. This\nkeeps the instrumentation isolated from the refactoring and with an\nintention to make bisection and review of the behavioral change easier.\n\nReordering as suggested would mix instrumentation with intermediate\nrefactoring states, which I'd prefer to avoid.\n\nI hope this reasoning makes sense, but I'm happy to discuss if there are\nstrong objections.",
          "reply_to": "Cheatham, Benjamin",
          "message_date": "2026-02-23"
        },
        {
          "author": "Cheatham, Benjamin",
          "summary": "Reviewer Cheatham expressed uncertainty about prioritizing zone lock wrappers over his suggested reading order for the patch series.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "uncertainty",
            "lack of clear objection"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "No that's fine, I figured as much. I just wasn't sure that was more important\nto you than what (I thought) was a better reading order for the series.\n\nThanks,\nBen",
          "reply_to": "Dmitry Ilvokhin",
          "message_date": "2026-02-23"
        },
        {
          "author": "Shakeel Butt",
          "summary": "Gave Acked-by",
          "sentiment": "positive",
          "sentiment_signals": [],
          "has_inline_review": false,
          "tags_given": [
            "Acked-by"
          ],
          "analysis_source": "heuristic",
          "raw_body": "",
          "reply_to": "Dmitry Ilvokhin",
          "message_date": "2026-02-23"
        }
      ],
      "analysis_source": "llm-per-reviewer",
      "patch_summary": "This patch introduces thin wrappers around zone lock acquire and release operations, allowing for future tracepoint instrumentation without modifying individual call sites. The wrappers are added to prepare the code for subsequent patches that will add tracing functionality. This change is a preparatory step and does not introduce any functional changes at this time."
    }
  }
}