{
  "thread_id": "aZY30X9bpsV-nPsm@linux.dev",
  "subject": "Re: [PATCH] arm64: remove HAVE_CMPXCHG_LOCAL",
  "url": "https://lore.kernel.org/all/aZY30X9bpsV-nPsm@linux.dev/",
  "dates": {
    "2026-02-18": {
      "report_file": "2026-02-18_ollama_llama3.1-8b.html",
      "developer": "Shakeel Butt",
      "reviews": [
        {
          "author": "Dev Jain",
          "summary": "Dev Jain agrees that the issue is not with LL/SC/LSE but rather preempt_disable()/enable() in this_cpu_*, and suggests keeping the code while removing the config selection.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "\nOn 15/02/26 9:09 am, Jisheng Zhang wrote:\n> It turns out the generic disable/enable irq this_cpu_cmpxchg\n> implementation is faster than LL/SC or lse implementation. Remove\n> HAVE_CMPXCHG_LOCAL for better performance on arm64.\n>\n> Tested on Quad 1.9GHZ CA55 platform:\n> average mod_node_page_state() cost decreases from 167ns to 103ns\n> the spawn (30 duration) benchmark in unixbench is improved\n> from 147494 lps to 150561 lps, improved by 2.1%\n>\n> Tested on Quad 2.1GHZ CA73 platform:\n> average mod_node_page_state() cost decreases from 113ns to 85ns\n> the spawn (30 duration) benchmark in unixbench is improved\n> from 209844 lps to 212581 lps, improved by 1.3%\n>\n> Signed-off-by: Jisheng Zhang <jszhang@kernel.org>\n> ---\n\nThanks. This concurs with my investigation on [1]. The problem\nisn't really LL/SC/LSE but preempt_disable()/enable() in\nthis_cpu_* [1, 2].\n\nI think you should only remove the selection of the config,\nbut keep the code? We may want to switch this on again if\nthe real issue gets solved.\n\n[1] https://lore.kernel.org/all/5a6782f3-d758-4d9c-975b-5ae4b5d80d4e@arm.com/\n[2] https://lore.kernel.org/all/CAHbLzkpcN-T8MH6=W3jCxcFj1gVZp8fRqe231yzZT-rV_E_org@mail.gmail.com/\n\n>  arch/arm64/Kconfig              |  1 -\n>  arch/arm64/include/asm/percpu.h | 24 ------------------------\n>  2 files changed, 25 deletions(-)\n>\n> diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig\n> index 38dba5f7e4d2..5e7e2e65d5a5 100644\n> --- a/arch/arm64/Kconfig\n> +++ b/arch/arm64/Kconfig\n> @@ -205,7 +205,6 @@ config ARM64\n>  \tselect HAVE_EBPF_JIT\n>  \tselect HAVE_C_RECORDMCOUNT\n>  \tselect HAVE_CMPXCHG_DOUBLE\n> -\tselect HAVE_CMPXCHG_LOCAL\n>  \tselect HAVE_CONTEXT_TRACKING_USER\n>  \tselect HAVE_DEBUG_KMEMLEAK\n>  \tselect HAVE_DMA_CONTIGUOUS\n> diff --git a/arch/arm64/include/asm/percpu.h b/arch/arm64/include/asm/percpu.h\n> index b57b2bb00967..70ffe566cb4b 100644\n> --- a/arch/arm64/include/asm/percpu.h\n> +++ b/arch/arm64/include/asm/percpu.h\n> @@ -232,30 +232,6 @@ PERCPU_RET_OP(add, add, ldadd)\n>  #define this_cpu_xchg_8(pcp, val)\t\\\n>  \t_pcp_protect_return(xchg_relaxed, pcp, val)\n>  \n> -#define this_cpu_cmpxchg_1(pcp, o, n)\t\\\n> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n> -#define this_cpu_cmpxchg_2(pcp, o, n)\t\\\n> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n> -#define this_cpu_cmpxchg_4(pcp, o, n)\t\\\n> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n> -#define this_cpu_cmpxchg_8(pcp, o, n)\t\\\n> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n> -\n> -#define this_cpu_cmpxchg64(pcp, o, n)\tthis_cpu_cmpxchg_8(pcp, o, n)\n> -\n> -#define this_cpu_cmpxchg128(pcp, o, n)\t\t\t\t\t\\\n> -({\t\t\t\t\t\t\t\t\t\\\n> -\ttypedef typeof(pcp) pcp_op_T__;\t\t\t\t\t\\\n> -\tu128 old__, new__, ret__;\t\t\t\t\t\\\n> -\tpcp_op_T__ *ptr__;\t\t\t\t\t\t\\\n> -\told__ = o;\t\t\t\t\t\t\t\\\n> -\tnew__ = n;\t\t\t\t\t\t\t\\\n> -\tpreempt_disable_notrace();\t\t\t\t\t\\\n> -\tptr__ = raw_cpu_ptr(&(pcp));\t\t\t\t\t\\\n> -\tret__ = cmpxchg128_local((void *)ptr__, old__, new__);\t\t\\\n> -\tpreempt_enable_notrace();\t\t\t\t\t\\\n> -\tret__;\t\t\t\t\t\t\t\t\\\n> -})\n>  \n>  #ifdef __KVM_NVHE_HYPERVISOR__\n>  extern unsigned long __hyp_per_cpu_offset(unsigned int cpu);\n",
          "reply_to": "Jisheng Zhang"
        },
        {
          "author": "Will Deacon",
          "summary": "Reviewer Will Deacon disagreed with the patch, citing that the performance improvement is system-dependent and not a good reason for micro-optimization.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "disagreement",
            "micro-optimization"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "On Sun, Feb 15, 2026 at 11:39:44AM +0800, Jisheng Zhang wrote:\n> It turns out the generic disable/enable irq this_cpu_cmpxchg\n> implementation is faster than LL/SC or lse implementation. Remove\n> HAVE_CMPXCHG_LOCAL for better performance on arm64.\n> \n> Tested on Quad 1.9GHZ CA55 platform:\n> average mod_node_page_state() cost decreases from 167ns to 103ns\n> the spawn (30 duration) benchmark in unixbench is improved\n> from 147494 lps to 150561 lps, improved by 2.1%\n> \n> Tested on Quad 2.1GHZ CA73 platform:\n> average mod_node_page_state() cost decreases from 113ns to 85ns\n> the spawn (30 duration) benchmark in unixbench is improved\n> from 209844 lps to 212581 lps, improved by 1.3%\n> \n> Signed-off-by: Jisheng Zhang <jszhang@kernel.org>\n> ---\n>  arch/arm64/Kconfig              |  1 -\n>  arch/arm64/include/asm/percpu.h | 24 ------------------------\n>  2 files changed, 25 deletions(-)\n\nThat is _entirely_ dependent on the system, so this isn't the right\napproach. I also don't think it's something we particularly want to\nmicro-optimise to accomodate systems that suck at atomics.\n\nWill\n\n> diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig\n> index 38dba5f7e4d2..5e7e2e65d5a5 100644\n> --- a/arch/arm64/Kconfig\n> +++ b/arch/arm64/Kconfig\n> @@ -205,7 +205,6 @@ config ARM64\n>  \tselect HAVE_EBPF_JIT\n>  \tselect HAVE_C_RECORDMCOUNT\n>  \tselect HAVE_CMPXCHG_DOUBLE\n> -\tselect HAVE_CMPXCHG_LOCAL\n>  \tselect HAVE_CONTEXT_TRACKING_USER\n>  \tselect HAVE_DEBUG_KMEMLEAK\n>  \tselect HAVE_DMA_CONTIGUOUS\n> diff --git a/arch/arm64/include/asm/percpu.h b/arch/arm64/include/asm/percpu.h\n> index b57b2bb00967..70ffe566cb4b 100644\n> --- a/arch/arm64/include/asm/percpu.h\n> +++ b/arch/arm64/include/asm/percpu.h\n> @@ -232,30 +232,6 @@ PERCPU_RET_OP(add, add, ldadd)\n>  #define this_cpu_xchg_8(pcp, val)\t\\\n>  \t_pcp_protect_return(xchg_relaxed, pcp, val)\n>  \n> -#define this_cpu_cmpxchg_1(pcp, o, n)\t\\\n> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n> -#define this_cpu_cmpxchg_2(pcp, o, n)\t\\\n> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n> -#define this_cpu_cmpxchg_4(pcp, o, n)\t\\\n> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n> -#define this_cpu_cmpxchg_8(pcp, o, n)\t\\\n> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n> -\n> -#define this_cpu_cmpxchg64(pcp, o, n)\tthis_cpu_cmpxchg_8(pcp, o, n)\n> -\n> -#define this_cpu_cmpxchg128(pcp, o, n)\t\t\t\t\t\\\n> -({\t\t\t\t\t\t\t\t\t\\\n> -\ttypedef typeof(pcp) pcp_op_T__;\t\t\t\t\t\\\n> -\tu128 old__, new__, ret__;\t\t\t\t\t\\\n> -\tpcp_op_T__ *ptr__;\t\t\t\t\t\t\\\n> -\told__ = o;\t\t\t\t\t\t\t\\\n> -\tnew__ = n;\t\t\t\t\t\t\t\\\n> -\tpreempt_disable_notrace();\t\t\t\t\t\\\n> -\tptr__ = raw_cpu_ptr(&(pcp));\t\t\t\t\t\\\n> -\tret__ = cmpxchg128_local((void *)ptr__, old__, new__);\t\t\\\n> -\tpreempt_enable_notrace();\t\t\t\t\t\\\n> -\tret__;\t\t\t\t\t\t\t\t\\\n> -})\n>  \n>  #ifdef __KVM_NVHE_HYPERVISOR__\n>  extern unsigned long __hyp_per_cpu_offset(unsigned int cpu);\n> -- \n> 2.51.0\n> \n",
          "reply_to": "Jisheng Zhang"
        },
        {
          "author": "Dev Jain",
          "summary": "Dev Jain suspects that the performance regression is not due to atomics, but rather preempt_disable(), and suggests testing this hypothesis on other hardware.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "further investigation"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "\nOn 16/02/26 4:30 pm, Will Deacon wrote:\n> On Sun, Feb 15, 2026 at 11:39:44AM +0800, Jisheng Zhang wrote:\n>> It turns out the generic disable/enable irq this_cpu_cmpxchg\n>> implementation is faster than LL/SC or lse implementation. Remove\n>> HAVE_CMPXCHG_LOCAL for better performance on arm64.\n>>\n>> Tested on Quad 1.9GHZ CA55 platform:\n>> average mod_node_page_state() cost decreases from 167ns to 103ns\n>> the spawn (30 duration) benchmark in unixbench is improved\n>> from 147494 lps to 150561 lps, improved by 2.1%\n>>\n>> Tested on Quad 2.1GHZ CA73 platform:\n>> average mod_node_page_state() cost decreases from 113ns to 85ns\n>> the spawn (30 duration) benchmark in unixbench is improved\n>> from 209844 lps to 212581 lps, improved by 1.3%\n>>\n>> Signed-off-by: Jisheng Zhang <jszhang@kernel.org>\n>> ---\n>>  arch/arm64/Kconfig              |  1 -\n>>  arch/arm64/include/asm/percpu.h | 24 ------------------------\n>>  2 files changed, 25 deletions(-)\n> That is _entirely_ dependent on the system, so this isn't the right\n> approach. I also don't think it's something we particularly want to\n> micro-optimise to accomodate systems that suck at atomics.\n\nHi Will,\n\nAs I mention in the other email, the suspect is not the atomics, but\npreempt_disable(). On Apple M3, the regression reported in [1] resolves\nby removing preempt_disable/enable in _pcp_protect_return. To prove\nthis another way, I disabled CONFIG_ARM64_HAS_LSE_ATOMICS and the\nregression worsened, indicating that at least on Apple M3 the\natomics are faster.\n\nIt may help to confirm this hypothesis on other hardware - perhaps\nJisheng can test with this change on his hardware and confirm\nwhether he gets the same performance improvement.\n\nBy coincidence, Yang Shi has been discussing the this_cpu_* overhead\nat [2].\n\n[1] https://lore.kernel.org/all/1052a452-9ba3-4da7-be47-7d27d27b3d1d@arm.com/\n[2] https://lore.kernel.org/all/CAHbLzkpcN-T8MH6=W3jCxcFj1gVZp8fRqe231yzZT-rV_E_org@mail.gmail.com/\n\n>\n> Will\n>\n>> diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig\n>> index 38dba5f7e4d2..5e7e2e65d5a5 100644\n>> --- a/arch/arm64/Kconfig\n>> +++ b/arch/arm64/Kconfig\n>> @@ -205,7 +205,6 @@ config ARM64\n>>  \tselect HAVE_EBPF_JIT\n>>  \tselect HAVE_C_RECORDMCOUNT\n>>  \tselect HAVE_CMPXCHG_DOUBLE\n>> -\tselect HAVE_CMPXCHG_LOCAL\n>>  \tselect HAVE_CONTEXT_TRACKING_USER\n>>  \tselect HAVE_DEBUG_KMEMLEAK\n>>  \tselect HAVE_DMA_CONTIGUOUS\n>> diff --git a/arch/arm64/include/asm/percpu.h b/arch/arm64/include/asm/percpu.h\n>> index b57b2bb00967..70ffe566cb4b 100644\n>> --- a/arch/arm64/include/asm/percpu.h\n>> +++ b/arch/arm64/include/asm/percpu.h\n>> @@ -232,30 +232,6 @@ PERCPU_RET_OP(add, add, ldadd)\n>>  #define this_cpu_xchg_8(pcp, val)\t\\\n>>  \t_pcp_protect_return(xchg_relaxed, pcp, val)\n>>  \n>> -#define this_cpu_cmpxchg_1(pcp, o, n)\t\\\n>> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n>> -#define this_cpu_cmpxchg_2(pcp, o, n)\t\\\n>> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n>> -#define this_cpu_cmpxchg_4(pcp, o, n)\t\\\n>> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n>> -#define this_cpu_cmpxchg_8(pcp, o, n)\t\\\n>> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n>> -\n>> -#define this_cpu_cmpxchg64(pcp, o, n)\tthis_cpu_cmpxchg_8(pcp, o, n)\n>> -\n>> -#define this_cpu_cmpxchg128(pcp, o, n)\t\t\t\t\t\\\n>> -({\t\t\t\t\t\t\t\t\t\\\n>> -\ttypedef typeof(pcp) pcp_op_T__;\t\t\t\t\t\\\n>> -\tu128 old__, new__, ret__;\t\t\t\t\t\\\n>> -\tpcp_op_T__ *ptr__;\t\t\t\t\t\t\\\n>> -\told__ = o;\t\t\t\t\t\t\t\\\n>> -\tnew__ = n;\t\t\t\t\t\t\t\\\n>> -\tpreempt_disable_notrace();\t\t\t\t\t\\\n>> -\tptr__ = raw_cpu_ptr(&(pcp));\t\t\t\t\t\\\n>> -\tret__ = cmpxchg128_local((void *)ptr__, old__, new__);\t\t\\\n>> -\tpreempt_enable_notrace();\t\t\t\t\t\\\n>> -\tret__;\t\t\t\t\t\t\t\t\\\n>> -})\n>>  \n>>  #ifdef __KVM_NVHE_HYPERVISOR__\n>>  extern unsigned long __hyp_per_cpu_offset(unsigned int cpu);\n>> -- \n>> 2.51.0\n>>\n",
          "reply_to": "Will Deacon"
        },
        {
          "author": "Catalin Marinas",
          "summary": "The reviewer questioned the patch by suggesting an alternative approach, replacing preempt disabling with local_irq_save(), and expressing concern about the atomicity of the generic cmpxchg implementation.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "alternative solution",
            "atomicity concern"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Mon, Feb 16, 2026 at 08:59:17PM +0530, Dev Jain wrote:\n> On 16/02/26 4:30 pm, Will Deacon wrote:\n> > On Sun, Feb 15, 2026 at 11:39:44AM +0800, Jisheng Zhang wrote:\n> >> It turns out the generic disable/enable irq this_cpu_cmpxchg\n> >> implementation is faster than LL/SC or lse implementation. Remove\n> >> HAVE_CMPXCHG_LOCAL for better performance on arm64.\n> >>\n> >> Tested on Quad 1.9GHZ CA55 platform:\n> >> average mod_node_page_state() cost decreases from 167ns to 103ns\n> >> the spawn (30 duration) benchmark in unixbench is improved\n> >> from 147494 lps to 150561 lps, improved by 2.1%\n> >>\n> >> Tested on Quad 2.1GHZ CA73 platform:\n> >> average mod_node_page_state() cost decreases from 113ns to 85ns\n> >> the spawn (30 duration) benchmark in unixbench is improved\n> >> from 209844 lps to 212581 lps, improved by 1.3%\n> >>\n> >> Signed-off-by: Jisheng Zhang <jszhang@kernel.org>\n> >> ---\n> >>  arch/arm64/Kconfig              |  1 -\n> >>  arch/arm64/include/asm/percpu.h | 24 ------------------------\n> >>  2 files changed, 25 deletions(-)\n> > That is _entirely_ dependent on the system, so this isn't the right\n> > approach. I also don't think it's something we particularly want to\n> > micro-optimise to accomodate systems that suck at atomics.\n> \n> Hi Will,\n> \n> As I mention in the other email, the suspect is not the atomics, but\n> preempt_disable(). On Apple M3, the regression reported in [1] resolves\n> by removing preempt_disable/enable in _pcp_protect_return. To prove\n> this another way, I disabled CONFIG_ARM64_HAS_LSE_ATOMICS and the\n> regression worsened, indicating that at least on Apple M3 the\n> atomics are faster.\n\nThen why don't we replace the preempt disabling with local_irq_save()\nin the arm64 code and still use the LSE atomics?\n\nIIUC (lots of macro indirection), the generic cmpxchg is not atomic, so\nanother CPU is allowed to mess this up if it accesses current CPU's\nvariable via per_cpu_ptr().\n\n-- \nCatalin\n",
          "reply_to": "Dev Jain"
        },
        {
          "author": "Will Deacon",
          "summary": "Reviewer Will Deacon suggested that instead of removing HAVE_CMPXCHG_LOCAL, the patch should focus on improving the performance of preempt_disable() as it is used in many other places.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Tue, Feb 17, 2026 at 01:53:19PM +0000, Catalin Marinas wrote:\n> On Mon, Feb 16, 2026 at 08:59:17PM +0530, Dev Jain wrote:\n> > On 16/02/26 4:30 pm, Will Deacon wrote:\n> > > On Sun, Feb 15, 2026 at 11:39:44AM +0800, Jisheng Zhang wrote:\n> > >> It turns out the generic disable/enable irq this_cpu_cmpxchg\n> > >> implementation is faster than LL/SC or lse implementation. Remove\n> > >> HAVE_CMPXCHG_LOCAL for better performance on arm64.\n> > >>\n> > >> Tested on Quad 1.9GHZ CA55 platform:\n> > >> average mod_node_page_state() cost decreases from 167ns to 103ns\n> > >> the spawn (30 duration) benchmark in unixbench is improved\n> > >> from 147494 lps to 150561 lps, improved by 2.1%\n> > >>\n> > >> Tested on Quad 2.1GHZ CA73 platform:\n> > >> average mod_node_page_state() cost decreases from 113ns to 85ns\n> > >> the spawn (30 duration) benchmark in unixbench is improved\n> > >> from 209844 lps to 212581 lps, improved by 1.3%\n> > >>\n> > >> Signed-off-by: Jisheng Zhang <jszhang@kernel.org>\n> > >> ---\n> > >>  arch/arm64/Kconfig              |  1 -\n> > >>  arch/arm64/include/asm/percpu.h | 24 ------------------------\n> > >>  2 files changed, 25 deletions(-)\n> > > That is _entirely_ dependent on the system, so this isn't the right\n> > > approach. I also don't think it's something we particularly want to\n> > > micro-optimise to accomodate systems that suck at atomics.\n> > \n> > Hi Will,\n> > \n> > As I mention in the other email, the suspect is not the atomics, but\n> > preempt_disable(). On Apple M3, the regression reported in [1] resolves\n> > by removing preempt_disable/enable in _pcp_protect_return. To prove\n> > this another way, I disabled CONFIG_ARM64_HAS_LSE_ATOMICS and the\n> > regression worsened, indicating that at least on Apple M3 the\n> > atomics are faster.\n> \n> Then why don't we replace the preempt disabling with local_irq_save()\n> in the arm64 code and still use the LSE atomics?\n\nEven better, work on making preempt_disable() faster as it's used in many\nother places. Of course, if people want to hack the .config, they could\nalso change the preemption mode...\n\nWill\n",
          "reply_to": "Catalin Marinas"
        },
        {
          "author": "Catalin Marinas",
          "summary": "The reviewer raised a concern about the preempt_enable_notrace() path calling preempt_schedule_notrace(), which can lead to an unconditional call to __schedule(). They suggested adding a simple change to the preempt macros to avoid this issue.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "concern",
            "suggested_change"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Tue, Feb 17, 2026 at 03:00:22PM +0000, Will Deacon wrote:\n> On Tue, Feb 17, 2026 at 01:53:19PM +0000, Catalin Marinas wrote:\n> > On Mon, Feb 16, 2026 at 08:59:17PM +0530, Dev Jain wrote:\n> > > On 16/02/26 4:30 pm, Will Deacon wrote:\n> > > > On Sun, Feb 15, 2026 at 11:39:44AM +0800, Jisheng Zhang wrote:\n> > > >> It turns out the generic disable/enable irq this_cpu_cmpxchg\n> > > >> implementation is faster than LL/SC or lse implementation. Remove\n> > > >> HAVE_CMPXCHG_LOCAL for better performance on arm64.\n> > > >>\n> > > >> Tested on Quad 1.9GHZ CA55 platform:\n> > > >> average mod_node_page_state() cost decreases from 167ns to 103ns\n> > > >> the spawn (30 duration) benchmark in unixbench is improved\n> > > >> from 147494 lps to 150561 lps, improved by 2.1%\n> > > >>\n> > > >> Tested on Quad 2.1GHZ CA73 platform:\n> > > >> average mod_node_page_state() cost decreases from 113ns to 85ns\n> > > >> the spawn (30 duration) benchmark in unixbench is improved\n> > > >> from 209844 lps to 212581 lps, improved by 1.3%\n[...]\n> > > > That is _entirely_ dependent on the system, so this isn't the right\n> > > > approach. I also don't think it's something we particularly want to\n> > > > micro-optimise to accomodate systems that suck at atomics.\n> > > \n> > > As I mention in the other email, the suspect is not the atomics, but\n> > > preempt_disable(). On Apple M3, the regression reported in [1] resolves\n> > > by removing preempt_disable/enable in _pcp_protect_return. To prove\n> > > this another way, I disabled CONFIG_ARM64_HAS_LSE_ATOMICS and the\n> > > regression worsened, indicating that at least on Apple M3 the\n> > > atomics are faster.\n> > \n> > Then why don't we replace the preempt disabling with local_irq_save()\n> > in the arm64 code and still use the LSE atomics?\n> \n> Even better, work on making preempt_disable() faster as it's used in many\n> other places.\n\nYes, that would be good. It's the preempt_enable_notrace() path that\nends up calling preempt_schedule_notrace() -> __schedule() pretty much\nunconditionally. Not sure what would go wrong but some simple change\nlike this (can be done at a higher in the preempt macros to even avoid\ngetting here):\n\ndiff --git a/kernel/sched/core.c b/kernel/sched/core.c\nindex 854984967fe2..d9a5d6438303 100644\n--- a/kernel/sched/core.c\n+++ b/kernel/sched/core.c\n@@ -7119,7 +7119,7 @@ asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)\n \tif (likely(!preemptible()))\n \t\treturn;\n \n-\tdo {\n+\twhile (need_resched()) {\n \t\t/*\n \t\t * Because the function tracer can trace preempt_count_sub()\n \t\t * and it also uses preempt_enable/disable_notrace(), if\n@@ -7146,7 +7146,7 @@ asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)\n \n \t\tpreempt_latency_stop(1);\n \t\tpreempt_enable_no_resched_notrace();\n-\t} while (need_resched());\n+\t}\n }\n EXPORT_SYMBOL_GPL(preempt_schedule_notrace);\n \n\nOf course, changing the preemption model solves this by making the\nmacros no-ops but I assume people want to keep preemption on.\n\n-- \nCatalin\n",
          "reply_to": "Will Deacon"
        },
        {
          "author": "Christoph (Ampere)",
          "summary": "Christoph raised concerns that the performance of cmpxchg varies by platform and kernel config, but noted that measurements on Ampere processors did not show a regression.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "requested additional context",
            "provided counterpoint"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Mon, 16 Feb 2026, Dev Jain wrote:\n\n> By coincidence, Yang Shi has been discussing the this_cpu_* overhead\n> at [2].\n\n\nYang Shi is on vacation but we have a patchset that removes\npreempt_enable/disable from this_cpu operations on ARM64.\n\nThe performance of cmpxchg varies by platform in use and with the kernel\nconfig. The measurements that I did 2 years ago indicated that the cmpxchg\nuse with Ampere processors did not cause a regression.\n\nNote that distro kernels often do not enable PREEMPT_FULL and therefore\npreempt_disable/enable overhead is not incurred in production systems.\n\nPREEMPT_VOLUNTARY does not use preemption for this_cpu ops.\n\n",
          "reply_to": "Dev Jain"
        },
        {
          "author": "K Nayak",
          "summary": "Reviewer K Nayak questioned the patch's description of the arm64 implementation as 'unconditional', pointing out that it checks __preempt_count_dec_and_test() before calling into __schedule(), similar to x86.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification_request"
          ],
          "has_inline_review": true,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Hello Catalin,\n\nOn 2/17/2026 10:18 PM, Catalin Marinas wrote:\n> Yes, that would be good. It's the preempt_enable_notrace() path that\n> ends up calling preempt_schedule_notrace() -> __schedule() pretty much\n> unconditionally.\n\nWhat do you mean by unconditionally? We always check\n__preempt_count_dec_and_test() before calling into __schedule().\n\nOn x86, We use MSB of preempt_count to indicate a resched and\nset_preempt_need_resched() would just clear this MSB.\n\nIf the preempt_count() turns 0, we immediately go into schedule\nor  or the next preempt_enable() -> __preempt_count_dec_and_test()\nwould see the entire preempt_count being clear and will call into\nschedule.\n\nThe arm64 implementation seems to be doing something similar too\nwith a separate \"ti->preempt.need_resched\" bit which is part of\nthe \"ti->preempt_count\"'s union so it isn't really unconditional.\n\n> Not sure what would go wrong but some simple change\n> like this (can be done at a higher in the preempt macros to even avoid\n> getting here):\n> \n> diff --git a/kernel/sched/core.c b/kernel/sched/core.c\n> index 854984967fe2..d9a5d6438303 100644\n> --- a/kernel/sched/core.c\n> +++ b/kernel/sched/core.c\n> @@ -7119,7 +7119,7 @@ asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)\n>  \tif (likely(!preemptible()))\n>  \t\treturn;\n>  \n> -\tdo {\n> +\twhile (need_resched()) {\n\nEssentially you are simply checking it twice now on entry since\nneed_resched() state would have already been communicated by\n__preempt_count_dec_and_test().\n\n>  \t\t/*\n>  \t\t * Because the function tracer can trace preempt_count_sub()\n>  \t\t * and it also uses preempt_enable/disable_notrace(), if\n> @@ -7146,7 +7146,7 @@ asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)\n>  \n>  \t\tpreempt_latency_stop(1);\n>  \t\tpreempt_enable_no_resched_notrace();\n> -\t} while (need_resched());\n> +\t}\n>  }\n>  EXPORT_SYMBOL_GPL(preempt_schedule_notrace);\n-- \nThanks and Regards,\nPrateek\n\n",
          "reply_to": "Catalin Marinas"
        },
        {
          "author": "Catalin Marinas",
          "summary": "The reviewer agrees that the patch is correct but notes that it doesn't address the root cause of the performance issue and suggests exploring other optimizations.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "suggested alternative solutions"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Hi Prateek,\n\nOn Wed, Feb 18, 2026 at 09:31:19AM +0530, K Prateek Nayak wrote:\n> On 2/17/2026 10:18 PM, Catalin Marinas wrote:\n> > Yes, that would be good. It's the preempt_enable_notrace() path that\n> > ends up calling preempt_schedule_notrace() -> __schedule() pretty much\n> > unconditionally.\n> \n> What do you mean by unconditionally? We always check\n> __preempt_count_dec_and_test() before calling into __schedule().\n> \n> On x86, We use MSB of preempt_count to indicate a resched and\n> set_preempt_need_resched() would just clear this MSB.\n> \n> If the preempt_count() turns 0, we immediately go into schedule\n> or  or the next preempt_enable() -> __preempt_count_dec_and_test()\n> would see the entire preempt_count being clear and will call into\n> schedule.\n> \n> The arm64 implementation seems to be doing something similar too\n> with a separate \"ti->preempt.need_resched\" bit which is part of\n> the \"ti->preempt_count\"'s union so it isn't really unconditional.\n\nAh, yes, you are right. I got the polarity of need_resched in\nthread_info wrong (we should have named it no_need_to_resched).\n\nSo in the common case, the overhead is caused by the additional\npointer chase and preempt_count update, on top of the cpu offset read.\nNot sure we can squeeze any more cycles out of these without some\nlarge overhaul like:\n\nhttps://git.kernel.org/mark/c/84ee5f23f93d4a650e828f831da9ed29c54623c5\n\nor Yang's per-CPU page tables. Well, there are more ideas like in-kernel\nrestartable sequences but they move the overhead elsewhere.\n\nThanks.\n\n-- \nCatalin\n",
          "reply_to": "K Nayak"
        },
        {
          "author": "Shakeel Butt",
          "summary": "Shakeel Butt raised concerns about the patch removing HAVE_CMPXCHG_LOCAL, specifically pointing out that mod_node_page_state() can be called in NMI context and generic disable/enable irq are not safe against NMIs.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "concerns about safety",
            "need for additional complexity"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Sun, Feb 15, 2026 at 11:39:44AM +0800, Jisheng Zhang wrote:\n> It turns out the generic disable/enable irq this_cpu_cmpxchg\n> implementation is faster than LL/SC or lse implementation. Remove\n> HAVE_CMPXCHG_LOCAL for better performance on arm64.\n> \n> Tested on Quad 1.9GHZ CA55 platform:\n> average mod_node_page_state() cost decreases from 167ns to 103ns\n> the spawn (30 duration) benchmark in unixbench is improved\n> from 147494 lps to 150561 lps, improved by 2.1%\n> \n> Tested on Quad 2.1GHZ CA73 platform:\n> average mod_node_page_state() cost decreases from 113ns to 85ns\n> the spawn (30 duration) benchmark in unixbench is improved\n> from 209844 lps to 212581 lps, improved by 1.3%\n> \n> Signed-off-by: Jisheng Zhang <jszhang@kernel.org>\n\nPlease note that mod_node_page_state() can be called in NMI context and\ngeneric disable/enable irq are not safe against NMIs (newer arm arch supports\nNMI).\n\n\n\n---\n\nOn Fri, Feb 20, 2026 at 02:20:54PM +0800, Jisheng Zhang wrote:\n> On Wed, Feb 18, 2026 at 02:07:57PM -0800, Shakeel Butt wrote:\n> > On Sun, Feb 15, 2026 at 11:39:44AM +0800, Jisheng Zhang wrote:\n> > > It turns out the generic disable/enable irq this_cpu_cmpxchg\n> > > implementation is faster than LL/SC or lse implementation. Remove\n> > > HAVE_CMPXCHG_LOCAL for better performance on arm64.\n> > > \n> > > Tested on Quad 1.9GHZ CA55 platform:\n> > > average mod_node_page_state() cost decreases from 167ns to 103ns\n> > > the spawn (30 duration) benchmark in unixbench is improved\n> > > from 147494 lps to 150561 lps, improved by 2.1%\n> > > \n> > > Tested on Quad 2.1GHZ CA73 platform:\n> > > average mod_node_page_state() cost decreases from 113ns to 85ns\n> > > the spawn (30 duration) benchmark in unixbench is improved\n> > > from 209844 lps to 212581 lps, improved by 1.3%\n> > > \n> > > Signed-off-by: Jisheng Zhang <jszhang@kernel.org>\n> > \n> > Please note that mod_node_page_state() can be called in NMI context and\n> > generic disable/enable irq are not safe against NMIs (newer arm arch supports\n> > NMI).\n> \n> hmm, interesting...\n> \n> fgrep HAVE_NMI arch/*/Kconfig\n> then\n> fgrep HAVE_CMPXCHG_LOCAL arch/*/Kconfig\n> \n> shows that only x86, arm64, s390 and loongarch are safe, while arm,\n> powerpc and mips enable HAVE_NMI but missing HAVE_CMPXCHG_LOCAL, so\n> they rely on generic generic disable/enable irq version, so you imply\n> that these three arch are not safe considering mod_node_page_state()\n> in NMI context.\n\nYes it seems like it. For memcg stats, we use ARCH_HAVE_NMI_SAFE_CMPXCHG and\nARCH_HAS_NMI_SAFE_THIS_CPU_OPS config options to correctly handle the updates\nfrom NMI context. Maybe we need something similar for vmstat as well.\n\nSo arm, powerpc and mips does not have ARCH_HAS_NMI_SAFE_THIS_CPU_OPS but\npowerpc does have ARCH_HAVE_NMI_SAFE_CMPXCHG and arm has\nit for CPU_V7, CPU_V7M & CPU_V6K models.\n\nI wonder if we need to add complexity for these archs.\n\n",
          "reply_to": "Jisheng Zhang"
        },
        {
          "author": "Jisheng Zhang (author)",
          "summary": "Reviewer Jisheng Zhang questioned whether removing HAVE_CMPXCHG_LOCAL would improve performance, citing a misunderstanding of the patch's implications.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "questioning",
            "clarification"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Mon, Feb 16, 2026 at 08:59:17PM +0530, Dev Jain wrote:\n> \n> On 16/02/26 4:30 pm, Will Deacon wrote:\n> > On Sun, Feb 15, 2026 at 11:39:44AM +0800, Jisheng Zhang wrote:\n> >> It turns out the generic disable/enable irq this_cpu_cmpxchg\n> >> implementation is faster than LL/SC or lse implementation. Remove\n> >> HAVE_CMPXCHG_LOCAL for better performance on arm64.\n> >>\n> >> Tested on Quad 1.9GHZ CA55 platform:\n> >> average mod_node_page_state() cost decreases from 167ns to 103ns\n> >> the spawn (30 duration) benchmark in unixbench is improved\n> >> from 147494 lps to 150561 lps, improved by 2.1%\n> >>\n> >> Tested on Quad 2.1GHZ CA73 platform:\n> >> average mod_node_page_state() cost decreases from 113ns to 85ns\n> >> the spawn (30 duration) benchmark in unixbench is improved\n> >> from 209844 lps to 212581 lps, improved by 1.3%\n> >>\n> >> Signed-off-by: Jisheng Zhang <jszhang@kernel.org>\n> >> ---\n> >>  arch/arm64/Kconfig              |  1 -\n> >>  arch/arm64/include/asm/percpu.h | 24 ------------------------\n> >>  2 files changed, 25 deletions(-)\n> > That is _entirely_ dependent on the system, so this isn't the right\n> > approach. I also don't think it's something we particularly want to\n> > micro-optimise to accomodate systems that suck at atomics.\n\nHi Will,\n\nI read this as an implication that the cmpxchg_local version is better\nthan generic disable/enable irq version on the newer arm64 systems. Is my\nunderstanding correct?\n\n> \n> Hi Will,\n> \n> As I mention in the other email, the suspect is not the atomics, but\n> preempt_disable(). On Apple M3, the regression reported in [1] resolves\n> by removing preempt_disable/enable in _pcp_protect_return. To prove\n> this another way, I disabled CONFIG_ARM64_HAS_LSE_ATOMICS and the\n> regression worsened, indicating that at least on Apple M3 the\n> atomics are faster.\n> \n> It may help to confirm this hypothesis on other hardware - perhaps\n> Jisheng can test with this change on his hardware and confirm\n> whether he gets the same performance improvement.\n\nHi Dev,\n\nThanks for the hints. I tried to remove the preempt_disable/enable from\n_pcp_protect_return, it improves, but the HAVE_CMPXCHG_LOCAL version is\nstill worse than generic disable/enable irq version on CA55 and CA73.\n\n> \n> By coincidence, Yang Shi has been discussing the this_cpu_* overhead\n> at [2].\n> \n> [1] https://lore.kernel.org/all/1052a452-9ba3-4da7-be47-7d27d27b3d1d@arm.com/\n> [2] https://lore.kernel.org/all/CAHbLzkpcN-T8MH6=W3jCxcFj1gVZp8fRqe231yzZT-rV_E_org@mail.gmail.com/\n> \n> >\n> > Will\n> >\n> >> diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig\n> >> index 38dba5f7e4d2..5e7e2e65d5a5 100644\n> >> --- a/arch/arm64/Kconfig\n> >> +++ b/arch/arm64/Kconfig\n> >> @@ -205,7 +205,6 @@ config ARM64\n> >>  \tselect HAVE_EBPF_JIT\n> >>  \tselect HAVE_C_RECORDMCOUNT\n> >>  \tselect HAVE_CMPXCHG_DOUBLE\n> >> -\tselect HAVE_CMPXCHG_LOCAL\n> >>  \tselect HAVE_CONTEXT_TRACKING_USER\n> >>  \tselect HAVE_DEBUG_KMEMLEAK\n> >>  \tselect HAVE_DMA_CONTIGUOUS\n> >> diff --git a/arch/arm64/include/asm/percpu.h b/arch/arm64/include/asm/percpu.h\n> >> index b57b2bb00967..70ffe566cb4b 100644\n> >> --- a/arch/arm64/include/asm/percpu.h\n> >> +++ b/arch/arm64/include/asm/percpu.h\n> >> @@ -232,30 +232,6 @@ PERCPU_RET_OP(add, add, ldadd)\n> >>  #define this_cpu_xchg_8(pcp, val)\t\\\n> >>  \t_pcp_protect_return(xchg_relaxed, pcp, val)\n> >>  \n> >> -#define this_cpu_cmpxchg_1(pcp, o, n)\t\\\n> >> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n> >> -#define this_cpu_cmpxchg_2(pcp, o, n)\t\\\n> >> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n> >> -#define this_cpu_cmpxchg_4(pcp, o, n)\t\\\n> >> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n> >> -#define this_cpu_cmpxchg_8(pcp, o, n)\t\\\n> >> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n> >> -\n> >> -#define this_cpu_cmpxchg64(pcp, o, n)\tthis_cpu_cmpxchg_8(pcp, o, n)\n> >> -\n> >> -#define this_cpu_cmpxchg128(pcp, o, n)\t\t\t\t\t\\\n> >> -({\t\t\t\t\t\t\t\t\t\\\n> >> -\ttypedef typeof(pcp) pcp_op_T__;\t\t\t\t\t\\\n> >> -\tu128 old__, new__, ret__;\t\t\t\t\t\\\n> >> -\tpcp_op_T__ *ptr__;\t\t\t\t\t\t\\\n> >> -\told__ = o;\t\t\t\t\t\t\t\\\n> >> -\tnew__ = n;\t\t\t\t\t\t\t\\\n> >> -\tpreempt_disable_notrace();\t\t\t\t\t\\\n> >> -\tptr__ = raw_cpu_ptr(&(pcp));\t\t\t\t\t\\\n> >> -\tret__ = cmpxchg128_local((void *)ptr__, old__, new__);\t\t\\\n> >> -\tpreempt_enable_notrace();\t\t\t\t\t\\\n> >> -\tret__;\t\t\t\t\t\t\t\t\\\n> >> -})\n> >>  \n> >>  #ifdef __KVM_NVHE_HYPERVISOR__\n> >>  extern unsigned long __hyp_per_cpu_offset(unsigned int cpu);\n> >> -- \n> >> 2.51.0\n> >>\n",
          "reply_to": "Dev Jain"
        },
        {
          "author": "Jisheng Zhang (author)",
          "summary": "The reviewer pointed out that the patch's assumption about arm64 being safe without HAVE_CMPXCHG_LOCAL is incorrect, as other architectures like arm, powerpc, and mips also rely on the generic implementation in NMI context.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Wed, Feb 18, 2026 at 02:07:57PM -0800, Shakeel Butt wrote:\n> On Sun, Feb 15, 2026 at 11:39:44AM +0800, Jisheng Zhang wrote:\n> > It turns out the generic disable/enable irq this_cpu_cmpxchg\n> > implementation is faster than LL/SC or lse implementation. Remove\n> > HAVE_CMPXCHG_LOCAL for better performance on arm64.\n> > \n> > Tested on Quad 1.9GHZ CA55 platform:\n> > average mod_node_page_state() cost decreases from 167ns to 103ns\n> > the spawn (30 duration) benchmark in unixbench is improved\n> > from 147494 lps to 150561 lps, improved by 2.1%\n> > \n> > Tested on Quad 2.1GHZ CA73 platform:\n> > average mod_node_page_state() cost decreases from 113ns to 85ns\n> > the spawn (30 duration) benchmark in unixbench is improved\n> > from 209844 lps to 212581 lps, improved by 1.3%\n> > \n> > Signed-off-by: Jisheng Zhang <jszhang@kernel.org>\n> \n> Please note that mod_node_page_state() can be called in NMI context and\n> generic disable/enable irq are not safe against NMIs (newer arm arch supports\n> NMI).\n\nhmm, interesting...\n\nfgrep HAVE_NMI arch/*/Kconfig\nthen\nfgrep HAVE_CMPXCHG_LOCAL arch/*/Kconfig\n\nshows that only x86, arm64, s390 and loongarch are safe, while arm,\npowerpc and mips enable HAVE_NMI but missing HAVE_CMPXCHG_LOCAL, so\nthey rely on generic generic disable/enable irq version, so you imply\nthat these three arch are not safe considering mod_node_page_state()\nin NMI context.\n",
          "reply_to": "Shakeel Butt"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    }
  }
}