{
  "thread_id": "aZY30X9bpsV-nPsm@linux.dev",
  "subject": "Re: [PATCH] arm64: remove HAVE_CMPXCHG_LOCAL",
  "url": "https://lore.kernel.org/all/aZY30X9bpsV-nPsm@linux.dev/",
  "dates": {
    "2026-02-18": {
      "report_file": "2026-02-18_ollama_llama3.1-8b.html",
      "developer": "Shakeel Butt",
      "reviews": [
        {
          "author": "Dev Jain",
          "summary": "Reviewer Dev Jain suggests that the patch should only remove the selection of the config, but keep the code, as the real issue may be related to preempt_disable() and not LL/SC/LSE or atomics.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "\nOn 15/02/26 9:09 am, Jisheng Zhang wrote:\n> It turns out the generic disable/enable irq this_cpu_cmpxchg\n> implementation is faster than LL/SC or lse implementation. Remove\n> HAVE_CMPXCHG_LOCAL for better performance on arm64.\n>\n> Tested on Quad 1.9GHZ CA55 platform:\n> average mod_node_page_state() cost decreases from 167ns to 103ns\n> the spawn (30 duration) benchmark in unixbench is improved\n> from 147494 lps to 150561 lps, improved by 2.1%\n>\n> Tested on Quad 2.1GHZ CA73 platform:\n> average mod_node_page_state() cost decreases from 113ns to 85ns\n> the spawn (30 duration) benchmark in unixbench is improved\n> from 209844 lps to 212581 lps, improved by 1.3%\n>\n> Signed-off-by: Jisheng Zhang <jszhang@kernel.org>\n> ---\n\nThanks. This concurs with my investigation on [1]. The problem\nisn't really LL/SC/LSE but preempt_disable()/enable() in\nthis_cpu_* [1, 2].\n\nI think you should only remove the selection of the config,\nbut keep the code? We may want to switch this on again if\nthe real issue gets solved.\n\n[1] https://lore.kernel.org/all/5a6782f3-d758-4d9c-975b-5ae4b5d80d4e@arm.com/\n[2] https://lore.kernel.org/all/CAHbLzkpcN-T8MH6=W3jCxcFj1gVZp8fRqe231yzZT-rV_E_org@mail.gmail.com/\n\n>  arch/arm64/Kconfig              |  1 -\n>  arch/arm64/include/asm/percpu.h | 24 ------------------------\n>  2 files changed, 25 deletions(-)\n>\n> diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig\n> index 38dba5f7e4d2..5e7e2e65d5a5 100644\n> --- a/arch/arm64/Kconfig\n> +++ b/arch/arm64/Kconfig\n> @@ -205,7 +205,6 @@ config ARM64\n>  \tselect HAVE_EBPF_JIT\n>  \tselect HAVE_C_RECORDMCOUNT\n>  \tselect HAVE_CMPXCHG_DOUBLE\n> -\tselect HAVE_CMPXCHG_LOCAL\n>  \tselect HAVE_CONTEXT_TRACKING_USER\n>  \tselect HAVE_DEBUG_KMEMLEAK\n>  \tselect HAVE_DMA_CONTIGUOUS\n> diff --git a/arch/arm64/include/asm/percpu.h b/arch/arm64/include/asm/percpu.h\n> index b57b2bb00967..70ffe566cb4b 100644\n> --- a/arch/arm64/include/asm/percpu.h\n> +++ b/arch/arm64/include/asm/percpu.h\n> @@ -232,30 +232,6 @@ PERCPU_RET_OP(add, add, ldadd)\n>  #define this_cpu_xchg_8(pcp, val)\t\\\n>  \t_pcp_protect_return(xchg_relaxed, pcp, val)\n>  \n> -#define this_cpu_cmpxchg_1(pcp, o, n)\t\\\n> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n> -#define this_cpu_cmpxchg_2(pcp, o, n)\t\\\n> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n> -#define this_cpu_cmpxchg_4(pcp, o, n)\t\\\n> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n> -#define this_cpu_cmpxchg_8(pcp, o, n)\t\\\n> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n> -\n> -#define this_cpu_cmpxchg64(pcp, o, n)\tthis_cpu_cmpxchg_8(pcp, o, n)\n> -\n> -#define this_cpu_cmpxchg128(pcp, o, n)\t\t\t\t\t\\\n> -({\t\t\t\t\t\t\t\t\t\\\n> -\ttypedef typeof(pcp) pcp_op_T__;\t\t\t\t\t\\\n> -\tu128 old__, new__, ret__;\t\t\t\t\t\\\n> -\tpcp_op_T__ *ptr__;\t\t\t\t\t\t\\\n> -\told__ = o;\t\t\t\t\t\t\t\\\n> -\tnew__ = n;\t\t\t\t\t\t\t\\\n> -\tpreempt_disable_notrace();\t\t\t\t\t\\\n> -\tptr__ = raw_cpu_ptr(&(pcp));\t\t\t\t\t\\\n> -\tret__ = cmpxchg128_local((void *)ptr__, old__, new__);\t\t\\\n> -\tpreempt_enable_notrace();\t\t\t\t\t\\\n> -\tret__;\t\t\t\t\t\t\t\t\\\n> -})\n>  \n>  #ifdef __KVM_NVHE_HYPERVISOR__\n>  extern unsigned long __hyp_per_cpu_offset(unsigned int cpu);\n\n\n---\n\n\nOn 16/02/26 4:30 pm, Will Deacon wrote:\n> On Sun, Feb 15, 2026 at 11:39:44AM +0800, Jisheng Zhang wrote:\n>> It turns out the generic disable/enable irq this_cpu_cmpxchg\n>> implementation is faster than LL/SC or lse implementation. Remove\n>> HAVE_CMPXCHG_LOCAL for better performance on arm64.\n>>\n>> Tested on Quad 1.9GHZ CA55 platform:\n>> average mod_node_page_state() cost decreases from 167ns to 103ns\n>> the spawn (30 duration) benchmark in unixbench is improved\n>> from 147494 lps to 150561 lps, improved by 2.1%\n>>\n>> Tested on Quad 2.1GHZ CA73 platform:\n>> average mod_node_page_state() cost decreases from 113ns to 85ns\n>> the spawn (30 duration) benchmark in unixbench is improved\n>> from 209844 lps to 212581 lps, improved by 1.3%\n>>\n>> Signed-off-by: Jisheng Zhang <jszhang@kernel.org>\n>> ---\n>>  arch/arm64/Kconfig              |  1 -\n>>  arch/arm64/include/asm/percpu.h | 24 ------------------------\n>>  2 files changed, 25 deletions(-)\n> That is _entirely_ dependent on the system, so this isn't the right\n> approach. I also don't think it's something we particularly want to\n> micro-optimise to accomodate systems that suck at atomics.\n\nHi Will,\n\nAs I mention in the other email, the suspect is not the atomics, but\npreempt_disable(). On Apple M3, the regression reported in [1] resolves\nby removing preempt_disable/enable in _pcp_protect_return. To prove\nthis another way, I disabled CONFIG_ARM64_HAS_LSE_ATOMICS and the\nregression worsened, indicating that at least on Apple M3 the\natomics are faster.\n\nIt may help to confirm this hypothesis on other hardware - perhaps\nJisheng can test with this change on his hardware and confirm\nwhether he gets the same performance improvement.\n\nBy coincidence, Yang Shi has been discussing the this_cpu_* overhead\nat [2].\n\n[1] https://lore.kernel.org/all/1052a452-9ba3-4da7-be47-7d27d27b3d1d@arm.com/\n[2] https://lore.kernel.org/all/CAHbLzkpcN-T8MH6=W3jCxcFj1gVZp8fRqe231yzZT-rV_E_org@mail.gmail.com/\n\n>\n> Will\n>\n>> diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig\n>> index 38dba5f7e4d2..5e7e2e65d5a5 100644\n>> --- a/arch/arm64/Kconfig\n>> +++ b/arch/arm64/Kconfig\n>> @@ -205,7 +205,6 @@ config ARM64\n>>  \tselect HAVE_EBPF_JIT\n>>  \tselect HAVE_C_RECORDMCOUNT\n>>  \tselect HAVE_CMPXCHG_DOUBLE\n>> -\tselect HAVE_CMPXCHG_LOCAL\n>>  \tselect HAVE_CONTEXT_TRACKING_USER\n>>  \tselect HAVE_DEBUG_KMEMLEAK\n>>  \tselect HAVE_DMA_CONTIGUOUS\n>> diff --git a/arch/arm64/include/asm/percpu.h b/arch/arm64/include/asm/percpu.h\n>> index b57b2bb00967..70ffe566cb4b 100644\n>> --- a/arch/arm64/include/asm/percpu.h\n>> +++ b/arch/arm64/include/asm/percpu.h\n>> @@ -232,30 +232,6 @@ PERCPU_RET_OP(add, add, ldadd)\n>>  #define this_cpu_xchg_8(pcp, val)\t\\\n>>  \t_pcp_protect_return(xchg_relaxed, pcp, val)\n>>  \n>> -#define this_cpu_cmpxchg_1(pcp, o, n)\t\\\n>> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n>> -#define this_cpu_cmpxchg_2(pcp, o, n)\t\\\n>> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n>> -#define this_cpu_cmpxchg_4(pcp, o, n)\t\\\n>> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n>> -#define this_cpu_cmpxchg_8(pcp, o, n)\t\\\n>> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n>> -\n>> -#define this_cpu_cmpxchg64(pcp, o, n)\tthis_cpu_cmpxchg_8(pcp, o, n)\n>> -\n>> -#define this_cpu_cmpxchg128(pcp, o, n)\t\t\t\t\t\\\n>> -({\t\t\t\t\t\t\t\t\t\\\n>> -\ttypedef typeof(pcp) pcp_op_T__;\t\t\t\t\t\\\n>> -\tu128 old__, new__, ret__;\t\t\t\t\t\\\n>> -\tpcp_op_T__ *ptr__;\t\t\t\t\t\t\\\n>> -\told__ = o;\t\t\t\t\t\t\t\\\n>> -\tnew__ = n;\t\t\t\t\t\t\t\\\n>> -\tpreempt_disable_notrace();\t\t\t\t\t\\\n>> -\tptr__ = raw_cpu_ptr(&(pcp));\t\t\t\t\t\\\n>> -\tret__ = cmpxchg128_local((void *)ptr__, old__, new__);\t\t\\\n>> -\tpreempt_enable_notrace();\t\t\t\t\t\\\n>> -\tret__;\t\t\t\t\t\t\t\t\\\n>> -})\n>>  \n>>  #ifdef __KVM_NVHE_HYPERVISOR__\n>>  extern unsigned long __hyp_per_cpu_offset(unsigned int cpu);\n>> -- \n>> 2.51.0\n>>\n"
        },
        {
          "author": "Will Deacon",
          "summary": "Reviewer Will Deacon raised concerns about the patch's approach to optimizing atomic operations on arm64, suggesting it is system-dependent and not a good candidate for micro-optimization.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "On Sun, Feb 15, 2026 at 11:39:44AM +0800, Jisheng Zhang wrote:\n> It turns out the generic disable/enable irq this_cpu_cmpxchg\n> implementation is faster than LL/SC or lse implementation. Remove\n> HAVE_CMPXCHG_LOCAL for better performance on arm64.\n> \n> Tested on Quad 1.9GHZ CA55 platform:\n> average mod_node_page_state() cost decreases from 167ns to 103ns\n> the spawn (30 duration) benchmark in unixbench is improved\n> from 147494 lps to 150561 lps, improved by 2.1%\n> \n> Tested on Quad 2.1GHZ CA73 platform:\n> average mod_node_page_state() cost decreases from 113ns to 85ns\n> the spawn (30 duration) benchmark in unixbench is improved\n> from 209844 lps to 212581 lps, improved by 1.3%\n> \n> Signed-off-by: Jisheng Zhang <jszhang@kernel.org>\n> ---\n>  arch/arm64/Kconfig              |  1 -\n>  arch/arm64/include/asm/percpu.h | 24 ------------------------\n>  2 files changed, 25 deletions(-)\n\nThat is _entirely_ dependent on the system, so this isn't the right\napproach. I also don't think it's something we particularly want to\nmicro-optimise to accomodate systems that suck at atomics.\n\nWill\n\n> diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig\n> index 38dba5f7e4d2..5e7e2e65d5a5 100644\n> --- a/arch/arm64/Kconfig\n> +++ b/arch/arm64/Kconfig\n> @@ -205,7 +205,6 @@ config ARM64\n>  \tselect HAVE_EBPF_JIT\n>  \tselect HAVE_C_RECORDMCOUNT\n>  \tselect HAVE_CMPXCHG_DOUBLE\n> -\tselect HAVE_CMPXCHG_LOCAL\n>  \tselect HAVE_CONTEXT_TRACKING_USER\n>  \tselect HAVE_DEBUG_KMEMLEAK\n>  \tselect HAVE_DMA_CONTIGUOUS\n> diff --git a/arch/arm64/include/asm/percpu.h b/arch/arm64/include/asm/percpu.h\n> index b57b2bb00967..70ffe566cb4b 100644\n> --- a/arch/arm64/include/asm/percpu.h\n> +++ b/arch/arm64/include/asm/percpu.h\n> @@ -232,30 +232,6 @@ PERCPU_RET_OP(add, add, ldadd)\n>  #define this_cpu_xchg_8(pcp, val)\t\\\n>  \t_pcp_protect_return(xchg_relaxed, pcp, val)\n>  \n> -#define this_cpu_cmpxchg_1(pcp, o, n)\t\\\n> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n> -#define this_cpu_cmpxchg_2(pcp, o, n)\t\\\n> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n> -#define this_cpu_cmpxchg_4(pcp, o, n)\t\\\n> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n> -#define this_cpu_cmpxchg_8(pcp, o, n)\t\\\n> -\t_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)\n> -\n> -#define this_cpu_cmpxchg64(pcp, o, n)\tthis_cpu_cmpxchg_8(pcp, o, n)\n> -\n> -#define this_cpu_cmpxchg128(pcp, o, n)\t\t\t\t\t\\\n> -({\t\t\t\t\t\t\t\t\t\\\n> -\ttypedef typeof(pcp) pcp_op_T__;\t\t\t\t\t\\\n> -\tu128 old__, new__, ret__;\t\t\t\t\t\\\n> -\tpcp_op_T__ *ptr__;\t\t\t\t\t\t\\\n> -\told__ = o;\t\t\t\t\t\t\t\\\n> -\tnew__ = n;\t\t\t\t\t\t\t\\\n> -\tpreempt_disable_notrace();\t\t\t\t\t\\\n> -\tptr__ = raw_cpu_ptr(&(pcp));\t\t\t\t\t\\\n> -\tret__ = cmpxchg128_local((void *)ptr__, old__, new__);\t\t\\\n> -\tpreempt_enable_notrace();\t\t\t\t\t\\\n> -\tret__;\t\t\t\t\t\t\t\t\\\n> -})\n>  \n>  #ifdef __KVM_NVHE_HYPERVISOR__\n>  extern unsigned long __hyp_per_cpu_offset(unsigned int cpu);\n> -- \n> 2.51.0\n> \n\n\n---\n\nOn Tue, Feb 17, 2026 at 01:53:19PM +0000, Catalin Marinas wrote:\n> On Mon, Feb 16, 2026 at 08:59:17PM +0530, Dev Jain wrote:\n> > On 16/02/26 4:30 pm, Will Deacon wrote:\n> > > On Sun, Feb 15, 2026 at 11:39:44AM +0800, Jisheng Zhang wrote:\n> > >> It turns out the generic disable/enable irq this_cpu_cmpxchg\n> > >> implementation is faster than LL/SC or lse implementation. Remove\n> > >> HAVE_CMPXCHG_LOCAL for better performance on arm64.\n> > >>\n> > >> Tested on Quad 1.9GHZ CA55 platform:\n> > >> average mod_node_page_state() cost decreases from 167ns to 103ns\n> > >> the spawn (30 duration) benchmark in unixbench is improved\n> > >> from 147494 lps to 150561 lps, improved by 2.1%\n> > >>\n> > >> Tested on Quad 2.1GHZ CA73 platform:\n> > >> average mod_node_page_state() cost decreases from 113ns to 85ns\n> > >> the spawn (30 duration) benchmark in unixbench is improved\n> > >> from 209844 lps to 212581 lps, improved by 1.3%\n> > >>\n> > >> Signed-off-by: Jisheng Zhang <jszhang@kernel.org>\n> > >> ---\n> > >>  arch/arm64/Kconfig              |  1 -\n> > >>  arch/arm64/include/asm/percpu.h | 24 ------------------------\n> > >>  2 files changed, 25 deletions(-)\n> > > That is _entirely_ dependent on the system, so this isn't the right\n> > > approach. I also don't think it's something we particularly want to\n> > > micro-optimise to accomodate systems that suck at atomics.\n> > \n> > Hi Will,\n> > \n> > As I mention in the other email, the suspect is not the atomics, but\n> > preempt_disable(). On Apple M3, the regression reported in [1] resolves\n> > by removing preempt_disable/enable in _pcp_protect_return. To prove\n> > this another way, I disabled CONFIG_ARM64_HAS_LSE_ATOMICS and the\n> > regression worsened, indicating that at least on Apple M3 the\n> > atomics are faster.\n> \n> Then why don't we replace the preempt disabling with local_irq_save()\n> in the arm64 code and still use the LSE atomics?\n\nEven better, work on making preempt_disable() faster as it's used in many\nother places. Of course, if people want to hack the .config, they could\nalso change the preemption mode...\n\nWill\n"
        },
        {
          "author": "Catalin Marinas",
          "summary": "The reviewer raised concerns about the patch removing HAVE_CMPXCHG_LOCAL, suggesting that using local_irq_save() instead of preempt_disable_notrace() would be a better approach to avoid potential issues with atomicity and preemption.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Mon, Feb 16, 2026 at 08:59:17PM +0530, Dev Jain wrote:\n> On 16/02/26 4:30 pm, Will Deacon wrote:\n> > On Sun, Feb 15, 2026 at 11:39:44AM +0800, Jisheng Zhang wrote:\n> >> It turns out the generic disable/enable irq this_cpu_cmpxchg\n> >> implementation is faster than LL/SC or lse implementation. Remove\n> >> HAVE_CMPXCHG_LOCAL for better performance on arm64.\n> >>\n> >> Tested on Quad 1.9GHZ CA55 platform:\n> >> average mod_node_page_state() cost decreases from 167ns to 103ns\n> >> the spawn (30 duration) benchmark in unixbench is improved\n> >> from 147494 lps to 150561 lps, improved by 2.1%\n> >>\n> >> Tested on Quad 2.1GHZ CA73 platform:\n> >> average mod_node_page_state() cost decreases from 113ns to 85ns\n> >> the spawn (30 duration) benchmark in unixbench is improved\n> >> from 209844 lps to 212581 lps, improved by 1.3%\n> >>\n> >> Signed-off-by: Jisheng Zhang <jszhang@kernel.org>\n> >> ---\n> >>  arch/arm64/Kconfig              |  1 -\n> >>  arch/arm64/include/asm/percpu.h | 24 ------------------------\n> >>  2 files changed, 25 deletions(-)\n> > That is _entirely_ dependent on the system, so this isn't the right\n> > approach. I also don't think it's something we particularly want to\n> > micro-optimise to accomodate systems that suck at atomics.\n> \n> Hi Will,\n> \n> As I mention in the other email, the suspect is not the atomics, but\n> preempt_disable(). On Apple M3, the regression reported in [1] resolves\n> by removing preempt_disable/enable in _pcp_protect_return. To prove\n> this another way, I disabled CONFIG_ARM64_HAS_LSE_ATOMICS and the\n> regression worsened, indicating that at least on Apple M3 the\n> atomics are faster.\n\nThen why don't we replace the preempt disabling with local_irq_save()\nin the arm64 code and still use the LSE atomics?\n\nIIUC (lots of macro indirection), the generic cmpxchg is not atomic, so\nanother CPU is allowed to mess this up if it accesses current CPU's\nvariable via per_cpu_ptr().\n\n-- \nCatalin\n\n\n---\n\nOn Tue, Feb 17, 2026 at 03:00:22PM +0000, Will Deacon wrote:\n> On Tue, Feb 17, 2026 at 01:53:19PM +0000, Catalin Marinas wrote:\n> > On Mon, Feb 16, 2026 at 08:59:17PM +0530, Dev Jain wrote:\n> > > On 16/02/26 4:30 pm, Will Deacon wrote:\n> > > > On Sun, Feb 15, 2026 at 11:39:44AM +0800, Jisheng Zhang wrote:\n> > > >> It turns out the generic disable/enable irq this_cpu_cmpxchg\n> > > >> implementation is faster than LL/SC or lse implementation. Remove\n> > > >> HAVE_CMPXCHG_LOCAL for better performance on arm64.\n> > > >>\n> > > >> Tested on Quad 1.9GHZ CA55 platform:\n> > > >> average mod_node_page_state() cost decreases from 167ns to 103ns\n> > > >> the spawn (30 duration) benchmark in unixbench is improved\n> > > >> from 147494 lps to 150561 lps, improved by 2.1%\n> > > >>\n> > > >> Tested on Quad 2.1GHZ CA73 platform:\n> > > >> average mod_node_page_state() cost decreases from 113ns to 85ns\n> > > >> the spawn (30 duration) benchmark in unixbench is improved\n> > > >> from 209844 lps to 212581 lps, improved by 1.3%\n[...]\n> > > > That is _entirely_ dependent on the system, so this isn't the right\n> > > > approach. I also don't think it's something we particularly want to\n> > > > micro-optimise to accomodate systems that suck at atomics.\n> > > \n> > > As I mention in the other email, the suspect is not the atomics, but\n> > > preempt_disable(). On Apple M3, the regression reported in [1] resolves\n> > > by removing preempt_disable/enable in _pcp_protect_return. To prove\n> > > this another way, I disabled CONFIG_ARM64_HAS_LSE_ATOMICS and the\n> > > regression worsened, indicating that at least on Apple M3 the\n> > > atomics are faster.\n> > \n> > Then why don't we replace the preempt disabling with local_irq_save()\n> > in the arm64 code and still use the LSE atomics?\n> \n> Even better, work on making preempt_disable() faster as it's used in many\n> other places.\n\nYes, that would be good. It's the preempt_enable_notrace() path that\nends up calling preempt_schedule_notrace() -> __schedule() pretty much\nunconditionally. Not sure what would go wrong but some simple change\nlike this (can be done at a higher in the preempt macros to even avoid\ngetting here):\n\ndiff --git a/kernel/sched/core.c b/kernel/sched/core.c\nindex 854984967fe2..d9a5d6438303 100644\n--- a/kernel/sched/core.c\n+++ b/kernel/sched/core.c\n@@ -7119,7 +7119,7 @@ asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)\n \tif (likely(!preemptible()))\n \t\treturn;\n \n-\tdo {\n+\twhile (need_resched()) {\n \t\t/*\n \t\t * Because the function tracer can trace preempt_count_sub()\n \t\t * and it also uses preempt_enable/disable_notrace(), if\n@@ -7146,7 +7146,7 @@ asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)\n \n \t\tpreempt_latency_stop(1);\n \t\tpreempt_enable_no_resched_notrace();\n-\t} while (need_resched());\n+\t}\n }\n EXPORT_SYMBOL_GPL(preempt_schedule_notrace);\n \n\nOf course, changing the preemption model solves this by making the\nmacros no-ops but I assume people want to keep preemption on.\n\n-- \nCatalin\n\n\n---\n\nHi Prateek,\n\nOn Wed, Feb 18, 2026 at 09:31:19AM +0530, K Prateek Nayak wrote:\n> On 2/17/2026 10:18 PM, Catalin Marinas wrote:\n> > Yes, that would be good. It's the preempt_enable_notrace() path that\n> > ends up calling preempt_schedule_notrace() -> __schedule() pretty much\n> > unconditionally.\n> \n> What do you mean by unconditionally? We always check\n> __preempt_count_dec_and_test() before calling into __schedule().\n> \n> On x86, We use MSB of preempt_count to indicate a resched and\n> set_preempt_need_resched() would just clear this MSB.\n> \n> If the preempt_count() turns 0, we immediately go into schedule\n> or  or the next preempt_enable() -> __preempt_count_dec_and_test()\n> would see the entire preempt_count being clear and will call into\n> schedule.\n> \n> The arm64 implementation seems to be doing something similar too\n> with a separate \"ti->preempt.need_resched\" bit which is part of\n> the \"ti->preempt_count\"'s union so it isn't really unconditional.\n\nAh, yes, you are right. I got the polarity of need_resched in\nthread_info wrong (we should have named it no_need_to_resched).\n\nSo in the common case, the overhead is caused by the additional\npointer chase and preempt_count update, on top of the cpu offset read.\nNot sure we can squeeze any more cycles out of these without some\nlarge overhaul like:\n\nhttps://git.kernel.org/mark/c/84ee5f23f93d4a650e828f831da9ed29c54623c5\n\nor Yang's per-CPU page tables. Well, there are more ideas like in-kernel\nrestartable sequences but they move the overhead elsewhere.\n\nThanks.\n\n-- \nCatalin\n"
        },
        {
          "author": "Christoph (Ampere)",
          "summary": "Christoph raised concerns that the patch may cause regressions on certain platforms, citing his own measurements from 2 years ago, but noted that preempt_disable/enable overhead is not incurred in production systems.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "regressions",
            "platform-specific"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Mon, 16 Feb 2026, Dev Jain wrote:\n\n> By coincidence, Yang Shi has been discussing the this_cpu_* overhead\n> at [2].\n\n\nYang Shi is on vacation but we have a patchset that removes\npreempt_enable/disable from this_cpu operations on ARM64.\n\nThe performance of cmpxchg varies by platform in use and with the kernel\nconfig. The measurements that I did 2 years ago indicated that the cmpxchg\nuse with Ampere processors did not cause a regression.\n\nNote that distro kernels often do not enable PREEMPT_FULL and therefore\npreempt_disable/enable overhead is not incurred in production systems.\n\nPREEMPT_VOLUNTARY does not use preemption for this_cpu ops.\n\n"
        },
        {
          "author": "K Nayak",
          "summary": "Reviewer K Nayak questioned the patch's description of removing HAVE_CMPXCHG_LOCAL as 'unconditional', pointing out that arm64 implementation checks preempt_count before calling __schedule() and has a similar need_resched bit.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": true,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Hello Catalin,\n\nOn 2/17/2026 10:18 PM, Catalin Marinas wrote:\n> Yes, that would be good. It's the preempt_enable_notrace() path that\n> ends up calling preempt_schedule_notrace() -> __schedule() pretty much\n> unconditionally.\n\nWhat do you mean by unconditionally? We always check\n__preempt_count_dec_and_test() before calling into __schedule().\n\nOn x86, We use MSB of preempt_count to indicate a resched and\nset_preempt_need_resched() would just clear this MSB.\n\nIf the preempt_count() turns 0, we immediately go into schedule\nor  or the next preempt_enable() -> __preempt_count_dec_and_test()\nwould see the entire preempt_count being clear and will call into\nschedule.\n\nThe arm64 implementation seems to be doing something similar too\nwith a separate \"ti->preempt.need_resched\" bit which is part of\nthe \"ti->preempt_count\"'s union so it isn't really unconditional.\n\n> Not sure what would go wrong but some simple change\n> like this (can be done at a higher in the preempt macros to even avoid\n> getting here):\n> \n> diff --git a/kernel/sched/core.c b/kernel/sched/core.c\n> index 854984967fe2..d9a5d6438303 100644\n> --- a/kernel/sched/core.c\n> +++ b/kernel/sched/core.c\n> @@ -7119,7 +7119,7 @@ asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)\n>  \tif (likely(!preemptible()))\n>  \t\treturn;\n>  \n> -\tdo {\n> +\twhile (need_resched()) {\n\nEssentially you are simply checking it twice now on entry since\nneed_resched() state would have already been communicated by\n__preempt_count_dec_and_test().\n\n>  \t\t/*\n>  \t\t * Because the function tracer can trace preempt_count_sub()\n>  \t\t * and it also uses preempt_enable/disable_notrace(), if\n> @@ -7146,7 +7146,7 @@ asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)\n>  \n>  \t\tpreempt_latency_stop(1);\n>  \t\tpreempt_enable_no_resched_notrace();\n> -\t} while (need_resched());\n> +\t}\n>  }\n>  EXPORT_SYMBOL_GPL(preempt_schedule_notrace);\n-- \nThanks and Regards,\nPrateek\n\n"
        },
        {
          "author": "Shakeel Butt",
          "summary": "Shakeel Butt raised a concern that mod_node_page_state() can be called in NMI context, and the generic disable/enable irq implementation is not safe against NMIs.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "concern about NMI safety"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "On Sun, Feb 15, 2026 at 11:39:44AM +0800, Jisheng Zhang wrote:\n> It turns out the generic disable/enable irq this_cpu_cmpxchg\n> implementation is faster than LL/SC or lse implementation. Remove\n> HAVE_CMPXCHG_LOCAL for better performance on arm64.\n> \n> Tested on Quad 1.9GHZ CA55 platform:\n> average mod_node_page_state() cost decreases from 167ns to 103ns\n> the spawn (30 duration) benchmark in unixbench is improved\n> from 147494 lps to 150561 lps, improved by 2.1%\n> \n> Tested on Quad 2.1GHZ CA73 platform:\n> average mod_node_page_state() cost decreases from 113ns to 85ns\n> the spawn (30 duration) benchmark in unixbench is improved\n> from 209844 lps to 212581 lps, improved by 1.3%\n> \n> Signed-off-by: Jisheng Zhang <jszhang@kernel.org>\n\nPlease note that mod_node_page_state() can be called in NMI context and\ngeneric disable/enable irq are not safe against NMIs (newer arm arch supports\nNMI).\n\n\n"
        }
      ],
      "analysis_source": "llm"
    }
  }
}