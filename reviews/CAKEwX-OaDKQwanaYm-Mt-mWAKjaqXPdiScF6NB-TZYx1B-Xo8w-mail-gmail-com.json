{
  "thread_id": "CAKEwX=OaDKQwanaYm=Mt+mWAKjaqXPdiScF6NB=TZYx1B-Xo8w@mail.gmail.com",
  "subject": "[PATCH RFC 00/15] mm, swap: swap table phase IV with dynamic ghost swapfile",
  "url": "https://lore.kernel.org/all/CAKEwX=OaDKQwanaYm=Mt+mWAKjaqXPdiScF6NB=TZYx1B-Xo8w@mail.gmail.com/",
  "dates": {
    "2026-02-20": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Nhat Pham",
      "reviews": [
        {
          "author": "Kairui Song",
          "summary": "Reviewer Kairui Song suggested replacing the existing logic in swap_cache_alloc_folio() to handle allocation failure by returning an error code instead of trying to return the existing folio if it's cached, and proposed introducing wrappers that handle allocation failure differently for async swapin and readahead, as well as zswap swap out.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Instead of trying to return the existing folio if the entry is already\ncached, just return an error code if the allocation failed. And\nintroduce proper wrappers that handle the allocation failure in\ndifferent ways.\n\nFor async swapin and readahead, the caller only wants to ensure that a\nswap in read if the allocation succeeded, and for zswap swap out, the\ncaller will just abort if the allocation failed because the entry is\ngone or cached already.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n mm/swap.h       |   3 +-\n mm/swap_state.c | 177 +++++++++++++++++++++++++++++---------------------------\n mm/zswap.c      |  15 ++---\n 3 files changed, 98 insertions(+), 97 deletions(-)\n\ndiff --git a/mm/swap.h b/mm/swap.h\nindex a77016f2423b..ad8b17a93758 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -281,8 +281,7 @@ struct folio *swap_cache_get_folio(swp_entry_t entry);\n void *swap_cache_get_shadow(swp_entry_t entry);\n void swap_cache_del_folio(struct folio *folio);\n struct folio *swap_cache_alloc_folio(swp_entry_t entry, gfp_t gfp_flags,\n-\t\t\t\t     struct mempolicy *mpol, pgoff_t ilx,\n-\t\t\t\t     bool *alloced);\n+\t\t\t\t     struct mempolicy *mpol, pgoff_t ilx);\n /* Below helpers require the caller to lock and pass in the swap cluster. */\n void __swap_cache_add_folio(struct swap_cluster_info *ci,\n \t\t\t    struct folio *folio, swp_entry_t entry);\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 32d9d877bda8..53fa95059012 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -459,41 +459,24 @@ void swap_update_readahead(struct folio *folio, struct vm_area_struct *vma,\n  * All swap slots covered by the folio must have a non-zero swap count.\n  *\n  * Context: Caller must protect the swap device with reference count or locks.\n- * Return: Returns the folio being added on success. Returns the existing folio\n- * if @entry is already cached. Returns NULL if raced with swapin or swapoff.\n+ * Return: 0 if success, error code if failed.\n  */\n-static struct folio *__swap_cache_prepare_and_add(swp_entry_t entry,\n-\t\t\t\t\t\t  struct folio *folio,\n-\t\t\t\t\t\t  gfp_t gfp, bool charged)\n+static int __swap_cache_prepare_and_add(swp_entry_t entry,\n+\t\t\t\t\tstruct folio *folio,\n+\t\t\t\t\tgfp_t gfp, bool charged)\n {\n-\tstruct folio *swapcache = NULL;\n \tvoid *shadow;\n \tint ret;\n \n \t__folio_set_locked(folio);\n \t__folio_set_swapbacked(folio);\n-\tfor (;;) {\n-\t\tret = swap_cache_add_folio(folio, entry, &shadow);\n-\t\tif (!ret)\n-\t\t\tbreak;\n-\n-\t\t/*\n-\t\t * Large order allocation needs special handling on\n-\t\t * race: if a smaller folio exists in cache, swapin needs\n-\t\t * to fallback to order 0, and doing a swap cache lookup\n-\t\t * might return a folio that is irrelevant to the faulting\n-\t\t * entry because @entry is aligned down. Just return NULL.\n-\t\t */\n-\t\tif (ret != -EEXIST || folio_test_large(folio))\n-\t\t\tgoto failed;\n-\n-\t\tswapcache = swap_cache_get_folio(entry);\n-\t\tif (swapcache)\n-\t\t\tgoto failed;\n-\t}\n+\tret = swap_cache_add_folio(folio, entry, &shadow);\n+\tif (ret)\n+\t\tgoto failed;\n \n \tif (!charged && mem_cgroup_swapin_charge_folio(folio, NULL, gfp, entry)) {\n \t\tswap_cache_del_folio(folio);\n+\t\tret = -ENOMEM;\n \t\tgoto failed;\n \t}\n \n@@ -503,11 +486,11 @@ static struct folio *__swap_cache_prepare_and_add(swp_entry_t entry,\n \n \t/* Caller will initiate read into locked folio */\n \tfolio_add_lru(folio);\n-\treturn folio;\n+\treturn 0;\n \n failed:\n \tfolio_unlock(folio);\n-\treturn swapcache;\n+\treturn ret;\n }\n \n /**\n@@ -516,7 +499,6 @@ static struct folio *__swap_cache_prepare_and_add(swp_entry_t entry,\n  * @gfp_mask: memory allocation flags\n  * @mpol: NUMA memory allocation policy to be applied\n  * @ilx: NUMA interleave index, for use only when MPOL_INTERLEAVE\n- * @new_page_allocated: sets true if allocation happened, false otherwise\n  *\n  * Allocate a folio in the swap cache for one swap slot, typically before\n  * doing IO (e.g. swap in or zswap writeback). The swap slot indicated by\n@@ -524,18 +506,40 @@ static struct folio *__swap_cache_prepare_and_add(swp_entry_t entry,\n  * Currently only supports order 0.\n  *\n  * Context: Caller must protect the swap device with reference count or locks.\n- * Return: Returns the existing folio if @entry is cached already. Returns\n- * NULL if failed due to -ENOMEM or @entry have a swap count < 1.\n+ * Return: Returns the folio if allocation succeeded and folio is added to\n+ * swap cache. Returns error code if allocation failed due to race.\n  */\n struct folio *swap_cache_alloc_folio(swp_entry_t entry, gfp_t gfp_mask,\n-\t\t\t\t     struct mempolicy *mpol, pgoff_t ilx,\n-\t\t\t\t     bool *new_page_allocated)\n+\t\t\t\t     struct mempolicy *mpol, pgoff_t ilx)\n+{\n+\tint ret;\n+\tstruct folio *folio;\n+\n+\t/* Allocate a new folio to be added into the swap cache. */\n+\tfolio = folio_alloc_mpol(gfp_mask, 0, mpol, ilx, numa_node_id());\n+\tif (!folio)\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\t/*\n+\t * Try add the new folio, it returns NULL if already exist,\n+\t * since folio is order 0.\n+\t */\n+\tret = __swap_cache_prepare_and_add(entry, folio, gfp_mask, false);\n+\tif (ret) {\n+\t\tfolio_put(folio);\n+\t\treturn ERR_PTR(ret);\n+\t}\n+\n+\treturn folio;\n+}\n+\n+static struct folio *swap_cache_read_folio(swp_entry_t entry, gfp_t gfp,\n+\t\t\t\t\t   struct mempolicy *mpol, pgoff_t ilx,\n+\t\t\t\t\t   struct swap_iocb **plug, bool readahead)\n {\n \tstruct swap_info_struct *si = __swap_entry_to_info(entry);\n \tstruct folio *folio;\n-\tstruct folio *result = NULL;\n \n-\t*new_page_allocated = false;\n \t/* Check the swap cache again for readahead path. */\n \tfolio = swap_cache_get_folio(entry);\n \tif (folio)\n@@ -545,17 +549,24 @@ struct folio *swap_cache_alloc_folio(swp_entry_t entry, gfp_t gfp_mask,\n \tif (!swap_entry_swapped(si, entry))\n \t\treturn NULL;\n \n-\t/* Allocate a new folio to be added into the swap cache. */\n-\tfolio = folio_alloc_mpol(gfp_mask, 0, mpol, ilx, numa_node_id());\n-\tif (!folio)\n+\tdo {\n+\t\tfolio = swap_cache_get_folio(entry);\n+\t\tif (folio)\n+\t\t\treturn folio;\n+\n+\t\tfolio = swap_cache_alloc_folio(entry, gfp, mpol, ilx);\n+\t} while (PTR_ERR(folio) == -EEXIST);\n+\n+\tif (IS_ERR_OR_NULL(folio))\n \t\treturn NULL;\n-\t/* Try add the new folio, returns existing folio or NULL on failure. */\n-\tresult = __swap_cache_prepare_and_add(entry, folio, gfp_mask, false);\n-\tif (result == folio)\n-\t\t*new_page_allocated = true;\n-\telse\n-\t\tfolio_put(folio);\n-\treturn result;\n+\n+\tswap_read_folio(folio, plug);\n+\tif (readahead) {\n+\t\tfolio_set_readahead(folio);\n+\t\tcount_vm_event(SWAP_RA);\n+\t}\n+\n+\treturn folio;\n }\n \n /**\n@@ -574,15 +585,35 @@ struct folio *swap_cache_alloc_folio(swp_entry_t entry, gfp_t gfp_mask,\n  */\n struct folio *swapin_folio(swp_entry_t entry, struct folio *folio)\n {\n+\tint ret;\n \tstruct folio *swapcache;\n \tpgoff_t offset = swp_offset(entry);\n \tunsigned long nr_pages = folio_nr_pages(folio);\n \n \tentry = swp_entry(swp_type(entry), round_down(offset, nr_pages));\n-\tswapcache = __swap_cache_prepare_and_add(entry, folio, 0, true);\n-\tif (swapcache == folio)\n-\t\tswap_read_folio(folio, NULL);\n-\treturn swapcache;\n+\tfor (;;) {\n+\t\tret = __swap_cache_prepare_and_add(entry, folio, 0, true);\n+\t\tif (!ret) {\n+\t\t\tswap_read_folio(folio, NULL);\n+\t\t\tbreak;\n+\t\t}\n+\n+\t\t/*\n+\t\t * Large order allocation needs special handling on\n+\t\t * race: if a smaller folio exists in cache, swapin needs\n+\t\t * to fallback to order 0, and doing a swap cache lookup\n+\t\t * might return a folio that is irrelevant to the faulting\n+\t\t * entry because @entry is aligned down. Just return NULL.\n+\t\t */\n+\t\tif (ret != -EEXIST || nr_pages > 1)\n+\t\t\treturn NULL;\n+\n+\t\tswapcache = swap_cache_get_folio(entry);\n+\t\tif (swapcache)\n+\t\t\treturn swapcache;\n+\t}\n+\n+\treturn folio;\n }\n \n /*\n@@ -596,7 +627,6 @@ struct folio *read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\n \t\tstruct swap_iocb **plug)\n {\n \tstruct swap_info_struct *si;\n-\tbool page_allocated;\n \tstruct mempolicy *mpol;\n \tpgoff_t ilx;\n \tstruct folio *folio;\n@@ -606,13 +636,9 @@ struct folio *read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,\n \t\treturn NULL;\n \n \tmpol = get_vma_policy(vma, addr, 0, &ilx);\n-\tfolio = swap_cache_alloc_folio(entry, gfp_mask, mpol, ilx,\n-\t\t\t\t       &page_allocated);\n+\tfolio = swap_cache_read_folio(entry, gfp_mask, mpol, ilx, plug, false);\n \tmpol_cond_put(mpol);\n \n-\tif (page_allocated)\n-\t\tswap_read_folio(folio, plug);\n-\n \tput_swap_device(si);\n \treturn folio;\n }\n@@ -697,7 +723,7 @@ static unsigned long swapin_nr_pages(unsigned long offset)\n  * are fairly likely to have been swapped out from the same node.\n  */\n struct folio *swap_cluster_readahead(swp_entry_t entry, gfp_t gfp_mask,\n-\t\t\t\t    struct mempolicy *mpol, pgoff_t ilx)\n+\t\t\t\t     struct mempolicy *mpol, pgoff_t ilx)\n {\n \tstruct folio *folio;\n \tunsigned long entry_offset = swp_offset(entry);\n@@ -707,7 +733,7 @@ struct folio *swap_cluster_readahead(swp_entry_t entry, gfp_t gfp_mask,\n \tstruct swap_info_struct *si = __swap_entry_to_info(entry);\n \tstruct blk_plug plug;\n \tstruct swap_iocb *splug = NULL;\n-\tbool page_allocated;\n+\tswp_entry_t ra_entry;\n \n \tmask = swapin_nr_pages(offset) - 1;\n \tif (!mask)\n@@ -724,18 +750,11 @@ struct folio *swap_cluster_readahead(swp_entry_t entry, gfp_t gfp_mask,\n \tblk_start_plug(&plug);\n \tfor (offset = start_offset; offset <= end_offset ; offset++) {\n \t\t/* Ok, do the async read-ahead now */\n-\t\tfolio = swap_cache_alloc_folio(\n-\t\t\tswp_entry(swp_type(entry), offset), gfp_mask, mpol, ilx,\n-\t\t\t&page_allocated);\n+\t\tra_entry = swp_entry(swp_type(entry), offset);\n+\t\tfolio = swap_cache_read_folio(ra_entry, gfp_mask, mpol, ilx,\n+\t\t\t\t\t      &splug, offset != entry_offset);\n \t\tif (!folio)\n \t\t\tcontinue;\n-\t\tif (page_allocated) {\n-\t\t\tswap_read_folio(folio, &splug);\n-\t\t\tif (offset != entry_offset) {\n-\t\t\t\tfolio_set_readahead(folio);\n-\t\t\t\tcount_vm_event(SWAP_RA);\n-\t\t\t}\n-\t\t}\n \t\tfolio_put(folio);\n \t}\n \tblk_finish_plug(&plug);\n@@ -743,11 +762,7 @@ struct folio *swap_cluster_readahead(swp_entry_t entry, gfp_t gfp_mask,\n \tlru_add_drain();\t/* Push any new pages onto the LRU now */\n skip:\n \t/* The page was likely read above, so no need for plugging here */\n-\tfolio = swap_cache_alloc_folio(entry, gfp_mask, mpol, ilx,\n-\t\t\t\t       &page_allocated);\n-\tif (unlikely(page_allocated))\n-\t\tswap_read_folio(folio, NULL);\n-\treturn folio;\n+\treturn swap_cache_read_folio(entry, gfp_mask, mpol, ilx, NULL, false);\n }\n \n static int swap_vma_ra_win(struct vm_fault *vmf, unsigned long *start,\n@@ -813,8 +828,7 @@ static struct folio *swap_vma_readahead(swp_entry_t targ_entry, gfp_t gfp_mask,\n \tpte_t *pte = NULL, pentry;\n \tint win;\n \tunsigned long start, end, addr;\n-\tpgoff_t ilx;\n-\tbool page_allocated;\n+\tpgoff_t ilx = targ_ilx;\n \n \twin = swap_vma_ra_win(vmf, &start, &end);\n \tif (win == 1)\n@@ -848,19 +862,12 @@ static struct folio *swap_vma_readahead(swp_entry_t targ_entry, gfp_t gfp_mask,\n \t\t\tif (!si)\n \t\t\t\tcontinue;\n \t\t}\n-\t\tfolio = swap_cache_alloc_folio(entry, gfp_mask, mpol, ilx,\n-\t\t\t\t\t       &page_allocated);\n+\t\tfolio = swap_cache_read_folio(entry, gfp_mask, mpol, ilx,\n+\t\t\t\t\t      &splug, addr != vmf->address);\n \t\tif (si)\n \t\t\tput_swap_device(si);\n \t\tif (!folio)\n \t\t\tcontinue;\n-\t\tif (page_allocated) {\n-\t\t\tswap_read_folio(folio, &splug);\n-\t\t\tif (addr != vmf->address) {\n-\t\t\t\tfolio_set_readahead(folio);\n-\t\t\t\tcount_vm_event(SWAP_RA);\n-\t\t\t}\n-\t\t}\n \t\tfolio_put(folio);\n \t}\n \tif (pte)\n@@ -870,10 +877,8 @@ static struct folio *swap_vma_readahead(swp_entry_t targ_entry, gfp_t gfp_mask,\n \tlru_add_drain();\n skip:\n \t/* The folio was likely read above, so no need for plugging here */\n-\tfolio = swap_cache_alloc_folio(targ_entry, gfp_mask, mpol, targ_ilx,\n-\t\t\t\t       &page_allocated);\n-\tif (unlikely(page_allocated))\n-\t\tswap_read_folio(folio, NULL);\n+\tfolio = swap_cache_read_folio(targ_entry, gfp_mask, mpol, targ_ilx,\n+\t\t\t\t      NULL, false);\n \treturn folio;\n }\n \ndiff --git a/mm/zswap.c b/mm/zswap.c\nindex af3f0fbb0558..f3aa83a99636 100644\n--- a/mm/zswap.c\n+++ b/mm/zswap.c\n@@ -992,7 +992,6 @@ static int zswap_writeback_entry(struct zswap_entry *entry,\n \tpgoff_t offset = swp_offset(swpentry);\n \tstruct folio *folio;\n \tstruct mempolicy *mpol;\n-\tbool folio_was_allocated;\n \tstruct swap_info_struct *si;\n \tint ret = 0;\n \n@@ -1003,22 +1002,20 @@ static int zswap_writeback_entry(struct zswap_entry *entry,\n \n \tmpol = get_task_policy(current);\n \tfolio = swap_cache_alloc_folio(swpentry, GFP_KERNEL, mpol,\n-\t\t\t\t       NO_INTERLEAVE_INDEX, &folio_was_allocated);\n+\t\t\t\t       NO_INTERLEAVE_INDEX);\n \tput_swap_device(si);\n-\tif (!folio)\n-\t\treturn -ENOMEM;\n \n \t/*\n+\t * Swap cache allocaiton might fail due to OOM, raced with free\n+\t * or existing folio when we due to concurrent swapin or free.\n \t * Found an existing folio, we raced with swapin or concurrent\n \t * shrinker. We generally writeback cold folios from zswap, and\n \t * swapin means the folio just became hot, so skip this folio.\n \t * For unlikely concurrent shrinker case, it will be unlinked\n \t * and freed when invalidated by the concurrent shrinker anyway.\n \t */\n-\tif (!folio_was_allocated) {\n-\t\tret = -EEXIST;\n-\t\tgoto out;\n-\t}\n+\tif (IS_ERR(folio))\n+\t\treturn PTR_ERR(folio);\n \n \t/*\n \t * folio is locked, and the swapcache is now secured against\n@@ -1058,7 +1055,7 @@ static int zswap_writeback_entry(struct zswap_entry *entry,\n \t__swap_writepage(folio, NULL);\n \n out:\n-\tif (ret && ret != -EEXIST) {\n+\tif (ret) {\n \t\tswap_cache_del_folio(folio);\n \t\tfolio_unlock(folio);\n \t}\n\n-- \n2.53.0",
          "reply_to": "",
          "message_date": "2026-02-20"
        },
        {
          "author": "Kairui Song",
          "summary": "The reviewer noted that the patch introduces a new function thp_limit_gfp_mask() in include/linux/huge_mm.h, which is identical to an existing function limit_gfp_mask() in mm/shmem.c, and requested that the duplicate code be removed.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "duplicate_code",
            "requested_changes"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "No feature change, to be used later.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n include/linux/huge_mm.h | 24 ++++++++++++++++++++++++\n mm/shmem.c              | 30 +++---------------------------\n 2 files changed, 27 insertions(+), 27 deletions(-)\n\ndiff --git a/include/linux/huge_mm.h b/include/linux/huge_mm.h\nindex a4d9f964dfde..d522e798822d 100644\n--- a/include/linux/huge_mm.h\n+++ b/include/linux/huge_mm.h\n@@ -237,6 +237,30 @@ static inline bool thp_vma_suitable_order(struct vm_area_struct *vma,\n \treturn true;\n }\n \n+/*\n+ * Make sure huge_gfp is always more limited than limit_gfp.\n+ * Some of the flags set permissions, while others set limitations.\n+ */\n+static inline gfp_t thp_limit_gfp_mask(gfp_t huge_gfp, gfp_t limit_gfp)\n+{\n+\tgfp_t allowflags = __GFP_IO | __GFP_FS | __GFP_RECLAIM;\n+\tgfp_t denyflags = __GFP_NOWARN | __GFP_NORETRY;\n+\tgfp_t zoneflags = limit_gfp & GFP_ZONEMASK;\n+\tgfp_t result = huge_gfp & ~(allowflags | GFP_ZONEMASK);\n+\n+\t/* Allow allocations only from the originally specified zones. */\n+\tresult |= zoneflags;\n+\n+\t/*\n+\t * Minimize the result gfp by taking the union with the deny flags,\n+\t * and the intersection of the allow flags.\n+\t */\n+\tresult |= (limit_gfp & denyflags);\n+\tresult |= (huge_gfp & limit_gfp) & allowflags;\n+\n+\treturn result;\n+}\n+\n /*\n  * Filter the bitfield of input orders to the ones suitable for use in the vma.\n  * See thp_vma_suitable_order().\ndiff --git a/mm/shmem.c b/mm/shmem.c\nindex b976b40fd442..9f054b5aae8e 100644\n--- a/mm/shmem.c\n+++ b/mm/shmem.c\n@@ -1788,30 +1788,6 @@ static struct folio *shmem_swapin_cluster(swp_entry_t swap, gfp_t gfp,\n \treturn folio;\n }\n \n-/*\n- * Make sure huge_gfp is always more limited than limit_gfp.\n- * Some of the flags set permissions, while others set limitations.\n- */\n-static gfp_t limit_gfp_mask(gfp_t huge_gfp, gfp_t limit_gfp)\n-{\n-\tgfp_t allowflags = __GFP_IO | __GFP_FS | __GFP_RECLAIM;\n-\tgfp_t denyflags = __GFP_NOWARN | __GFP_NORETRY;\n-\tgfp_t zoneflags = limit_gfp & GFP_ZONEMASK;\n-\tgfp_t result = huge_gfp & ~(allowflags | GFP_ZONEMASK);\n-\n-\t/* Allow allocations only from the originally specified zones. */\n-\tresult |= zoneflags;\n-\n-\t/*\n-\t * Minimize the result gfp by taking the union with the deny flags,\n-\t * and the intersection of the allow flags.\n-\t */\n-\tresult |= (limit_gfp & denyflags);\n-\tresult |= (huge_gfp & limit_gfp) & allowflags;\n-\n-\treturn result;\n-}\n-\n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\n bool shmem_hpage_pmd_enabled(void)\n {\n@@ -2062,7 +2038,7 @@ static struct folio *shmem_swap_alloc_folio(struct inode *inode,\n \t\t     non_swapcache_batch(entry, nr_pages) != nr_pages)\n \t\t\tgoto fallback;\n \n-\t\talloc_gfp = limit_gfp_mask(vma_thp_gfp_mask(vma), gfp);\n+\t\talloc_gfp = thp_limit_gfp_mask(vma_thp_gfp_mask(vma), gfp);\n \t}\n retry:\n \tnew = shmem_alloc_folio(alloc_gfp, order, info, index);\n@@ -2138,7 +2114,7 @@ static int shmem_replace_folio(struct folio **foliop, gfp_t gfp,\n \tif (nr_pages > 1) {\n \t\tgfp_t huge_gfp = vma_thp_gfp_mask(vma);\n \n-\t\tgfp = limit_gfp_mask(huge_gfp, gfp);\n+\t\tgfp = thp_limit_gfp_mask(huge_gfp, gfp);\n \t}\n #endif\n \n@@ -2545,7 +2521,7 @@ static int shmem_get_folio_gfp(struct inode *inode, pgoff_t index,\n \t\tgfp_t huge_gfp;\n \n \t\thuge_gfp = vma_thp_gfp_mask(vma);\n-\t\thuge_gfp = limit_gfp_mask(huge_gfp, gfp);\n+\t\thuge_gfp = thp_limit_gfp_mask(huge_gfp, gfp);\n \t\tfolio = shmem_alloc_and_add_folio(vmf, huge_gfp,\n \t\t\t\tinode, index, fault_mm, orders);\n \t\tif (!IS_ERR(folio)) {\n\n-- \n2.53.0",
          "reply_to": "",
          "message_date": "2026-02-20"
        },
        {
          "author": "Kairui Song",
          "summary": "The reviewer requested that the patch be split into smaller, more manageable commits to make it easier for others to review.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "No feature change, make later commits easier to review.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n mm/swap_state.c | 55 ++++++++++++++++++++++++++++++-------------------------\n 1 file changed, 30 insertions(+), 25 deletions(-)\n\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 53fa95059012..1e340faea9ac 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -137,6 +137,28 @@ void *swap_cache_get_shadow(swp_entry_t entry)\n \treturn NULL;\n }\n \n+static int __swap_cache_add_check(struct swap_cluster_info *ci,\n+\t\t\t\t  unsigned int ci_off, unsigned int nr,\n+\t\t\t\t  void **shadow)\n+{\n+\tunsigned int ci_end = ci_off + nr;\n+\tunsigned long old_tb;\n+\n+\tif (unlikely(!ci->table))\n+\t\treturn -ENOENT;\n+\tdo {\n+\t\told_tb = __swap_table_get(ci, ci_off);\n+\t\tif (unlikely(swp_tb_is_folio(old_tb)))\n+\t\t\treturn -EEXIST;\n+\t\tif (unlikely(!__swp_tb_get_count(old_tb)))\n+\t\t\treturn -ENOENT;\n+\t\tif (swp_tb_is_shadow(old_tb))\n+\t\t\t*shadow = swp_tb_to_shadow(old_tb);\n+\t} while (++ci_off < ci_end);\n+\n+\treturn 0;\n+}\n+\n void __swap_cache_add_folio(struct swap_cluster_info *ci,\n \t\t\t    struct folio *folio, swp_entry_t entry)\n {\n@@ -179,43 +201,26 @@ static int swap_cache_add_folio(struct folio *folio, swp_entry_t entry,\n {\n \tint err;\n \tvoid *shadow = NULL;\n-\tunsigned long old_tb;\n+\tunsigned int ci_off;\n \tstruct swap_info_struct *si;\n \tstruct swap_cluster_info *ci;\n-\tunsigned int ci_start, ci_off, ci_end;\n \tunsigned long nr_pages = folio_nr_pages(folio);\n \n \tsi = __swap_entry_to_info(entry);\n-\tci_start = swp_cluster_offset(entry);\n-\tci_end = ci_start + nr_pages;\n-\tci_off = ci_start;\n \tci = swap_cluster_lock(si, swp_offset(entry));\n-\tif (unlikely(!ci->table)) {\n-\t\terr = -ENOENT;\n-\t\tgoto failed;\n+\tci_off = swp_cluster_offset(entry);\n+\terr = __swap_cache_add_check(ci, ci_off, nr_pages, &shadow);\n+\tif (err) {\n+\t\tswap_cluster_unlock(ci);\n+\t\treturn err;\n \t}\n-\tdo {\n-\t\told_tb = __swap_table_get(ci, ci_off);\n-\t\tif (unlikely(swp_tb_is_folio(old_tb))) {\n-\t\t\terr = -EEXIST;\n-\t\t\tgoto failed;\n-\t\t}\n-\t\tif (unlikely(!__swp_tb_get_count(old_tb))) {\n-\t\t\terr = -ENOENT;\n-\t\t\tgoto failed;\n-\t\t}\n-\t\tif (swp_tb_is_shadow(old_tb))\n-\t\t\tshadow = swp_tb_to_shadow(old_tb);\n-\t} while (++ci_off < ci_end);\n+\n \t__swap_cache_add_folio(ci, folio, entry);\n \tswap_cluster_unlock(ci);\n \tif (shadowp)\n \t\t*shadowp = shadow;\n-\treturn 0;\n \n-failed:\n-\tswap_cluster_unlock(ci);\n-\treturn err;\n+\treturn 0;\n }\n \n /**\n\n-- \n2.53.0",
          "reply_to": "",
          "message_date": "2026-02-20"
        },
        {
          "author": "Kairui Song",
          "summary": "Reviewer Kairui Song suggested modifying swap_cache_alloc_folio to handle larger orders, which would allow for direct allocation of large folios in the swap cache. This change also adjusts synchronization and introduces a fallback mechanism for large order allocations.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "To make it possible to allocate large folios directly in swap cache, let\nswap_cache_alloc_folio handle larger orders too.\n\nThis slightly changes how allocation is synchronized. Now, whoever first\nsuccessfully allocates a folio in the swap cache will be the one who\ncharges it and performs the swap-in. Raced swapin now should avoid a\nredundant charge and just wait for the swapin to finish.\n\nLarge order fallback is also moved to the swap cache layer. This should\nmake the fallback process less racy, too.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n mm/swap.h       |   3 +-\n mm/swap_state.c | 193 +++++++++++++++++++++++++++++++++++++++++---------------\n mm/zswap.c      |   2 +-\n 3 files changed, 145 insertions(+), 53 deletions(-)\n\ndiff --git a/mm/swap.h b/mm/swap.h\nindex ad8b17a93758..6774af10a943 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -280,7 +280,8 @@ bool swap_cache_has_folio(swp_entry_t entry);\n struct folio *swap_cache_get_folio(swp_entry_t entry);\n void *swap_cache_get_shadow(swp_entry_t entry);\n void swap_cache_del_folio(struct folio *folio);\n-struct folio *swap_cache_alloc_folio(swp_entry_t entry, gfp_t gfp_flags,\n+struct folio *swap_cache_alloc_folio(swp_entry_t target_entry, gfp_t gfp_mask,\n+\t\t\t\t     unsigned long orders, struct vm_fault *vmf,\n \t\t\t\t     struct mempolicy *mpol, pgoff_t ilx);\n /* Below helpers require the caller to lock and pass in the swap cluster. */\n void __swap_cache_add_folio(struct swap_cluster_info *ci,\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 1e340faea9ac..e32b06a1f229 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -137,26 +137,39 @@ void *swap_cache_get_shadow(swp_entry_t entry)\n \treturn NULL;\n }\n \n-static int __swap_cache_add_check(struct swap_cluster_info *ci,\n-\t\t\t\t  unsigned int ci_off, unsigned int nr,\n-\t\t\t\t  void **shadow)\n+static int __swap_cache_check_batch(struct swap_cluster_info *ci,\n+\t\t\t\t    unsigned int ci_off, unsigned int ci_targ,\n+\t\t\t\t    unsigned int nr, void **shadowp)\n {\n \tunsigned int ci_end = ci_off + nr;\n \tunsigned long old_tb;\n \n \tif (unlikely(!ci->table))\n \t\treturn -ENOENT;\n+\n \tdo {\n \t\told_tb = __swap_table_get(ci, ci_off);\n-\t\tif (unlikely(swp_tb_is_folio(old_tb)))\n-\t\t\treturn -EEXIST;\n-\t\tif (unlikely(!__swp_tb_get_count(old_tb)))\n-\t\t\treturn -ENOENT;\n+\t\tif (unlikely(swp_tb_is_folio(old_tb)) ||\n+\t\t    unlikely(!__swp_tb_get_count(old_tb)))\n+\t\t\tbreak;\n \t\tif (swp_tb_is_shadow(old_tb))\n-\t\t\t*shadow = swp_tb_to_shadow(old_tb);\n+\t\t\t*shadowp = swp_tb_to_shadow(old_tb);\n \t} while (++ci_off < ci_end);\n \n-\treturn 0;\n+\tif (likely(ci_off == ci_end))\n+\t\treturn 0;\n+\n+\t/*\n+\t * If the target slot is not suitable for adding swap cache, return\n+\t * -EEXIST or -ENOENT. If the batch is not suitable, could be a\n+\t * race with concurrent free or cache add, return -EBUSY.\n+\t */\n+\told_tb = __swap_table_get(ci, ci_targ);\n+\tif (swp_tb_is_folio(old_tb))\n+\t\treturn -EEXIST;\n+\tif (!__swp_tb_get_count(old_tb))\n+\t\treturn -ENOENT;\n+\treturn -EBUSY;\n }\n \n void __swap_cache_add_folio(struct swap_cluster_info *ci,\n@@ -209,7 +222,7 @@ static int swap_cache_add_folio(struct folio *folio, swp_entry_t entry,\n \tsi = __swap_entry_to_info(entry);\n \tci = swap_cluster_lock(si, swp_offset(entry));\n \tci_off = swp_cluster_offset(entry);\n-\terr = __swap_cache_add_check(ci, ci_off, nr_pages, &shadow);\n+\terr = __swap_cache_check_batch(ci, ci_off, ci_off, nr_pages, &shadow);\n \tif (err) {\n \t\tswap_cluster_unlock(ci);\n \t\treturn err;\n@@ -223,6 +236,124 @@ static int swap_cache_add_folio(struct folio *folio, swp_entry_t entry,\n \treturn 0;\n }\n \n+static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n+\t\t\t\t\tswp_entry_t targ_entry, gfp_t gfp,\n+\t\t\t\t\tunsigned int order, struct vm_fault *vmf,\n+\t\t\t\t\tstruct mempolicy *mpol, pgoff_t ilx)\n+{\n+\tint err;\n+\tswp_entry_t entry;\n+\tstruct folio *folio;\n+\tvoid *shadow = NULL, *shadow_check = NULL;\n+\tunsigned long address, nr_pages = 1 << order;\n+\tunsigned int ci_off, ci_targ = swp_cluster_offset(targ_entry);\n+\n+\tentry.val = round_down(targ_entry.val, nr_pages);\n+\tci_off = round_down(ci_targ, nr_pages);\n+\n+\t/* First check if the range is available */\n+\tspin_lock(&ci->lock);\n+\terr = __swap_cache_check_batch(ci, ci_off, ci_targ, nr_pages, &shadow);\n+\tspin_unlock(&ci->lock);\n+\tif (unlikely(err))\n+\t\treturn ERR_PTR(err);\n+\n+\tif (vmf) {\n+\t\tif (order)\n+\t\t\tgfp = thp_limit_gfp_mask(vma_thp_gfp_mask(vmf->vma), gfp);\n+\t\taddress = round_down(vmf->address, PAGE_SIZE << order);\n+\t\tfolio = vma_alloc_folio(gfp, order, vmf->vma, address);\n+\t} else {\n+\t\tfolio = folio_alloc_mpol(gfp, order, mpol, ilx, numa_node_id());\n+\t}\n+\tif (unlikely(!folio))\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\t/* Double check the range is still not in conflict */\n+\tspin_lock(&ci->lock);\n+\terr = __swap_cache_check_batch(ci, ci_off, ci_targ, nr_pages, &shadow_check);\n+\tif (unlikely(err) || shadow_check != shadow) {\n+\t\tspin_unlock(&ci->lock);\n+\t\tfolio_put(folio);\n+\n+\t\t/* If shadow changed, just try again */\n+\t\treturn ERR_PTR(err ? err : -EAGAIN);\n+\t}\n+\n+\t__folio_set_locked(folio);\n+\t__folio_set_swapbacked(folio);\n+\t__swap_cache_add_folio(ci, folio, entry);\n+\tspin_unlock(&ci->lock);\n+\n+\tif (mem_cgroup_swapin_charge_folio(folio, vmf ? vmf->vma->vm_mm : NULL,\n+\t\t\t\t\t   gfp, entry)) {\n+\t\tspin_lock(&ci->lock);\n+\t\t__swap_cache_del_folio(ci, folio, shadow);\n+\t\tspin_unlock(&ci->lock);\n+\t\tfolio_unlock(folio);\n+\t\tfolio_put(folio);\n+\t\tcount_mthp_stat(order, MTHP_STAT_SWPIN_FALLBACK_CHARGE);\n+\t\treturn ERR_PTR(-ENOMEM);\n+\t}\n+\n+\t/* For memsw accouting, swap is uncharged when folio is added to swap cache */\n+\tmemcg1_swapin(entry, 1 << order);\n+\tif (shadow)\n+\t\tworkingset_refault(folio, shadow);\n+\n+\t/* Caller will initiate read into locked new_folio */\n+\tfolio_add_lru(folio);\n+\n+\treturn folio;\n+}\n+\n+/**\n+ * swap_cache_alloc_folio - Allocate folio for swapped out slot in swap cache.\n+ * @targ_entry: swap entry indicating the target slot\n+ * @orders: allocation orders\n+ * @vmf: fault information\n+ * @gfp_mask: memory allocation flags\n+ * @mpol: NUMA memory allocation policy to be applied\n+ * @ilx: NUMA interleave index, for use only when MPOL_INTERLEAVE\n+ *\n+ * Allocate a folio in the swap cache for one swap slot, typically before\n+ * doing IO (e.g. swap in or zswap writeback). The swap slot indicated by\n+ * @targ_entry must have a non-zero swap count (swapped out).\n+ *\n+ * Context: Caller must protect the swap device with reference count or locks.\n+ * Return: Returns the folio if allocation successed and folio is added to\n+ * swap cache. Returns error code if allocation failed due to race.\n+ */\n+struct folio *swap_cache_alloc_folio(swp_entry_t targ_entry, gfp_t gfp_mask,\n+\t\t\t\t     unsigned long orders, struct vm_fault *vmf,\n+\t\t\t\t     struct mempolicy *mpol, pgoff_t ilx)\n+{\n+\tint order;\n+\tstruct folio *folio;\n+\tstruct swap_cluster_info *ci;\n+\n+\tci = __swap_entry_to_cluster(targ_entry);\n+\torder = orders ? highest_order(orders) : 0;\n+\tfor (;;) {\n+\t\tfolio = __swap_cache_alloc(ci, targ_entry, gfp_mask, order,\n+\t\t\t\t\t   vmf, mpol, ilx);\n+\t\tif (!IS_ERR(folio))\n+\t\t\treturn folio;\n+\t\tif (PTR_ERR(folio) == -EAGAIN)\n+\t\t\tcontinue;\n+\t\t/* Only -EBUSY means we should fallback and retry. */\n+\t\tif (PTR_ERR(folio) != -EBUSY)\n+\t\t\treturn folio;\n+\t\tcount_mthp_stat(order, MTHP_STAT_SWPIN_FALLBACK);\n+\t\torder = next_order(&orders, order);\n+\t\tif (!orders)\n+\t\t\tbreak;\n+\t}\n+\t/* Should never reach here, order 0 should not fail with -EBUSY. */\n+\tWARN_ON_ONCE(1);\n+\treturn ERR_PTR(-EINVAL);\n+}\n+\n /**\n  * __swap_cache_del_folio - Removes a folio from the swap cache.\n  * @ci: The locked swap cluster.\n@@ -498,46 +629,6 @@ static int __swap_cache_prepare_and_add(swp_entry_t entry,\n \treturn ret;\n }\n \n-/**\n- * swap_cache_alloc_folio - Allocate folio for swapped out slot in swap cache.\n- * @entry: the swapped out swap entry to be binded to the folio.\n- * @gfp_mask: memory allocation flags\n- * @mpol: NUMA memory allocation policy to be applied\n- * @ilx: NUMA interleave index, for use only when MPOL_INTERLEAVE\n- *\n- * Allocate a folio in the swap cache for one swap slot, typically before\n- * doing IO (e.g. swap in or zswap writeback). The swap slot indicated by\n- * @entry must have a non-zero swap count (swapped out).\n- * Currently only supports order 0.\n- *\n- * Context: Caller must protect the swap device with reference count or locks.\n- * Return: Returns the folio if allocation succeeded and folio is added to\n- * swap cache. Returns error code if allocation failed due to race.\n- */\n-struct folio *swap_cache_alloc_folio(swp_entry_t entry, gfp_t gfp_mask,\n-\t\t\t\t     struct mempolicy *mpol, pgoff_t ilx)\n-{\n-\tint ret;\n-\tstruct folio *folio;\n-\n-\t/* Allocate a new folio to be added into the swap cache. */\n-\tfolio = folio_alloc_mpol(gfp_mask, 0, mpol, ilx, numa_node_id());\n-\tif (!folio)\n-\t\treturn ERR_PTR(-ENOMEM);\n-\n-\t/*\n-\t * Try add the new folio, it returns NULL if already exist,\n-\t * since folio is order 0.\n-\t */\n-\tret = __swap_cache_prepare_and_add(entry, folio, gfp_mask, false);\n-\tif (ret) {\n-\t\tfolio_put(folio);\n-\t\treturn ERR_PTR(ret);\n-\t}\n-\n-\treturn folio;\n-}\n-\n static struct folio *swap_cache_read_folio(swp_entry_t entry, gfp_t gfp,\n \t\t\t\t\t   struct mempolicy *mpol, pgoff_t ilx,\n \t\t\t\t\t   struct swap_iocb **plug, bool readahead)\n@@ -559,7 +650,7 @@ static struct folio *swap_cache_read_folio(swp_entry_t entry, gfp_t gfp,\n \t\tif (folio)\n \t\t\treturn folio;\n \n-\t\tfolio = swap_cache_alloc_folio(entry, gfp, mpol, ilx);\n+\t\tfolio = swap_cache_alloc_folio(entry, gfp, 0, NULL, mpol, ilx);\n \t} while (PTR_ERR(folio) == -EEXIST);\n \n \tif (IS_ERR_OR_NULL(folio))\ndiff --git a/mm/zswap.c b/mm/zswap.c\nindex f3aa83a99636..5d83539a8bba 100644\n--- a/mm/zswap.c\n+++ b/mm/zswap.c\n@@ -1001,7 +1001,7 @@ static int zswap_writeback_entry(struct zswap_entry *entry,\n \t\treturn -EEXIST;\n \n \tmpol = get_task_policy(current);\n-\tfolio = swap_cache_alloc_folio(swpentry, GFP_KERNEL, mpol,\n+\tfolio = swap_cache_alloc_folio(swpentry, GFP_KERNEL, 0, NULL, mpol,\n \t\t\t\t       NO_INTERLEAVE_INDEX);\n \tput_swap_device(si);\n \n\n-- \n2.53.0",
          "reply_to": "",
          "message_date": "2026-02-20"
        },
        {
          "author": "Kairui Song",
          "summary": "The reviewer noted that the patch always charges swapin folios into the dead cgroup's parent cgroup, which can lead to a situation where the folio->swap entry belongs to a cgroup that is not folio->memcg. They suggested several possible solutions, including dynamically allocating a swap cluster trampoline cgroup table and tolerating a 2-byte per slot overhead.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "As a result this will always charge the swapin folio into the dead\ncgroup's parent cgroup, and ensure folio->swap belongs to folio_memcg.\nThis only affects some uncommon behavior if we move the process between\nmemcg.\n\nWhen a process that previously swapped some memory is moved to another\ncgroup, and the cgroup where the swap occurred is dead, folios for\nswap in of old swap entries will be charged into the new cgroup.\nCombined with the lazy freeing of swap cache, this leads to a strange\nsituation where the folio->swap entry belongs to a cgroup that is not\nfolio->memcg.\n\nSwapin from dead zombie memcg might be rare in practise, cgroups are\nofflined only after the workload in it is gone, which requires zapping\nthe page table first, and releases all swap entries. Shmem is\na bit different, but shmem always has swap count == 1, and force\nreleases the swap cache. So, for shmem charging into the new memcg and\nrelease entry does look more sensible.\n\nHowever, to make things easier to understand for an RFC, let's just\nalways charge to the parent cgroup if the leaf cgroup is dead. This may\nnot be the best design, but it makes the following work much easier to\ndemonstrate.\n\nFor a better solution, we can later:\n\n- Dynamically allocate a swap cluster trampoline cgroup table\n  (ci->memcg_table) and use that for zombie swapin only. Which is\n  actually OK and may not cause a mess in the code level, since the\n  incoming swap table compaction will require table expansion on swap-in\n  as well.\n\n- Just tolerate a 2-byte per slot overhead all the time, which is also\n  acceptable.\n\n- Limit the charge to parent behavior to only one situation: when the\n  swap count > 2 and the process is migrated to another cgroup after\n  swapout, these entries. This is even more rare to see in practice, I\n  think.\n\nFor reference, the memory ownership model of cgroup v2:\n\n\"\"\"\nA memory area is charged to the cgroup which instantiated it and stays\ncharged to the cgroup until the area is released.  Migrating a process\nto a different cgroup doesn't move the memory usages that it\ninstantiated while in the previous cgroup to the new cgroup.\n\nA memory area may be used by processes belonging to different cgroups.\nTo which cgroup the area will be charged is in-deterministic; however,\nover time, the memory area is likely to end up in a cgroup which has\nenough memory allowance to avoid high reclaim pressure.\n\nIf a cgroup sweeps a considerable amount of memory which is expected\nto be accessed repeatedly by other cgroups, it may make sense to use\nPOSIX_FADV_DONTNEED to relinquish the ownership of memory areas\nbelonging to the affected files to ensure correct memory ownership.\n\"\"\"\n\nSo I think all of the solutions mentioned above, including this commit,\nare not wrong.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n mm/memcontrol.c | 53 +++++++++++++++++++++++++++++++++++++++++++++++++----\n 1 file changed, 49 insertions(+), 4 deletions(-)\n\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 73f622f7a72b..b2898719e935 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -4803,22 +4803,67 @@ int mem_cgroup_charge_hugetlb(struct folio *folio, gfp_t gfp)\n int mem_cgroup_swapin_charge_folio(struct folio *folio, struct mm_struct *mm,\n \t\t\t\t  gfp_t gfp, swp_entry_t entry)\n {\n-\tstruct mem_cgroup *memcg;\n-\tunsigned short id;\n+\tstruct mem_cgroup *memcg, *swap_memcg;\n+\tunsigned short id, parent_id;\n+\tunsigned int nr_pages;\n \tint ret;\n \n \tif (mem_cgroup_disabled())\n \t\treturn 0;\n \n \tid = lookup_swap_cgroup_id(entry);\n+\tnr_pages = folio_nr_pages(folio);\n+\n \trcu_read_lock();\n-\tmemcg = mem_cgroup_from_private_id(id);\n-\tif (!memcg || !css_tryget_online(&memcg->css))\n+\tswap_memcg = mem_cgroup_from_private_id(id);\n+\tif (!swap_memcg) {\n+\t\tWARN_ON_ONCE(id);\n \t\tmemcg = get_mem_cgroup_from_mm(mm);\n+\t} else {\n+\t\tmemcg = swap_memcg;\n+\t\t/* Find the nearest online ancestor if dead, for reparent */\n+\t\twhile (!css_tryget_online(&memcg->css))\n+\t\t\tmemcg = parent_mem_cgroup(memcg);\n+\t}\n \trcu_read_unlock();\n \n \tret = charge_memcg(folio, memcg, gfp);\n+\tif (ret)\n+\t\tgoto out;\n+\n+\t/*\n+\t * If the swap entry's memcg is dead, reparent the swap charge\n+\t * from swap_memcg to memcg.\n+\t *\n+\t * If memcg is also being offlined, the charge will be moved to\n+\t * its parent again.\n+\t */\n+\tif (swap_memcg && memcg != swap_memcg) {\n+\t\tstruct mem_cgroup *parent_memcg;\n \n+\t\tparent_memcg = mem_cgroup_private_id_get_online(memcg, nr_pages);\n+\t\tparent_id = mem_cgroup_private_id(parent_memcg);\n+\n+\t\tWARN_ON(id != swap_cgroup_clear(entry, nr_pages));\n+\t\tswap_cgroup_record(folio, parent_id, entry);\n+\n+\t\tif (do_memsw_account()) {\n+\t\t\tif (!mem_cgroup_is_root(parent_memcg))\n+\t\t\t\tpage_counter_charge(&parent_memcg->memsw, nr_pages);\n+\t\t\tpage_counter_uncharge(&swap_memcg->memsw, nr_pages);\n+\t\t} else {\n+\t\t\tif (!mem_cgroup_is_root(parent_memcg))\n+\t\t\t\tpage_counter_charge(&parent_memcg->swap, nr_pages);\n+\t\t\tpage_counter_uncharge(&swap_memcg->swap, nr_pages);\n+\t\t}\n+\n+\t\tmod_memcg_state(parent_memcg, MEMCG_SWAP, nr_pages);\n+\t\tmod_memcg_state(swap_memcg, MEMCG_SWAP, -nr_pages);\n+\n+\t\t/* Release the dead cgroup after reparent */\n+\t\tmem_cgroup_private_id_put(swap_memcg, nr_pages);\n+\t}\n+out:\n \tcss_put(&memcg->css);\n \treturn ret;\n }\n\n-- \n2.53.0",
          "reply_to": "",
          "message_date": "2026-02-20"
        },
        {
          "author": "Kairui Song",
          "summary": "The reviewer noted that the patch introduces a new function thp_swapin_suiltable_orders() which returns an unsigned long value representing the highest order of swap allocation, but this function is not properly used in the do_swap_page() path. The reviewer suggested replacing the return value with a more meaningful result.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "patch review",
            "code analysis"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Now the large order allocation is supported in swap cache, making both\nanon and shmem use this instead of implementing their own different\nmethod for doing so.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n mm/memory.c     |  77 +++++---------------------\n mm/shmem.c      |  94 ++++++++------------------------\n mm/swap.h       |  30 ++---------\n mm/swap_state.c | 163 ++++++++++++--------------------------------------------\n mm/swapfile.c   |   3 +-\n 5 files changed, 76 insertions(+), 291 deletions(-)\n\ndiff --git a/mm/memory.c b/mm/memory.c\nindex 21bf2517fbce..e58f976508b3 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -4520,26 +4520,6 @@ static vm_fault_t handle_pte_marker(struct vm_fault *vmf)\n \treturn VM_FAULT_SIGBUS;\n }\n \n-static struct folio *__alloc_swap_folio(struct vm_fault *vmf)\n-{\n-\tstruct vm_area_struct *vma = vmf->vma;\n-\tstruct folio *folio;\n-\tsoftleaf_t entry;\n-\n-\tfolio = vma_alloc_folio(GFP_HIGHUSER_MOVABLE, 0, vma, vmf->address);\n-\tif (!folio)\n-\t\treturn NULL;\n-\n-\tentry = softleaf_from_pte(vmf->orig_pte);\n-\tif (mem_cgroup_swapin_charge_folio(folio, vma->vm_mm,\n-\t\t\t\t\t   GFP_KERNEL, entry)) {\n-\t\tfolio_put(folio);\n-\t\treturn NULL;\n-\t}\n-\n-\treturn folio;\n-}\n-\n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\n /*\n  * Check if the PTEs within a range are contiguous swap entries\n@@ -4569,8 +4549,6 @@ static bool can_swapin_thp(struct vm_fault *vmf, pte_t *ptep, int nr_pages)\n \t */\n \tif (unlikely(swap_zeromap_batch(entry, nr_pages, NULL) != nr_pages))\n \t\treturn false;\n-\tif (unlikely(non_swapcache_batch(entry, nr_pages) != nr_pages))\n-\t\treturn false;\n \n \treturn true;\n }\n@@ -4598,16 +4576,14 @@ static inline unsigned long thp_swap_suitable_orders(pgoff_t swp_offset,\n \treturn orders;\n }\n \n-static struct folio *alloc_swap_folio(struct vm_fault *vmf)\n+static unsigned long thp_swapin_suiltable_orders(struct vm_fault *vmf)\n {\n \tstruct vm_area_struct *vma = vmf->vma;\n \tunsigned long orders;\n-\tstruct folio *folio;\n \tunsigned long addr;\n \tsoftleaf_t entry;\n \tspinlock_t *ptl;\n \tpte_t *pte;\n-\tgfp_t gfp;\n \tint order;\n \n \t/*\n@@ -4615,7 +4591,7 @@ static struct folio *alloc_swap_folio(struct vm_fault *vmf)\n \t * maintain the uffd semantics.\n \t */\n \tif (unlikely(userfaultfd_armed(vma)))\n-\t\tgoto fallback;\n+\t\treturn 0;\n \n \t/*\n \t * A large swapped out folio could be partially or fully in zswap. We\n@@ -4623,7 +4599,7 @@ static struct folio *alloc_swap_folio(struct vm_fault *vmf)\n \t * folio.\n \t */\n \tif (!zswap_never_enabled())\n-\t\tgoto fallback;\n+\t\treturn 0;\n \n \tentry = softleaf_from_pte(vmf->orig_pte);\n \t/*\n@@ -4637,12 +4613,12 @@ static struct folio *alloc_swap_folio(struct vm_fault *vmf)\n \t\t\t\t\t  vmf->address, orders);\n \n \tif (!orders)\n-\t\tgoto fallback;\n+\t\treturn 0;\n \n \tpte = pte_offset_map_lock(vmf->vma->vm_mm, vmf->pmd,\n \t\t\t\t  vmf->address & PMD_MASK, &ptl);\n \tif (unlikely(!pte))\n-\t\tgoto fallback;\n+\t\treturn 0;\n \n \t/*\n \t * For do_swap_page, find the highest order where the aligned range is\n@@ -4658,29 +4634,12 @@ static struct folio *alloc_swap_folio(struct vm_fault *vmf)\n \n \tpte_unmap_unlock(pte, ptl);\n \n-\t/* Try allocating the highest of the remaining orders. */\n-\tgfp = vma_thp_gfp_mask(vma);\n-\twhile (orders) {\n-\t\taddr = ALIGN_DOWN(vmf->address, PAGE_SIZE << order);\n-\t\tfolio = vma_alloc_folio(gfp, order, vma, addr);\n-\t\tif (folio) {\n-\t\t\tif (!mem_cgroup_swapin_charge_folio(folio, vma->vm_mm,\n-\t\t\t\t\t\t\t    gfp, entry))\n-\t\t\t\treturn folio;\n-\t\t\tcount_mthp_stat(order, MTHP_STAT_SWPIN_FALLBACK_CHARGE);\n-\t\t\tfolio_put(folio);\n-\t\t}\n-\t\tcount_mthp_stat(order, MTHP_STAT_SWPIN_FALLBACK);\n-\t\torder = next_order(&orders, order);\n-\t}\n-\n-fallback:\n-\treturn __alloc_swap_folio(vmf);\n+\treturn orders;\n }\n #else /* !CONFIG_TRANSPARENT_HUGEPAGE */\n-static struct folio *alloc_swap_folio(struct vm_fault *vmf)\n+static unsigned long thp_swapin_suiltable_orders(struct vm_fault *vmf)\n {\n-\treturn __alloc_swap_folio(vmf);\n+\treturn 0;\n }\n #endif /* CONFIG_TRANSPARENT_HUGEPAGE */\n \n@@ -4785,21 +4744,13 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)\n \tif (folio)\n \t\tswap_update_readahead(folio, vma, vmf->address);\n \tif (!folio) {\n-\t\tif (data_race(si->flags & SWP_SYNCHRONOUS_IO)) {\n-\t\t\tfolio = alloc_swap_folio(vmf);\n-\t\t\tif (folio) {\n-\t\t\t\t/*\n-\t\t\t\t * folio is charged, so swapin can only fail due\n-\t\t\t\t * to raced swapin and return NULL.\n-\t\t\t\t */\n-\t\t\t\tswapcache = swapin_folio(entry, folio);\n-\t\t\t\tif (swapcache != folio)\n-\t\t\t\t\tfolio_put(folio);\n-\t\t\t\tfolio = swapcache;\n-\t\t\t}\n-\t\t} else {\n+\t\t/* Swapin bypass readahead for SWP_SYNCHRONOUS_IO devices */\n+\t\tif (data_race(si->flags & SWP_SYNCHRONOUS_IO))\n+\t\t\tfolio = swapin_entry(entry, GFP_HIGHUSER_MOVABLE,\n+\t\t\t\t\t     thp_swapin_suiltable_orders(vmf),\n+\t\t\t\t\t     vmf, NULL, 0);\n+\t\telse\n \t\t\tfolio = swapin_readahead(entry, GFP_HIGHUSER_MOVABLE, vmf);\n-\t\t}\n \n \t\tif (!folio) {\n \t\t\t/*\ndiff --git a/mm/shmem.c b/mm/shmem.c\nindex 9f054b5aae8e..0a19ac82ec77 100644\n--- a/mm/shmem.c\n+++ b/mm/shmem.c\n@@ -159,7 +159,7 @@ static unsigned long shmem_default_max_inodes(void)\n \n static int shmem_swapin_folio(struct inode *inode, pgoff_t index,\n \t\t\tstruct folio **foliop, enum sgp_type sgp, gfp_t gfp,\n-\t\t\tstruct vm_area_struct *vma, vm_fault_t *fault_type);\n+\t\t\tstruct vm_fault *vmf, vm_fault_t *fault_type);\n \n static inline struct shmem_sb_info *SHMEM_SB(struct super_block *sb)\n {\n@@ -2014,68 +2014,24 @@ static struct folio *shmem_alloc_and_add_folio(struct vm_fault *vmf,\n }\n \n static struct folio *shmem_swap_alloc_folio(struct inode *inode,\n-\t\tstruct vm_area_struct *vma, pgoff_t index,\n+\t\tstruct vm_fault *vmf, pgoff_t index,\n \t\tswp_entry_t entry, int order, gfp_t gfp)\n {\n+\tpgoff_t ilx;\n+\tstruct folio *folio;\n+\tstruct mempolicy *mpol;\n+\tunsigned long orders = BIT(order);\n \tstruct shmem_inode_info *info = SHMEM_I(inode);\n-\tstruct folio *new, *swapcache;\n-\tint nr_pages = 1 << order;\n-\tgfp_t alloc_gfp = gfp;\n-\n-\tif (!IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE)) {\n-\t\tif (WARN_ON_ONCE(order))\n-\t\t\treturn ERR_PTR(-EINVAL);\n-\t} else if (order) {\n-\t\t/*\n-\t\t * If uffd is active for the vma, we need per-page fault\n-\t\t * fidelity to maintain the uffd semantics, then fallback\n-\t\t * to swapin order-0 folio, as well as for zswap case.\n-\t\t * Any existing sub folio in the swap cache also blocks\n-\t\t * mTHP swapin.\n-\t\t */\n-\t\tif ((vma && unlikely(userfaultfd_armed(vma))) ||\n-\t\t     !zswap_never_enabled() ||\n-\t\t     non_swapcache_batch(entry, nr_pages) != nr_pages)\n-\t\t\tgoto fallback;\n \n-\t\talloc_gfp = thp_limit_gfp_mask(vma_thp_gfp_mask(vma), gfp);\n-\t}\n-retry:\n-\tnew = shmem_alloc_folio(alloc_gfp, order, info, index);\n-\tif (!new) {\n-\t\tnew = ERR_PTR(-ENOMEM);\n-\t\tgoto fallback;\n-\t}\n+\tif ((vmf && unlikely(userfaultfd_armed(vmf->vma))) ||\n+\t     !zswap_never_enabled())\n+\t\torders = 0;\n \n-\tif (mem_cgroup_swapin_charge_folio(new, vma ? vma->vm_mm : NULL,\n-\t\t\t\t\t   alloc_gfp, entry)) {\n-\t\tfolio_put(new);\n-\t\tnew = ERR_PTR(-ENOMEM);\n-\t\tgoto fallback;\n-\t}\n+\tmpol = shmem_get_pgoff_policy(info, index, order, &ilx);\n+\tfolio = swapin_entry(entry, gfp, orders, vmf, mpol, ilx);\n+\tmpol_cond_put(mpol);\n \n-\tswapcache = swapin_folio(entry, new);\n-\tif (swapcache != new) {\n-\t\tfolio_put(new);\n-\t\tif (!swapcache) {\n-\t\t\t/*\n-\t\t\t * The new folio is charged already, swapin can\n-\t\t\t * only fail due to another raced swapin.\n-\t\t\t */\n-\t\t\tnew = ERR_PTR(-EEXIST);\n-\t\t\tgoto fallback;\n-\t\t}\n-\t}\n-\treturn swapcache;\n-fallback:\n-\t/* Order 0 swapin failed, nothing to fallback to, abort */\n-\tif (!order)\n-\t\treturn new;\n-\tentry.val += index - round_down(index, nr_pages);\n-\talloc_gfp = gfp;\n-\tnr_pages = 1;\n-\torder = 0;\n-\tgoto retry;\n+\treturn folio;\n }\n \n /*\n@@ -2262,11 +2218,12 @@ static int shmem_split_large_entry(struct inode *inode, pgoff_t index,\n  */\n static int shmem_swapin_folio(struct inode *inode, pgoff_t index,\n \t\t\t     struct folio **foliop, enum sgp_type sgp,\n-\t\t\t     gfp_t gfp, struct vm_area_struct *vma,\n+\t\t\t     gfp_t gfp, struct vm_fault *vmf,\n \t\t\t     vm_fault_t *fault_type)\n {\n \tstruct address_space *mapping = inode->i_mapping;\n-\tstruct mm_struct *fault_mm = vma ? vma->vm_mm : NULL;\n+\tstruct vm_area_struct *vma = vmf ? vmf->vma : NULL;\n+\tstruct mm_struct *fault_mm = vmf ? vmf->vma->vm_mm : NULL;\n \tstruct shmem_inode_info *info = SHMEM_I(inode);\n \tswp_entry_t swap;\n \tsoftleaf_t index_entry;\n@@ -2307,20 +2264,15 @@ static int shmem_swapin_folio(struct inode *inode, pgoff_t index,\n \tif (!folio) {\n \t\tif (data_race(si->flags & SWP_SYNCHRONOUS_IO)) {\n \t\t\t/* Direct swapin skipping swap cache & readahead */\n-\t\t\tfolio = shmem_swap_alloc_folio(inode, vma, index,\n-\t\t\t\t\t\t       index_entry, order, gfp);\n-\t\t\tif (IS_ERR(folio)) {\n-\t\t\t\terror = PTR_ERR(folio);\n-\t\t\t\tfolio = NULL;\n-\t\t\t\tgoto failed;\n-\t\t\t}\n+\t\t\tfolio = shmem_swap_alloc_folio(inode, vmf, index,\n+\t\t\t\t\t\t       swap, order, gfp);\n \t\t} else {\n \t\t\t/* Cached swapin only supports order 0 folio */\n \t\t\tfolio = shmem_swapin_cluster(swap, gfp, info, index);\n-\t\t\tif (!folio) {\n-\t\t\t\terror = -ENOMEM;\n-\t\t\t\tgoto failed;\n-\t\t\t}\n+\t\t}\n+\t\tif (!folio) {\n+\t\t\terror = -ENOMEM;\n+\t\t\tgoto failed;\n \t\t}\n \t\tif (fault_type) {\n \t\t\t*fault_type |= VM_FAULT_MAJOR;\n@@ -2468,7 +2420,7 @@ static int shmem_get_folio_gfp(struct inode *inode, pgoff_t index,\n \n \tif (xa_is_value(folio)) {\n \t\terror = shmem_swapin_folio(inode, index, &folio,\n-\t\t\t\t\t   sgp, gfp, vma, fault_type);\n+\t\t\t\t\t   sgp, gfp, vmf, fault_type);\n \t\tif (error == -EEXIST)\n \t\t\tgoto repeat;\n \ndiff --git a/mm/swap.h b/mm/swap.h\nindex 6774af10a943..80c2f1bf7a57 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -300,7 +300,8 @@ struct folio *swap_cluster_readahead(swp_entry_t entry, gfp_t flag,\n \t\tstruct mempolicy *mpol, pgoff_t ilx);\n struct folio *swapin_readahead(swp_entry_t entry, gfp_t flag,\n \t\tstruct vm_fault *vmf);\n-struct folio *swapin_folio(swp_entry_t entry, struct folio *folio);\n+struct folio *swapin_entry(swp_entry_t entry, gfp_t flag, unsigned long orders,\n+\t\t\t   struct vm_fault *vmf, struct mempolicy *mpol, pgoff_t ilx);\n void swap_update_readahead(struct folio *folio, struct vm_area_struct *vma,\n \t\t\t   unsigned long addr);\n \n@@ -334,24 +335,6 @@ static inline int swap_zeromap_batch(swp_entry_t entry, int max_nr,\n \t\treturn find_next_bit(sis->zeromap, end, start) - start;\n }\n \n-static inline int non_swapcache_batch(swp_entry_t entry, int max_nr)\n-{\n-\tint i;\n-\n-\t/*\n-\t * While allocating a large folio and doing mTHP swapin, we need to\n-\t * ensure all entries are not cached, otherwise, the mTHP folio will\n-\t * be in conflict with the folio in swap cache.\n-\t */\n-\tfor (i = 0; i < max_nr; i++) {\n-\t\tif (swap_cache_has_folio(entry))\n-\t\t\treturn i;\n-\t\tentry.val++;\n-\t}\n-\n-\treturn i;\n-}\n-\n #else /* CONFIG_SWAP */\n struct swap_iocb;\n static inline struct swap_cluster_info *swap_cluster_lock(\n@@ -433,7 +416,9 @@ static inline struct folio *swapin_readahead(swp_entry_t swp, gfp_t gfp_mask,\n \treturn NULL;\n }\n \n-static inline struct folio *swapin_folio(swp_entry_t entry, struct folio *folio)\n+static inline struct folio *swapin_entry(\n+\tswp_entry_t entry, gfp_t flag, unsigned long orders,\n+\tstruct vm_fault *vmf, struct mempolicy *mpol, pgoff_t ilx)\n {\n \treturn NULL;\n }\n@@ -493,10 +478,5 @@ static inline int swap_zeromap_batch(swp_entry_t entry, int max_nr,\n {\n \treturn 0;\n }\n-\n-static inline int non_swapcache_batch(swp_entry_t entry, int max_nr)\n-{\n-\treturn 0;\n-}\n #endif /* CONFIG_SWAP */\n #endif /* _MM_SWAP_H */\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex e32b06a1f229..0a2a4e084cf2 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -199,43 +199,6 @@ void __swap_cache_add_folio(struct swap_cluster_info *ci,\n \tlruvec_stat_mod_folio(folio, NR_SWAPCACHE, nr_pages);\n }\n \n-/**\n- * swap_cache_add_folio - Add a folio into the swap cache.\n- * @folio: The folio to be added.\n- * @entry: The swap entry corresponding to the folio.\n- * @gfp: gfp_mask for XArray node allocation.\n- * @shadowp: If a shadow is found, return the shadow.\n- *\n- * Context: Caller must ensure @entry is valid and protect the swap device\n- * with reference count or locks.\n- */\n-static int swap_cache_add_folio(struct folio *folio, swp_entry_t entry,\n-\t\t\t\tvoid **shadowp)\n-{\n-\tint err;\n-\tvoid *shadow = NULL;\n-\tunsigned int ci_off;\n-\tstruct swap_info_struct *si;\n-\tstruct swap_cluster_info *ci;\n-\tunsigned long nr_pages = folio_nr_pages(folio);\n-\n-\tsi = __swap_entry_to_info(entry);\n-\tci = swap_cluster_lock(si, swp_offset(entry));\n-\tci_off = swp_cluster_offset(entry);\n-\terr = __swap_cache_check_batch(ci, ci_off, ci_off, nr_pages, &shadow);\n-\tif (err) {\n-\t\tswap_cluster_unlock(ci);\n-\t\treturn err;\n-\t}\n-\n-\t__swap_cache_add_folio(ci, folio, entry);\n-\tswap_cluster_unlock(ci);\n-\tif (shadowp)\n-\t\t*shadowp = shadow;\n-\n-\treturn 0;\n-}\n-\n static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n \t\t\t\t\tswp_entry_t targ_entry, gfp_t gfp,\n \t\t\t\t\tunsigned int order, struct vm_fault *vmf,\n@@ -328,30 +291,28 @@ struct folio *swap_cache_alloc_folio(swp_entry_t targ_entry, gfp_t gfp_mask,\n \t\t\t\t     unsigned long orders, struct vm_fault *vmf,\n \t\t\t\t     struct mempolicy *mpol, pgoff_t ilx)\n {\n-\tint order;\n+\tint order, err;\n \tstruct folio *folio;\n \tstruct swap_cluster_info *ci;\n \n+\t/* Always allow order 0 so swap won't fail under pressure. */\n+\torder = orders ? highest_order(orders |= BIT(0)) : 0;\n \tci = __swap_entry_to_cluster(targ_entry);\n-\torder = orders ? highest_order(orders) : 0;\n \tfor (;;) {\n \t\tfolio = __swap_cache_alloc(ci, targ_entry, gfp_mask, order,\n \t\t\t\t\t   vmf, mpol, ilx);\n \t\tif (!IS_ERR(folio))\n \t\t\treturn folio;\n-\t\tif (PTR_ERR(folio) == -EAGAIN)\n+\t\terr = PTR_ERR(folio);\n+\t\tif (err == -EAGAIN)\n \t\t\tcontinue;\n-\t\t/* Only -EBUSY means we should fallback and retry. */\n-\t\tif (PTR_ERR(folio) != -EBUSY)\n-\t\t\treturn folio;\n+\t\tif (!order || (err != -EBUSY && err != -ENOMEM))\n+\t\t\tbreak;\n \t\tcount_mthp_stat(order, MTHP_STAT_SWPIN_FALLBACK);\n \t\torder = next_order(&orders, order);\n-\t\tif (!orders)\n-\t\t\tbreak;\n \t}\n-\t/* Should never reach here, order 0 should not fail with -EBUSY. */\n-\tWARN_ON_ONCE(1);\n-\treturn ERR_PTR(-EINVAL);\n+\n+\treturn ERR_PTR(err);\n }\n \n /**\n@@ -584,51 +545,6 @@ void swap_update_readahead(struct folio *folio, struct vm_area_struct *vma,\n \t}\n }\n \n-/**\n- * __swap_cache_prepare_and_add - Prepare the folio and add it to swap cache.\n- * @entry: swap entry to be bound to the folio.\n- * @folio: folio to be added.\n- * @gfp: memory allocation flags for charge, can be 0 if @charged if true.\n- * @charged: if the folio is already charged.\n- *\n- * Update the swap_map and add folio as swap cache, typically before swapin.\n- * All swap slots covered by the folio must have a non-zero swap count.\n- *\n- * Context: Caller must protect the swap device with reference count or locks.\n- * Return: 0 if success, error code if failed.\n- */\n-static int __swap_cache_prepare_and_add(swp_entry_t entry,\n-\t\t\t\t\tstruct folio *folio,\n-\t\t\t\t\tgfp_t gfp, bool charged)\n-{\n-\tvoid *shadow;\n-\tint ret;\n-\n-\t__folio_set_locked(folio);\n-\t__folio_set_swapbacked(folio);\n-\tret = swap_cache_add_folio(folio, entry, &shadow);\n-\tif (ret)\n-\t\tgoto failed;\n-\n-\tif (!charged && mem_cgroup_swapin_charge_folio(folio, NULL, gfp, entry)) {\n-\t\tswap_cache_del_folio(folio);\n-\t\tret = -ENOMEM;\n-\t\tgoto failed;\n-\t}\n-\n-\tmemcg1_swapin(entry, folio_nr_pages(folio));\n-\tif (shadow)\n-\t\tworkingset_refault(folio, shadow);\n-\n-\t/* Caller will initiate read into locked folio */\n-\tfolio_add_lru(folio);\n-\treturn 0;\n-\n-failed:\n-\tfolio_unlock(folio);\n-\treturn ret;\n-}\n-\n static struct folio *swap_cache_read_folio(swp_entry_t entry, gfp_t gfp,\n \t\t\t\t\t   struct mempolicy *mpol, pgoff_t ilx,\n \t\t\t\t\t   struct swap_iocb **plug, bool readahead)\n@@ -649,7 +565,6 @@ static struct folio *swap_cache_read_folio(swp_entry_t entry, gfp_t gfp,\n \t\tfolio = swap_cache_get_folio(entry);\n \t\tif (folio)\n \t\t\treturn folio;\n-\n \t\tfolio = swap_cache_alloc_folio(entry, gfp, 0, NULL, mpol, ilx);\n \t} while (PTR_ERR(folio) == -EEXIST);\n \n@@ -666,49 +581,37 @@ static struct folio *swap_cache_read_folio(swp_entry_t entry, gfp_t gfp,\n }\n \n /**\n- * swapin_folio - swap-in one or multiple entries skipping readahead.\n- * @entry: starting swap entry to swap in\n- * @folio: a new allocated and charged folio\n+ * swapin_entry - swap-in one or multiple entries skipping readahead.\n+ * @entry: swap entry indicating the target slot\n+ * @gfp_mask: memory allocation flags\n+ * @orders: allocation orders\n+ * @vmf: fault information\n+ * @mpol: NUMA memory allocation policy to be applied\n+ * @ilx: NUMA interleave index, for use only when MPOL_INTERLEAVE\n  *\n- * Reads @entry into @folio, @folio will be added to the swap cache.\n- * If @folio is a large folio, the @entry will be rounded down to align\n- * with the folio size.\n+ * This would allocate a folio suit given @orders, or return the existing\n+ * folio in the swap cache for @entry. This initiates the IO, too, if needed.\n+ * @entry could be rounded down if @orders allows large allocation.\n  *\n- * Return: returns pointer to @folio on success. If folio is a large folio\n- * and this raced with another swapin, NULL will be returned to allow fallback\n- * to order 0. Else, if another folio was already added to the swap cache,\n- * return that swap cache folio instead.\n+ * Context: Caller must ensure @entry is valid and pin the swap device with refcount.\n+ * Return: Returns the folio on success, returns error code if failed.\n  */\n-struct folio *swapin_folio(swp_entry_t entry, struct folio *folio)\n+struct folio *swapin_entry(swp_entry_t entry, gfp_t gfp, unsigned long orders,\n+\t\t\t   struct vm_fault *vmf, struct mempolicy *mpol, pgoff_t ilx)\n {\n-\tint ret;\n-\tstruct folio *swapcache;\n-\tpgoff_t offset = swp_offset(entry);\n-\tunsigned long nr_pages = folio_nr_pages(folio);\n-\n-\tentry = swp_entry(swp_type(entry), round_down(offset, nr_pages));\n-\tfor (;;) {\n-\t\tret = __swap_cache_prepare_and_add(entry, folio, 0, true);\n-\t\tif (!ret) {\n-\t\t\tswap_read_folio(folio, NULL);\n-\t\t\tbreak;\n-\t\t}\n+\tstruct folio *folio;\n \n-\t\t/*\n-\t\t * Large order allocation needs special handling on\n-\t\t * race: if a smaller folio exists in cache, swapin needs\n-\t\t * to fallback to order 0, and doing a swap cache lookup\n-\t\t * might return a folio that is irrelevant to the faulting\n-\t\t * entry because @entry is aligned down. Just return NULL.\n-\t\t */\n-\t\tif (ret != -EEXIST || nr_pages > 1)\n-\t\t\treturn NULL;\n+\tdo {\n+\t\tfolio = swap_cache_get_folio(entry);\n+\t\tif (folio)\n+\t\t\treturn folio;\n+\t\tfolio = swap_cache_alloc_folio(entry, gfp, orders, vmf, mpol, ilx);\n+\t} while (PTR_ERR(folio) == -EEXIST);\n \n-\t\tswapcache = swap_cache_get_folio(entry);\n-\t\tif (swapcache)\n-\t\t\treturn swapcache;\n-\t}\n+\tif (IS_ERR(folio))\n+\t\treturn NULL;\n \n+\tswap_read_folio(folio, NULL);\n \treturn folio;\n }\n \ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 06b37efad2bd..7e7614a5181a 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -1833,8 +1833,7 @@ void folio_put_swap(struct folio *folio, struct page *subpage)\n  *   do_swap_page()\n  *     ...\t\t\t\tswapoff+swapon\n  *     swap_cache_alloc_folio()\n- *       swap_cache_add_folio()\n- *         // check swap_map\n+ *       // check swap_map\n  *     // verify PTE not changed\n  *\n  * In __swap_duplicate(), the swap_map need to be checked before\n\n-- \n2.53.0",
          "reply_to": "",
          "message_date": "2026-02-20"
        },
        {
          "author": "Kairui Song",
          "summary": "Reviewer Kairui Song noted that the patch changes refault counting at the nearest online memcg level, which is different from file folios and may not be accurate for anon shadows, and requested further investigation.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "To prepare for merging the swap_cgroup_ctrl into the swap table, store\nthe memcg info in the swap table on swapout.\n\nThis is done by using the existing shadow format.\n\nNote this also changes the refault counting at the nearest online memcg\nlevel:\n\nUnlike file folios, anon folios are mostly exclusive to one mem cgroup,\nand each cgroup is likely to have different characteristics.\n\nWhen commit b910718a948a (\"mm: vmscan: detect file thrashing at the\nreclaim root\") moved the refault accounting to the reclaim root level,\nanon shadows don't even exist, and it's explicitly for file pages. Later\ncommit aae466b0052e (\"mm/swap: implement workingset detection for\nanonymous LRU\") added anon shadows following a similar design. And in\nshrink_lruvec, an active LRU's shrinking is done regardlessly when it's\nlow.\n\nFor MGLRU, it's a bit different, but with the PID refault control, it's\nmore accurate to let the nearest online memcg take the refault feedback\ntoo.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n mm/internal.h   | 20 ++++++++++++++++++++\n mm/swap.h       |  7 ++++---\n mm/swap_state.c | 50 +++++++++++++++++++++++++++++++++-----------------\n mm/swapfile.c   |  4 +++-\n mm/vmscan.c     |  6 +-----\n mm/workingset.c | 16 +++++++++++-----\n 6 files changed, 72 insertions(+), 31 deletions(-)\n\ndiff --git a/mm/internal.h b/mm/internal.h\nindex cb0af847d7d9..5bbe081c9048 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -1714,6 +1714,7 @@ static inline void shrinker_debugfs_remove(struct dentry *debugfs_entry,\n #endif /* CONFIG_SHRINKER_DEBUG */\n \n /* Only track the nodes of mappings with shadow entries */\n+#define WORKINGSET_SHIFT 1\n void workingset_update_node(struct xa_node *node);\n extern struct list_lru shadow_nodes;\n #define mapping_set_update(xas, mapping) do {\t\t\t\\\n@@ -1722,6 +1723,25 @@ extern struct list_lru shadow_nodes;\n \t\txas_set_lru(xas, &shadow_nodes);\t\t\\\n \t}\t\t\t\t\t\t\t\\\n } while (0)\n+static inline unsigned short shadow_to_memcgid(void *shadow)\n+{\n+\tunsigned long entry = xa_to_value(shadow);\n+\tunsigned short memcgid;\n+\n+\tentry >>= (WORKINGSET_SHIFT + NODES_SHIFT);\n+\tmemcgid = entry & ((1UL << MEM_CGROUP_ID_SHIFT) - 1);\n+\n+\treturn memcgid;\n+}\n+static inline void *memcgid_to_shadow(unsigned short memcgid)\n+{\n+\tunsigned long val;\n+\n+\tval = memcgid;\n+\tval <<= (NODES_SHIFT + WORKINGSET_SHIFT);\n+\n+\treturn xa_mk_value(val);\n+}\n \n /* mremap.c */\n unsigned long move_page_tables(struct pagetable_move_control *pmc);\ndiff --git a/mm/swap.h b/mm/swap.h\nindex da41e9cea46d..c95f5fafea42 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -265,6 +265,8 @@ static inline bool folio_matches_swap_entry(const struct folio *folio,\n \treturn folio_entry.val == round_down(entry.val, nr_pages);\n }\n \n+bool folio_maybe_swapped(struct folio *folio);\n+\n /*\n  * All swap cache helpers below require the caller to ensure the swap entries\n  * used are valid and stabilize the device by any of the following ways:\n@@ -286,9 +288,8 @@ struct folio *swap_cache_alloc_folio(swp_entry_t target_entry, gfp_t gfp_mask,\n /* Below helpers require the caller to lock and pass in the swap cluster. */\n void __swap_cache_add_folio(struct swap_cluster_info *ci,\n \t\t\t    struct folio *folio, swp_entry_t entry);\n-void __swap_cache_del_folio(struct swap_cluster_info *ci,\n-\t\t\t    struct folio *folio, void *shadow,\n-\t\t\t    bool charged, bool reclaim);\n+void __swap_cache_del_folio(struct swap_cluster_info *ci, struct folio *folio,\n+\t\t\t    void *shadow, bool charged, bool reclaim);\n void __swap_cache_replace_folio(struct swap_cluster_info *ci,\n \t\t\t\tstruct folio *old, struct folio *new);\n \ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 40f037576c5f..cc4bf40320ef 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -143,22 +143,11 @@ static int __swap_cache_check_batch(struct swap_cluster_info *ci,\n {\n \tunsigned int ci_end = ci_off + nr;\n \tunsigned long old_tb;\n+\tunsigned int memcgid;\n \n \tif (unlikely(!ci->table))\n \t\treturn -ENOENT;\n \n-\tdo {\n-\t\told_tb = __swap_table_get(ci, ci_off);\n-\t\tif (unlikely(swp_tb_is_folio(old_tb)) ||\n-\t\t    unlikely(!__swp_tb_get_count(old_tb)))\n-\t\t\tbreak;\n-\t\tif (swp_tb_is_shadow(old_tb))\n-\t\t\t*shadowp = swp_tb_to_shadow(old_tb);\n-\t} while (++ci_off < ci_end);\n-\n-\tif (likely(ci_off == ci_end))\n-\t\treturn 0;\n-\n \t/*\n \t * If the target slot is not suitable for adding swap cache, return\n \t * -EEXIST or -ENOENT. If the batch is not suitable, could be a\n@@ -169,7 +158,21 @@ static int __swap_cache_check_batch(struct swap_cluster_info *ci,\n \t\treturn -EEXIST;\n \tif (!__swp_tb_get_count(old_tb))\n \t\treturn -ENOENT;\n-\treturn -EBUSY;\n+\tif (WARN_ON_ONCE(!swp_tb_is_shadow(old_tb)))\n+\t\treturn -ENOENT;\n+\t*shadowp = swp_tb_to_shadow(old_tb);\n+\tmemcgid = shadow_to_memcgid(*shadowp);\n+\n+\tWARN_ON_ONCE(!mem_cgroup_disabled() && !memcgid);\n+\tdo {\n+\t\told_tb = __swap_table_get(ci, ci_off);\n+\t\tif (unlikely(swp_tb_is_folio(old_tb)) ||\n+\t\t    unlikely(!__swp_tb_get_count(old_tb)) ||\n+\t\t    memcgid != shadow_to_memcgid(swp_tb_to_shadow(old_tb)))\n+\t\t\treturn -EBUSY;\n+\t} while (++ci_off < ci_end);\n+\n+\treturn 0;\n }\n \n void __swap_cache_add_folio(struct swap_cluster_info *ci,\n@@ -261,8 +264,7 @@ static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n \n \t/* For memsw accouting, swap is uncharged when folio is added to swap cache */\n \tmemcg1_swapin(folio);\n-\tif (shadow)\n-\t\tworkingset_refault(folio, shadow);\n+\tworkingset_refault(folio, shadow);\n \n \t/* Caller will initiate read into locked new_folio */\n \tfolio_add_lru(folio);\n@@ -319,7 +321,8 @@ struct folio *swap_cache_alloc_folio(swp_entry_t targ_entry, gfp_t gfp_mask,\n  * __swap_cache_del_folio - Removes a folio from the swap cache.\n  * @ci: The locked swap cluster.\n  * @folio: The folio.\n- * @shadow: shadow value to be filled in the swap cache.\n+ * @shadow: Shadow to restore when the folio is not charged. Ignored when\n+ *          @charged is true, as the shadow is computed internally.\n  * @charged: If folio->swap is charged to folio->memcg.\n  * @reclaim: If the folio is being reclaimed. When true on cgroup v1,\n  *           the memory charge is transferred from memory to swap.\n@@ -336,6 +339,7 @@ void __swap_cache_del_folio(struct swap_cluster_info *ci, struct folio *folio,\n \tint count;\n \tunsigned long old_tb;\n \tstruct swap_info_struct *si;\n+\tstruct mem_cgroup *memcg = NULL;\n \tswp_entry_t entry = folio->swap;\n \tunsigned int ci_start, ci_off, ci_end;\n \tbool folio_swapped = false, need_free = false;\n@@ -353,7 +357,13 @@ void __swap_cache_del_folio(struct swap_cluster_info *ci, struct folio *folio,\n \t * charging (e.g. swapin charge failure, or swap alloc charge failure).\n \t */\n \tif (charged)\n-\t\tmem_cgroup_swap_free_folio(folio, reclaim);\n+\t\tmemcg = mem_cgroup_swap_free_folio(folio, reclaim);\n+\tif (reclaim) {\n+\t\tWARN_ON(!charged);\n+\t\tshadow = workingset_eviction(folio, memcg);\n+\t} else if (memcg) {\n+\t\tshadow = memcgid_to_shadow(mem_cgroup_private_id(memcg));\n+\t}\n \n \tsi = __swap_entry_to_info(entry);\n \tci_start = swp_cluster_offset(entry);\n@@ -392,6 +402,11 @@ void __swap_cache_del_folio(struct swap_cluster_info *ci, struct folio *folio,\n  * swap_cache_del_folio - Removes a folio from the swap cache.\n  * @folio: The folio.\n  *\n+ * Force delete a folio from the swap cache. This is only safe to use for\n+ * folios that are not swapped out (swap count == 0) to release the swap\n+ * space from being pinned by swap cache, or remove a clean and charged\n+ * folio that no one modified or is still using.\n+ *\n  * Same as __swap_cache_del_folio, but handles lock and refcount. The\n  * caller must ensure the folio is either clean or has a swap count\n  * equal to zero, or it may cause data loss.\n@@ -404,6 +419,7 @@ void swap_cache_del_folio(struct folio *folio)\n \tswp_entry_t entry = folio->swap;\n \n \tci = swap_cluster_lock(__swap_entry_to_info(entry), swp_offset(entry));\n+\tVM_WARN_ON_ONCE(folio_test_dirty(folio) && folio_maybe_swapped(folio));\n \t__swap_cache_del_folio(ci, folio, NULL, true, false);\n \tswap_cluster_unlock(ci);\n \ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex c0169bce46c9..2cd3e260f1bf 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -1972,9 +1972,11 @@ int swp_swapcount(swp_entry_t entry)\n  * decrease of swap count is possible through swap_put_entries_direct, so this\n  * may return a false positive.\n  *\n+ * Caller can hold the ci lock to get a stable result.\n+ *\n  * Context: Caller must ensure the folio is locked and in the swap cache.\n  */\n-static bool folio_maybe_swapped(struct folio *folio)\n+bool folio_maybe_swapped(struct folio *folio)\n {\n \tswp_entry_t entry = folio->swap;\n \tstruct swap_cluster_info *ci;\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 5112f81cf875..4565c9c3ac60 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -755,11 +755,7 @@ static int __remove_mapping(struct address_space *mapping, struct folio *folio,\n \t}\n \n \tif (folio_test_swapcache(folio)) {\n-\t\tswp_entry_t swap = folio->swap;\n-\n-\t\tif (reclaimed && !mapping_exiting(mapping))\n-\t\t\tshadow = workingset_eviction(folio, target_memcg);\n-\t\t__swap_cache_del_folio(ci, folio, shadow, true, true);\n+\t\t__swap_cache_del_folio(ci, folio, NULL, true, true);\n \t\tswap_cluster_unlock_irq(ci);\n \t} else {\n \t\tvoid (*free_folio)(struct folio *);\ndiff --git a/mm/workingset.c b/mm/workingset.c\nindex 37a94979900f..765a954baefa 100644\n--- a/mm/workingset.c\n+++ b/mm/workingset.c\n@@ -202,12 +202,18 @@ static unsigned int bucket_order[ANON_AND_FILE] __read_mostly;\n static void *pack_shadow(int memcgid, pg_data_t *pgdat, unsigned long eviction,\n \t\t\t bool workingset, bool file)\n {\n+\tvoid *shadow;\n+\n \teviction &= file ? EVICTION_MASK : EVICTION_MASK_ANON;\n \teviction = (eviction << MEM_CGROUP_ID_SHIFT) | memcgid;\n \teviction = (eviction << NODES_SHIFT) | pgdat->node_id;\n \teviction = (eviction << WORKINGSET_SHIFT) | workingset;\n \n-\treturn xa_mk_value(eviction);\n+\tshadow = xa_mk_value(eviction);\n+\t/* Sanity check for retrieving memcgid from anon shadow. */\n+\tVM_WARN_ON_ONCE(shadow_to_memcgid(shadow) != memcgid);\n+\n+\treturn shadow;\n }\n \n static void unpack_shadow(void *shadow, int *memcgidp, pg_data_t **pgdat,\n@@ -232,7 +238,7 @@ static void unpack_shadow(void *shadow, int *memcgidp, pg_data_t **pgdat,\n \n #ifdef CONFIG_LRU_GEN\n \n-static void *lru_gen_eviction(struct folio *folio)\n+static void *lru_gen_eviction(struct folio *folio, struct mem_cgroup *memcg)\n {\n \tint hist;\n \tunsigned long token;\n@@ -244,7 +250,6 @@ static void *lru_gen_eviction(struct folio *folio)\n \tint refs = folio_lru_refs(folio);\n \tbool workingset = folio_test_workingset(folio);\n \tint tier = lru_tier_from_refs(refs, workingset);\n-\tstruct mem_cgroup *memcg = folio_memcg(folio);\n \tstruct pglist_data *pgdat = folio_pgdat(folio);\n \n \tBUILD_BUG_ON(LRU_GEN_WIDTH + LRU_REFS_WIDTH >\n@@ -252,6 +257,7 @@ static void *lru_gen_eviction(struct folio *folio)\n \n \tlruvec = mem_cgroup_lruvec(memcg, pgdat);\n \tlrugen = &lruvec->lrugen;\n+\tmemcg = lruvec_memcg(lruvec);\n \tmin_seq = READ_ONCE(lrugen->min_seq[type]);\n \ttoken = (min_seq << LRU_REFS_WIDTH) | max(refs - 1, 0);\n \n@@ -329,7 +335,7 @@ static void lru_gen_refault(struct folio *folio, void *shadow)\n \n #else /* !CONFIG_LRU_GEN */\n \n-static void *lru_gen_eviction(struct folio *folio)\n+static void *lru_gen_eviction(struct folio *folio, struct mem_cgroup *target_memcg)\n {\n \treturn NULL;\n }\n@@ -396,7 +402,7 @@ void *workingset_eviction(struct folio *folio, struct mem_cgroup *target_memcg)\n \tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n \n \tif (lru_gen_enabled())\n-\t\treturn lru_gen_eviction(folio);\n+\t\treturn lru_gen_eviction(folio, target_memcg);\n \n \tlruvec = mem_cgroup_lruvec(target_memcg, pgdat);\n \t/* XXX: target_memcg can be NULL, go through lruvec */\n\n-- \n2.53.0",
          "reply_to": "",
          "message_date": "2026-02-20"
        },
        {
          "author": "Kairui Song",
          "summary": "Reviewer Kairui Song noted that vswap_free() acquires the per-vswap spinlock while holding the folio lock, creating a potential lock ordering violation with reclaim paths, and suggested dropping the lock before calling try_to_unmap().",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "potential deadlock",
            "lock ordering issue"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "To make sure folio->swap always belongs to folio->memcg, when doing the\ncharge, charge against folio->memcg. Defer the recording of swap cgroup\ninfo, do a reparent, and record the nearest online ancestor on swap cache\nremoval only.\n\nThen, a folio is in the swap cache, and the folio itself is owned by the\nmemcg. Hence, through the folio, the memcg also owns folio->swap. The\nextra pinning of the swap cgroup info record is not needed and can be\nreleased.\n\nThis should be fine for both cgroup v2 and v1. There should be no\nuserspace observable behavior.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n include/linux/memcontrol.h |   8 ++--\n include/linux/swap.h       |  24 +++++++++--\n mm/memcontrol-v1.c         |  77 ++++++++++++++++-----------------\n mm/memcontrol.c            | 104 ++++++++++++++++++++++++++++++++-------------\n mm/swap.h                  |   6 ++-\n mm/swap_cgroup.c           |   5 +--\n mm/swap_state.c            |  26 +++++++++---\n mm/swapfile.c              |  15 +++++--\n mm/vmscan.c                |   3 +-\n 9 files changed, 173 insertions(+), 95 deletions(-)\n\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex 70b685a85bf4..0b37d4faf785 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -1896,8 +1896,8 @@ static inline void mem_cgroup_exit_user_fault(void)\n \tcurrent->in_user_fault = 0;\n }\n \n-void memcg1_swapout(struct folio *folio, swp_entry_t entry);\n-void memcg1_swapin(swp_entry_t entry, unsigned int nr_pages);\n+void memcg1_swapout(struct folio *folio, struct mem_cgroup *swap_memcg);\n+void memcg1_swapin(struct folio *folio);\n \n #else /* CONFIG_MEMCG_V1 */\n static inline\n@@ -1926,11 +1926,11 @@ static inline void mem_cgroup_exit_user_fault(void)\n {\n }\n \n-static inline void memcg1_swapout(struct folio *folio, swp_entry_t entry)\n+static inline void memcg1_swapout(struct folio *folio, struct mem_cgroup *_memcg)\n {\n }\n \n-static inline void memcg1_swapin(swp_entry_t entry, unsigned int nr_pages)\n+static inline void memcg1_swapin(struct folio *folio)\n {\n }\n \ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 0effe3cc50f5..66cf657a1f35 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -580,12 +580,22 @@ static inline int mem_cgroup_try_charge_swap(struct folio *folio,\n \treturn __mem_cgroup_try_charge_swap(folio, entry);\n }\n \n-extern void __mem_cgroup_uncharge_swap(swp_entry_t entry, unsigned int nr_pages);\n-static inline void mem_cgroup_uncharge_swap(swp_entry_t entry, unsigned int nr_pages)\n+extern void __mem_cgroup_uncharge_swap(unsigned short id, unsigned int nr_pages);\n+static inline void mem_cgroup_uncharge_swap(unsigned short id, unsigned int nr_pages)\n {\n \tif (mem_cgroup_disabled())\n \t\treturn;\n-\t__mem_cgroup_uncharge_swap(entry, nr_pages);\n+\t__mem_cgroup_uncharge_swap(id, nr_pages);\n+}\n+\n+struct mem_cgroup *__mem_cgroup_swap_free_folio(struct folio *folio,\n+\t\t\t\t\t       bool reclaim);\n+static inline struct mem_cgroup *mem_cgroup_swap_free_folio(struct folio *folio,\n+\t\t\t\t\t\t\t    bool reclaim)\n+{\n+\tif (mem_cgroup_disabled())\n+\t\treturn NULL;\n+\treturn __mem_cgroup_swap_free_folio(folio, reclaim);\n }\n \n extern long mem_cgroup_get_nr_swap_pages(struct mem_cgroup *memcg);\n@@ -597,11 +607,17 @@ static inline int mem_cgroup_try_charge_swap(struct folio *folio,\n \treturn 0;\n }\n \n-static inline void mem_cgroup_uncharge_swap(swp_entry_t entry,\n+static inline void mem_cgroup_uncharge_swap(unsigned short id,\n \t\t\t\t\t    unsigned int nr_pages)\n {\n }\n \n+static inline struct mem_cgroup *mem_cgroup_swap_free_folio(struct folio *folio,\n+\t\t\t\t\t\t\t    bool reclaim)\n+{\n+\treturn NULL;\n+}\n+\n static inline long mem_cgroup_get_nr_swap_pages(struct mem_cgroup *memcg)\n {\n \treturn get_nr_swap_pages();\ndiff --git a/mm/memcontrol-v1.c b/mm/memcontrol-v1.c\nindex a7c78b0987df..038e630dc7e1 100644\n--- a/mm/memcontrol-v1.c\n+++ b/mm/memcontrol-v1.c\n@@ -606,29 +606,21 @@ void memcg1_commit_charge(struct folio *folio, struct mem_cgroup *memcg)\n /**\n  * memcg1_swapout - transfer a memsw charge to swap\n  * @folio: folio whose memsw charge to transfer\n- * @entry: swap entry to move the charge to\n- *\n- * Transfer the memsw charge of @folio to @entry.\n+ * @swap_memcg: cgroup that will be charged, must be online ancestor\n+ *              of folio's memcg.\n  */\n-void memcg1_swapout(struct folio *folio, swp_entry_t entry)\n+void memcg1_swapout(struct folio *folio, struct mem_cgroup *swap_memcg)\n {\n-\tstruct mem_cgroup *memcg, *swap_memcg;\n+\tstruct mem_cgroup *memcg;\n \tunsigned int nr_entries;\n+\tunsigned long flags;\n \n-\tVM_BUG_ON_FOLIO(folio_test_lru(folio), folio);\n-\tVM_BUG_ON_FOLIO(folio_ref_count(folio), folio);\n-\n-\tif (mem_cgroup_disabled())\n-\t\treturn;\n-\n-\tif (!do_memsw_account())\n-\t\treturn;\n+\t/* The folio must be getting reclaimed. */\n+\tVM_WARN_ON_ONCE_FOLIO(folio_mapped(folio), folio);\n \n \tmemcg = folio_memcg(folio);\n \n \tVM_WARN_ON_ONCE_FOLIO(!memcg, folio);\n-\tif (!memcg)\n-\t\treturn;\n \n \t/*\n \t * In case the memcg owning these pages has been offlined and doesn't\n@@ -636,14 +628,15 @@ void memcg1_swapout(struct folio *folio, swp_entry_t entry)\n \t * ancestor for the swap instead and transfer the memory+swap charge.\n \t */\n \tnr_entries = folio_nr_pages(folio);\n-\tswap_memcg = mem_cgroup_private_id_get_online(memcg, nr_entries);\n \tmod_memcg_state(swap_memcg, MEMCG_SWAP, nr_entries);\n \n-\tswap_cgroup_record(folio, mem_cgroup_private_id(swap_memcg), entry);\n-\n \tfolio_unqueue_deferred_split(folio);\n-\tfolio->memcg_data = 0;\n \n+\t/*\n+\t * Free the folio charge now so memsw won't be double uncharged:\n+\t * memsw is now charged by the swap record.\n+\t */\n+\tfolio->memcg_data = 0;\n \tif (!mem_cgroup_is_root(memcg))\n \t\tpage_counter_uncharge(&memcg->memory, nr_entries);\n \n@@ -653,33 +646,34 @@ void memcg1_swapout(struct folio *folio, swp_entry_t entry)\n \t\tpage_counter_uncharge(&memcg->memsw, nr_entries);\n \t}\n \n-\t/*\n-\t * Interrupts should be disabled here because the caller holds the\n-\t * i_pages lock which is taken with interrupts-off. It is\n-\t * important here to have the interrupts disabled because it is the\n-\t * only synchronisation we have for updating the per-CPU variables.\n-\t */\n+\tlocal_irq_save(flags);\n \tpreempt_disable_nested();\n-\tVM_WARN_ON_IRQS_ENABLED();\n \tmemcg1_charge_statistics(memcg, -folio_nr_pages(folio));\n \tpreempt_enable_nested();\n+\tlocal_irq_restore(flags);\n \tmemcg1_check_events(memcg, folio_nid(folio));\n \n \tcss_put(&memcg->css);\n }\n \n /*\n- * memcg1_swapin - uncharge swap slot\n- * @entry: the first swap entry for which the pages are charged\n- * @nr_pages: number of pages which will be uncharged\n+ * memcg1_swapin - uncharge memsw for the swap slot on swapin\n+ * @folio: the folio being swapped in, already charged to memory\n  *\n  * Call this function after successfully adding the charged page to swapcache.\n- *\n- * Note: This function assumes the page for which swap slot is being uncharged\n- * is order 0 page.\n+ * The swap cgroup tracking has already been released by\n+ * mem_cgroup_swapin_charge_folio(), so we only need to drop the duplicate\n+ * memsw charge that was placed on the swap entry during swapout.\n  */\n-void memcg1_swapin(swp_entry_t entry, unsigned int nr_pages)\n+void memcg1_swapin(struct folio *folio)\n {\n+\tstruct mem_cgroup *memcg;\n+\tunsigned int nr_pages;\n+\n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_locked(folio), folio);\n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapcache(folio), folio);\n+\tVM_WARN_ON_ONCE_FOLIO(!folio_memcg_charged(folio), folio);\n+\n \t/*\n \t * Cgroup1's unified memory+swap counter has been charged with the\n \t * new swapcache page, finish the transfer by uncharging the swap\n@@ -692,14 +686,15 @@ void memcg1_swapin(swp_entry_t entry, unsigned int nr_pages)\n \t * correspond 1:1 to page and swap slot lifetimes: we charge the\n \t * page to memory here, and uncharge swap when the slot is freed.\n \t */\n-\tif (do_memsw_account()) {\n-\t\t/*\n-\t\t * The swap entry might not get freed for a long time,\n-\t\t * let's not wait for it.  The page already received a\n-\t\t * memory+swap charge, drop the swap entry duplicate.\n-\t\t */\n-\t\tmem_cgroup_uncharge_swap(entry, nr_pages);\n-\t}\n+\tif (!do_memsw_account())\n+\t\treturn;\n+\n+\tmemcg = folio_memcg(folio);\n+\tnr_pages = folio_nr_pages(folio);\n+\n+\tif (!mem_cgroup_is_root(memcg))\n+\t\tpage_counter_uncharge(&memcg->memsw, nr_pages);\n+\tmod_memcg_state(memcg, MEMCG_SWAP, -nr_pages);\n }\n \n void memcg1_uncharge_batch(struct mem_cgroup *memcg, unsigned long pgpgout,\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex b2898719e935..d9ff44b77409 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -4804,8 +4804,8 @@ int mem_cgroup_swapin_charge_folio(struct folio *folio, struct mm_struct *mm,\n \t\t\t\t  gfp_t gfp, swp_entry_t entry)\n {\n \tstruct mem_cgroup *memcg, *swap_memcg;\n-\tunsigned short id, parent_id;\n \tunsigned int nr_pages;\n+\tunsigned short id;\n \tint ret;\n \n \tif (mem_cgroup_disabled())\n@@ -4831,37 +4831,31 @@ int mem_cgroup_swapin_charge_folio(struct folio *folio, struct mm_struct *mm,\n \tif (ret)\n \t\tgoto out;\n \n+\t/*\n+\t * On successful charge, the folio itself now belongs to the memcg,\n+\t * so is folio->swap. So we can release the swap cgroup table's\n+\t * pinning of the private id.\n+\t */\n+\tswap_cgroup_clear(folio->swap, nr_pages);\n+\tmem_cgroup_private_id_put(swap_memcg, nr_pages);\n+\n \t/*\n \t * If the swap entry's memcg is dead, reparent the swap charge\n \t * from swap_memcg to memcg.\n-\t *\n-\t * If memcg is also being offlined, the charge will be moved to\n-\t * its parent again.\n \t */\n \tif (swap_memcg && memcg != swap_memcg) {\n-\t\tstruct mem_cgroup *parent_memcg;\n-\n-\t\tparent_memcg = mem_cgroup_private_id_get_online(memcg, nr_pages);\n-\t\tparent_id = mem_cgroup_private_id(parent_memcg);\n-\n-\t\tWARN_ON(id != swap_cgroup_clear(entry, nr_pages));\n-\t\tswap_cgroup_record(folio, parent_id, entry);\n-\n \t\tif (do_memsw_account()) {\n-\t\t\tif (!mem_cgroup_is_root(parent_memcg))\n-\t\t\t\tpage_counter_charge(&parent_memcg->memsw, nr_pages);\n+\t\t\tif (!mem_cgroup_is_root(memcg))\n+\t\t\t\tpage_counter_charge(&memcg->memsw, nr_pages);\n \t\t\tpage_counter_uncharge(&swap_memcg->memsw, nr_pages);\n \t\t} else {\n-\t\t\tif (!mem_cgroup_is_root(parent_memcg))\n-\t\t\t\tpage_counter_charge(&parent_memcg->swap, nr_pages);\n+\t\t\tif (!mem_cgroup_is_root(memcg))\n+\t\t\t\tpage_counter_charge(&memcg->swap, nr_pages);\n \t\t\tpage_counter_uncharge(&swap_memcg->swap, nr_pages);\n \t\t}\n \n-\t\tmod_memcg_state(parent_memcg, MEMCG_SWAP, nr_pages);\n+\t\tmod_memcg_state(memcg, MEMCG_SWAP, nr_pages);\n \t\tmod_memcg_state(swap_memcg, MEMCG_SWAP, -nr_pages);\n-\n-\t\t/* Release the dead cgroup after reparent */\n-\t\tmem_cgroup_private_id_put(swap_memcg, nr_pages);\n \t}\n out:\n \tcss_put(&memcg->css);\n@@ -5260,33 +5254,32 @@ int __mem_cgroup_try_charge_swap(struct folio *folio, swp_entry_t entry)\n \t\treturn 0;\n \t}\n \n-\tmemcg = mem_cgroup_private_id_get_online(memcg, nr_pages);\n-\n+\t/*\n+\t * Charge the swap counter against the folio's memcg directly.\n+\t * The private id pinning and swap cgroup recording are deferred\n+\t * to __mem_cgroup_swap_free_folio() when the folio leaves the\n+\t * swap cache.  No _id_get_online here means no _id_put on error.\n+\t */\n \tif (!mem_cgroup_is_root(memcg) &&\n \t    !page_counter_try_charge(&memcg->swap, nr_pages, &counter)) {\n \t\tmemcg_memory_event(memcg, MEMCG_SWAP_MAX);\n \t\tmemcg_memory_event(memcg, MEMCG_SWAP_FAIL);\n-\t\tmem_cgroup_private_id_put(memcg, nr_pages);\n \t\treturn -ENOMEM;\n \t}\n \tmod_memcg_state(memcg, MEMCG_SWAP, nr_pages);\n \n-\tswap_cgroup_record(folio, mem_cgroup_private_id(memcg), entry);\n-\n \treturn 0;\n }\n \n /**\n  * __mem_cgroup_uncharge_swap - uncharge swap space\n- * @entry: swap entry to uncharge\n+ * @id: private id of the mem_cgroup to uncharge\n  * @nr_pages: the amount of swap space to uncharge\n  */\n-void __mem_cgroup_uncharge_swap(swp_entry_t entry, unsigned int nr_pages)\n+void __mem_cgroup_uncharge_swap(unsigned short id, unsigned int nr_pages)\n {\n \tstruct mem_cgroup *memcg;\n-\tunsigned short id;\n \n-\tid = swap_cgroup_clear(entry, nr_pages);\n \trcu_read_lock();\n \tmemcg = mem_cgroup_from_private_id(id);\n \tif (memcg) {\n@@ -5302,6 +5295,59 @@ void __mem_cgroup_uncharge_swap(swp_entry_t entry, unsigned int nr_pages)\n \trcu_read_unlock();\n }\n \n+/**\n+ * __mem_cgroup_swap_free_folio - Folio is being freed from swap cache.\n+ * @folio: folio being freed.\n+ * @reclaim: true if the folio is being reclaimed.\n+ *\n+ * For cgroup V2, swap entries are charged to folio's memcg by the time\n+ * swap allocator adds it into the swap cache by mem_cgroup_try_charge_swap.\n+ * The ownership of folio->swap to folio->memcg is constrained by the folio\n+ * in swap cache. If the folio is being removed from swap cache, the\n+ * constraint will be gone so need to grab the memcg's private id for long\n+ * term tracking.\n+ *\n+ * For cgroup V1, the memory-to-swap charge transfer is also performed on\n+ * the folio reclaim path.\n+ *\n+ * It's unlikely but possible that the folio's memcg is dead, in that case\n+ * we reparent and recharge the parent. Recorded cgroup is changed to\n+ * parent too.\n+ *\n+ * Return: Pointer to the mem cgroup being pinned by the charge.\n+ */\n+struct mem_cgroup *__mem_cgroup_swap_free_folio(struct folio *folio,\n+\t\t\t\t\t       bool reclaim)\n+{\n+\tunsigned int nr_pages = folio_nr_pages(folio);\n+\tstruct mem_cgroup *memcg, *swap_memcg;\n+\tswp_entry_t entry = folio->swap;\n+\tunsigned short id;\n+\n+\tVM_WARN_ON_ONCE_FOLIO(!folio_memcg_charged(folio), folio);\n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapcache(folio), folio);\n+\n+\t/*\n+\t * Pin the nearest online ancestor's private id for long term\n+\t * swap cgroup tracking.  If memcg is still alive, swap_memcg\n+\t * will be the same as memcg. Else, it's reparented.\n+\t */\n+\tmemcg = folio_memcg(folio);\n+\tswap_memcg = mem_cgroup_private_id_get_online(memcg, nr_pages);\n+\tid = mem_cgroup_private_id(swap_memcg);\n+\tswap_cgroup_record(folio, id, entry);\n+\n+\tif (reclaim && do_memsw_account()) {\n+\t\tmemcg1_swapout(folio, swap_memcg);\n+\t} else if (memcg != swap_memcg) {\n+\t\tif (!mem_cgroup_is_root(swap_memcg))\n+\t\t\tpage_counter_charge(&swap_memcg->swap, nr_pages);\n+\t\tpage_counter_uncharge(&memcg->swap, nr_pages);\n+\t}\n+\n+\treturn swap_memcg;\n+}\n+\n long mem_cgroup_get_nr_swap_pages(struct mem_cgroup *memcg)\n {\n \tlong nr_swap_pages = get_nr_swap_pages();\ndiff --git a/mm/swap.h b/mm/swap.h\nindex 80c2f1bf7a57..da41e9cea46d 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -287,7 +287,8 @@ struct folio *swap_cache_alloc_folio(swp_entry_t target_entry, gfp_t gfp_mask,\n void __swap_cache_add_folio(struct swap_cluster_info *ci,\n \t\t\t    struct folio *folio, swp_entry_t entry);\n void __swap_cache_del_folio(struct swap_cluster_info *ci,\n-\t\t\t    struct folio *folio, swp_entry_t entry, void *shadow);\n+\t\t\t    struct folio *folio, void *shadow,\n+\t\t\t    bool charged, bool reclaim);\n void __swap_cache_replace_folio(struct swap_cluster_info *ci,\n \t\t\t\tstruct folio *old, struct folio *new);\n \n@@ -459,7 +460,8 @@ static inline void swap_cache_del_folio(struct folio *folio)\n }\n \n static inline void __swap_cache_del_folio(struct swap_cluster_info *ci,\n-\t\tstruct folio *folio, swp_entry_t entry, void *shadow)\n+\t\tstruct folio *folio, void *shadow,\n+\t\tbool charged, bool reclaim)\n {\n }\n \ndiff --git a/mm/swap_cgroup.c b/mm/swap_cgroup.c\nindex de779fed8c21..b5a7f21c3afe 100644\n--- a/mm/swap_cgroup.c\n+++ b/mm/swap_cgroup.c\n@@ -54,8 +54,7 @@ static unsigned short __swap_cgroup_id_xchg(struct swap_cgroup *map,\n /**\n  * swap_cgroup_record - record mem_cgroup for a set of swap entries.\n  * These entries must belong to one single folio, and that folio\n- * must be being charged for swap space (swap out), and these\n- * entries must not have been charged\n+ * must be being charged for swap space (swap out).\n  *\n  * @folio: the folio that the swap entry belongs to\n  * @id: mem_cgroup ID to be recorded\n@@ -75,7 +74,7 @@ void swap_cgroup_record(struct folio *folio, unsigned short id,\n \n \tdo {\n \t\told = __swap_cgroup_id_xchg(map, offset, id);\n-\t\tVM_BUG_ON(old);\n+\t\tVM_WARN_ON_ONCE(old);\n \t} while (++offset != end);\n }\n \ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 0a2a4e084cf2..40f037576c5f 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -251,7 +251,7 @@ static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n \tif (mem_cgroup_swapin_charge_folio(folio, vmf ? vmf->vma->vm_mm : NULL,\n \t\t\t\t\t   gfp, entry)) {\n \t\tspin_lock(&ci->lock);\n-\t\t__swap_cache_del_folio(ci, folio, shadow);\n+\t\t__swap_cache_del_folio(ci, folio, shadow, false, false);\n \t\tspin_unlock(&ci->lock);\n \t\tfolio_unlock(folio);\n \t\tfolio_put(folio);\n@@ -260,7 +260,7 @@ static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n \t}\n \n \t/* For memsw accouting, swap is uncharged when folio is added to swap cache */\n-\tmemcg1_swapin(entry, 1 << order);\n+\tmemcg1_swapin(folio);\n \tif (shadow)\n \t\tworkingset_refault(folio, shadow);\n \n@@ -319,21 +319,24 @@ struct folio *swap_cache_alloc_folio(swp_entry_t targ_entry, gfp_t gfp_mask,\n  * __swap_cache_del_folio - Removes a folio from the swap cache.\n  * @ci: The locked swap cluster.\n  * @folio: The folio.\n- * @entry: The first swap entry that the folio corresponds to.\n  * @shadow: shadow value to be filled in the swap cache.\n+ * @charged: If folio->swap is charged to folio->memcg.\n+ * @reclaim: If the folio is being reclaimed. When true on cgroup v1,\n+ *           the memory charge is transferred from memory to swap.\n  *\n  * Removes a folio from the swap cache and fills a shadow in place.\n  * This won't put the folio's refcount. The caller has to do that.\n  *\n- * Context: Caller must ensure the folio is locked and in the swap cache\n- * using the index of @entry, and lock the cluster that holds the entries.\n+ * Context: Caller must ensure the folio is locked and in the swap cache,\n+ * and lock the cluster that holds the entries.\n  */\n void __swap_cache_del_folio(struct swap_cluster_info *ci, struct folio *folio,\n-\t\t\t    swp_entry_t entry, void *shadow)\n+\t\t\t    void *shadow, bool charged, bool reclaim)\n {\n \tint count;\n \tunsigned long old_tb;\n \tstruct swap_info_struct *si;\n+\tswp_entry_t entry = folio->swap;\n \tunsigned int ci_start, ci_off, ci_end;\n \tbool folio_swapped = false, need_free = false;\n \tunsigned long nr_pages = folio_nr_pages(folio);\n@@ -343,6 +346,15 @@ void __swap_cache_del_folio(struct swap_cluster_info *ci, struct folio *folio,\n \tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapcache(folio), folio);\n \tVM_WARN_ON_ONCE_FOLIO(folio_test_writeback(folio), folio);\n \n+\t/*\n+\t * If the folio's swap entry is charged to its memcg, record the\n+\t * swap cgroup for long-term tracking before the folio leaves the\n+\t * swap cache.  Not charged when the folio never completed memcg\n+\t * charging (e.g. swapin charge failure, or swap alloc charge failure).\n+\t */\n+\tif (charged)\n+\t\tmem_cgroup_swap_free_folio(folio, reclaim);\n+\n \tsi = __swap_entry_to_info(entry);\n \tci_start = swp_cluster_offset(entry);\n \tci_end = ci_start + nr_pages;\n@@ -392,7 +404,7 @@ void swap_cache_del_folio(struct folio *folio)\n \tswp_entry_t entry = folio->swap;\n \n \tci = swap_cluster_lock(__swap_entry_to_info(entry), swp_offset(entry));\n-\t__swap_cache_del_folio(ci, folio, entry, NULL);\n+\t__swap_cache_del_folio(ci, folio, NULL, true, false);\n \tswap_cluster_unlock(ci);\n \n \tfolio_ref_sub(folio, folio_nr_pages(folio));\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 7e7614a5181a..c0169bce46c9 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -1703,6 +1703,7 @@ int folio_alloc_swap(struct folio *folio)\n {\n \tunsigned int order = folio_order(folio);\n \tunsigned int size = 1 << order;\n+\tstruct swap_cluster_info *ci;\n \n \tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n \tVM_BUG_ON_FOLIO(!folio_test_uptodate(folio), folio);\n@@ -1737,8 +1738,12 @@ int folio_alloc_swap(struct folio *folio)\n \t}\n \n \t/* Need to call this even if allocation failed, for MEMCG_SWAP_FAIL. */\n-\tif (unlikely(mem_cgroup_try_charge_swap(folio, folio->swap)))\n-\t\tswap_cache_del_folio(folio);\n+\tif (unlikely(mem_cgroup_try_charge_swap(folio, folio->swap))) {\n+\t\tci = swap_cluster_lock(__swap_entry_to_info(folio->swap),\n+\t\t\t\t       swp_offset(folio->swap));\n+\t\t__swap_cache_del_folio(ci, folio, NULL, false, false);\n+\t\tswap_cluster_unlock(ci);\n+\t}\n \n \tif (unlikely(!folio_test_swapcache(folio)))\n \t\treturn -ENOMEM;\n@@ -1879,6 +1884,7 @@ void __swap_cluster_free_entries(struct swap_info_struct *si,\n \t\t\t\t unsigned int ci_start, unsigned int nr_pages)\n {\n \tunsigned long old_tb;\n+\tunsigned short id;\n \tunsigned int ci_off = ci_start, ci_end = ci_start + nr_pages;\n \tunsigned long offset = cluster_offset(si, ci) + ci_start;\n \n@@ -1892,7 +1898,10 @@ void __swap_cluster_free_entries(struct swap_info_struct *si,\n \t\t__swap_table_set(ci, ci_off, null_to_swp_tb());\n \t} while (++ci_off < ci_end);\n \n-\tmem_cgroup_uncharge_swap(swp_entry(si->type, offset), nr_pages);\n+\tid = swap_cgroup_clear(swp_entry(si->type, offset), nr_pages);\n+\tif (id)\n+\t\tmem_cgroup_uncharge_swap(id, nr_pages);\n+\n \tswap_range_free(si, offset, nr_pages);\n \tswap_cluster_assert_empty(ci, ci_start, nr_pages, false);\n \ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 44e4fcd6463c..5112f81cf875 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -759,8 +759,7 @@ static int __remove_mapping(struct address_space *mapping, struct folio *folio,\n \n \t\tif (reclaimed && !mapping_exiting(mapping))\n \t\t\tshadow = workingset_eviction(folio, target_memcg);\n-\t\tmemcg1_swapout(folio, swap);\n-\t\t__swap_cache_del_folio(ci, folio, swap, shadow);\n+\t\t__swap_cache_del_folio(ci, folio, shadow, true, true);\n \t\tswap_cluster_unlock_irq(ci);\n \t} else {\n \t\tvoid (*free_folio)(struct folio *);\n\n-- \n2.53.0",
          "reply_to": "",
          "message_date": "2026-02-20"
        },
        {
          "author": "Kairui Song",
          "summary": "The reviewer suggested modifying the code to handle different memory control groups (memcg) at once, instead of relying on the caller to ensure all slots are in the same memcg.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Instead of let the caller ensures all slots are in the same memcg, the\nmake it be able to handle different memcg at once.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n mm/swapfile.c | 31 +++++++++++++++++++++++++------\n 1 file changed, 25 insertions(+), 6 deletions(-)\n\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 2cd3e260f1bf..cd2d3b2ca6f0 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -1883,10 +1883,13 @@ void __swap_cluster_free_entries(struct swap_info_struct *si,\n \t\t\t\t struct swap_cluster_info *ci,\n \t\t\t\t unsigned int ci_start, unsigned int nr_pages)\n {\n+\tvoid *shadow;\n \tunsigned long old_tb;\n-\tunsigned short id;\n+\tunsigned int type = si->type;\n+\tunsigned int id = 0, id_iter, id_check;\n \tunsigned int ci_off = ci_start, ci_end = ci_start + nr_pages;\n-\tunsigned long offset = cluster_offset(si, ci) + ci_start;\n+\tunsigned long offset = cluster_offset(si, ci);\n+\tunsigned int ci_batch = ci_off;\n \n \tVM_WARN_ON(ci->count < nr_pages);\n \n@@ -1896,13 +1899,29 @@ void __swap_cluster_free_entries(struct swap_info_struct *si,\n \t\t/* Release the last ref, or after swap cache is dropped */\n \t\tVM_WARN_ON(!swp_tb_is_shadow(old_tb) || __swp_tb_get_count(old_tb) > 1);\n \t\t__swap_table_set(ci, ci_off, null_to_swp_tb());\n+\n+\t\tshadow = swp_tb_to_shadow(old_tb);\n+\t\tid_iter = shadow_to_memcgid(shadow);\n+\t\tif (id != id_iter) {\n+\t\t\tif (id) {\n+\t\t\t\tid_check = swap_cgroup_clear(swp_entry(type, offset + ci_batch),\n+\t\t\t\t\t\t\t     ci_off - ci_batch);\n+\t\t\t\tWARN_ON(id != id_check);\n+\t\t\t\tmem_cgroup_uncharge_swap(id, ci_off - ci_batch);\n+\t\t\t}\n+\t\t\tid = id_iter;\n+\t\t\tci_batch = ci_off;\n+\t\t}\n \t} while (++ci_off < ci_end);\n \n-\tid = swap_cgroup_clear(swp_entry(si->type, offset), nr_pages);\n-\tif (id)\n-\t\tmem_cgroup_uncharge_swap(id, nr_pages);\n+\tif (id) {\n+\t\tid_check = swap_cgroup_clear(swp_entry(type, offset + ci_batch),\n+\t\t\t\t\t     ci_off - ci_batch);\n+\t\tWARN_ON(id != id_check);\n+\t\tmem_cgroup_uncharge_swap(id, ci_off - ci_batch);\n+\t}\n \n-\tswap_range_free(si, offset, nr_pages);\n+\tswap_range_free(si, offset + ci_start, nr_pages);\n \tswap_cluster_assert_empty(ci, ci_start, nr_pages, false);\n \n \tif (!ci->count)\n\n-- \n2.53.0",
          "reply_to": "",
          "message_date": "2026-02-20"
        },
        {
          "author": "Kairui Song",
          "summary": "The reviewer suggested transitioning mem_cgroup_swapin_charge_folio() to receive the memcg id from the caller via the swap table shadow entry, demoting the old swap cgroup array lookup to a sanity check and removing per-PTE cgroup id batching break from swap_pte_batch().",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "patch suggestion",
            "code change request"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Transition mem_cgroup_swapin_charge_folio() to receive the memcg id\nfrom the caller via the swap table shadow entry, demoting the old\nswap cgroup array lookup to a sanity check. Also removes the per-PTE\ncgroup id batching break from swap_pte_batch() since now swap is able to\nfree slots across mem cgroups.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n include/linux/memcontrol.h | 6 ++++--\n mm/internal.h              | 4 ----\n mm/memcontrol.c            | 9 ++++++---\n mm/swap_state.c            | 5 ++++-\n 4 files changed, 14 insertions(+), 10 deletions(-)\n\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex 0b37d4faf785..8fc794baf736 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -667,7 +667,8 @@ static inline int mem_cgroup_charge(struct folio *folio, struct mm_struct *mm,\n int mem_cgroup_charge_hugetlb(struct folio* folio, gfp_t gfp);\n \n int mem_cgroup_swapin_charge_folio(struct folio *folio, struct mm_struct *mm,\n-\t\t\t\t  gfp_t gfp, swp_entry_t entry);\n+\t\t\t\t   gfp_t gfp, swp_entry_t entry,\n+\t\t\t\t   unsigned short id);\n \n void __mem_cgroup_uncharge(struct folio *folio);\n \n@@ -1145,7 +1146,8 @@ static inline int mem_cgroup_charge_hugetlb(struct folio* folio, gfp_t gfp)\n }\n \n static inline int mem_cgroup_swapin_charge_folio(struct folio *folio,\n-\t\t\tstruct mm_struct *mm, gfp_t gfp, swp_entry_t entry)\n+\t\t\tstruct mm_struct *mm, gfp_t gfp, swp_entry_t entry,\n+\t\t\tunsigned short id)\n {\n \treturn 0;\n }\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 5bbe081c9048..416d3401aa17 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -452,12 +452,10 @@ static inline int swap_pte_batch(pte_t *start_ptep, int max_nr, pte_t pte)\n \tconst pte_t *end_ptep = start_ptep + max_nr;\n \tconst softleaf_t entry = softleaf_from_pte(pte);\n \tpte_t *ptep = start_ptep + 1;\n-\tunsigned short cgroup_id;\n \n \tVM_WARN_ON(max_nr < 1);\n \tVM_WARN_ON(!softleaf_is_swap(entry));\n \n-\tcgroup_id = lookup_swap_cgroup_id(entry);\n \twhile (ptep < end_ptep) {\n \t\tsoftleaf_t entry;\n \n@@ -466,8 +464,6 @@ static inline int swap_pte_batch(pte_t *start_ptep, int max_nr, pte_t pte)\n \t\tif (!pte_same(pte, expected_pte))\n \t\t\tbreak;\n \t\tentry = softleaf_from_pte(pte);\n-\t\tif (lookup_swap_cgroup_id(entry) != cgroup_id)\n-\t\t\tbreak;\n \t\texpected_pte = pte_next_swp_offset(expected_pte);\n \t\tptep++;\n \t}\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex d9ff44b77409..d0f50019d733 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -4794,6 +4794,7 @@ int mem_cgroup_charge_hugetlb(struct folio *folio, gfp_t gfp)\n  * @mm: mm context of the victim\n  * @gfp: reclaim mode\n  * @entry: swap entry for which the folio is allocated\n+ * @id: the mem cgroup id\n  *\n  * This function charges a folio allocated for swapin. Please call this before\n  * adding the folio to the swapcache.\n@@ -4801,19 +4802,21 @@ int mem_cgroup_charge_hugetlb(struct folio *folio, gfp_t gfp)\n  * Returns 0 on success. Otherwise, an error code is returned.\n  */\n int mem_cgroup_swapin_charge_folio(struct folio *folio, struct mm_struct *mm,\n-\t\t\t\t  gfp_t gfp, swp_entry_t entry)\n+\t\t\t\t   gfp_t gfp, swp_entry_t entry, unsigned short id)\n {\n \tstruct mem_cgroup *memcg, *swap_memcg;\n+\tunsigned short memcg_id;\n \tunsigned int nr_pages;\n-\tunsigned short id;\n \tint ret;\n \n \tif (mem_cgroup_disabled())\n \t\treturn 0;\n \n-\tid = lookup_swap_cgroup_id(entry);\n+\tmemcg_id = lookup_swap_cgroup_id(entry);\n \tnr_pages = folio_nr_pages(folio);\n \n+\tWARN_ON_ONCE(id != memcg_id);\n+\n \trcu_read_lock();\n \tswap_memcg = mem_cgroup_from_private_id(id);\n \tif (!swap_memcg) {\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex cc4bf40320ef..5ab3a41fe42c 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -251,8 +251,11 @@ static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n \t__swap_cache_add_folio(ci, folio, entry);\n \tspin_unlock(&ci->lock);\n \n+\t/* With swap table, we must have a shadow, for memcg tracking */\n+\tWARN_ON(!shadow);\n+\n \tif (mem_cgroup_swapin_charge_folio(folio, vmf ? vmf->vma->vm_mm : NULL,\n-\t\t\t\t\t   gfp, entry)) {\n+\t\t\t\t\t   gfp, entry, shadow_to_memcgid(shadow))) {\n \t\tspin_lock(&ci->lock);\n \t\t__swap_cache_del_folio(ci, folio, shadow, false, false);\n \t\tspin_unlock(&ci->lock);\n\n-- \n2.53.0",
          "reply_to": "",
          "message_date": "2026-02-20"
        },
        {
          "author": "Kairui Song",
          "summary": "The reviewer noted that the swap table now contains swap cgroup info all the time, and suggested dropping the swap cgroup array.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "suggested improvement"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Now swap table contains the swap cgropu info all the time, the swap\ncgroup array can be dropped.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n MAINTAINERS                 |   1 -\n include/linux/memcontrol.h  |   6 +-\n include/linux/swap_cgroup.h |  47 ------------\n mm/Makefile                 |   3 -\n mm/internal.h               |   1 -\n mm/memcontrol-v1.c          |   1 -\n mm/memcontrol.c             |  19 ++---\n mm/swap_cgroup.c            | 171 --------------------------------------------\n mm/swap_state.c             |   3 +-\n mm/swapfile.c               |  23 +-----\n 10 files changed, 11 insertions(+), 264 deletions(-)\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex aa1734a12887..05e633611e0b 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -6571,7 +6571,6 @@ F:\tmm/memcontrol.c\n F:\tmm/memcontrol-v1.c\n F:\tmm/memcontrol-v1.h\n F:\tmm/page_counter.c\n-F:\tmm/swap_cgroup.c\n F:\tsamples/cgroup/*\n F:\ttools/testing/selftests/cgroup/memcg_protection.m\n F:\ttools/testing/selftests/cgroup/test_hugetlb_memcg.c\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex 8fc794baf736..4bfe905bffb0 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -667,8 +667,7 @@ static inline int mem_cgroup_charge(struct folio *folio, struct mm_struct *mm,\n int mem_cgroup_charge_hugetlb(struct folio* folio, gfp_t gfp);\n \n int mem_cgroup_swapin_charge_folio(struct folio *folio, struct mm_struct *mm,\n-\t\t\t\t   gfp_t gfp, swp_entry_t entry,\n-\t\t\t\t   unsigned short id);\n+\t\t\t\t   gfp_t gfp, unsigned short id);\n \n void __mem_cgroup_uncharge(struct folio *folio);\n \n@@ -1146,8 +1145,7 @@ static inline int mem_cgroup_charge_hugetlb(struct folio* folio, gfp_t gfp)\n }\n \n static inline int mem_cgroup_swapin_charge_folio(struct folio *folio,\n-\t\t\tstruct mm_struct *mm, gfp_t gfp, swp_entry_t entry,\n-\t\t\tunsigned short id)\n+\t\t\tstruct mm_struct *mm, gfp_t gfp, unsigned short id)\n {\n \treturn 0;\n }\ndiff --git a/include/linux/swap_cgroup.h b/include/linux/swap_cgroup.h\ndeleted file mode 100644\nindex 91cdf12190a0..000000000000\n--- a/include/linux/swap_cgroup.h\n+++ /dev/null\n@@ -1,47 +0,0 @@\n-/* SPDX-License-Identifier: GPL-2.0 */\n-#ifndef __LINUX_SWAP_CGROUP_H\n-#define __LINUX_SWAP_CGROUP_H\n-\n-#include <linux/swap.h>\n-\n-#if defined(CONFIG_MEMCG) && defined(CONFIG_SWAP)\n-\n-extern void swap_cgroup_record(struct folio *folio, unsigned short id, swp_entry_t ent);\n-extern unsigned short swap_cgroup_clear(swp_entry_t ent, unsigned int nr_ents);\n-extern unsigned short lookup_swap_cgroup_id(swp_entry_t ent);\n-extern int swap_cgroup_swapon(int type, unsigned long max_pages);\n-extern void swap_cgroup_swapoff(int type);\n-\n-#else\n-\n-static inline\n-void swap_cgroup_record(struct folio *folio, unsigned short id, swp_entry_t ent)\n-{\n-}\n-\n-static inline\n-unsigned short swap_cgroup_clear(swp_entry_t ent, unsigned int nr_ents)\n-{\n-\treturn 0;\n-}\n-\n-static inline\n-unsigned short lookup_swap_cgroup_id(swp_entry_t ent)\n-{\n-\treturn 0;\n-}\n-\n-static inline int\n-swap_cgroup_swapon(int type, unsigned long max_pages)\n-{\n-\treturn 0;\n-}\n-\n-static inline void swap_cgroup_swapoff(int type)\n-{\n-\treturn;\n-}\n-\n-#endif\n-\n-#endif /* __LINUX_SWAP_CGROUP_H */\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 8ad2ab08244e..eff9f9e7e061 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -103,9 +103,6 @@ obj-$(CONFIG_PAGE_COUNTER) += page_counter.o\n obj-$(CONFIG_LIVEUPDATE_MEMFD) += memfd_luo.o\n obj-$(CONFIG_MEMCG_V1) += memcontrol-v1.o\n obj-$(CONFIG_MEMCG) += memcontrol.o vmpressure.o\n-ifdef CONFIG_SWAP\n-obj-$(CONFIG_MEMCG) += swap_cgroup.o\n-endif\n ifdef CONFIG_BPF_SYSCALL\n obj-$(CONFIG_MEMCG) += bpf_memcontrol.o\n endif\ndiff --git a/mm/internal.h b/mm/internal.h\nindex 416d3401aa17..26691885d75f 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -16,7 +16,6 @@\n #include <linux/rmap.h>\n #include <linux/swap.h>\n #include <linux/leafops.h>\n-#include <linux/swap_cgroup.h>\n #include <linux/tracepoint-defs.h>\n \n /* Internal core VMA manipulation functions. */\ndiff --git a/mm/memcontrol-v1.c b/mm/memcontrol-v1.c\nindex 038e630dc7e1..eff18eda0707 100644\n--- a/mm/memcontrol-v1.c\n+++ b/mm/memcontrol-v1.c\n@@ -5,7 +5,6 @@\n #include <linux/mm_inline.h>\n #include <linux/pagewalk.h>\n #include <linux/backing-dev.h>\n-#include <linux/swap_cgroup.h>\n #include <linux/eventfd.h>\n #include <linux/poll.h>\n #include <linux/sort.h>\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex d0f50019d733..8d0c9f3a011e 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -54,7 +54,6 @@\n #include <linux/vmpressure.h>\n #include <linux/memremap.h>\n #include <linux/mm_inline.h>\n-#include <linux/swap_cgroup.h>\n #include <linux/cpu.h>\n #include <linux/oom.h>\n #include <linux/lockdep.h>\n@@ -4793,7 +4792,6 @@ int mem_cgroup_charge_hugetlb(struct folio *folio, gfp_t gfp)\n  * @folio: folio to charge.\n  * @mm: mm context of the victim\n  * @gfp: reclaim mode\n- * @entry: swap entry for which the folio is allocated\n  * @id: the mem cgroup id\n  *\n  * This function charges a folio allocated for swapin. Please call this before\n@@ -4802,21 +4800,17 @@ int mem_cgroup_charge_hugetlb(struct folio *folio, gfp_t gfp)\n  * Returns 0 on success. Otherwise, an error code is returned.\n  */\n int mem_cgroup_swapin_charge_folio(struct folio *folio, struct mm_struct *mm,\n-\t\t\t\t   gfp_t gfp, swp_entry_t entry, unsigned short id)\n+\t\t\t\t   gfp_t gfp, unsigned short id)\n {\n \tstruct mem_cgroup *memcg, *swap_memcg;\n-\tunsigned short memcg_id;\n \tunsigned int nr_pages;\n \tint ret;\n \n \tif (mem_cgroup_disabled())\n \t\treturn 0;\n \n-\tmemcg_id = lookup_swap_cgroup_id(entry);\n \tnr_pages = folio_nr_pages(folio);\n \n-\tWARN_ON_ONCE(id != memcg_id);\n-\n \trcu_read_lock();\n \tswap_memcg = mem_cgroup_from_private_id(id);\n \tif (!swap_memcg) {\n@@ -4836,10 +4830,11 @@ int mem_cgroup_swapin_charge_folio(struct folio *folio, struct mm_struct *mm,\n \n \t/*\n \t * On successful charge, the folio itself now belongs to the memcg,\n-\t * so is folio->swap. So we can release the swap cgroup table's\n-\t * pinning of the private id.\n+\t * so is folio->swap. And the folio takes place of the shadow in\n+\t * the swap table so we can release the shadow's pinning of the\n+\t * private id.\n \t */\n-\tswap_cgroup_clear(folio->swap, nr_pages);\n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapcache(folio), folio);\n \tmem_cgroup_private_id_put(swap_memcg, nr_pages);\n \n \t/*\n@@ -5324,8 +5319,6 @@ struct mem_cgroup *__mem_cgroup_swap_free_folio(struct folio *folio,\n {\n \tunsigned int nr_pages = folio_nr_pages(folio);\n \tstruct mem_cgroup *memcg, *swap_memcg;\n-\tswp_entry_t entry = folio->swap;\n-\tunsigned short id;\n \n \tVM_WARN_ON_ONCE_FOLIO(!folio_memcg_charged(folio), folio);\n \tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapcache(folio), folio);\n@@ -5337,8 +5330,6 @@ struct mem_cgroup *__mem_cgroup_swap_free_folio(struct folio *folio,\n \t */\n \tmemcg = folio_memcg(folio);\n \tswap_memcg = mem_cgroup_private_id_get_online(memcg, nr_pages);\n-\tid = mem_cgroup_private_id(swap_memcg);\n-\tswap_cgroup_record(folio, id, entry);\n \n \tif (reclaim && do_memsw_account()) {\n \t\tmemcg1_swapout(folio, swap_memcg);\ndiff --git a/mm/swap_cgroup.c b/mm/swap_cgroup.c\ndeleted file mode 100644\nindex b5a7f21c3afe..000000000000\n--- a/mm/swap_cgroup.c\n+++ /dev/null\n@@ -1,171 +0,0 @@\n-// SPDX-License-Identifier: GPL-2.0\n-#include <linux/swap_cgroup.h>\n-#include <linux/vmalloc.h>\n-#include <linux/mm.h>\n-\n-#include <linux/swapops.h> /* depends on mm.h include */\n-\n-static DEFINE_MUTEX(swap_cgroup_mutex);\n-\n-/* Pack two cgroup id (short) of two entries in one swap_cgroup (atomic_t) */\n-#define ID_PER_SC (sizeof(struct swap_cgroup) / sizeof(unsigned short))\n-#define ID_SHIFT (BITS_PER_TYPE(unsigned short))\n-#define ID_MASK (BIT(ID_SHIFT) - 1)\n-struct swap_cgroup {\n-\tatomic_t ids;\n-};\n-\n-struct swap_cgroup_ctrl {\n-\tstruct swap_cgroup *map;\n-};\n-\n-static struct swap_cgroup_ctrl swap_cgroup_ctrl[MAX_SWAPFILES];\n-\n-static unsigned short __swap_cgroup_id_lookup(struct swap_cgroup *map,\n-\t\t\t\t\t      pgoff_t offset)\n-{\n-\tunsigned int shift = (offset % ID_PER_SC) * ID_SHIFT;\n-\tunsigned int old_ids = atomic_read(&map[offset / ID_PER_SC].ids);\n-\n-\tBUILD_BUG_ON(!is_power_of_2(ID_PER_SC));\n-\tBUILD_BUG_ON(sizeof(struct swap_cgroup) != sizeof(atomic_t));\n-\n-\treturn (old_ids >> shift) & ID_MASK;\n-}\n-\n-static unsigned short __swap_cgroup_id_xchg(struct swap_cgroup *map,\n-\t\t\t\t\t    pgoff_t offset,\n-\t\t\t\t\t    unsigned short new_id)\n-{\n-\tunsigned short old_id;\n-\tstruct swap_cgroup *sc = &map[offset / ID_PER_SC];\n-\tunsigned int shift = (offset % ID_PER_SC) * ID_SHIFT;\n-\tunsigned int new_ids, old_ids = atomic_read(&sc->ids);\n-\n-\tdo {\n-\t\told_id = (old_ids >> shift) & ID_MASK;\n-\t\tnew_ids = (old_ids & ~(ID_MASK << shift));\n-\t\tnew_ids |= ((unsigned int)new_id) << shift;\n-\t} while (!atomic_try_cmpxchg(&sc->ids, &old_ids, new_ids));\n-\n-\treturn old_id;\n-}\n-\n-/**\n- * swap_cgroup_record - record mem_cgroup for a set of swap entries.\n- * These entries must belong to one single folio, and that folio\n- * must be being charged for swap space (swap out).\n- *\n- * @folio: the folio that the swap entry belongs to\n- * @id: mem_cgroup ID to be recorded\n- * @ent: the first swap entry to be recorded\n- */\n-void swap_cgroup_record(struct folio *folio, unsigned short id,\n-\t\t\tswp_entry_t ent)\n-{\n-\tunsigned int nr_ents = folio_nr_pages(folio);\n-\tstruct swap_cgroup *map;\n-\tpgoff_t offset, end;\n-\tunsigned short old;\n-\n-\toffset = swp_offset(ent);\n-\tend = offset + nr_ents;\n-\tmap = swap_cgroup_ctrl[swp_type(ent)].map;\n-\n-\tdo {\n-\t\told = __swap_cgroup_id_xchg(map, offset, id);\n-\t\tVM_WARN_ON_ONCE(old);\n-\t} while (++offset != end);\n-}\n-\n-/**\n- * swap_cgroup_clear - clear mem_cgroup for a set of swap entries.\n- * These entries must be being uncharged from swap. They either\n- * belongs to one single folio in the swap cache (swap in for\n- * cgroup v1), or no longer have any users (slot freeing).\n- *\n- * @ent: the first swap entry to be recorded into\n- * @nr_ents: number of swap entries to be recorded\n- *\n- * Returns the existing old value.\n- */\n-unsigned short swap_cgroup_clear(swp_entry_t ent, unsigned int nr_ents)\n-{\n-\tpgoff_t offset, end;\n-\tstruct swap_cgroup *map;\n-\tunsigned short old, iter = 0;\n-\n-\toffset = swp_offset(ent);\n-\tend = offset + nr_ents;\n-\tmap = swap_cgroup_ctrl[swp_type(ent)].map;\n-\n-\tdo {\n-\t\told = __swap_cgroup_id_xchg(map, offset, 0);\n-\t\tif (!iter)\n-\t\t\titer = old;\n-\t\tVM_BUG_ON(iter != old);\n-\t} while (++offset != end);\n-\n-\treturn old;\n-}\n-\n-/**\n- * lookup_swap_cgroup_id - lookup mem_cgroup id tied to swap entry\n- * @ent: swap entry to be looked up.\n- *\n- * Returns ID of mem_cgroup at success. 0 at failure. (0 is invalid ID)\n- */\n-unsigned short lookup_swap_cgroup_id(swp_entry_t ent)\n-{\n-\tstruct swap_cgroup_ctrl *ctrl;\n-\n-\tif (mem_cgroup_disabled())\n-\t\treturn 0;\n-\n-\tctrl = &swap_cgroup_ctrl[swp_type(ent)];\n-\treturn __swap_cgroup_id_lookup(ctrl->map, swp_offset(ent));\n-}\n-\n-int swap_cgroup_swapon(int type, unsigned long max_pages)\n-{\n-\tstruct swap_cgroup *map;\n-\tstruct swap_cgroup_ctrl *ctrl;\n-\n-\tif (mem_cgroup_disabled())\n-\t\treturn 0;\n-\n-\tBUILD_BUG_ON(sizeof(unsigned short) * ID_PER_SC !=\n-\t\t     sizeof(struct swap_cgroup));\n-\tmap = vzalloc(DIV_ROUND_UP(max_pages, ID_PER_SC) *\n-\t\t      sizeof(struct swap_cgroup));\n-\tif (!map)\n-\t\tgoto nomem;\n-\n-\tctrl = &swap_cgroup_ctrl[type];\n-\tmutex_lock(&swap_cgroup_mutex);\n-\tctrl->map = map;\n-\tmutex_unlock(&swap_cgroup_mutex);\n-\n-\treturn 0;\n-nomem:\n-\tpr_info(\"couldn't allocate enough memory for swap_cgroup\\n\");\n-\tpr_info(\"swap_cgroup can be disabled by swapaccount=0 boot option\\n\");\n-\treturn -ENOMEM;\n-}\n-\n-void swap_cgroup_swapoff(int type)\n-{\n-\tstruct swap_cgroup *map;\n-\tstruct swap_cgroup_ctrl *ctrl;\n-\n-\tif (mem_cgroup_disabled())\n-\t\treturn;\n-\n-\tmutex_lock(&swap_cgroup_mutex);\n-\tctrl = &swap_cgroup_ctrl[type];\n-\tmap = ctrl->map;\n-\tctrl->map = NULL;\n-\tmutex_unlock(&swap_cgroup_mutex);\n-\n-\tvfree(map);\n-}\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 5ab3a41fe42c..c6ba15de4094 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -163,7 +163,6 @@ static int __swap_cache_check_batch(struct swap_cluster_info *ci,\n \t*shadowp = swp_tb_to_shadow(old_tb);\n \tmemcgid = shadow_to_memcgid(*shadowp);\n \n-\tWARN_ON_ONCE(!mem_cgroup_disabled() && !memcgid);\n \tdo {\n \t\told_tb = __swap_table_get(ci, ci_off);\n \t\tif (unlikely(swp_tb_is_folio(old_tb)) ||\n@@ -255,7 +254,7 @@ static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n \tWARN_ON(!shadow);\n \n \tif (mem_cgroup_swapin_charge_folio(folio, vmf ? vmf->vma->vm_mm : NULL,\n-\t\t\t\t\t   gfp, entry, shadow_to_memcgid(shadow))) {\n+\t\t\t\t\t   gfp, shadow_to_memcgid(shadow))) {\n \t\tspin_lock(&ci->lock);\n \t\t__swap_cache_del_folio(ci, folio, shadow, false, false);\n \t\tspin_unlock(&ci->lock);\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex cd2d3b2ca6f0..de34f1990209 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -45,7 +45,6 @@\n \n #include <asm/tlbflush.h>\n #include <linux/leafops.h>\n-#include <linux/swap_cgroup.h>\n #include \"swap_table.h\"\n #include \"internal.h\"\n #include \"swap_table.h\"\n@@ -1885,8 +1884,7 @@ void __swap_cluster_free_entries(struct swap_info_struct *si,\n {\n \tvoid *shadow;\n \tunsigned long old_tb;\n-\tunsigned int type = si->type;\n-\tunsigned int id = 0, id_iter, id_check;\n+\tunsigned int id = 0, id_iter;\n \tunsigned int ci_off = ci_start, ci_end = ci_start + nr_pages;\n \tunsigned long offset = cluster_offset(si, ci);\n \tunsigned int ci_batch = ci_off;\n@@ -1903,23 +1901,15 @@ void __swap_cluster_free_entries(struct swap_info_struct *si,\n \t\tshadow = swp_tb_to_shadow(old_tb);\n \t\tid_iter = shadow_to_memcgid(shadow);\n \t\tif (id != id_iter) {\n-\t\t\tif (id) {\n-\t\t\t\tid_check = swap_cgroup_clear(swp_entry(type, offset + ci_batch),\n-\t\t\t\t\t\t\t     ci_off - ci_batch);\n-\t\t\t\tWARN_ON(id != id_check);\n+\t\t\tif (id)\n \t\t\t\tmem_cgroup_uncharge_swap(id, ci_off - ci_batch);\n-\t\t\t}\n \t\t\tid = id_iter;\n \t\t\tci_batch = ci_off;\n \t\t}\n \t} while (++ci_off < ci_end);\n \n-\tif (id) {\n-\t\tid_check = swap_cgroup_clear(swp_entry(type, offset + ci_batch),\n-\t\t\t\t\t     ci_off - ci_batch);\n-\t\tWARN_ON(id != id_check);\n+\tif (id)\n \t\tmem_cgroup_uncharge_swap(id, ci_off - ci_batch);\n-\t}\n \n \tswap_range_free(si, offset + ci_start, nr_pages);\n \tswap_cluster_assert_empty(ci, ci_start, nr_pages, false);\n@@ -3034,8 +3024,6 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \tp->global_cluster = NULL;\n \tkvfree(zeromap);\n \tfree_swap_cluster_info(cluster_info, maxpages);\n-\t/* Destroy swap account information */\n-\tswap_cgroup_swapoff(p->type);\n \n \tinode = mapping->host;\n \n@@ -3567,10 +3555,6 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tif (error)\n \t\tgoto bad_swap_unlock_inode;\n \n-\terror = swap_cgroup_swapon(si->type, maxpages);\n-\tif (error)\n-\t\tgoto bad_swap_unlock_inode;\n-\n \t/*\n \t * Use kvmalloc_array instead of bitmap_zalloc as the allocation order might\n \t * be above MAX_PAGE_ORDER incase of a large swap file.\n@@ -3681,7 +3665,6 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tsi->global_cluster = NULL;\n \tinode = NULL;\n \tdestroy_swap_extents(si, swap_file);\n-\tswap_cgroup_swapoff(si->type);\n \tfree_swap_cluster_info(si->cluster_info, si->max);\n \tsi->cluster_info = NULL;\n \tkvfree(si->zeromap);\n\n-- \n2.53.0",
          "reply_to": "",
          "message_date": "2026-02-20"
        },
        {
          "author": "Kairui Song",
          "summary": "Reviewer Kairui Song noted that the patch can simplify the code by merging the zeromap into the swap table, reserving one bit for counting, and removing unnecessary checks.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "suggested improvement",
            "code simplification"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "By reserving one bit for the counting part, we can easily merge the\nzeromap into the swap table.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n include/linux/swap.h |   1 -\n mm/memory.c          |  12 ++----\n mm/page_io.c         |  28 ++++++++++----\n mm/swap.h            |  31 ----------------\n mm/swap_state.c      |  23 ++++++++----\n mm/swap_table.h      | 103 +++++++++++++++++++++++++++++++++++++++++++--------\n mm/swapfile.c        |  27 +-------------\n 7 files changed, 127 insertions(+), 98 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 66cf657a1f35..bc871d8a1e99 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -254,7 +254,6 @@ struct swap_info_struct {\n \tstruct plist_node list;\t\t/* entry in swap_active_head */\n \tsigned char\ttype;\t\t/* strange name for an index */\n \tunsigned int\tmax;\t\t/* size of this swap device */\n-\tunsigned long *zeromap;\t\t/* kvmalloc'ed bitmap to track zero pages */\n \tstruct swap_cluster_info *cluster_info; /* cluster info. Only for SSD */\n \tstruct list_head free_clusters; /* free clusters list */\n \tstruct list_head full_clusters; /* full clusters list */\ndiff --git a/mm/memory.c b/mm/memory.c\nindex e58f976508b3..8df169fced0d 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -88,6 +88,7 @@\n \n #include \"pgalloc-track.h\"\n #include \"internal.h\"\n+#include \"swap_table.h\"\n #include \"swap.h\"\n \n #if defined(LAST_CPUPID_NOT_IN_PAGE_FLAGS) && !defined(CONFIG_COMPILE_TEST)\n@@ -4522,13 +4523,11 @@ static vm_fault_t handle_pte_marker(struct vm_fault *vmf)\n \n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\n /*\n- * Check if the PTEs within a range are contiguous swap entries\n- * and have consistent swapcache, zeromap.\n+ * Check if the PTEs within a range are contiguous swap entries.\n  */\n static bool can_swapin_thp(struct vm_fault *vmf, pte_t *ptep, int nr_pages)\n {\n \tunsigned long addr;\n-\tsoftleaf_t entry;\n \tint idx;\n \tpte_t pte;\n \n@@ -4538,18 +4537,13 @@ static bool can_swapin_thp(struct vm_fault *vmf, pte_t *ptep, int nr_pages)\n \n \tif (!pte_same(pte, pte_move_swp_offset(vmf->orig_pte, -idx)))\n \t\treturn false;\n-\tentry = softleaf_from_pte(pte);\n-\tif (swap_pte_batch(ptep, nr_pages, pte) != nr_pages)\n-\t\treturn false;\n-\n \t/*\n \t * swap_read_folio() can't handle the case a large folio is hybridly\n \t * from different backends. And they are likely corner cases. Similar\n \t * things might be added once zswap support large folios.\n \t */\n-\tif (unlikely(swap_zeromap_batch(entry, nr_pages, NULL) != nr_pages))\n+\tif (swap_pte_batch(ptep, nr_pages, pte) != nr_pages)\n \t\treturn false;\n-\n \treturn true;\n }\n \ndiff --git a/mm/page_io.c b/mm/page_io.c\nindex a2c034660c80..5a0b5034489b 100644\n--- a/mm/page_io.c\n+++ b/mm/page_io.c\n@@ -26,6 +26,7 @@\n #include <linux/delayacct.h>\n #include <linux/zswap.h>\n #include \"swap.h\"\n+#include \"swap_table.h\"\n \n static void __end_swap_bio_write(struct bio *bio)\n {\n@@ -204,15 +205,20 @@ static bool is_folio_zero_filled(struct folio *folio)\n static void swap_zeromap_folio_set(struct folio *folio)\n {\n \tstruct obj_cgroup *objcg = get_obj_cgroup_from_folio(folio);\n-\tstruct swap_info_struct *sis = __swap_entry_to_info(folio->swap);\n \tint nr_pages = folio_nr_pages(folio);\n+\tstruct swap_cluster_info *ci;\n \tswp_entry_t entry;\n \tunsigned int i;\n \n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapcache(folio), folio);\n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_locked(folio), folio);\n+\n+\tci = swap_cluster_get_and_lock(folio);\n \tfor (i = 0; i < folio_nr_pages(folio); i++) {\n \t\tentry = page_swap_entry(folio_page(folio, i));\n-\t\tset_bit(swp_offset(entry), sis->zeromap);\n+\t\t__swap_table_set_zero(ci, swp_cluster_offset(entry));\n \t}\n+\tswap_cluster_unlock(ci);\n \n \tcount_vm_events(SWPOUT_ZERO, nr_pages);\n \tif (objcg) {\n@@ -223,14 +229,19 @@ static void swap_zeromap_folio_set(struct folio *folio)\n \n static void swap_zeromap_folio_clear(struct folio *folio)\n {\n-\tstruct swap_info_struct *sis = __swap_entry_to_info(folio->swap);\n+\tstruct swap_cluster_info *ci;\n \tswp_entry_t entry;\n \tunsigned int i;\n \n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapcache(folio), folio);\n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_locked(folio), folio);\n+\n+\tci = swap_cluster_get_and_lock(folio);\n \tfor (i = 0; i < folio_nr_pages(folio); i++) {\n \t\tentry = page_swap_entry(folio_page(folio, i));\n-\t\tclear_bit(swp_offset(entry), sis->zeromap);\n+\t\t__swap_table_clear_zero(ci, swp_cluster_offset(entry));\n \t}\n+\tswap_cluster_unlock(ci);\n }\n \n /*\n@@ -255,10 +266,9 @@ int swap_writeout(struct folio *folio, struct swap_iocb **swap_plug)\n \t}\n \n \t/*\n-\t * Use a bitmap (zeromap) to avoid doing IO for zero-filled pages.\n-\t * The bits in zeromap are protected by the locked swapcache folio\n-\t * and atomic updates are used to protect against read-modify-write\n-\t * corruption due to other zero swap entries seeing concurrent updates.\n+\t * Use the swap table zero mark to avoid doing IO for zero-filled\n+\t * pages. The zero mark is protected by the cluster lock, which is\n+\t * acquired internally by swap_zeromap_folio_set/clear.\n \t */\n \tif (is_folio_zero_filled(folio)) {\n \t\tswap_zeromap_folio_set(folio);\n@@ -511,6 +521,8 @@ static bool swap_read_folio_zeromap(struct folio *folio)\n \tstruct obj_cgroup *objcg;\n \tbool is_zeromap;\n \n+\tVM_WARN_ON_ONCE_FOLIO(!folio_test_locked(folio), folio);\n+\n \t/*\n \t * Swapping in a large folio that is partially in the zeromap is not\n \t * currently handled. Return true without marking the folio uptodate so\ndiff --git a/mm/swap.h b/mm/swap.h\nindex c95f5fafea42..cb1ab20d83d5 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -312,31 +312,6 @@ static inline unsigned int folio_swap_flags(struct folio *folio)\n \treturn __swap_entry_to_info(folio->swap)->flags;\n }\n \n-/*\n- * Return the count of contiguous swap entries that share the same\n- * zeromap status as the starting entry. If is_zeromap is not NULL,\n- * it will return the zeromap status of the starting entry.\n- */\n-static inline int swap_zeromap_batch(swp_entry_t entry, int max_nr,\n-\t\tbool *is_zeromap)\n-{\n-\tstruct swap_info_struct *sis = __swap_entry_to_info(entry);\n-\tunsigned long start = swp_offset(entry);\n-\tunsigned long end = start + max_nr;\n-\tbool first_bit;\n-\n-\tfirst_bit = test_bit(start, sis->zeromap);\n-\tif (is_zeromap)\n-\t\t*is_zeromap = first_bit;\n-\n-\tif (max_nr <= 1)\n-\t\treturn max_nr;\n-\tif (first_bit)\n-\t\treturn find_next_zero_bit(sis->zeromap, end, start) - start;\n-\telse\n-\t\treturn find_next_bit(sis->zeromap, end, start) - start;\n-}\n-\n #else /* CONFIG_SWAP */\n struct swap_iocb;\n static inline struct swap_cluster_info *swap_cluster_lock(\n@@ -475,11 +450,5 @@ static inline unsigned int folio_swap_flags(struct folio *folio)\n {\n \treturn 0;\n }\n-\n-static inline int swap_zeromap_batch(swp_entry_t entry, int max_nr,\n-\t\tbool *has_zeromap)\n-{\n-\treturn 0;\n-}\n #endif /* CONFIG_SWAP */\n #endif /* _MM_SWAP_H */\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex c6ba15de4094..419419e18a47 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -138,6 +138,7 @@ void *swap_cache_get_shadow(swp_entry_t entry)\n }\n \n static int __swap_cache_check_batch(struct swap_cluster_info *ci,\n+\t\t\t\t    swp_entry_t entry,\n \t\t\t\t    unsigned int ci_off, unsigned int ci_targ,\n \t\t\t\t    unsigned int nr, void **shadowp)\n {\n@@ -148,6 +149,13 @@ static int __swap_cache_check_batch(struct swap_cluster_info *ci,\n \tif (unlikely(!ci->table))\n \t\treturn -ENOENT;\n \n+\t/*\n+\t * TODO: Swap of large folio that is partially in the zeromap\n+\t * is not supported.\n+\t */\n+\tif (nr > 1 && swap_zeromap_batch(entry, nr, NULL) != nr)\n+\t\treturn -EBUSY;\n+\n \t/*\n \t * If the target slot is not suitable for adding swap cache, return\n \t * -EEXIST or -ENOENT. If the batch is not suitable, could be a\n@@ -190,7 +198,7 @@ void __swap_cache_add_folio(struct swap_cluster_info *ci,\n \tdo {\n \t\told_tb = __swap_table_get(ci, ci_off);\n \t\tVM_WARN_ON_ONCE(swp_tb_is_folio(old_tb));\n-\t\t__swap_table_set(ci, ci_off, pfn_to_swp_tb(pfn, __swp_tb_get_count(old_tb)));\n+\t\t__swap_table_set(ci, ci_off, pfn_to_swp_tb(pfn, __swp_tb_get_flags(old_tb)));\n \t} while (++ci_off < ci_end);\n \n \tfolio_ref_add(folio, nr_pages);\n@@ -218,7 +226,7 @@ static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n \n \t/* First check if the range is available */\n \tspin_lock(&ci->lock);\n-\terr = __swap_cache_check_batch(ci, ci_off, ci_targ, nr_pages, &shadow);\n+\terr = __swap_cache_check_batch(ci, entry, ci_off, ci_targ, nr_pages, &shadow);\n \tspin_unlock(&ci->lock);\n \tif (unlikely(err))\n \t\treturn ERR_PTR(err);\n@@ -236,7 +244,7 @@ static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n \n \t/* Double check the range is still not in conflict */\n \tspin_lock(&ci->lock);\n-\terr = __swap_cache_check_batch(ci, ci_off, ci_targ, nr_pages, &shadow_check);\n+\terr = __swap_cache_check_batch(ci, entry, ci_off, ci_targ, nr_pages, &shadow_check);\n \tif (unlikely(err) || shadow_check != shadow) {\n \t\tspin_unlock(&ci->lock);\n \t\tfolio_put(folio);\n@@ -338,7 +346,6 @@ struct folio *swap_cache_alloc_folio(swp_entry_t targ_entry, gfp_t gfp_mask,\n void __swap_cache_del_folio(struct swap_cluster_info *ci, struct folio *folio,\n \t\t\t    void *shadow, bool charged, bool reclaim)\n {\n-\tint count;\n \tunsigned long old_tb;\n \tstruct swap_info_struct *si;\n \tstruct mem_cgroup *memcg = NULL;\n@@ -375,13 +382,13 @@ void __swap_cache_del_folio(struct swap_cluster_info *ci, struct folio *folio,\n \t\told_tb = __swap_table_get(ci, ci_off);\n \t\tWARN_ON_ONCE(!swp_tb_is_folio(old_tb) ||\n \t\t\t     swp_tb_to_folio(old_tb) != folio);\n-\t\tcount = __swp_tb_get_count(old_tb);\n-\t\tif (count)\n+\t\tif (__swp_tb_get_count(old_tb))\n \t\t\tfolio_swapped = true;\n \t\telse\n \t\t\tneed_free = true;\n \t\t/* If shadow is NULL, we sets an empty shadow. */\n-\t\t__swap_table_set(ci, ci_off, shadow_to_swp_tb(shadow, count));\n+\t\t__swap_table_set(ci, ci_off, shadow_to_swp_tb(shadow,\n+\t\t\t\t __swp_tb_get_flags(old_tb)));\n \t} while (++ci_off < ci_end);\n \n \tfolio->swap.val = 0;\n@@ -460,7 +467,7 @@ void __swap_cache_replace_folio(struct swap_cluster_info *ci,\n \tdo {\n \t\told_tb = __swap_table_get(ci, ci_off);\n \t\tWARN_ON_ONCE(!swp_tb_is_folio(old_tb) || swp_tb_to_folio(old_tb) != old);\n-\t\t__swap_table_set(ci, ci_off, pfn_to_swp_tb(pfn, __swp_tb_get_count(old_tb)));\n+\t\t__swap_table_set(ci, ci_off, pfn_to_swp_tb(pfn, __swp_tb_get_flags(old_tb)));\n \t} while (++ci_off < ci_end);\n \n \t/*\ndiff --git a/mm/swap_table.h b/mm/swap_table.h\nindex 8415ffbe2b9c..6d3d773e1908 100644\n--- a/mm/swap_table.h\n+++ b/mm/swap_table.h\n@@ -21,12 +21,14 @@ struct swap_table {\n  * Swap table entry type and bits layouts:\n  *\n  * NULL:     |---------------- 0 ---------------| - Free slot\n- * Shadow:   | SWAP_COUNT |---- SHADOW_VAL ---|1| - Swapped out slot\n- * PFN:      | SWAP_COUNT |------ PFN -------|10| - Cached slot\n+ * Shadow:   |SWAP_COUNT|Z|---- SHADOW_VAL ---|1| - Swapped out slot\n+ * PFN:      |SWAP_COUNT|Z|------ PFN -------|10| - Cached slot\n  * Pointer:  |----------- Pointer ----------|100| - (Unused)\n  * Bad:      |------------- 1 -------------|1000| - Bad slot\n  *\n- * SWAP_COUNT is `SWP_TB_COUNT_BITS` long, each entry is an atomic long.\n+ * COUNT is `SWP_TB_COUNT_BITS` long, Z is the `SWP_TB_ZERO_MARK` bit,\n+ * and together they form the `SWP_TB_FLAGS_BITS` wide flags field.\n+ * Each entry is an atomic long.\n  *\n  * Usages:\n  *\n@@ -70,16 +72,21 @@ struct swap_table {\n #define SWP_TB_PFN_MARK_MASK\t(BIT(SWP_TB_PFN_MARK_BITS) - 1)\n \n /* SWAP_COUNT part for PFN or shadow, the width can be shrunk or extended */\n-#define SWP_TB_COUNT_BITS      min(4, BITS_PER_LONG - SWP_TB_PFN_BITS)\n+#define SWP_TB_FLAGS_BITS\tmin(5, BITS_PER_LONG - SWP_TB_PFN_BITS)\n+#define SWP_TB_COUNT_BITS\t(SWP_TB_FLAGS_BITS - 1)\n+#define SWP_TB_FLAGS_MASK\t(~((~0UL) >> SWP_TB_FLAGS_BITS))\n #define SWP_TB_COUNT_MASK      (~((~0UL) >> SWP_TB_COUNT_BITS))\n+#define SWP_TB_FLAGS_SHIFT     (BITS_PER_LONG - SWP_TB_FLAGS_BITS)\n #define SWP_TB_COUNT_SHIFT     (BITS_PER_LONG - SWP_TB_COUNT_BITS)\n #define SWP_TB_COUNT_MAX       ((1 << SWP_TB_COUNT_BITS) - 1)\n \n+#define SWP_TB_ZERO_MARK\tBIT(BITS_PER_LONG - SWP_TB_COUNT_BITS - 1)\n+\n /* Bad slot: ends with 0b1000 and rests of bits are all 1 */\n #define SWP_TB_BAD\t\t((~0UL) << 3)\n \n /* Macro for shadow offset calculation */\n-#define SWAP_COUNT_SHIFT\tSWP_TB_COUNT_BITS\n+#define SWAP_COUNT_SHIFT\tSWP_TB_FLAGS_BITS\n \n /*\n  * Helpers for casting one type of info into a swap table entry.\n@@ -102,35 +109,43 @@ static inline unsigned long __count_to_swp_tb(unsigned char count)\n \treturn ((unsigned long)count) << SWP_TB_COUNT_SHIFT;\n }\n \n-static inline unsigned long pfn_to_swp_tb(unsigned long pfn, unsigned int count)\n+static inline unsigned long __flags_to_swp_tb(unsigned char flags)\n+{\n+\tBUILD_BUG_ON(SWP_TB_FLAGS_BITS > BITS_PER_BYTE);\n+\tVM_WARN_ON((flags >> 1) > SWP_TB_COUNT_MAX);\n+\treturn ((unsigned long)flags) << SWP_TB_FLAGS_SHIFT;\n+}\n+\n+\n+static inline unsigned long pfn_to_swp_tb(unsigned long pfn, unsigned char flags)\n {\n \tunsigned long swp_tb;\n \n \tBUILD_BUG_ON(sizeof(unsigned long) != sizeof(void *));\n \tBUILD_BUG_ON(SWAP_CACHE_PFN_BITS >\n-\t\t     (BITS_PER_LONG - SWP_TB_PFN_MARK_BITS - SWP_TB_COUNT_BITS));\n+\t\t     (BITS_PER_LONG - SWP_TB_PFN_MARK_BITS - SWP_TB_FLAGS_BITS));\n \n \tswp_tb = (pfn << SWP_TB_PFN_MARK_BITS) | SWP_TB_PFN_MARK;\n-\tVM_WARN_ON_ONCE(swp_tb & SWP_TB_COUNT_MASK);\n+\tVM_WARN_ON_ONCE(swp_tb & SWP_TB_FLAGS_MASK);\n \n-\treturn swp_tb | __count_to_swp_tb(count);\n+\treturn swp_tb | __flags_to_swp_tb(flags);\n }\n \n-static inline unsigned long folio_to_swp_tb(struct folio *folio, unsigned int count)\n+static inline unsigned long folio_to_swp_tb(struct folio *folio, unsigned char flags)\n {\n-\treturn pfn_to_swp_tb(folio_pfn(folio), count);\n+\treturn pfn_to_swp_tb(folio_pfn(folio), flags);\n }\n \n-static inline unsigned long shadow_to_swp_tb(void *shadow, unsigned int count)\n+static inline unsigned long shadow_to_swp_tb(void *shadow, unsigned char flags)\n {\n \tBUILD_BUG_ON((BITS_PER_XA_VALUE + 1) !=\n \t\t     BITS_PER_BYTE * sizeof(unsigned long));\n \tBUILD_BUG_ON((unsigned long)xa_mk_value(0) != SWP_TB_SHADOW_MARK);\n \n \tVM_WARN_ON_ONCE(shadow && !xa_is_value(shadow));\n-\tVM_WARN_ON_ONCE(shadow && ((unsigned long)shadow & SWP_TB_COUNT_MASK));\n+\tVM_WARN_ON_ONCE(shadow && ((unsigned long)shadow & SWP_TB_FLAGS_MASK));\n \n-\treturn (unsigned long)shadow | __count_to_swp_tb(count) | SWP_TB_SHADOW_MARK;\n+\treturn (unsigned long)shadow | SWP_TB_SHADOW_MARK | __flags_to_swp_tb(flags);\n }\n \n /*\n@@ -168,14 +183,14 @@ static inline bool swp_tb_is_countable(unsigned long swp_tb)\n static inline struct folio *swp_tb_to_folio(unsigned long swp_tb)\n {\n \tVM_WARN_ON(!swp_tb_is_folio(swp_tb));\n-\treturn pfn_folio((swp_tb & ~SWP_TB_COUNT_MASK) >> SWP_TB_PFN_MARK_BITS);\n+\treturn pfn_folio((swp_tb & ~SWP_TB_FLAGS_MASK) >> SWP_TB_PFN_MARK_BITS);\n }\n \n static inline void *swp_tb_to_shadow(unsigned long swp_tb)\n {\n \tVM_WARN_ON(!swp_tb_is_shadow(swp_tb));\n \t/* No shift needed, xa_value is stored as it is in the lower bits. */\n-\treturn (void *)(swp_tb & ~SWP_TB_COUNT_MASK);\n+\treturn (void *)(swp_tb & ~SWP_TB_FLAGS_MASK);\n }\n \n static inline unsigned char __swp_tb_get_count(unsigned long swp_tb)\n@@ -184,6 +199,12 @@ static inline unsigned char __swp_tb_get_count(unsigned long swp_tb)\n \treturn ((swp_tb & SWP_TB_COUNT_MASK) >> SWP_TB_COUNT_SHIFT);\n }\n \n+static inline unsigned char __swp_tb_get_flags(unsigned long swp_tb)\n+{\n+\tVM_WARN_ON(!swp_tb_is_countable(swp_tb));\n+\treturn ((swp_tb & SWP_TB_FLAGS_MASK) >> SWP_TB_FLAGS_SHIFT);\n+}\n+\n static inline int swp_tb_get_count(unsigned long swp_tb)\n {\n \tif (swp_tb_is_countable(swp_tb))\n@@ -247,4 +268,54 @@ static inline unsigned long swap_table_get(struct swap_cluster_info *ci,\n \n \treturn swp_tb;\n }\n+\n+static inline void __swap_table_set_zero(struct swap_cluster_info *ci,\n+\t\t\t\t\t unsigned int ci_off)\n+{\n+\tunsigned long swp_tb = __swap_table_get(ci, ci_off);\n+\n+\tVM_WARN_ON(!swp_tb_is_countable(swp_tb));\n+\tswp_tb |= SWP_TB_ZERO_MARK;\n+\t__swap_table_set(ci, ci_off, swp_tb);\n+}\n+\n+static inline void __swap_table_clear_zero(struct swap_cluster_info *ci,\n+\t\t\t\t\t   unsigned int ci_off)\n+{\n+\tunsigned long swp_tb = __swap_table_get(ci, ci_off);\n+\n+\tVM_WARN_ON(!swp_tb_is_countable(swp_tb));\n+\tswp_tb &= ~SWP_TB_ZERO_MARK;\n+\t__swap_table_set(ci, ci_off, swp_tb);\n+}\n+\n+/**\n+ * Return the count of contiguous swap entries that share the same\n+ * zeromap status as the starting entry. If is_zerop is not NULL,\n+ * it will return the zeromap status of the starting entry.\n+ *\n+ * Context: Caller must ensure the cluster containing the entries\n+ * that will be checked won't be freed.\n+ */\n+static inline int swap_zeromap_batch(swp_entry_t entry, int max_nr,\n+\t\t\t\t     bool *is_zerop)\n+{\n+\tbool is_zero;\n+\tunsigned long swp_tb;\n+\tstruct swap_cluster_info *ci = __swap_entry_to_cluster(entry);\n+\tunsigned int ci_start = swp_cluster_offset(entry), ci_off, ci_end;\n+\n+\tci_off = ci_start;\n+\tci_end = ci_off + max_nr;\n+\tswp_tb = swap_table_get(ci, ci_off);\n+\tis_zero = !!(swp_tb & SWP_TB_ZERO_MARK);\n+\tif (is_zerop)\n+\t\t*is_zerop = is_zero;\n+\twhile (++ci_off < ci_end) {\n+\t\tswp_tb = swap_table_get(ci, ci_off);\n+\t\tif (is_zero != !!(swp_tb & SWP_TB_ZERO_MARK))\n+\t\t\tbreak;\n+\t}\n+\treturn ci_off - ci_start;\n+}\n #endif\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex de34f1990209..4018e8694b72 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -918,7 +918,7 @@ static bool __swap_cluster_alloc_entries(struct swap_info_struct *si,\n \t\tnr_pages = 1;\n \t\tswap_cluster_assert_empty(ci, ci_off, 1, false);\n \t\t/* Sets a fake shadow as placeholder */\n-\t\t__swap_table_set(ci, ci_off, shadow_to_swp_tb(NULL, 1));\n+\t\t__swap_table_set(ci, ci_off, __swp_tb_mk_count(shadow_to_swp_tb(NULL, 0), 1));\n \t} else {\n \t\t/* Allocation without folio is only possible with hibernation */\n \t\tWARN_ON_ONCE(1);\n@@ -1308,14 +1308,8 @@ static void swap_range_free(struct swap_info_struct *si, unsigned long offset,\n \tvoid (*swap_slot_free_notify)(struct block_device *, unsigned long);\n \tunsigned int i;\n \n-\t/*\n-\t * Use atomic clear_bit operations only on zeromap instead of non-atomic\n-\t * bitmap_clear to prevent adjacent bits corruption due to simultaneous writes.\n-\t */\n-\tfor (i = 0; i < nr_entries; i++) {\n-\t\tclear_bit(offset + i, si->zeromap);\n+\tfor (i = 0; i < nr_entries; i++)\n \t\tzswap_invalidate(swp_entry(si->type, offset + i));\n-\t}\n \n \tif (si->flags & SWP_BLKDEV)\n \t\tswap_slot_free_notify =\n@@ -2921,7 +2915,6 @@ static void flush_percpu_swap_cluster(struct swap_info_struct *si)\n SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n {\n \tstruct swap_info_struct *p = NULL;\n-\tunsigned long *zeromap;\n \tstruct swap_cluster_info *cluster_info;\n \tstruct file *swap_file, *victim;\n \tstruct address_space *mapping;\n@@ -3009,8 +3002,6 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \n \tswap_file = p->swap_file;\n \tp->swap_file = NULL;\n-\tzeromap = p->zeromap;\n-\tp->zeromap = NULL;\n \tmaxpages = p->max;\n \tcluster_info = p->cluster_info;\n \tp->max = 0;\n@@ -3022,7 +3013,6 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \tmutex_unlock(&swapon_mutex);\n \tkfree(p->global_cluster);\n \tp->global_cluster = NULL;\n-\tkvfree(zeromap);\n \tfree_swap_cluster_info(cluster_info, maxpages);\n \n \tinode = mapping->host;\n@@ -3555,17 +3545,6 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tif (error)\n \t\tgoto bad_swap_unlock_inode;\n \n-\t/*\n-\t * Use kvmalloc_array instead of bitmap_zalloc as the allocation order might\n-\t * be above MAX_PAGE_ORDER incase of a large swap file.\n-\t */\n-\tsi->zeromap = kvmalloc_array(BITS_TO_LONGS(maxpages), sizeof(long),\n-\t\t\t\t     GFP_KERNEL | __GFP_ZERO);\n-\tif (!si->zeromap) {\n-\t\terror = -ENOMEM;\n-\t\tgoto bad_swap_unlock_inode;\n-\t}\n-\n \tif (si->bdev && bdev_stable_writes(si->bdev))\n \t\tsi->flags |= SWP_STABLE_WRITES;\n \n@@ -3667,8 +3646,6 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tdestroy_swap_extents(si, swap_file);\n \tfree_swap_cluster_info(si->cluster_info, si->max);\n \tsi->cluster_info = NULL;\n-\tkvfree(si->zeromap);\n-\tsi->zeromap = NULL;\n \t/*\n \t * Clear the SWP_USED flag after all resources are freed so\n \t * alloc_swap_info can reuse this si safely.\n\n-- \n2.53.0",
          "reply_to": "",
          "message_date": "2026-02-20"
        },
        {
          "author": "Kairui Song",
          "summary": "Reviewer noted that the current zswap implementation requires a backing swapfile, which wastes swapfile space, and suggested using a ghost swapfile instead, which only contains the swapfile header and does not waste swapfile space.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "From: Chris Li <chrisl@kernel.org>\n\nThe current zswap requires a backing swapfile. The swap slot used\nby zswap is not able to be used by the swapfile. That waste swapfile\nspace.\n\nThe ghost swapfile is a swapfile that only contains the swapfile header\nfor zswap. The swapfile header indicate the size of the swapfile. There\nis no swap data section in the ghost swapfile, therefore, no waste of\nswapfile space.  As such, any write to a ghost swapfile will fail. To\nprevents accidental read or write of ghost swapfile, bdev of\nswap_info_struct is set to NULL. Ghost swapfile will also set the SSD\nflag because there is no rotation disk access when using zswap.\n\nThe zswap write back has been disabled if all swapfiles in the system\nare ghost swap files.\n\nSigned-off-by: Chris Li <chrisl@kernel.org>\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n include/linux/swap.h |  2 ++\n mm/page_io.c         | 18 +++++++++++++++---\n mm/swap.h            |  2 +-\n mm/swapfile.c        | 42 +++++++++++++++++++++++++++++++++++++-----\n mm/zswap.c           | 12 +++++++++---\n 5 files changed, 64 insertions(+), 12 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex bc871d8a1e99..3b2efd319f44 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -215,6 +215,7 @@ enum {\n \tSWP_PAGE_DISCARD = (1 << 10),\t/* freed swap page-cluster discards */\n \tSWP_STABLE_WRITES = (1 << 11),\t/* no overwrite PG_writeback pages */\n \tSWP_SYNCHRONOUS_IO = (1 << 12),\t/* synchronous IO is efficient */\n+\tSWP_GHOST\t= (1 << 13),\t/* not backed by anything */\n \t\t\t\t\t/* add others here before... */\n };\n \n@@ -419,6 +420,7 @@ void free_folio_and_swap_cache(struct folio *folio);\n void free_pages_and_swap_cache(struct encoded_page **, int);\n /* linux/mm/swapfile.c */\n extern atomic_long_t nr_swap_pages;\n+extern atomic_t nr_real_swapfiles;\n extern long total_swap_pages;\n extern atomic_t nr_rotate_swap;\n \ndiff --git a/mm/page_io.c b/mm/page_io.c\nindex 5a0b5034489b..f4a5fc0863f5 100644\n--- a/mm/page_io.c\n+++ b/mm/page_io.c\n@@ -291,8 +291,7 @@ int swap_writeout(struct folio *folio, struct swap_iocb **swap_plug)\n \t\treturn AOP_WRITEPAGE_ACTIVATE;\n \t}\n \n-\t__swap_writepage(folio, swap_plug);\n-\treturn 0;\n+\treturn __swap_writepage(folio, swap_plug);\n out_unlock:\n \tfolio_unlock(folio);\n \treturn ret;\n@@ -454,11 +453,18 @@ static void swap_writepage_bdev_async(struct folio *folio,\n \tsubmit_bio(bio);\n }\n \n-void __swap_writepage(struct folio *folio, struct swap_iocb **swap_plug)\n+int __swap_writepage(struct folio *folio, struct swap_iocb **swap_plug)\n {\n \tstruct swap_info_struct *sis = __swap_entry_to_info(folio->swap);\n \n \tVM_BUG_ON_FOLIO(!folio_test_swapcache(folio), folio);\n+\n+\tif (sis->flags & SWP_GHOST) {\n+\t\t/* Prevent the page from getting reclaimed. */\n+\t\tfolio_set_dirty(folio);\n+\t\treturn AOP_WRITEPAGE_ACTIVATE;\n+\t}\n+\n \t/*\n \t * ->flags can be updated non-atomically (scan_swap_map_slots),\n \t * but that will never affect SWP_FS_OPS, so the data_race\n@@ -475,6 +481,7 @@ void __swap_writepage(struct folio *folio, struct swap_iocb **swap_plug)\n \t\tswap_writepage_bdev_sync(folio, sis);\n \telse\n \t\tswap_writepage_bdev_async(folio, sis);\n+\treturn 0;\n }\n \n void swap_write_unplug(struct swap_iocb *sio)\n@@ -649,6 +656,11 @@ void swap_read_folio(struct folio *folio, struct swap_iocb **plug)\n \tif (zswap_load(folio) != -ENOENT)\n \t\tgoto finish;\n \n+\tif (unlikely(sis->flags & SWP_GHOST)) {\n+\t\tfolio_unlock(folio);\n+\t\tgoto finish;\n+\t}\n+\n \t/* We have to read from slower devices. Increase zswap protection. */\n \tzswap_folio_swapin(folio);\n \ndiff --git a/mm/swap.h b/mm/swap.h\nindex cb1ab20d83d5..55aa6d904afd 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -226,7 +226,7 @@ static inline void swap_read_unplug(struct swap_iocb *plug)\n }\n void swap_write_unplug(struct swap_iocb *sio);\n int swap_writeout(struct folio *folio, struct swap_iocb **swap_plug);\n-void __swap_writepage(struct folio *folio, struct swap_iocb **swap_plug);\n+int __swap_writepage(struct folio *folio, struct swap_iocb **swap_plug);\n \n /* linux/mm/swap_state.c */\n extern struct address_space swap_space __read_mostly;\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 4018e8694b72..65666c43cbd5 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -67,6 +67,7 @@ static void move_cluster(struct swap_info_struct *si,\n static DEFINE_SPINLOCK(swap_lock);\n static unsigned int nr_swapfiles;\n atomic_long_t nr_swap_pages;\n+atomic_t nr_real_swapfiles;\n /*\n  * Some modules use swappable objects and may try to swap them out under\n  * memory pressure (via the shrinker). Before doing so, they may wish to\n@@ -1211,6 +1212,8 @@ static void del_from_avail_list(struct swap_info_struct *si, bool swapoff)\n \t\t\tgoto skip;\n \t}\n \n+\tif (!(si->flags & SWP_GHOST))\n+\t\tatomic_sub(1, &nr_real_swapfiles);\n \tplist_del(&si->avail_list, &swap_avail_head);\n \n skip:\n@@ -1253,6 +1256,8 @@ static void add_to_avail_list(struct swap_info_struct *si, bool swapon)\n \t}\n \n \tplist_add(&si->avail_list, &swap_avail_head);\n+\tif (!(si->flags & SWP_GHOST))\n+\t\tatomic_add(1, &nr_real_swapfiles);\n \n skip:\n \tspin_unlock(&swap_avail_lock);\n@@ -2793,6 +2798,11 @@ static int setup_swap_extents(struct swap_info_struct *sis,\n \tstruct inode *inode = mapping->host;\n \tint ret;\n \n+\tif (sis->flags & SWP_GHOST) {\n+\t\t*span = 0;\n+\t\treturn 0;\n+\t}\n+\n \tif (S_ISBLK(inode->i_mode)) {\n \t\tret = add_swap_extent(sis, 0, sis->max, 0);\n \t\t*span = sis->pages;\n@@ -2992,7 +3002,8 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \n \tdestroy_swap_extents(p, p->swap_file);\n \n-\tif (!(p->flags & SWP_SOLIDSTATE))\n+\tif (!(p->flags & SWP_GHOST) &&\n+\t    !(p->flags & SWP_SOLIDSTATE))\n \t\tatomic_dec(&nr_rotate_swap);\n \n \tmutex_lock(&swapon_mutex);\n@@ -3102,6 +3113,19 @@ static void swap_stop(struct seq_file *swap, void *v)\n \tmutex_unlock(&swapon_mutex);\n }\n \n+static const char *swap_type_str(struct swap_info_struct *si)\n+{\n+\tstruct file *file = si->swap_file;\n+\n+\tif (si->flags & SWP_GHOST)\n+\t\treturn \"ghost\\t\";\n+\n+\tif (S_ISBLK(file_inode(file)->i_mode))\n+\t\treturn \"partition\";\n+\n+\treturn \"file\\t\";\n+}\n+\n static int swap_show(struct seq_file *swap, void *v)\n {\n \tstruct swap_info_struct *si = v;\n@@ -3121,8 +3145,7 @@ static int swap_show(struct seq_file *swap, void *v)\n \tlen = seq_file_path(swap, file, \" \\t\\n\\\\\");\n \tseq_printf(swap, \"%*s%s\\t%lu\\t%s%lu\\t%s%d\\n\",\n \t\t\tlen < 40 ? 40 - len : 1, \" \",\n-\t\t\tS_ISBLK(file_inode(file)->i_mode) ?\n-\t\t\t\t\"partition\" : \"file\\t\",\n+\t\t\tswap_type_str(si),\n \t\t\tbytes, bytes < 10000000 ? \"\\t\" : \"\",\n \t\t\tinuse, inuse < 10000000 ? \"\\t\" : \"\",\n \t\t\tsi->prio);\n@@ -3254,7 +3277,6 @@ static int claim_swapfile(struct swap_info_struct *si, struct inode *inode)\n \treturn 0;\n }\n \n-\n /*\n  * Find out how many pages are allowed for a single swap device. There\n  * are two limiting factors:\n@@ -3300,6 +3322,7 @@ static unsigned long read_swap_header(struct swap_info_struct *si,\n \tunsigned long maxpages;\n \tunsigned long swapfilepages;\n \tunsigned long last_page;\n+\tloff_t size;\n \n \tif (memcmp(\"SWAPSPACE2\", swap_header->magic.magic, 10)) {\n \t\tpr_err(\"Unable to find swap-space signature\\n\");\n@@ -3342,7 +3365,16 @@ static unsigned long read_swap_header(struct swap_info_struct *si,\n \n \tif (!maxpages)\n \t\treturn 0;\n-\tswapfilepages = i_size_read(inode) >> PAGE_SHIFT;\n+\n+\tsize = i_size_read(inode);\n+\tif (size == PAGE_SIZE) {\n+\t\t/* Ghost swapfile */\n+\t\tsi->bdev = NULL;\n+\t\tsi->flags |= SWP_GHOST | SWP_SOLIDSTATE;\n+\t\treturn maxpages;\n+\t}\n+\n+\tswapfilepages = size >> PAGE_SHIFT;\n \tif (swapfilepages && maxpages > swapfilepages) {\n \t\tpr_warn(\"Swap area shorter than signature indicates\\n\");\n \t\treturn 0;\ndiff --git a/mm/zswap.c b/mm/zswap.c\nindex 5d83539a8bba..e470f697e770 100644\n--- a/mm/zswap.c\n+++ b/mm/zswap.c\n@@ -995,11 +995,16 @@ static int zswap_writeback_entry(struct zswap_entry *entry,\n \tstruct swap_info_struct *si;\n \tint ret = 0;\n \n-\t/* try to allocate swap cache folio */\n \tsi = get_swap_device(swpentry);\n \tif (!si)\n \t\treturn -EEXIST;\n \n+\tif (si->flags & SWP_GHOST) {\n+\t\tput_swap_device(si);\n+\t\treturn -EINVAL;\n+\t}\n+\n+\t/* try to allocate swap cache folio */\n \tmpol = get_task_policy(current);\n \tfolio = swap_cache_alloc_folio(swpentry, GFP_KERNEL, 0, NULL, mpol,\n \t\t\t\t       NO_INTERLEAVE_INDEX);\n@@ -1052,7 +1057,8 @@ static int zswap_writeback_entry(struct zswap_entry *entry,\n \tfolio_set_reclaim(folio);\n \n \t/* start writeback */\n-\t__swap_writepage(folio, NULL);\n+\tret = __swap_writepage(folio, NULL);\n+\tWARN_ON_ONCE(ret);\n \n out:\n \tif (ret) {\n@@ -1536,7 +1542,7 @@ bool zswap_store(struct folio *folio)\n \tzswap_pool_put(pool);\n put_objcg:\n \tobj_cgroup_put(objcg);\n-\tif (!ret && zswap_pool_reached_full)\n+\tif (!ret && zswap_pool_reached_full && atomic_read(&nr_real_swapfiles))\n \t\tqueue_work(shrink_wq, &zswap_shrink_work);\n check_old:\n \t/*\n\n-- \n2.53.0",
          "reply_to": "",
          "message_date": "2026-02-20"
        },
        {
          "author": "Kairui Song",
          "summary": "The reviewer suggested using /dev/ghostswap as a special device to allow userspace setup of ghost swap easily without extra tools.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "suggestion",
            "improvement"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Use /dev/ghostswap as a special device so userspace can setup ghost\nswap easily without any extra tools.\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n drivers/char/mem.c   | 39 +++++++++++++++++++++++++++++++++++++++\n include/linux/swap.h |  2 ++\n mm/swapfile.c        | 22 +++++++++++++++++++---\n 3 files changed, 60 insertions(+), 3 deletions(-)\n\ndiff --git a/drivers/char/mem.c b/drivers/char/mem.c\nindex cca4529431f8..8d0eb3f7d191 100644\n--- a/drivers/char/mem.c\n+++ b/drivers/char/mem.c\n@@ -30,6 +30,7 @@\n #include <linux/uio.h>\n #include <linux/uaccess.h>\n #include <linux/security.h>\n+#include <linux/swap.h>\n \n #define DEVMEM_MINOR\t1\n #define DEVPORT_MINOR\t4\n@@ -667,6 +668,41 @@ static const struct file_operations null_fops = {\n \t.uring_cmd\t= uring_cmd_null,\n };\n \n+#ifdef CONFIG_SWAP\n+static ssize_t read_ghostswap(struct file *file, char __user *buf,\n+\t\t\t      size_t count, loff_t *ppos)\n+{\n+\tunion swap_header *hdr;\n+\tsize_t to_copy;\n+\n+\tif (*ppos >= PAGE_SIZE)\n+\t\treturn 0;\n+\n+\thdr = kzalloc(PAGE_SIZE, GFP_KERNEL);\n+\tif (!hdr)\n+\t\treturn -ENOMEM;\n+\n+\thdr->info.version = 1;\n+\thdr->info.last_page = totalram_pages() - 1;\n+\tmemcpy(hdr->magic.magic, \"SWAPSPACE2\", 10);\n+\tto_copy = min_t(size_t, count, PAGE_SIZE - *ppos);\n+\tif (copy_to_user(buf, (char *)hdr + *ppos, to_copy)) {\n+\t\tkfree(hdr);\n+\t\treturn -EFAULT;\n+\t}\n+\n+\tkfree(hdr);\n+\t*ppos += to_copy;\n+\treturn to_copy;\n+}\n+\n+static const struct file_operations ghostswap_fops = {\n+\t.llseek\t\t= null_lseek,\n+\t.read\t\t= read_ghostswap,\n+\t.write\t\t= write_null,\n+};\n+#endif\n+\n #ifdef CONFIG_DEVPORT\n static const struct file_operations port_fops = {\n \t.llseek\t\t= memory_lseek,\n@@ -718,6 +754,9 @@ static const struct memdev {\n #ifdef CONFIG_PRINTK\n \t[11] = { \"kmsg\", &kmsg_fops, 0, 0644 },\n #endif\n+#ifdef CONFIG_SWAP\n+\t[DEVGHOST_MINOR] = { \"ghostswap\", &ghostswap_fops, 0, 0660 },\n+#endif\n };\n \n static int memory_open(struct inode *inode, struct file *filp)\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 3b2efd319f44..b57a4a40f4fe 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -421,6 +421,8 @@ void free_pages_and_swap_cache(struct encoded_page **, int);\n /* linux/mm/swapfile.c */\n extern atomic_long_t nr_swap_pages;\n extern atomic_t nr_real_swapfiles;\n+\n+#define DEVGHOST_MINOR\t13\t/* /dev/ghostswap char device minor */\n extern long total_swap_pages;\n extern atomic_t nr_rotate_swap;\n \ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 65666c43cbd5..d054f40ec75f 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -42,6 +42,7 @@\n #include <linux/suspend.h>\n #include <linux/zswap.h>\n #include <linux/plist.h>\n+#include <linux/major.h>\n \n #include <asm/tlbflush.h>\n #include <linux/leafops.h>\n@@ -1703,6 +1704,7 @@ int folio_alloc_swap(struct folio *folio)\n \tunsigned int size = 1 << order;\n \tstruct swap_cluster_info *ci;\n \n+\tVM_WARN_ON_FOLIO(folio_test_swapcache(folio), folio);\n \tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n \tVM_BUG_ON_FOLIO(!folio_test_uptodate(folio), folio);\n \n@@ -3421,6 +3423,10 @@ static int setup_swap_clusters_info(struct swap_info_struct *si,\n \terr = swap_cluster_setup_bad_slot(si, cluster_info, 0, false);\n \tif (err)\n \t\tgoto err;\n+\n+\tif (!swap_header)\n+\t\tgoto setup_cluster_info;\n+\n \tfor (i = 0; i < swap_header->info.nr_badpages; i++) {\n \t\tunsigned int page_nr = swap_header->info.badpages[i];\n \n@@ -3440,6 +3446,7 @@ static int setup_swap_clusters_info(struct swap_info_struct *si,\n \t\t\tgoto err;\n \t}\n \n+setup_cluster_info:\n \tINIT_LIST_HEAD(&si->free_clusters);\n \tINIT_LIST_HEAD(&si->full_clusters);\n \tINIT_LIST_HEAD(&si->discard_clusters);\n@@ -3476,7 +3483,7 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \tstruct dentry *dentry;\n \tint prio;\n \tint error;\n-\tunion swap_header *swap_header;\n+\tunion swap_header *swap_header = NULL;\n \tint nr_extents;\n \tsector_t span;\n \tunsigned long maxpages;\n@@ -3528,6 +3535,15 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \t\tgoto bad_swap_unlock_inode;\n \t}\n \n+\t/* /dev/ghostswap: synthesize a ghost swap device. */\n+\tif (S_ISCHR(inode->i_mode) &&\n+\t    imajor(inode) == MEM_MAJOR && iminor(inode) == DEVGHOST_MINOR) {\n+\t\tmaxpages = round_up(totalram_pages(), SWAPFILE_CLUSTER);\n+\t\tsi->flags |= SWP_GHOST | SWP_SOLIDSTATE;\n+\t\tsi->bdev = NULL;\n+\t\tgoto setup;\n+\t}\n+\n \t/*\n \t * The swap subsystem needs a major overhaul to support this.\n \t * It doesn't work yet so just disable it for now.\n@@ -3550,13 +3566,13 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \t\tgoto bad_swap_unlock_inode;\n \t}\n \tswap_header = kmap_local_folio(folio, 0);\n-\n \tmaxpages = read_swap_header(si, swap_header, inode);\n \tif (unlikely(!maxpages)) {\n \t\terror = -EINVAL;\n \t\tgoto bad_swap_unlock_inode;\n \t}\n \n+setup:\n \tsi->max = maxpages;\n \tsi->pages = maxpages - 1;\n \tnr_extents = setup_swap_extents(si, swap_file, &span);\n@@ -3585,7 +3601,7 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \n \tif (si->bdev && bdev_nonrot(si->bdev)) {\n \t\tsi->flags |= SWP_SOLIDSTATE;\n-\t} else {\n+\t} else if (!(si->flags & SWP_SOLIDSTATE)) {\n \t\tatomic_inc(&nr_rotate_swap);\n \t\tinced_nr_rotate_swap = true;\n \t}\n\n-- \n2.53.0",
          "reply_to": "",
          "message_date": "2026-02-20"
        },
        {
          "author": "Kairui Song",
          "summary": "The reviewer noted that the folio lock can be used to stabilize virtual table data, and suggested using a folio_realloc_swap for swap entry writeback, which would simplify the implementation by removing the global percpu cluster cache.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "suggested improvements"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Now, the ghost swap file is completely dynamic. For easier testing, this\ncommit makes the /dev/ghostswap 8 times the size of total ram by\ndefault.\n\nNOTE: This commit is still a minimal proof of concept, so many parts of\nthe implementation can be improved.\n\nAnd we have a ci_dyn->virtual_table that's is ready to be used (not used\nyet). For example, storing zswap's metadata. In theory the folio lock can\nbe used to stablize it's virtual table data.\n\ne.g., Swap entry writeback can also be done easily using a\nfolio_realloc_swap, skip the folio->swap's device and use underlying\ndevices, it will be easier to do if we remove the global percpu cluster\ncache as suggested by [1] and should just work with tiering and priority.\nJust put the folio->swap as a reverse entry in the lower layer's swap\ntable, and collect lower level's swap entry in the virtual_table, then\nit's all good.\n\nAnd right now all allocations are using atomic, which can also be\nimproved as the swap table already has sleep allocation support,\njust need to adapt it.\n\nThe RCU lock protection convention can also be simplified.\n\nBut without all that, this works pretty well. We can have a \"virtual\nswap\" of any size with zero overhead, common stress tests are showing\na very nice performance, while ordinary swaps have zero overhead,\nand everything is runtime configurable.\n\nBut don't be too surprised if some corner cases are not well covered\nyet, as most works are still focusing on the infrastructure.\n\nLink: https://lore.kernel.org/linux-mm/20260126065242.1221862-5-youngjun.park@lge.com/ [1]\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\n include/linux/swap.h |   1 +\n mm/swap.h            |  44 +++++++++++++---\n mm/swap_state.c      |  35 ++++++++-----\n mm/swap_table.h      |   2 +\n mm/swapfile.c        | 145 +++++++++++++++++++++++++++++++++++++++++++++++----\n 5 files changed, 199 insertions(+), 28 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex b57a4a40f4fe..41d7eae56d65 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -284,6 +284,7 @@ struct swap_info_struct {\n \tstruct work_struct reclaim_work; /* reclaim worker */\n \tstruct list_head discard_clusters; /* discard clusters list */\n \tstruct plist_node avail_list;   /* entry in swap_avail_head */\n+\tstruct xarray cluster_info_pool; /* Xarray for ghost swap cluster info */\n };\n \n static inline swp_entry_t page_swap_entry(struct page *page)\ndiff --git a/mm/swap.h b/mm/swap.h\nindex 55aa6d904afd..7a4d1d939842 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -41,6 +41,13 @@ struct swap_cluster_info {\n \tstruct list_head list;\n };\n \n+struct swap_cluster_info_dynamic {\n+\tstruct swap_cluster_info ci;\t/* Underlying cluster info */\n+\tunsigned int index;\t\t/* for cluster_index() */\n+\tstruct rcu_head rcu;\t\t/* For kfree_rcu deferred free */\n+\t/* unsigned long *virtual_table; And we can easily have a virtual table */\n+};\n+\n /* All on-list cluster must have a non-zero flag. */\n enum swap_cluster_flags {\n \tCLUSTER_FLAG_NONE = 0, /* For temporary off-list cluster */\n@@ -51,6 +58,7 @@ enum swap_cluster_flags {\n \tCLUSTER_FLAG_USABLE = CLUSTER_FLAG_FRAG,\n \tCLUSTER_FLAG_FULL,\n \tCLUSTER_FLAG_DISCARD,\n+\tCLUSTER_FLAG_DEAD,\t/* Ghost cluster pending kfree_rcu */\n \tCLUSTER_FLAG_MAX,\n };\n \n@@ -84,9 +92,19 @@ static inline struct swap_info_struct *__swap_entry_to_info(swp_entry_t entry)\n static inline struct swap_cluster_info *__swap_offset_to_cluster(\n \t\tstruct swap_info_struct *si, pgoff_t offset)\n {\n+\tunsigned int cluster_idx = offset / SWAPFILE_CLUSTER;\n+\n \tVM_WARN_ON_ONCE(percpu_ref_is_zero(&si->users)); /* race with swapoff */\n \tVM_WARN_ON_ONCE(offset >= roundup(si->max, SWAPFILE_CLUSTER));\n-\treturn &si->cluster_info[offset / SWAPFILE_CLUSTER];\n+\n+\tif (si->flags & SWP_GHOST) {\n+\t\tstruct swap_cluster_info_dynamic *ci_dyn;\n+\n+\t\tci_dyn = xa_load(&si->cluster_info_pool, cluster_idx);\n+\t\treturn ci_dyn ? &ci_dyn->ci : NULL;\n+\t}\n+\n+\treturn &si->cluster_info[cluster_idx];\n }\n \n static inline struct swap_cluster_info *__swap_entry_to_cluster(swp_entry_t entry)\n@@ -98,7 +116,7 @@ static inline struct swap_cluster_info *__swap_entry_to_cluster(swp_entry_t entr\n static __always_inline struct swap_cluster_info *__swap_cluster_lock(\n \t\tstruct swap_info_struct *si, unsigned long offset, bool irq)\n {\n-\tstruct swap_cluster_info *ci = __swap_offset_to_cluster(si, offset);\n+\tstruct swap_cluster_info *ci;\n \n \t/*\n \t * Nothing modifies swap cache in an IRQ context. All access to\n@@ -111,10 +129,24 @@ static __always_inline struct swap_cluster_info *__swap_cluster_lock(\n \t */\n \tVM_WARN_ON_ONCE(!in_task());\n \tVM_WARN_ON_ONCE(percpu_ref_is_zero(&si->users)); /* race with swapoff */\n-\tif (irq)\n-\t\tspin_lock_irq(&ci->lock);\n-\telse\n-\t\tspin_lock(&ci->lock);\n+\n+\trcu_read_lock();\n+\tci = __swap_offset_to_cluster(si, offset);\n+\tif (ci) {\n+\t\tif (irq)\n+\t\t\tspin_lock_irq(&ci->lock);\n+\t\telse\n+\t\t\tspin_lock(&ci->lock);\n+\n+\t\tif (ci->flags == CLUSTER_FLAG_DEAD) {\n+\t\t\tif (irq)\n+\t\t\t\tspin_unlock_irq(&ci->lock);\n+\t\t\telse\n+\t\t\t\tspin_unlock(&ci->lock);\n+\t\t\tci = NULL;\n+\t\t}\n+\t}\n+\trcu_read_unlock();\n \treturn ci;\n }\n \ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 419419e18a47..1c3600a93ecd 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -90,8 +90,10 @@ struct folio *swap_cache_get_folio(swp_entry_t entry)\n \tstruct folio *folio;\n \n \tfor (;;) {\n+\t\trcu_read_lock();\n \t\tswp_tb = swap_table_get(__swap_entry_to_cluster(entry),\n \t\t\t\t\tswp_cluster_offset(entry));\n+\t\trcu_read_unlock();\n \t\tif (!swp_tb_is_folio(swp_tb))\n \t\t\treturn NULL;\n \t\tfolio = swp_tb_to_folio(swp_tb);\n@@ -113,8 +115,10 @@ bool swap_cache_has_folio(swp_entry_t entry)\n {\n \tunsigned long swp_tb;\n \n+\trcu_read_lock();\n \tswp_tb = swap_table_get(__swap_entry_to_cluster(entry),\n \t\t\t\tswp_cluster_offset(entry));\n+\trcu_read_unlock();\n \treturn swp_tb_is_folio(swp_tb);\n }\n \n@@ -130,8 +134,10 @@ void *swap_cache_get_shadow(swp_entry_t entry)\n {\n \tunsigned long swp_tb;\n \n+\trcu_read_lock();\n \tswp_tb = swap_table_get(__swap_entry_to_cluster(entry),\n \t\t\t\tswp_cluster_offset(entry));\n+\trcu_read_unlock();\n \tif (swp_tb_is_shadow(swp_tb))\n \t\treturn swp_tb_to_shadow(swp_tb);\n \treturn NULL;\n@@ -209,14 +215,14 @@ void __swap_cache_add_folio(struct swap_cluster_info *ci,\n \tlruvec_stat_mod_folio(folio, NR_SWAPCACHE, nr_pages);\n }\n \n-static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n-\t\t\t\t\tswp_entry_t targ_entry, gfp_t gfp,\n+static struct folio *__swap_cache_alloc(swp_entry_t targ_entry, gfp_t gfp,\n \t\t\t\t\tunsigned int order, struct vm_fault *vmf,\n \t\t\t\t\tstruct mempolicy *mpol, pgoff_t ilx)\n {\n \tint err;\n \tswp_entry_t entry;\n \tstruct folio *folio;\n+\tstruct swap_cluster_info *ci;\n \tvoid *shadow = NULL, *shadow_check = NULL;\n \tunsigned long address, nr_pages = 1 << order;\n \tunsigned int ci_off, ci_targ = swp_cluster_offset(targ_entry);\n@@ -225,9 +231,12 @@ static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n \tci_off = round_down(ci_targ, nr_pages);\n \n \t/* First check if the range is available */\n-\tspin_lock(&ci->lock);\n-\terr = __swap_cache_check_batch(ci, entry, ci_off, ci_targ, nr_pages, &shadow);\n-\tspin_unlock(&ci->lock);\n+\terr = -ENOENT;\n+\tci = swap_cluster_lock(__swap_entry_to_info(entry), swp_offset(entry));\n+\tif (ci) {\n+\t\terr = __swap_cache_check_batch(ci, entry, ci_off, ci_targ, nr_pages, &shadow);\n+\t\tswap_cluster_unlock(ci);\n+\t}\n \tif (unlikely(err))\n \t\treturn ERR_PTR(err);\n \n@@ -243,10 +252,13 @@ static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n \t\treturn ERR_PTR(-ENOMEM);\n \n \t/* Double check the range is still not in conflict */\n-\tspin_lock(&ci->lock);\n-\terr = __swap_cache_check_batch(ci, entry, ci_off, ci_targ, nr_pages, &shadow_check);\n+\terr = -ENOENT;\n+\tci = swap_cluster_lock(__swap_entry_to_info(entry), swp_offset(entry));\n+\tif (ci)\n+\t\terr = __swap_cache_check_batch(ci, entry, ci_off, ci_targ, nr_pages, &shadow_check);\n \tif (unlikely(err) || shadow_check != shadow) {\n-\t\tspin_unlock(&ci->lock);\n+\t\tif (ci)\n+\t\t\tswap_cluster_unlock(ci);\n \t\tfolio_put(folio);\n \n \t\t/* If shadow changed, just try again */\n@@ -256,13 +268,14 @@ static struct folio *__swap_cache_alloc(struct swap_cluster_info *ci,\n \t__folio_set_locked(folio);\n \t__folio_set_swapbacked(folio);\n \t__swap_cache_add_folio(ci, folio, entry);\n-\tspin_unlock(&ci->lock);\n+\tswap_cluster_unlock(ci);\n \n \t/* With swap table, we must have a shadow, for memcg tracking */\n \tWARN_ON(!shadow);\n \n \tif (mem_cgroup_swapin_charge_folio(folio, vmf ? vmf->vma->vm_mm : NULL,\n \t\t\t\t\t   gfp, shadow_to_memcgid(shadow))) {\n+\t\t/* The folio pins the cluster */\n \t\tspin_lock(&ci->lock);\n \t\t__swap_cache_del_folio(ci, folio, shadow, false, false);\n \t\tspin_unlock(&ci->lock);\n@@ -305,13 +318,11 @@ struct folio *swap_cache_alloc_folio(swp_entry_t targ_entry, gfp_t gfp_mask,\n {\n \tint order, err;\n \tstruct folio *folio;\n-\tstruct swap_cluster_info *ci;\n \n \t/* Always allow order 0 so swap won't fail under pressure. */\n \torder = orders ? highest_order(orders |= BIT(0)) : 0;\n-\tci = __swap_entry_to_cluster(targ_entry);\n \tfor (;;) {\n-\t\tfolio = __swap_cache_alloc(ci, targ_entry, gfp_mask, order,\n+\t\tfolio = __swap_cache_alloc(targ_entry, gfp_mask, order,\n \t\t\t\t\t   vmf, mpol, ilx);\n \t\tif (!IS_ERR(folio))\n \t\t\treturn folio;\ndiff --git a/mm/swap_table.h b/mm/swap_table.h\nindex 6d3d773e1908..867bcfff0e3c 100644\n--- a/mm/swap_table.h\n+++ b/mm/swap_table.h\n@@ -260,6 +260,8 @@ static inline unsigned long swap_table_get(struct swap_cluster_info *ci,\n \tunsigned long swp_tb;\n \n \tVM_WARN_ON_ONCE(off >= SWAPFILE_CLUSTER);\n+\tif (!ci)\n+\t\treturn SWP_TB_NULL;\n \n \trcu_read_lock();\n \ttable = rcu_dereference(ci->table);\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex d054f40ec75f..f0682c8c8f53 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -404,6 +404,8 @@ static inline bool cluster_is_usable(struct swap_cluster_info *ci, int order)\n static inline unsigned int cluster_index(struct swap_info_struct *si,\n \t\t\t\t\t struct swap_cluster_info *ci)\n {\n+\tif (si->flags & SWP_GHOST)\n+\t\treturn container_of(ci, struct swap_cluster_info_dynamic, ci)->index;\n \treturn ci - si->cluster_info;\n }\n \n@@ -708,6 +710,22 @@ static void free_cluster(struct swap_info_struct *si, struct swap_cluster_info *\n \t\treturn;\n \t}\n \n+\tif (si->flags & SWP_GHOST) {\n+\t\tstruct swap_cluster_info_dynamic *ci_dyn;\n+\n+\t\tci_dyn = container_of(ci, struct swap_cluster_info_dynamic, ci);\n+\t\tif (ci->flags != CLUSTER_FLAG_NONE) {\n+\t\t\tspin_lock(&si->lock);\n+\t\t\tlist_del(&ci->list);\n+\t\t\tspin_unlock(&si->lock);\n+\t\t}\n+\t\tswap_cluster_free_table(ci);\n+\t\txa_erase(&si->cluster_info_pool, ci_dyn->index);\n+\t\tci->flags = CLUSTER_FLAG_DEAD;\n+\t\tkfree_rcu(ci_dyn, rcu);\n+\t\treturn;\n+\t}\n+\n \t__free_cluster(si, ci);\n }\n \n@@ -814,15 +832,17 @@ static int swap_cluster_setup_bad_slot(struct swap_info_struct *si,\n  * stolen by a lower order). @usable will be set to false if that happens.\n  */\n static bool cluster_reclaim_range(struct swap_info_struct *si,\n-\t\t\t\t  struct swap_cluster_info *ci,\n+\t\t\t\t  struct swap_cluster_info **pcip,\n \t\t\t\t  unsigned long start, unsigned int order,\n \t\t\t\t  bool *usable)\n {\n+\tstruct swap_cluster_info *ci = *pcip;\n \tunsigned int nr_pages = 1 << order;\n \tunsigned long offset = start, end = start + nr_pages;\n \tunsigned long swp_tb;\n \n \tspin_unlock(&ci->lock);\n+\trcu_read_lock();\n \tdo {\n \t\tswp_tb = swap_table_get(ci, offset % SWAPFILE_CLUSTER);\n \t\tif (swp_tb_get_count(swp_tb))\n@@ -831,7 +851,15 @@ static bool cluster_reclaim_range(struct swap_info_struct *si,\n \t\t\tif (__try_to_reclaim_swap(si, offset, TTRS_ANYWAY) < 0)\n \t\t\t\tbreak;\n \t} while (++offset < end);\n-\tspin_lock(&ci->lock);\n+\trcu_read_unlock();\n+\n+\t/* Re-lookup: ghost cluster may have been freed while lock was dropped */\n+\tci = swap_cluster_lock(si, start);\n+\t*pcip = ci;\n+\tif (!ci) {\n+\t\t*usable = false;\n+\t\treturn false;\n+\t}\n \n \t/*\n \t * We just dropped ci->lock so cluster could be used by another\n@@ -979,7 +1007,8 @@ static unsigned int alloc_swap_scan_cluster(struct swap_info_struct *si,\n \t\tif (!cluster_scan_range(si, ci, offset, nr_pages, &need_reclaim))\n \t\t\tcontinue;\n \t\tif (need_reclaim) {\n-\t\t\tret = cluster_reclaim_range(si, ci, offset, order, &usable);\n+\t\t\tret = cluster_reclaim_range(si, &ci, offset, order,\n+\t\t\t\t\t\t    &usable);\n \t\t\tif (!usable)\n \t\t\t\tgoto out;\n \t\t\tif (cluster_is_empty(ci))\n@@ -1005,8 +1034,10 @@ static unsigned int alloc_swap_scan_cluster(struct swap_info_struct *si,\n \t * should use a new cluster, and move the failed cluster to where it\n \t * should be.\n \t */\n-\trelocate_cluster(si, ci);\n-\tswap_cluster_unlock(ci);\n+\tif (ci) {\n+\t\trelocate_cluster(si, ci);\n+\t\tswap_cluster_unlock(ci);\n+\t}\n \tif (si->flags & SWP_SOLIDSTATE) {\n \t\tthis_cpu_write(percpu_swap_cluster.offset[order], next);\n \t\tthis_cpu_write(percpu_swap_cluster.si[order], si);\n@@ -1038,6 +1069,44 @@ static unsigned int alloc_swap_scan_list(struct swap_info_struct *si,\n \treturn found;\n }\n \n+static unsigned int alloc_swap_scan_dynamic(struct swap_info_struct *si,\n+\t\t\t\t\t    struct folio *folio)\n+{\n+\tstruct swap_cluster_info_dynamic *ci_dyn;\n+\tstruct swap_cluster_info *ci;\n+\tstruct swap_table *table;\n+\tunsigned long offset;\n+\n+\tWARN_ON(!(si->flags & SWP_GHOST));\n+\n+\tci_dyn = kzalloc(sizeof(*ci_dyn), GFP_ATOMIC);\n+\tif (!ci_dyn)\n+\t\treturn SWAP_ENTRY_INVALID;\n+\n+\ttable = swap_table_alloc(GFP_ATOMIC);\n+\tif (!table) {\n+\t\tkfree(ci_dyn);\n+\t\treturn SWAP_ENTRY_INVALID;\n+\t}\n+\n+\tspin_lock_init(&ci_dyn->ci.lock);\n+\tINIT_LIST_HEAD(&ci_dyn->ci.list);\n+\trcu_assign_pointer(ci_dyn->ci.table, table);\n+\n+\tif (xa_alloc(&si->cluster_info_pool, &ci_dyn->index, ci_dyn,\n+\t\t     XA_LIMIT(1, DIV_ROUND_UP(si->max, SWAPFILE_CLUSTER) - 1),\n+\t\t     GFP_ATOMIC)) {\n+\t\tswap_table_free(table);\n+\t\tkfree(ci_dyn);\n+\t\treturn SWAP_ENTRY_INVALID;\n+\t}\n+\n+\tci = &ci_dyn->ci;\n+\tspin_lock(&ci->lock);\n+\toffset = cluster_offset(si, ci);\n+\treturn alloc_swap_scan_cluster(si, ci, folio, offset);\n+}\n+\n static void swap_reclaim_full_clusters(struct swap_info_struct *si, bool force)\n {\n \tlong to_scan = 1;\n@@ -1060,7 +1129,9 @@ static void swap_reclaim_full_clusters(struct swap_info_struct *si, bool force)\n \t\t\t\tspin_unlock(&ci->lock);\n \t\t\t\tnr_reclaim = __try_to_reclaim_swap(si, offset,\n \t\t\t\t\t\t\t\t   TTRS_ANYWAY);\n-\t\t\t\tspin_lock(&ci->lock);\n+\t\t\t\tci = swap_cluster_lock(si, offset);\n+\t\t\t\tif (!ci)\n+\t\t\t\t\tgoto next;\n \t\t\t\tif (nr_reclaim) {\n \t\t\t\t\toffset += abs(nr_reclaim);\n \t\t\t\t\tcontinue;\n@@ -1074,6 +1145,7 @@ static void swap_reclaim_full_clusters(struct swap_info_struct *si, bool force)\n \t\t\trelocate_cluster(si, ci);\n \n \t\tswap_cluster_unlock(ci);\n+next:\n \t\tif (to_scan <= 0)\n \t\t\tbreak;\n \t}\n@@ -1136,6 +1208,12 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \t\t\tgoto done;\n \t}\n \n+\tif (si->flags & SWP_GHOST) {\n+\t\tfound = alloc_swap_scan_dynamic(si, folio);\n+\t\tif (found)\n+\t\t\tgoto done;\n+\t}\n+\n \tif (!(si->flags & SWP_PAGE_DISCARD)) {\n \t\tfound = alloc_swap_scan_list(si, &si->free_clusters, folio, false);\n \t\tif (found)\n@@ -1375,7 +1453,8 @@ static bool swap_alloc_fast(struct folio *folio)\n \t\treturn false;\n \n \tci = swap_cluster_lock(si, offset);\n-\talloc_swap_scan_cluster(si, ci, folio, offset);\n+\tif (ci)\n+\t\talloc_swap_scan_cluster(si, ci, folio, offset);\n \tput_swap_device(si);\n \treturn folio_test_swapcache(folio);\n }\n@@ -1476,6 +1555,7 @@ int swap_retry_table_alloc(swp_entry_t entry, gfp_t gfp)\n \tif (!si)\n \t\treturn 0;\n \n+\t/* Entry is in use (being faulted in), so its cluster is alive. */\n \tci = __swap_offset_to_cluster(si, offset);\n \tret = swap_extend_table_alloc(si, ci, gfp);\n \n@@ -1996,6 +2076,7 @@ bool folio_maybe_swapped(struct folio *folio)\n \tVM_WARN_ON_ONCE_FOLIO(!folio_test_locked(folio), folio);\n \tVM_WARN_ON_ONCE_FOLIO(!folio_test_swapcache(folio), folio);\n \n+\t/* Folio is locked and in swap cache, so ci->count > 0: cluster is alive. */\n \tci = __swap_entry_to_cluster(entry);\n \tci_off = swp_cluster_offset(entry);\n \tci_end = ci_off + folio_nr_pages(folio);\n@@ -2124,7 +2205,8 @@ swp_entry_t swap_alloc_hibernation_slot(int type)\n \tpcp_offset = this_cpu_read(percpu_swap_cluster.offset[0]);\n \tif (pcp_si == si && pcp_offset) {\n \t\tci = swap_cluster_lock(si, pcp_offset);\n-\t\toffset = alloc_swap_scan_cluster(si, ci, NULL, pcp_offset);\n+\t\tif (ci)\n+\t\t\toffset = alloc_swap_scan_cluster(si, ci, NULL, pcp_offset);\n \t}\n \tif (offset == SWAP_ENTRY_INVALID)\n \t\toffset = cluster_alloc_swap_entry(si, NULL);\n@@ -2413,8 +2495,10 @@ static int unuse_pte_range(struct vm_area_struct *vma, pmd_t *pmd,\n \t\t\t\t\t\t&vmf);\n \t\t}\n \t\tif (!folio) {\n+\t\t\trcu_read_lock();\n \t\t\tswp_tb = swap_table_get(__swap_entry_to_cluster(entry),\n \t\t\t\t\t\tswp_cluster_offset(entry));\n+\t\t\trcu_read_unlock();\n \t\t\tif (swp_tb_get_count(swp_tb) <= 0)\n \t\t\t\tcontinue;\n \t\t\treturn -ENOMEM;\n@@ -2560,8 +2644,10 @@ static unsigned int find_next_to_unuse(struct swap_info_struct *si,\n \t * allocations from this area (while holding swap_lock).\n \t */\n \tfor (i = prev + 1; i < si->max; i++) {\n+\t\trcu_read_lock();\n \t\tswp_tb = swap_table_get(__swap_offset_to_cluster(si, i),\n \t\t\t\t\ti % SWAPFILE_CLUSTER);\n+\t\trcu_read_unlock();\n \t\tif (!swp_tb_is_null(swp_tb) && !swp_tb_is_bad(swp_tb))\n \t\t\tbreak;\n \t\tif ((i % LATENCY_LIMIT) == 0)\n@@ -2874,6 +2960,8 @@ static void wait_for_allocation(struct swap_info_struct *si)\n \tstruct swap_cluster_info *ci;\n \n \tBUG_ON(si->flags & SWP_WRITEOK);\n+\tif (si->flags & SWP_GHOST)\n+\t\treturn;\n \n \tfor (offset = 0; offset < end; offset += SWAPFILE_CLUSTER) {\n \t\tci = swap_cluster_lock(si, offset);\n@@ -3394,10 +3482,47 @@ static int setup_swap_clusters_info(struct swap_info_struct *si,\n \t\t\t\t    unsigned long maxpages)\n {\n \tunsigned long nr_clusters = DIV_ROUND_UP(maxpages, SWAPFILE_CLUSTER);\n-\tstruct swap_cluster_info *cluster_info;\n+\tstruct swap_cluster_info *cluster_info = NULL;\n+\tstruct swap_cluster_info_dynamic *ci_dyn;\n \tint err = -ENOMEM;\n \tunsigned long i;\n \n+\t/* For SWP_GHOST files, initialize Xarray pool instead of static array */\n+\tif (si->flags & SWP_GHOST) {\n+\t\t/*\n+\t\t * Pre-allocate cluster 0 and mark slot 0 (header page)\n+\t\t * as bad so the allocator never hands out page offset 0.\n+\t\t */\n+\t\tci_dyn = kzalloc(sizeof(*ci_dyn), GFP_KERNEL);\n+\t\tif (!ci_dyn)\n+\t\t\tgoto err;\n+\t\tspin_lock_init(&ci_dyn->ci.lock);\n+\t\tINIT_LIST_HEAD(&ci_dyn->ci.list);\n+\n+\t\tnr_clusters = 0;\n+\t\txa_init_flags(&si->cluster_info_pool, XA_FLAGS_ALLOC);\n+\t\terr = xa_insert(&si->cluster_info_pool, 0, ci_dyn, GFP_KERNEL);\n+\t\tif (err) {\n+\t\t\tkfree(ci_dyn);\n+\t\t\tgoto err;\n+\t\t}\n+\n+\t\terr = swap_cluster_setup_bad_slot(si, &ci_dyn->ci, 0, false);\n+\t\tif (err) {\n+\t\t\tstruct swap_table *table;\n+\n+\t\t\txa_erase(&si->cluster_info_pool, 0);\n+\t\t\ttable = (void *)rcu_dereference_protected(ci_dyn->ci.table, true);\n+\t\t\tif (table)\n+\t\t\t\tswap_table_free(table);\n+\t\t\tkfree(ci_dyn);\n+\t\t\txa_destroy(&si->cluster_info_pool);\n+\t\t\tgoto err;\n+\t\t}\n+\n+\t\tgoto setup_cluster_info;\n+\t}\n+\n \tcluster_info = kvcalloc(nr_clusters, sizeof(*cluster_info), GFP_KERNEL);\n \tif (!cluster_info)\n \t\tgoto err;\n@@ -3538,7 +3663,7 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n \t/* /dev/ghostswap: synthesize a ghost swap device. */\n \tif (S_ISCHR(inode->i_mode) &&\n \t    imajor(inode) == MEM_MAJOR && iminor(inode) == DEVGHOST_MINOR) {\n-\t\tmaxpages = round_up(totalram_pages(), SWAPFILE_CLUSTER);\n+\t\tmaxpages = round_up(totalram_pages(), SWAPFILE_CLUSTER) * 8;\n \t\tsi->flags |= SWP_GHOST | SWP_SOLIDSTATE;\n \t\tsi->bdev = NULL;\n \t\tgoto setup;\n\n-- \n2.53.0",
          "reply_to": "",
          "message_date": "2026-02-20"
        },
        {
          "author": "Kairui Song",
          "summary": "Reviewer Kairui Song noted that the use of Xarray for dynamic swap file cluster info is a good intermediate step, but suggested using a dynamic cluster array instead to avoid extra overhead and risk of swapout allocation failure.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "NEUTRAL",
            "NO_CONCERN"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "NOTE for an RFC quality series: Swap table P4 is patch 1 - 12, and the\ndynamic ghost file is patch 13 - 15. Putting them together as RFC for\neasier review and discussions. Swap table P4 is stable and good to merge\nif we are OK with a few memcg reparent behavior (there is also a\nsolution if we don't), dynamic ghost swap is yet a minimal proof of\nconcept. See patch 15 for more details. And see below for Swap table 4\ncover letter (nice performance gain and memory save).\n\nThis is based on the latest mm-unstable, swap table P3 [1] and patches\n[2] and [3], [4]. Sending this out early, as it might be helpful for us\nto get a cleaner picture of the ongoing efforts, make the discussions easier.\n\nSummary: With this approach, we can have an infinitely or dynamically\nlarge ghost which could be identical to \"virtual swap\", and support\nevery feature we need while being *runtime configurable* with *zero\noverhead* for plain swap and keep the infrastructure unified. Also\nhighly compatible with YoungJun's swap tiering [5], and other ideas like\nswap table compaction, swapops, as it aligns with a few proposals [6]\n[7] [8] [9] [10].\n\nIn the past two years, most efforts have focused on the swap\ninfrastructure, and we have made tremendous gains in performance,\nkeeping the memory usage reasonable or lower, and also greatly cleaned\nup and simplified the API and conventions.\n\nNow the infrastructures are almost ready, after P4, implementing an\ninfinitely or dynamically large swapfile can be done in a very easy to\nmaintain and flexible way, code change is minimal and progressive\nfor review, and makes future optimization like swap table compaction\ndoable too, since the infrastructure is all the same for all swaps.\n\nThe dynamic swap file is now using Xarray for the cluster info, and\ninside the cluster, it's all the same swap allocator, swap table, and\nexisting infrastructures. A virtual table is available for any extra\ndata or usage. See below for the benefits and what we can achieve.\n\nHuge thanks to Chris Li for the layered swap table and ghost swapfile\nidea, without whom the work here can't be archived. Also, thanks to Nhat\nfor pushing and suggesting using an Xarray for the swapfile [11] for\ndynamic size. I was originally planning to use a dynamic cluster\narray, which requires a bit more adaptation, cleanup, and convention\nchanges. But during the discussion there, I got the inspiration that\nXarray can be used as the intermediate step, making this approach\ndoable with minimal changes. Just keep using it in the future, it\nmight not hurt too, as Xarray is only limited to ghost / virtual\nfiles, so plain swaps won't have any extra overhead for lookup or high\nrisk of swapout allocation failure.\n\nI'm fully open and totally fine for suggestions on naming or API\nstrategy, and others are highly welcome to keep the work going using\nthis flexible approach. Following this approach, we will have all the\nfollowing things progressively (some are already or almost there):\n\n- 8 bytes per slot memory usage, when using only plain swap.\n  - And the memory usage can be reduced to 3 or only 1 byte.\n- 16 bytes per slot memory usage, when using ghost / virtual zswap.\n  - Zswap can just use ci_dyn->virtual_table to free up it's content\n    completely.\n  - And the memory usage can be reduced to 11 or 8 bytes using the same\n    code above.\n  - 24 bytes only if including reverse mapping is in use.\n- Minimal code review or maintenance burden. All layers are using the exact\n  same infrastructure for metadata / allocation / synchronization, making\n  all API and conventions consistent and easy to maintain.\n- Writeback, migration and compaction are easily supportable since both\n  reverse mapping and reallocation are prepared. We just need a\n  folio_realloc_swap to allocate new entries for the existing entry, and\n  fill the swap table with a reserve map entry.\n- Fast swapoff: Just read into ghost / virtual swap cache.\n- Zero static data (mostly due to swap table P4), even the clusters are\n  dynamic (If using Xarray, only for ghost / virtual swap file).\n- So we can have an infinitely sized swap space with no static data\n  overhead.\n- Everything is runtime configurable, and high-performance. An\n  uncompressible workload or an offline batch workload can directly use a\n  plain or remote swap for the lowest interference, memory usage, or for\n  best performance.\n- Highly compatible with YoungJun's swap tiering, even the ghost / virtual\n  file can be just a tier. For example, if you have a huge NBD that doesn't\n  care about fragmentation and compression, or the workload is\n  uncompressible, setting the workload to use NBD's tier will give you only\n  8 bytes of overhead per slot and peak performance, bypassing everything.\n  Meanwhile, other workloads or cgroups can still use the ghost layer with\n  compression or defragmentation using 16 bytes (zswap only) or 24 bytes\n  (ghost swap with physical writeback) overhead.\n- No force or breaking change to any existing allocation, priority, swap\n  setup, or reclaim strategy. Ghost / virtual swap can be enabled or\n  disabled using swapon / swapoff.\n\nAnd if you consider these ops are too complex to set up and maintain, we\ncan then only allow one ghost / virtual file, make it infinitely large,\nand be the default one and top tier, then it achieves the identical thing\nto virtual swap space, but with much fewer LOC changed and being runtime\noptional.\n\nCurrently, the dynamic ghost files are just reported as ordinary swap files\nin /proc/swaps and we can have multiple ones, so users will have a full\nview of what's going on. This is a very easy-to-change design decision.\nI'm open to ideas about how we should present this to users. e.g., Hiding\nit will make it more \"virtual\", but I don't think that's a good idea.\n\nThe size of the swapfile (si->max) is now just a number, which could be\nchangeable at runtime if we have a proper idea how to expose that and\nmight need some audit of a few remaining users. But right now, we can\nalready easily have a huge swap device with no overhead, for example:\n\nfree -m\n               total        used        free      shared  buff/cache   available\nMem:            1465         250         927           1         356        1215\nSwap:       15269887           0    15269887\n\nAnd for easier testing, I added a /dev/ghostswap in this RFC. `swapon\n/dev/ghostswap` enables that. Without swapon /dev/ghostswap, any existing\nusers, including ZRAM, won't observe any change.\n\n===\n\nOriginal cover letter for swap table phase IV:\n\nThis series unifies the allocation and charging process of anon and shmem,\nprovides better synchronization, and consolidates cgroup tracking, hence\ndropping the cgroup array and improving the performance of mTHP by about\n~15%.\n\nStill testing with build kernel under great pressure, enabling mTHP 256kB,\non an EPYC 7K62 using 16G ZRAM, make -j48 with 1G memory limit, 12 test\nruns:\n\nBefore: 2215.55s system, 2:53.03 elapsed\nAfter:  1852.14s system, 2:41.44 elapsed (16.4% faster system time)\n\nIn some workloads, the speed gain is more than that since this reduces\nmemory thrashing, so even IO-bound work could benefit a lot, and I no\nlonger see any: \"Huh VM_FAULT_OOM leaked out to the #PF handler. Retrying\nPF\", it was shown from time to time before this series.\n\nNow, the swap cache layer ensures a folio will be the exclusive owner of\nthe swap slot, then charge it, which leads to much smaller thrashing when\nunder pressure.\n\nAnd besides, the swap cgroup static array is gone, so for example, mounting\na 1TB swap device saves about 512MB of memory:\n\nBefore:\n        total     used     free     shared  buff/cache available\nMem:    1465      854      331      1       347        610\nSwap:   1048575   0        1048575\n\nAfter:\n        total     used     free     shared  buff/cache available\nMem:    1465      332      838      1       363        1133\nSwap:   1048575   0        1048575\n\nIt saves us ~512M of memory, we now have close to 0 static overhead.\n\nLink: https://lore.kernel.org/linux-mm/20260218-swap-table-p3-v3-0-f4e34be021a7@tencent.com/ [1]\nLink: https://lore.kernel.org/linux-mm/20260213-memcg-privid-v1-1-d8cb7afcf831@tencent.com/ [2]\nLink: https://lore.kernel.org/linux-mm/20260211-shmem-swap-gfp-v1-1-e9781099a861@tencent.com/ [3]\nLink: https://lore.kernel.org/linux-mm/20260216-hibernate-perf-v4-0-1ba9f0bf1ec9@tencent.com/ [4]\nLink: https://lore.kernel.org/linux-mm/20260217000950.4015880-1-youngjun.park@lge.com/ [5]\nLink: https://lore.kernel.org/all/CAMgjq7BvQ0ZXvyLGp2YP96+i+6COCBBJCYmjXHGBnfisCAb8VA@mail.gmail.com/ [6]\nLink: https://lwn.net/Articles/974587/ [7]\nLink: https://lwn.net/Articles/932077/ [8]\nLink: https://lwn.net/Articles/1016136/ [9]\nLink: https://lore.kernel.org/linux-mm/20260208215839.87595-1-nphamcs@gmail.com/ [10]\nLink: https://lore.kernel.org/linux-mm/CAKEwX=OUni7PuUqGQUhbMDtErurFN_i=1RgzyQsNXy4LABhXoA@mail.gmail.com/ [11]\n\nSigned-off-by: Kairui Song <kasong@tencent.com>\n---\nChris Li (1):\n      mm: ghost swapfile support for zswap\n\nKairui Song (14):\n      mm: move thp_limit_gfp_mask to header\n      mm, swap: simplify swap_cache_alloc_folio\n      mm, swap: move conflict checking logic of out swap cache adding\n      mm, swap: add support for large order folios in swap cache directly\n      mm, swap: unify large folio allocation\n      memcg, swap: reparent the swap entry on swapin if swapout cgroup is dead\n      memcg, swap: defer the recording of memcg info and reparent flexibly\n      mm, swap: store and check memcg info in the swap table\n      mm, swap: support flexible batch freeing of slots in different memcg\n      mm, swap: always retrieve memcg id from swap table\n      mm/swap, memcg: remove swap cgroup array\n      mm, swap: merge zeromap into swap table\n      mm, swap: add a special device for ghost swap setup\n      mm, swap: allocate cluster dynamically for ghost swapfile\n\n MAINTAINERS                 |   1 -\n drivers/char/mem.c          |  39 ++++\n include/linux/huge_mm.h     |  24 +++\n include/linux/memcontrol.h  |  12 +-\n include/linux/swap.h        |  30 ++-\n include/linux/swap_cgroup.h |  47 -----\n mm/Makefile                 |   3 -\n mm/internal.h               |  25 ++-\n mm/memcontrol-v1.c          |  78 ++++----\n mm/memcontrol.c             | 119 ++++++++++--\n mm/memory.c                 |  89 ++-------\n mm/page_io.c                |  46 +++--\n mm/shmem.c                  | 122 +++---------\n mm/swap.h                   | 122 +++++-------\n mm/swap_cgroup.c            | 172 ----------------\n mm/swap_state.c             | 464 ++++++++++++++++++++++++--------------------\n mm/swap_table.h             | 105 ++++++++--\n mm/swapfile.c               | 278 ++++++++++++++++++++------\n mm/vmscan.c                 |   7 +-\n mm/workingset.c             |  16 +-\n mm/zswap.c                  |  29 +--\n 21 files changed, 977 insertions(+), 851 deletions(-)\n---\nbase-commit: 4750368e2cd365ac1e02c6919013c8871f35d8f9\nchange-id: 20260111-swap-table-p4-98ee92baa7c4\n\nBest regards,\n-- \nKairui Song <kasong@tencent.com>",
          "reply_to": "",
          "message_date": "2026-02-20"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-21": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Nhat Pham",
      "reviews": [
        {
          "author": "Barry Song",
          "summary": "Reviewer Barry Song noted that the dynamic ghost swapfile implementation may not be compatible with existing swap tiering proposals, specifically YoungJun's swap tiering, and requested clarification on how the two approaches align.\n\nReviewer Barry Song expressed strong disagreement with the naming choice of 'ghost' for the dynamic swapfile, finding it arbitrary and not descriptive of its functionality, and suggested replacing it with a more fitting name such as 'vswap', while also noting that Nhat is already using this name.\n\nReviewer Barry Song expressed concerns about the representation of the dynamic ghost swapfile, preferring it not to be visible as a real file in any filesystem, specifically mentioning ext4.\n\nReviewer Barry Song questioned the nature and implementation of /dev/ghostswap, specifically its size relationship to si->size and its characterization as a block or character device.\n\nReviewer Barry Song noted that the patch introduces a new swapfile type, but did not provide any details about how it interacts with existing swapfiles or how it affects swap table management.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "compatibility concern",
            "request for clarification",
            "strong disagreement",
            "naming choice",
            "concerns",
            "unnatural",
            "unclear technical question",
            "lack of interaction details",
            "concerns about swap table management"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Fri, Feb 20, 2026 at 7:42AM Kairui Song via B4 Relay\n<devnull+kasong.tencent.com@kernel.org> wrote:\n\n---\n\nTo be honest, I really dislike the name \"ghost.\" I would\nprefer something that reflects its actual functionality.\n\"Ghost\" does not describe what it does and feels rather\narbitrary.\n\nI suggest retiring the name \"ghost\" and replacing it with\nsomething more appropriate. \"vswap\" could be a good option,\nbut Nhat is already using that name.\n\n---\n\nEven if it remains visible in /proc/swaps, I would rather\nnot represent it as a real file in any filesystem. Putting\na \"ghost\" swapfile on something like ext4 seems unnatural.\n\n---\n\n/dev/ghostswap is assumed to be a virtual block device or\nsomething similar? If it is a block device, how is its size\nrelated to si->size?\n\nLooking at [PATCH RFC 14/15] mm, swap: add a special device\nfor ghost swap setup, it appears to be a character device.\nThis feels very odd to me. Im not in favor of coupling the\nghost swapfile with a memdev character device.\nA cdev should be a true character device.\n\n---\n\nThanks\nBarry",
          "reply_to": "Kairui Song",
          "message_date": "2026-02-21"
        },
        {
          "author": "Kairui Song",
          "summary": "Reviewer noted that the dynamic ghost swapfile patches (13-15) are directly from Chris Li, and suggested keeping the original naming convention as it is.\n\nReviewer Kairui Song expressed agreement with the patch, mentioning a slide from LSFMM last year that illustrates their initial vision for the swap table layout, and noted that while the actual implementation may differ slightly, plain swap still has zero overhead.\n\nReviewer Kairui Song pointed out that the dynamic ghost swapfile is visible in the output of 'swapon', and suggested renaming it to '/dev/xswap' or hiding it altogether, also mentioning that enabling/disabling it can be done using 'swapon /dev/xswap' and 'swapoff /dev/xswap'.\n\nThe reviewer clarified that the dynamic ghost swapfile is a placeholder for testing purposes, unrelated to the si->size field.\n\nReviewer noted that the patch introduces a placeholder for a virtual device, which is an alternative to Chris's dummy header approach, but expressed uncertainty about how to pass the size of this virtual device to the kernel.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "no specific technical concerns raised",
            "agreement",
            "approval",
            "requested changes",
            "clarification",
            "context",
            "uncertainty",
            "open-ended question"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Hi Barry,\n\nThat can be easily changed by \"search and replace\", I just kept the\nname since patch 13 is directly from Chris and I just didn't change\nit.\n\n---\n\nThat looks good to me too, you can also check the slide from LSFMM\nlast year page 23 to see how I imaged thing would workout at that\ntime:\nhttps://drive.google.com/file/d/1_QKlXErUkQ-TXmJJy79fJoLPui9TGK1S/view\n\nThe actual layout will be a bit different from that slide, since the\nredirect entry will be in the lower devices, the virtual device will\nhave an extra virtual table to hold its redirect entry. But still I'm\nglad that plain swap still has zero overhead so ZRAM or high\nperformance NVME is still good.\n\n---\n\nHow do you think about this? Here is the output after this sereis:\n# swapon\nNAME           TYPE       SIZE USED PRIO\n/dev/ghostswap ghost     11.5G 821M   -1\n/dev/ram0      partition 1024G 9.9M   -1\n/dev/vdb2      partition    2G 112K   -1\n\nOr we can rename it to:\n# swapon\nNAME           TYPE       SIZE USED PRIO\n/dev/xswap     xswap     11.5G 821M   -1\n/dev/ram0      partition 1024G 9.9M   -1\n/dev/vdb2      partition    2G 112K   -1\n\nswapon /dev/xswap will enable this layer (for now I just hardcoded it\nto be 8 times the size of total ram). swapoff /dev/xswap disables it.\nWe can also change the priority.\n\nWe can also hide it.\n\n---\n\nIt's not a real device, just a placeholder to make swapon usable\nwithout any modification for easier testing (some user space\nimplementation doesn't work well with dummy header). And it has\nnothing to do with the si->size.\n\n---\n\nNo coupling at all, it's just a place holder so swapon (the syscall)\nknows it's a virtual device, which is just an alternative to the dummy\nheader approach from Chris, so people can test it easier.\n\nThe si->size is just a number and any value can be given. I just\nhaven't decided how we should pass the number to the kernel or just\nmake it dynamic: e.g. set it to total ram size and increase by 2M\nevery time a new cluster is used.",
          "reply_to": "Barry Song",
          "message_date": "2026-02-21"
        },
        {
          "author": "Barry Song",
          "summary": "Reviewer Barry Song suggested replacing the dynamic ghost swapfile with a 'virtual' block device, /dev/xswap, which would display its size as 11.5G via ls -l, and considered this approach more natural than using a cdev placeholder.\n\nThe reviewer noted that the dynamic ghost swapfile appears as a character device in /dev/, which could lead to user confusion and unintended interactions with udev rules, requesting a different naming convention or approach.\n\nReviewer Barry Song noted that using a character device (cdev) as a placeholder introduces behavioral coupling between swap and non-swap use cases, where the cdev behaves differently depending on its context.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "behavioral coupling",
            "introduced"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Id rather have a virtual block device, /dev/xswap, with\nits size displayed as 11.5G via `ls -l filename`. This is\nalso more natural than relying on a cdev placeholder.\n\nIf\n\n---\n\nI understand it is a placeholder for swap, but if it appears\nas /dev/ghostfile, users browsing /dev/ will see it as a\nreal cdev. A /dev/chardev is intended for user read/write\naccess.\nAlso, udev rules can act on an exported cdev. This couples\nus with a lot of userspace behavior.\n\n---\n\nUsing a cdev as a placeholder has introduced behavioral\ncoupling. For swap, it serves as a placeholder; for anything\noutside swap, it behaves as a regular cdev.",
          "reply_to": "Kairui Song",
          "message_date": "2026-02-21"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-23": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Nhat Pham",
      "reviews": [
        {
          "author": "Johannes Weiner",
          "summary": "The reviewer found the proposed semantics of the dynamic ghost swapfile to be an improvement over the current state.",
          "sentiment": "positive",
          "sentiment_signals": [
            "NEEDS_WORK"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Those semantics look good to me. I think it's better than the status\nquo, actually.",
          "reply_to": "Kairui Song",
          "message_date": "2026-02-23"
        },
        {
          "author": "Johannes Weiner",
          "summary": "The reviewer noted that the refault evaluation needs to happen at the level that drove eviction, specifically when pages get reclaimed in a round-robin fashion, both components must compete freely and behave as if there is only one LRU, otherwise it risks retaining stale workingset in one subgroup while the other one is thrashing.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "This is not correct.\n\nAs much as I like the idea of storing the swap_cgroup association\ninside the shadow entry, the refault evaluation needs to happen at the\nlevel that drove eviction.\n\nConsider a workload that is split into cgroups purely for accounting,\nnot for setting different limits:\n\nworkload (limit domain)\n`- component A\n`- component B\n\nThis means the two components must compete freely, and it must behave\nas if there is only one LRU. When pages get reclaimed in a round-robin\nfashion, both A and B get aged at the same pace. Likewise, when pages\nin A refault, they must challenge the *combined* workingset of both A\nand B, not just the local pages.\n\nOtherwise, you risk retaining stale workingset in one subgroup while\nthe other one is thrashing. This breaks userspace expectations.",
          "reply_to": "Kairui Song",
          "message_date": "2026-02-23"
        },
        {
          "author": "Johannes Weiner",
          "summary": "Reviewer noted that the proposed design creates duplicate metadata for every page written to disk through zswap, resulting in unnecessary storage usage.\n\nReviewer Johannes Weiner requested extension of the patch to include disk swap functionality, noting that current zswap swapoff is CPU-intensive but slow due to scattered I/O.\n\nThe reviewer expressed concerns that the dynamic ghost swapfile approach makes free(1) output misleading, as it does not accurately represent actual swap capacity, and that users or distributions would have difficulty choosing a suitable size due to the arbitrary limit imposed by compression ratio.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "arbitrary restriction",
            "misleading output"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "That seems to tie us pretty permanently to duplicate metadata.\n\nFor every page that was written to disk through zswap, we have an\nentry in the ghost swapfile, and an entry in the backend swapfile, no?\n\n---\n\nCan we get this for disk swap as well? ;)\n\nZswap swapoff is already fairly fast, albeit CPU intense. It's the\nscattered IO that makes swapoff on disks so terrible.\n\n---\n\nI'm not a fan of this. This makes free(1) output kind of useless, and\nvery misleading. The swap space presented here has nothing to do with\nactual swap capacity, and the actual disk swap capacity is obscured.\n\nAnd how would a user choose this size? How would a distribution?\n\nThe only limit is compression ratio, and you don't know this in\nadvance. This restriction seems pretty arbitrary and avoidable.\n\nThere is no good technical reason to present this in any sort of\nstatic fashion.",
          "reply_to": "Kairui Song",
          "message_date": "2026-02-23"
        },
        {
          "author": "Nhat Pham",
          "summary": "Reviewer Nhat Pham noted that the dynamic ghost swapfile uses Xarray for cluster info, but expressed concern about potential overhead and risk of swapout allocation failure for plain swaps.\n\nReviewer Nhat Pham expressed concerns about the placement of metadata (swap count, swap cgroup) in the dynamic ghost swapfile design, specifically noting that storing it at the bottom layer has issues such as not knowing the suitable backend at swap allocation time and limited space for metadata. He suggested storing metadata in the top layer instead.\n\nReviewer Nhat Pham pointed out that the patch series lacks key features for a virtual swap setup, including charging, backend decision making, efficient transfer, and swapoff logic, which would make the implementation more complex than claimed.\n\nReviewer Nhat Pham expressed concern that exposing virtual swap state to users in the swapfile summary view is confusing and poorly reflects physical state, suggesting that only sysfs debug counters should be exposed for troubleshooting purposes.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "concern",
            "potential issue",
            "requested changes",
            "misleading claim",
            "additional complexity"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Thu, Feb 19, 2026 at 3:42PM Kairui Song via B4 Relay\n<devnull+kasong.tencent.com@kernel.org> wrote:\n\n---\n\nThanks for your effort. Dynamic swap space is a very important\nconsideration anyone deploying compressed swapping backend on large\nmemory systems in general. And yeah, I think using a radix tree/xarray\nis easiest out-of-the-box solution for this - thanks for citing me :P\n\nI still have some confusion and concerns though. Johannes already made\nsome good points - I'll just add some thoughts from my point of view,\nhaving gone back and forth with virtual swap designs:\n\n1. At which layer should the metadata (swap count, swap cgroup, etc.) live?\n\nI remember that in your LSFMMBPF presentation (time flies), your\nproposal was to store a redirection entry in the top layer, and keep\nall the metadata at the bottom (i.e backend) layer? This has problems\n- for once, you might not know suitable backend at swap allocation\ntime, but only at writeout time. For e.g, in certain zswap setups, we\nreject the incompressible page and cycle it back to the active LRU, so\nwe have no space in zswap layer to store swap entry metadata (note\nthat at this point the swap entry cannot be freed, because we have\nalready unmapped the page from the PTEs (and would require a page\ntable walk to undo this a la swapoff). Similarly, when we\nexclusive-load a page from zswap, we invalidate the zswap metadata\nstruct, so we will no longer have a space for the swap entry metadata.\n\nThe zero-filled (or same-filled) swap entry case is an even more\negregious example :) It really shouldn't be a state in any backend -\nit should be a completely independent backend.\n\nThe only design that makes sense is to store metadata in the top layer\nas well. It's what I'm doing for my virtual swap patch series, but if\nwe're pursuing this opt-in swapfile direction we are going to\nduplicate metadata :)\n\n---\n\n2. I think the \"fewer LOC changed\" claim here is misleading ;)\n\nA lot of the behaviors that is required in a virtual swap setup is\nmissing from this patch series. You are essentially just implementing\na swapfile with a dynamic allocator. You still need a bunch more logic\nto support a proper multi-tier virtual swap setup - just on top of my\nmind:\n\na. Charging: virtual swap usage not be charged the same as the\nphysical swap usage, especially when you have a zswap + disk swap\nsetup, powered by virtual swap. For once, I don't believe in sizing\nvirtual swap, but also a latency-sensitive cgroup allowe to use only\nzswap (backed by virtual swap) is using and competing for resources\nvery differently from a cgroup whose memory is incompressible and only\nallowed to use disk swap.\n\nb. Backend decision making and efficient backend transfer - as you\nsaid, \"folio_realloc_swap\" is yet to be implemented :) And as I\nmention earlier, we CANNOT determine swap backend before PTE unmap\ntime, because backend suitability is content-dependent. You will have\nto add extra logic to handle this nuanced swap allocation behavior.\n\nc. Virtual swap freeing - it requires more work, as you have to free\nboth the virtual swap entry itself, as well as digging into the\nphysical backend layer.\n\nd. Swapoff - now you have to both page tables and virtual swap table.\n\nBy the time you implement all of this, I think it will be MORE\ncomplex, especially since you want to maintain BOTH the new setup and\nthe old non-virtual swap setup. You'll have to litter the codes with a\nbunch of ifs (or ifdefs) to check - hey do we have a virtual swapfile?\nHey is this a virtual swap slot? Etc. Etc. everywhere, from the PTE\ninfra (zapping, page fault, etc.), to cgroup infra, to physical swap\narchitecture.\n\nComparing this line of work by itself with the vswap series, which\nalready comes with all of these included, is a bit apples-to-oranges\n(and especially with the fact that vswap simplifies logic and removes\nLoCs in a lot of places too, such as in swapoff. The delta LoC is only\n300-400 IIRC?).\n\n---\n\n3. I don't think we should expose virtual swap state to users (in this\ncase, in the swapfile summary view i.e in free). It is just confusing,\nas it poorly reflects the physical state (be it compressed memory\nfootprint, or actual disk usage). We obviously should expose a bunch\nof sysfs debug counters for troubleshootings, but for average users,\nit should be all transparent.",
          "reply_to": "Kairui Song",
          "message_date": "2026-02-23"
        },
        {
          "author": "Shakeel Butt",
          "summary": "Reviewer Shakeel Butt requested that the commit message be made self-contained, specifically pointing out that the current opening statement is confusing.\n\nReviewer Shakeel Butt questioned the reason behind the difference in cgroup between folio->swap and folio->memcg, implying that this discrepancy may be problematic.\n\nReviewer Shakeel Butt questioned whether the behavior of dynamic ghost swapfile is consistent across all types of memory backed by shmem, including MAP_SHARED and memfd, as well as cow anon memory shared between parent and child processes.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "requested clarification",
            "implied potential issue",
            "requested clarification on consistency"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "I directly jump to this patch and the opening statement is confusing. Please\nmake the commit message self contained.\n\n---\n\nWhy is this an issue (i.e. folio->swap's cgroup different from\nfolio->memcg)?\n\n---\n\nIs this behavior same for all types of memory backed by shmem (i.e. MAP_SHARED,\nmemfd etc)? What about cow anon memory shared between parent and child\nprocesses?",
          "reply_to": "Kairui Song",
          "message_date": "2026-02-23"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-24": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Nhat Pham",
      "reviews": [
        {
          "author": "Kairui Song",
          "summary": "It's already doing that, storing metadata at the top layer, only a reverse mapping in the lower layer. So none of these issues are still there. Don't worry, I do remember that conversation and kept that in mind :) I left that part undone kind of on purpose, since it's only RFC, and in hope that there could be collaboration. And the dynamic allocator is only ~200 LOC now. Other parts of this series are not only for virtual swap. For example the unified folio alloc for swapin, which gives us 15% performance gain in real workloads, can still get merged and benifit all of us without involving the virtual swap or memcg part. And meanwhile, with the later patches, we don't have to re-implement the whole infrastructure to have a virtual table.",
          "sentiment": "positive",
          "sentiment_signals": [
            "merged"
          ],
          "has_inline_review": true,
          "tags_given": [],
          "analysis_source": "heuristic",
          "raw_body": "On Tue, Feb 24, 2026 at 2:22AM Nhat Pham <nphamcs@gmail.com> wrote:\n>\n> On Thu, Feb 19, 2026 at 3:42PM Kairui Song via B4 Relay\n> <devnull+kasong.tencent.com@kernel.org> wrote:\n> > Huge thanks to Chris Li for the layered swap table and ghost swapfile\n> > idea, without whom the work here can't be archived. Also, thanks to Nhat\n> > for pushing and suggesting using an Xarray for the swapfile [11] for\n> > dynamic size. I was originally planning to use a dynamic cluster\n> > array, which requires a bit more adaptation, cleanup, and convention\n> > changes. But during the discussion there, I got the inspiration that\n> > Xarray can be used as the intermediate step, making this approach\n> > doable with minimal changes. Just keep using it in the future, it\n> > might not hurt too, as Xarray is only limited to ghost / virtual\n> > files, so plain swaps won't have any extra overhead for lookup or high\n> > risk of swapout allocation failure.\n>\n> Thanks for your effort. Dynamic swap space is a very important\n> consideration anyone deploying compressed swapping backend on large\n> memory systems in general. And yeah, I think using a radix tree/xarray\n> is easiest out-of-the-box solution for this - thanks for citing me :P\n\nThanks for the discussion :)\n\n>\n> I still have some confusion and concerns though. Johannes already made\n> some good points - I'll just add some thoughts from my point of view,\n> having gone back and forth with virtual swap designs:\n>\n> 1. At which layer should the metadata (swap count, swap cgroup, etc.) live?\n>\n> I remember that in your LSFMMBPF presentation (time flies), your\n> proposal was to store a redirection entry in the top layer, and keep\n> all the metadata at the bottom (i.e backend) layer? This has problems\n> - for once, you might not know suitable backend at swap allocation\n> time, but only at writeout time. For e.g, in certain zswap setups, we\n> reject the incompressible page and cycle it back to the active LRU, so\n> we have no space in zswap layer to store swap entry metadata (note\n> that at this point the swap entry cannot be freed, because we have\n> already unmapped the page from the PTEs (and would require a page\n> table walk to undo this a la swapoff). Similarly, when we\n> exclusive-load a page from zswap, we invalidate the zswap metadata\n> struct, so we will no longer have a space for the swap entry metadata.\n>\n> The zero-filled (or same-filled) swap entry case is an even more\n> egregious example :) It really shouldn't be a state in any backend -\n> it should be a completely independent backend.\n>\n> The only design that makes sense is to store metadata in the top layer\n> as well. It's what I'm doing for my virtual swap patch series, but if\n> we're pursuing this opt-in swapfile direction we are going to\n> duplicate metadata :)\n\nIt's already doing that, storing metadata at the top layer, only a\nreverse mapping in the lower layer.\n\nSo none of these issues are still there. Don't worry, I do remember\nthat conversation and kept that in mind :)\n\n> > And if you consider these ops are too complex to set up and maintain, we\n> > can then only allow one ghost / virtual file, make it infinitely large,\n> > and be the default one and top tier, then it achieves the identical thing\n> > to virtual swap space, but with much fewer LOC changed and being runtime\n> > optional.\n>\n> 2. I think the \"fewer LOC changed\" claim here is misleading ;)\n>\n> A lot of the behaviors that is required in a virtual swap setup is\n> missing from this patch series. You are essentially just implementing\n> a swapfile with a dynamic allocator. You still need a bunch more logic\n> to support a proper multi-tier virtual swap setup - just on top of my\n> mind:\n\nI left that part undone kind of on purpose, since it's only RFC, and\nin hope that there could be collaboration.\n\nAnd the dynamic allocator is only ~200 LOC now. Other parts of this\nseries are not only for virtual swap. For example the unified folio\nalloc for swapin, which gives us 15% performance gain in real\nworkloads, can still get merged and benifit all of us without\ninvolving the virtual swap or memcg part.\n\nAnd meanwhile, with the later patches, we don't have to re-implement\nthe whole infrastructure to have a virtual table. And future plans\nlike data compaction should benifit every layer naturally (same\ninfra).\n\n> a. Charging: virtual swap usage not be charged the same as the\n> physical swap usage, especially when you have a zswap + disk swap\n> setup, powered by virtual swap. For once, I don't believe in sizing\n> virtual swap, but also a latency-sensitive cgroup allowe to use only\n> zswap (backed by virtual swap) is using and competing for resources\n> very differently from a cgroup whose memory is incompressible and only\n> allowed to use disk swap.\n\nAh, now as you mention it, I see in the beginning of this series I\nadded: \"Swap table P4 is stable and good to merge if we are OK with a\nfew memcg reparent behavior (there is also a solution if we don't)\".\nThe \"other solution\" also fits your different charge idea here. Just\nhave a ci->memcg_table, then each layer can have their own charge\ndesign, and the shadow is still only used for refault check. That\ngives us 10 bytes per slot overhead though, but still lower than\nbefore and stays completely dynamic.\n\nAlso, no duplicated memcg, since the upper layer and lower layer\nshould be charged differently. If they don't, then just let\nci->memcg_table stay NULL.\n\n>\n> b. Backend decision making and efficient backend transfer - as you\n> said, \"folio_realloc_swap\" is yet to be implemented :) And as I\n> mention earlier, we CANNOT determine swap backend before PTE unmap\n\nAnd we are not doing that at all. folio_alloc_swap happens before\nunmap, but realloc happens after that. VSS does the same thing.\n\n> time, because backend suitability is content-dependent. You will have\n> to add extra logic to handle this nuanced swap allocation behavior.\n>\n> c. Virtual swap freeing - it requires more work, as you have to free\n> both the virtual swap entry itself, as well as digging into the\n> physical backend layer.\n>\n> d. Swapoff - now you have to both page tables and virtual swap table.\n\nSwapoff is actually easy here... If it sees a reverse map slot, read\ninto the upper layer. Else goto the old logic. Then it's done. If\nghost swap is the layer with highest priority, then every slot is a\nreverse map slot.\n\n>\n> By the time you implement all of this, I think it will be MORE\n> complex, especially since you want to maintain BOTH the new setup and\n> the old non-virtual swap setup. You'll have to litter the codes with a\n> bunch of ifs (or ifdefs) to check - hey do we have a virtual swapfile?\n> Hey is this a virtual swap slot? Etc. Etc. everywhere, from the PTE\n> infra (zapping, page fault, etc.), to cgroup infra, to physical swap\n> architecture.\n\nIt is using the same infrastructure, which means a lot of things are\nreused and unified. Isn't that a good sign? And again we don't need to\nre-implement the whole infra.\n\nAnd if you need multiple layers then there will be more \"if\"s and\noverhead however you implement it. But with unified infra, each layer\ncan stay optional. And checking \"si->flags & GHOST / VIRTUAL\" really\nshouldn't be costly or trouble some at all, compared to a mandatory\nlayer with layers of Xarray walk.\n\nAnd we can move, maintain the virt part in a separate place.\n\n> Comparing this line of work by itself with the vswap series, which\n> already comes with all of these included, is a bit apples-to-oranges\n> (and especially with the fact that vswap simplifies logic and removes\n> LoCs in a lot of places too, such as in swapoff. The delta LoC is only\n> 300-400 IIRC?).\n\nOne thing I want to highlight here is that the old swapoff really\nshouldn't just die. That gives us no chance to clear up the swap cache\nat all (vss holding swap data in RAM is also just swap cache). Pages\nstill in swap cache means minor page faults will still trigger. If the\nworkload is opaque but we knows a high load of traffic is coming and\nwants to get rid of any performance bottleneck by reading all folios\ninto the right place, swapoff gives the guarantee that no anon fault\nwill be ever triggered, that happens a lot in multiple tenant cloud\nenvironments, and these workload are opaque so madvise doesn't apply.\n\n> > The size of the swapfile (si->max) is now just a number, which could be\n> > changeable at runtime if we have a proper idea how to expose that and\n> > might need some audit of a few remaining users. But right now, we can\n> > already easily have a huge swap device with no overhead, for example:\n> >\n> > free -m\n> >                total        used        free      shared  buff/cache   available\n> > Mem:            1465         250         927           1         356        1215\n> > Swap:       15269887           0    15269887\n> >\n>\n> 3. I don't think we should expose virtual swap state to users (in this\n> case, in the swapfile summary view i.e in free). It is just confusing,\n> as it poorly reflects the physical state (be it compressed memory\n> footprint, or actual disk usage). We obviously should expose a bunch\n> of sysfs debug counters for troubleshootings, but for average users,\n> it should be all transparent.\n\nUsing sysfs can also be a choice, that's really just a demonstration\ninterface. But I do think it's worse if the user has no idea what is\nactually going on.\n",
          "reply_to": "",
          "message_date": "2026-02-24",
          "message_id": ""
        },
        {
          "author": "Nhat Pham (author)",
          "summary": "The point is not that it's hard to do. That's the whole sale pitch of vswap - once you have it all the use case is neatly facilitated ;) I'm just pointing out that \"minimal LoC\" is not exactly fair here, as we still have (in my estimate) quite a sizable amount of work. I somewhat agree with Johannes that the problem is quite academic in nature here, but I will think more about it. I think the users should know that virtual swap is enabled or not, and some diagnostics stats - allocated, used, rejected/failure etc. But from users perspective, the other traditional swapfile states don't seem that useful, and might give users misconceptions. When you see swapfile stats, you know that you are occupying a limited physical resource, and how much of it is left.",
          "sentiment": "positive",
          "sentiment_signals": [
            "merged"
          ],
          "has_inline_review": true,
          "tags_given": [],
          "analysis_source": "heuristic",
          "raw_body": "On Mon, Feb 23, 2026 at 7:35PM Kairui Song <ryncsn@gmail.com> wrote:\n>\n> On Tue, Feb 24, 2026 at 2:22AM Nhat Pham <nphamcs@gmail.com> wrote:\n> >\n> > On Thu, Feb 19, 2026 at 3:42PM Kairui Song via B4 Relay\n> > <devnull+kasong.tencent.com@kernel.org> wrote:\n> > > Huge thanks to Chris Li for the layered swap table and ghost swapfile\n> > > idea, without whom the work here can't be archived. Also, thanks to Nhat\n> > > for pushing and suggesting using an Xarray for the swapfile [11] for\n> > > dynamic size. I was originally planning to use a dynamic cluster\n> > > array, which requires a bit more adaptation, cleanup, and convention\n> > > changes. But during the discussion there, I got the inspiration that\n> > > Xarray can be used as the intermediate step, making this approach\n> > > doable with minimal changes. Just keep using it in the future, it\n> > > might not hurt too, as Xarray is only limited to ghost / virtual\n> > > files, so plain swaps won't have any extra overhead for lookup or high\n> > > risk of swapout allocation failure.\n> >\n> > Thanks for your effort. Dynamic swap space is a very important\n> > consideration anyone deploying compressed swapping backend on large\n> > memory systems in general. And yeah, I think using a radix tree/xarray\n> > is easiest out-of-the-box solution for this - thanks for citing me :P\n>\n> Thanks for the discussion :)\n>\n> >\n> > I still have some confusion and concerns though. Johannes already made\n> > some good points - I'll just add some thoughts from my point of view,\n> > having gone back and forth with virtual swap designs:\n> >\n> > 1. At which layer should the metadata (swap count, swap cgroup, etc.) live?\n> >\n> > I remember that in your LSFMMBPF presentation (time flies), your\n> > proposal was to store a redirection entry in the top layer, and keep\n> > all the metadata at the bottom (i.e backend) layer? This has problems\n> > - for once, you might not know suitable backend at swap allocation\n> > time, but only at writeout time. For e.g, in certain zswap setups, we\n> > reject the incompressible page and cycle it back to the active LRU, so\n> > we have no space in zswap layer to store swap entry metadata (note\n> > that at this point the swap entry cannot be freed, because we have\n> > already unmapped the page from the PTEs (and would require a page\n> > table walk to undo this a la swapoff). Similarly, when we\n> > exclusive-load a page from zswap, we invalidate the zswap metadata\n> > struct, so we will no longer have a space for the swap entry metadata.\n> >\n> > The zero-filled (or same-filled) swap entry case is an even more\n> > egregious example :) It really shouldn't be a state in any backend -\n> > it should be a completely independent backend.\n> >\n> > The only design that makes sense is to store metadata in the top layer\n> > as well. It's what I'm doing for my virtual swap patch series, but if\n> > we're pursuing this opt-in swapfile direction we are going to\n> > duplicate metadata :)\n>\n> It's already doing that, storing metadata at the top layer, only a\n> reverse mapping in the lower layer.\n>\n> So none of these issues are still there. Don't worry, I do remember\n> that conversation and kept that in mind :)\n>\n> > > And if you consider these ops are too complex to set up and maintain, we\n> > > can then only allow one ghost / virtual file, make it infinitely large,\n> > > and be the default one and top tier, then it achieves the identical thing\n> > > to virtual swap space, but with much fewer LOC changed and being runtime\n> > > optional.\n> >\n> > 2. I think the \"fewer LOC changed\" claim here is misleading ;)\n> >\n> > A lot of the behaviors that is required in a virtual swap setup is\n> > missing from this patch series. You are essentially just implementing\n> > a swapfile with a dynamic allocator. You still need a bunch more logic\n> > to support a proper multi-tier virtual swap setup - just on top of my\n> > mind:\n>\n> I left that part undone kind of on purpose, since it's only RFC, and\n> in hope that there could be collaboration.\n>\n> And the dynamic allocator is only ~200 LOC now. Other parts of this\n> series are not only for virtual swap. For example the unified folio\n> alloc for swapin, which gives us 15% performance gain in real\n> workloads, can still get merged and benifit all of us without\n> involving the virtual swap or memcg part.\n>\n> And meanwhile, with the later patches, we don't have to re-implement\n> the whole infrastructure to have a virtual table. And future plans\n> like data compaction should benifit every layer naturally (same\n> infra).\n>\n> > a. Charging: virtual swap usage not be charged the same as the\n> > physical swap usage, especially when you have a zswap + disk swap\n> > setup, powered by virtual swap. For once, I don't believe in sizing\n> > virtual swap, but also a latency-sensitive cgroup allowe to use only\n> > zswap (backed by virtual swap) is using and competing for resources\n> > very differently from a cgroup whose memory is incompressible and only\n> > allowed to use disk swap.\n>\n> Ah, now as you mention it, I see in the beginning of this series I\n> added: \"Swap table P4 is stable and good to merge if we are OK with a\n> few memcg reparent behavior (there is also a solution if we don't)\".\n> The \"other solution\" also fits your different charge idea here. Just\n> have a ci->memcg_table, then each layer can have their own charge\n> design, and the shadow is still only used for refault check. That\n> gives us 10 bytes per slot overhead though, but still lower than\n> before and stays completely dynamic.\n>\n> Also, no duplicated memcg, since the upper layer and lower layer\n> should be charged differently. If they don't, then just let\n> ci->memcg_table stay NULL.\n>\n> >\n> > b. Backend decision making and efficient backend transfer - as you\n> > said, \"folio_realloc_swap\" is yet to be implemented :) And as I\n> > mention earlier, we CANNOT determine swap backend before PTE unmap\n>\n> And we are not doing that at all. folio_alloc_swap happens before\n> unmap, but realloc happens after that. VSS does the same thing.\n>\n> > time, because backend suitability is content-dependent. You will have\n> > to add extra logic to handle this nuanced swap allocation behavior.\n> >\n> > c. Virtual swap freeing - it requires more work, as you have to free\n> > both the virtual swap entry itself, as well as digging into the\n> > physical backend layer.\n> >\n> > d. Swapoff - now you have to both page tables and virtual swap table.\n>\n> Swapoff is actually easy here... If it sees a reverse map slot, read\n> into the upper layer. Else goto the old logic. Then it's done. If\n> ghost swap is the layer with highest priority, then every slot is a\n> reverse map slot.\n>\n> >\n> > By the time you implement all of this, I think it will be MORE\n> > complex, especially since you want to maintain BOTH the new setup and\n> > the old non-virtual swap setup. You'll have to litter the codes with a\n> > bunch of ifs (or ifdefs) to check - hey do we have a virtual swapfile?\n> > Hey is this a virtual swap slot? Etc. Etc. everywhere, from the PTE\n> > infra (zapping, page fault, etc.), to cgroup infra, to physical swap\n> > architecture.\n>\n> It is using the same infrastructure, which means a lot of things are\n> reused and unified. Isn't that a good sign? And again we don't need to\n> re-implement the whole infra.\n>\n> And if you need multiple layers then there will be more \"if\"s and\n> overhead however you implement it. But with unified infra, each layer\n> can stay optional. And checking \"si->flags & GHOST / VIRTUAL\" really\n> shouldn't be costly or trouble some at all, compared to a mandatory\n> layer with layers of Xarray walk.\n>\n> And we can move, maintain the virt part in a separate place.\n\nThe point is not that it's hard to do. That's the whole sale pitch of\nvswap - once you have it all the use case is neatly facilitated ;)\n\nI'm just pointing out that \"minimal LoC\" is not exactly fair here, as\nwe still have (in my estimate) quite a sizable amount of work.\n\n>\n> > Comparing this line of work by itself with the vswap series, which\n> > already comes with all of these included, is a bit apples-to-oranges\n> > (and especially with the fact that vswap simplifies logic and removes\n> > LoCs in a lot of places too, such as in swapoff. The delta LoC is only\n> > 300-400 IIRC?).\n>\n> One thing I want to highlight here is that the old swapoff really\n> shouldn't just die. That gives us no chance to clear up the swap cache\n> at all (vss holding swap data in RAM is also just swap cache). Pages\n> still in swap cache means minor page faults will still trigger. If the\n> workload is opaque but we knows a high load of traffic is coming and\n> wants to get rid of any performance bottleneck by reading all folios\n> into the right place, swapoff gives the guarantee that no anon fault\n> will be ever triggered, that happens a lot in multiple tenant cloud\n> environments, and these workload are opaque so madvise doesn't apply.\n\nI somewhat agree with Johannes that the problem is quite academic in\nnature here, but I will think more about it.\n\n>\n> > > The size of the swapfile (si->max) is now just a number, which could be\n> > > changeable at runtime if we have a proper idea how to expose that and\n> > > might need some audit of a few remaining users. But right now, we can\n> > > already easily have a huge swap device with no overhead, for example:\n> > >\n> > > free -m\n> > >                total        used        free      shared  buff/cache   available\n> > > Mem:            1465         250         927           1         356        1215\n> > > Swap:       15269887           0    15269887\n> > >\n> >\n> > 3. I don't think we should expose virtual swap state to users (in this\n> > case, in the swapfile summary view i.e in free). It is just confusing,\n> > as it poorly reflects the physical state (be it compressed memory\n> > footprint, or actual disk usage). We obviously should expose a bunch\n> > of sysfs debug counters for troubleshootings, but for average users,\n> > it should be all transparent.\n>\n> Using sysfs can also be a choice, that's really just a demonstration\n> interface. But I do think it's worse if the user has no idea what is\n> actually going on.\n\nI think the users should know that virtual swap is enabled or not, and\nsome diagnostics stats - allocated, used, rejected/failure etc.\n\nBut from users perspective, the other traditional swapfile states\ndon't seem that useful, and might give users misconceptions. When you\nsee swapfile stats, you know that you are occupying a limited physical\nresource, and how much of it is left. I don't think there's even a\ngood reason to statically size virtual swap space - it's just a\nfacility to enable use cases, not an actual resource in the same way\nas memory, or disk drive, and is dynamic (on-demand) in nature.\n",
          "reply_to": "",
          "message_date": "2026-02-24",
          "message_id": ""
        }
      ],
      "analysis_source": "heuristic"
    }
  }
}