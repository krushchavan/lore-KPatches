{
  "thread_id": "aZhErt9DZcWI24_v@thinkstation",
  "subject": "[LSF/MM/BPF TOPIC] 64k (or 16k) base page size on x86",
  "url": "https://lore.kernel.org/all/aZhErt9DZcWI24_v@thinkstation/",
  "dates": {
    "2026-02-20": {
      "report_file": "2026-02-20.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "David (Arm)",
          "summary": "I'd assume that many applications nowadays can deal with differing page sizes (thanks to some other architectures paving the way). But yes, some real legacy stuff, or stuff that ever only cared about intel still hardcodes pagesize=4k. In Meta's fleet, I'd be quite interesting how much conversion there would have to be done. For legacy apps, you could still run them as 4k pagesize on the same system, of course. I still have to wrap my head around the sub-page mapping here as well. It's scary. Re mapcount: I think if any part of the page is mapped, it would be considered mapped -> mapcount += 1. I'd assume that would work. Devil is in the detail with these things before we have memdescs. E.g., page table have a dedicated type (PGTY_table) and store separate metadata in the ptdesc.",
          "sentiment": "neutral",
          "sentiment_signals": [],
          "has_inline_review": true,
          "tags_given": [],
          "analysis_source": "heuristic",
          "raw_body": "On 2/20/26 13:07, Kiryl Shutsemau wrote:\n> On Fri, Feb 20, 2026 at 11:24:37AM +0100, David Hildenbrand (Arm) wrote:\n>>>\n>>> Just to clarify, do you want it to be enforced on userspace ABI.\n>>> Like, all mappings are 64k aligned?\n>>\n>> Right, see the proposal from Dev on the list.\n>>\n>>  From user-space POV, the pagesize would be 64K for these emulated processes.\n>> That is, VMAs must be suitable aligned etc.\n> \n> Well, it will drastically limit the adoption. We have too much legacy\n> stuff on x86.\n\nI'd assume that many applications nowadays can deal with differing page \nsizes (thanks to some other architectures paving the way).\n\nBut yes, some real legacy stuff, or stuff that ever only cared about \nintel still hardcodes pagesize=4k.\n\nIn Meta's fleet, I'd be quite interesting how much conversion there \nwould have to be done.\n\nFor legacy apps, you could still run them as 4k pagesize on the same \nsystem, of course.\n\n> \n>>>\n>>> Waste of memory for page table is solvable and pretty straight forward.\n>>> Most of such cases can be solve mechanically by switching to slab.\n>>\n>> Well, yes, like Willy says, there are already similar custom solutions for\n>> s390x and ppc.\n>>\n>> Pasha talked recently about the memory waste of 16k kernel stacks and how we\n>> would want to reduce that to 4k. In your proposal, it would be 64k, unless\n>> you somehow manage to allocate multiple kernel stacks from the same 64k\n>> page. My head hurts thinking about whether that could work, maybe it could\n>> (no idea about guard pages in there, though).\n> \n> Kernel stack is allocated from vmalloc. I think mapping them with\n> sub-page granularity should be doable.\n\nI still have to wrap my head around the sub-page mapping here as well. \nIt's scary.\n\nRe mapcount: I think if any part of the page is mapped, it would be \nconsidered mapped -> mapcount += 1.\n\n> \n> BTW, do you see any reason why slab-allocated stack wouldn't work for\n> large base page sizes? There's no requirement for it be aligned to page\n> or PTE, right?\n\nI'd assume that would work. Devil is in the detail with these things \nbefore we have memdescs.\n\nE.g., page table have a dedicated type (PGTY_table) and store separate \nmetadata in the ptdesc. For kernel stack there was once a proposal to \nhave a type but it is not upstream.\n\n> \n>> Let's take a look at the history of page size usage on Arm (people can feel\n>> free to correct me):\n>>\n>> (1) Most distros were using 64k on Arm.\n>>\n>> (2) People realized that 64k was suboptimal many use cases (memory\n>>      waste for stacks, pagecache, etc) and started to switch to 4k. I\n>>      remember that mostly HPC-centric users sticked to 64k, but there was\n>>      also demand from others to be able to stay on 64k.\n>>\n>> (3) Arm improved performance on a 4k kernel by adding cont-pte support,\n>>      trying to get closer to 64k native performance.\n>>\n>> (4) Achieving 64k native performance is hard, which is why per-process\n>>      page sizes are being explored to get the best out of both worlds\n>>      (use 64k page size only where it really matters for performance).\n>>\n>> Arm clearly has the added benefit of actually benefiting from hardware\n>> support for 64k.\n>>\n>> IIUC, what you are proposing feels a bit like traveling back in time when it\n>> comes to the memory waste problem that Arm users encountered.\n>>\n>> Where do you see the big difference to 64k on Arm in your proposal? Would\n>> you currently also be running 64k Arm in production and the memory waste etc\n>> is acceptable?\n> \n> That's the point. I don't see a big difference to 64k Arm. I want to\n> bring this option to x86: at some machine size it makes sense trade\n> memory consumption for scalability. I am targeting it to machines with\n> over 2TiB of RAM.\n> \n> BTW, we do run 64k Arm in our fleet. There's some growing pains, but it\n> looks good in general We have no plans to switch to 4k (or 16k) at the\n> moment. 512M THPs also look good on some workloads.\n\nOkay, that's valuable information, thanks!\n\nBeing able to remove the sub-page mapping part (or being able to just \nhide it somewhere deep down in arch code) would make this a lot easier \nto digest.\n\n-- \nCheers,\n\nDavid\n",
          "reply_to": "",
          "message_date": "2026-02-20",
          "message_id": ""
        },
        {
          "author": "Kalesh Singh",
          "summary": "On Fri, Feb 20, 2026 at 8:30 AM David Hildenbrand (Arm) I think most issues will stem from linkers setting the default ELF segment alignment (max-page-size) for x86 to 4096. So those ELFs will not load correctly or at all on the larger emulated granularity.",
          "sentiment": "neutral",
          "sentiment_signals": [],
          "has_inline_review": true,
          "tags_given": [],
          "analysis_source": "heuristic",
          "raw_body": "On Fri, Feb 20, 2026 at 8:30 AM David Hildenbrand (Arm)\n<david@kernel.org> wrote:\n>\n> On 2/20/26 13:07, Kiryl Shutsemau wrote:\n> > On Fri, Feb 20, 2026 at 11:24:37AM +0100, David Hildenbrand (Arm) wrote:\n> >>>\n> >>> Just to clarify, do you want it to be enforced on userspace ABI.\n> >>> Like, all mappings are 64k aligned?\n> >>\n> >> Right, see the proposal from Dev on the list.\n> >>\n> >>  From user-space POV, the pagesize would be 64K for these emulated processes.\n> >> That is, VMAs must be suitable aligned etc.\n> >\n> > Well, it will drastically limit the adoption. We have too much legacy\n> > stuff on x86.\n>\n> I'd assume that many applications nowadays can deal with differing page\n> sizes (thanks to some other architectures paving the way).\n>\n> But yes, some real legacy stuff, or stuff that ever only cared about\n> intel still hardcodes pagesize=4k.\n\nI think most issues will stem from linkers setting the default ELF\nsegment alignment (max-page-size) for x86 to 4096. So those ELFs will\nnot load correctly or at all on the larger emulated granularity.\n\n-- Kalesh\n\n>\n> In Meta's fleet, I'd be quite interesting how much conversion there\n> would have to be done.\n>\n> For legacy apps, you could still run them as 4k pagesize on the same\n> system, of course.\n>\n> >\n> >>>\n> >>> Waste of memory for page table is solvable and pretty straight forward.\n> >>> Most of such cases can be solve mechanically by switching to slab.\n> >>\n> >> Well, yes, like Willy says, there are already similar custom solutions for\n> >> s390x and ppc.\n> >>\n> >> Pasha talked recently about the memory waste of 16k kernel stacks and how we\n> >> would want to reduce that to 4k. In your proposal, it would be 64k, unless\n> >> you somehow manage to allocate multiple kernel stacks from the same 64k\n> >> page. My head hurts thinking about whether that could work, maybe it could\n> >> (no idea about guard pages in there, though).\n> >\n> > Kernel stack is allocated from vmalloc. I think mapping them with\n> > sub-page granularity should be doable.\n>\n> I still have to wrap my head around the sub-page mapping here as well.\n> It's scary.\n>\n> Re mapcount: I think if any part of the page is mapped, it would be\n> considered mapped -> mapcount += 1.\n>\n> >\n> > BTW, do you see any reason why slab-allocated stack wouldn't work for\n> > large base page sizes? There's no requirement for it be aligned to page\n> > or PTE, right?\n>\n> I'd assume that would work. Devil is in the detail with these things\n> before we have memdescs.\n>\n> E.g., page table have a dedicated type (PGTY_table) and store separate\n> metadata in the ptdesc. For kernel stack there was once a proposal to\n> have a type but it is not upstream.\n>\n> >\n> >> Let's take a look at the history of page size usage on Arm (people can feel\n> >> free to correct me):\n> >>\n> >> (1) Most distros were using 64k on Arm.\n> >>\n> >> (2) People realized that 64k was suboptimal many use cases (memory\n> >>      waste for stacks, pagecache, etc) and started to switch to 4k. I\n> >>      remember that mostly HPC-centric users sticked to 64k, but there was\n> >>      also demand from others to be able to stay on 64k.\n> >>\n> >> (3) Arm improved performance on a 4k kernel by adding cont-pte support,\n> >>      trying to get closer to 64k native performance.\n> >>\n> >> (4) Achieving 64k native performance is hard, which is why per-process\n> >>      page sizes are being explored to get the best out of both worlds\n> >>      (use 64k page size only where it really matters for performance).\n> >>\n> >> Arm clearly has the added benefit of actually benefiting from hardware\n> >> support for 64k.\n> >>\n> >> IIUC, what you are proposing feels a bit like traveling back in time when it\n> >> comes to the memory waste problem that Arm users encountered.\n> >>\n> >> Where do you see the big difference to 64k on Arm in your proposal? Would\n> >> you currently also be running 64k Arm in production and the memory waste etc\n> >> is acceptable?\n> >\n> > That's the point. I don't see a big difference to 64k Arm. I want to\n> > bring this option to x86: at some machine size it makes sense trade\n> > memory consumption for scalability. I am targeting it to machines with\n> > over 2TiB of RAM.\n> >\n> > BTW, we do run 64k Arm in our fleet. There's some growing pains, but it\n> > looks good in general We have no plans to switch to 4k (or 16k) at the\n> > moment. 512M THPs also look good on some workloads.\n>\n> Okay, that's valuable information, thanks!\n>\n> Being able to remove the sub-page mapping part (or being able to just\n> hide it somewhere deep down in arch code) would make this a lot easier\n> to digest.\n>\n> --\n> Cheers,\n>\n> David\n>\n",
          "reply_to": "",
          "message_date": "2026-02-20",
          "message_id": ""
        }
      ],
      "analysis_source": "heuristic"
    },
    "2026-02-19": {
      "report_file": "2026-02-20_ollama_llama3.1-8b.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "Pedro Falcato",
          "summary": "Reviewer Pedro Falcato questioned the relevance of the patch's idea, citing that modern systems can use mTHP (memory Transparent Huge Pages) to achieve similar benefits by simply toggling a sysfs entry.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Doesn't this idea make less sense these days, with mTHP? Simply by toggling one\nof the entries in /sys/kernel/mm/transparent_hugepage.",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-19"
        },
        {
          "author": "Pedro Falcato",
          "summary": "Reviewer Pedro Falcato suggested adding a global minimum page cache order to address scalability concerns, noting that some issues are being addressed separately (e.g., memdesc work) and expressing confusion about the benefits of larger allocation sizes.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "confusion"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "We could perhaps add a way to enforce a min_order globally on the page cache,\nas a way to address it.\n\nThere are some points there which aren't addressed by mTHP work in any way\n(1G THPs for one), others which are being addressed separately (memdesc work\ntrying to cut down on struct page overhead).\n\n(I also don't understand your point about order-5 allocation, AFAIK pcp will\ncache up to COSTLY_ORDER (3) and PMD order, but I'm probably not seeing the\nfull picture)\n\n\n-- \nPedro",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-19"
        },
        {
          "author": "David (Arm)",
          "summary": "Reviewer suggested emulating a larger page size (64k) for user space on x86 while still using 4k pages internally, reducing zone lock contention and other issues.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "When discussing per-process page sizes with Ryan and Dev, I mentioned \nthat having a larger emulated page size could be interesting for other \narchitectures as well.\n\nThat is, we would emulate a 64K page size on Intel for user space as \nwell, but let the OS work with 4K pages.\n\nWe'd only allocate+map large folios into user space + pagecache, but \nstill allow for page tables etc. to not waste memory.\n\nSo \"most\" of your allocations in the system would actually be at least \n64k, reducing zone lock contention etc.\n\n\nIt doesn't solve all the problems you wanted to tackle on your list \n(e.g., \"struct page\" overhead, which will be sorted out by memdescs).\n\n-- \nCheers,\n\nDavid",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-19"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "Author responded to a concern that the patch does not handle fragmentation properly by explaining that mTHP is best effort and fragmentation is not a concern, implying that the patch will still work as long as there is free memory.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "no clear resolution"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "mTHP is still best effort. This is way you don't need to care about\nfragmentation, you will get your 64k page as long as you have free\nmemory.",
          "reply_to": "Pedro Falcato",
          "message_date": "2026-02-19"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author addressed Pedro's concern about the efficiency of the page allocator, explaining that a higher base page size reduces the work required to merge/split buddy pages.",
          "sentiment": "positive",
          "sentiment_signals": [
            "acknowledged the benefit",
            "explained reasoning"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "With higher base page size, page allocator doesn't need to do as much\nwork to merge/split buddy pages. So serving the same 2M as order-5 is\ncheaper than order-9.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
          "reply_to": "Pedro Falcato",
          "message_date": "2026-02-19"
        },
        {
          "author": "David (Arm)",
          "summary": "Reviewer David noted that the proposed change would lead to reduced page table merging and splitting due to larger allocation sizes, which is a natural consequence of using 64k pages instead of 4k",
          "sentiment": "neutral",
          "sentiment_signals": [
            "no clear technical objection or suggestion"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "I think the idea is that if most of your allocations (anon + pagecache) \nare 64k instead of 4k, on average, you'll just naturally do less merging \nsplitting.\n\n-- \nCheers,\n\nDavid",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-19"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "Author disagrees that emulation can help reduce zone lock contention, citing potential for increased contention due to mixed page sizes.",
          "sentiment": "contentious",
          "sentiment_signals": [
            "disagreement",
            "potential for increased contention"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "I am not convinced emulation would help zone lock contention. I expect\ncontention to be higher if page allocator would see a mix of 4k and 64k\nrequests. It sounds like constant split/merge under the lock.",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-19"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "Author addressed David's concern about the feasibility of serving 1G pages from the buddy allocator, agreeing that it is not possible and questioning how to achieve viable 1G THPs without this capability.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "acknowledging a technical limitation",
            "questioning the approach"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "I don't think we can serve 1G pages out of buddy allocator with 4k\norder-0. And without it, I don't see how to get to a viable 1G THPs.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-19"
        },
        {
          "author": "David (Arm)",
          "summary": "Reviewer David (Arm) noted that if most allocations are larger than 64k, the benefits of splitting page size into PTE_SIZE and PG_SIZE may be reduced, as there will still be some splitting/merging required for smaller allocations.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "If most your allocations are larger, then there isn't that much \nsplitting/merging.\n\nThere will be some for the < 64k allocations of course, but when all \nuser space+page cache is >= 64 then the split/merge + zone lock should \nbe heavily reduced.",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-19"
        },
        {
          "author": "David (Arm)",
          "summary": "Reviewer David expressed skepticism about the proposed change, suggesting that previous work by Zi Yan could be leveraged to achieve the desired outcome.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "skepticism",
            "previous work"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Zi Yan was one working on this, and I think we had ideas on how to make \nthat work in the long run.\n\n-- \nCheers,\n\nDavid",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-19"
        },
        {
          "author": "Dave Hansen",
          "summary": "Reviewer Dave Hansen noted that a 64k page cache would consume significantly more memory than expected, approximately 5GB extra for a kernel tree, and questioned the potential memory savings of the proposed change.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "First of all, this looks like fun. Nice work! I'm not opposed at all in\nconcept to cleaning up things and doing the logical separation you\ndescribed to split buddy granularity and mapping granularity. That seems\nlike a worthy endeavor and some of the union/#define tricks look like a\nlikely viable way to do it incrementally.\n\nBut I don't think there's going to be a lot of memory savings in the\nend. Maybe this would bring the mem= hyperscalers back into the fold and\nhave them actually start using 'struct page' again for their VM memory.\nDunno.\n\nBut, let's look at my kernel directory and round the file sizes up to\n4k, 16k and 64k:\n\nfind .  -printf '%s\\n' | while read size; do echo\t\\\n\t\t$(((size + 0x0fff) & 0xfffff000))\t\\\n\t\t$(((size + 0x3fff) & 0xffffc000))\t\\\n\t\t$(((size + 0xffff) & 0xffff0000));\ndone\n\n... and add them all up:\n\n11,297,648 KB - on disk\n11,297,712 KB - in a 4k page cache\n12,223,488 KB - in a 16k page cache\n16,623,296 KB - in a 64k page cache\n\nSo a 64k page cache eats ~5GB of extra memory for a kernel tree (well,\n_my_ kernel tree). In other words, if you are looking for memory savings\non my laptop, you'll need ~300GB of RAM before 'struct page' overhead\noverwhelms the page cache bloat from a single kernel tree.\n\nThe whole kernel obviously isn't in the page cache all at the same time.\nThe page cache across the system is also obviously different than a\nkernel tree, but you get the point.\n\nThat's not to diminish how useful something like this might be,\nespecially for folks that are sensitive to 'struct page' overhead or\nallocator performance.\n\nBut, it will mostly be getting better performance at the _cost_ of\nconsuming more RAM, not saving RAM.",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-19"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "Author clarifies whether the proposed 64k page size should enforce alignment for user-space mappings, indicating a need to revisit the patch's impact on userspace ABI.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "need_for_revision"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Just to clarify, do you want it to be enforced on userspace ABI.\nLike, all mappings are 64k aligned?",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-19"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author acknowledges that memory waste due to page table overhead is a solvable issue, suggesting that switching to slab allocation can address it.",
          "sentiment": "positive",
          "sentiment_signals": [
            "acknowledged a problem",
            "proposed a solution"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Waste of memory for page table is solvable and pretty straight forward.\nMost of such cases can be solve mechanically by switching to slab.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-19"
        },
        {
          "author": "Dave Hansen",
          "summary": "Reviewer Dave Hansen noted that the patch's approach to separating PAGE_SIZE into PTE_SIZE and PG_SIZE may not be suitable for architectures other than x86, as it relies on the assumption that PTE entries are aligned to a power of two, which is not guaranteed by the architecture.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "architecture",
            "assumption"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On 2/19/26 07:08, Kiryl Shutsemau wrote:\n...",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-19"
        },
        {
          "author": "Dave Hansen",
          "summary": "Reviewer noted that the patch introduces a large number of changes and renames across multiple files, which may make it difficult for others to review and understand the code.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "large number of changes",
            "renames"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "A few notes about the diffstats:\n\n$ git diff v6.17..HEAD arch/x86 | diffstat | tail -1\n 105 files changed, 874 insertions(+), 843 deletions(-)\n$ git diff v6.17..HEAD mm | diffstat | tail -1\n 53 files changed, 1136 insertions(+), 1069 deletions(-)\n\nThe vast, vast majority of this seems to be the renames. Stuff like:",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-19"
        },
        {
          "author": "Dave Hansen",
          "summary": "Reviewer Dave Hansen expressed concern about the complexity and potential bugs introduced by the logic changes in the patch, requesting a clear separation between mechanical and logical modifications.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested_changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "That stuff obviously needs to be audited but it's far less concerning\nthan the logic changes.\n\nSo just for review sanity, if you go forward with this, I'd very much\nappreciate a strong separation of the purely mechanical bits from any\nlogic changes.",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-19"
        },
        {
          "author": "Dave Hansen",
          "summary": "Reviewer Dave Hansen noted that the proposed patch is not independent of other changes, specifically a full tree conversion to ptdescs, and suggested that the series should focus on identifying additional work needed before such a conversion can be considered.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "gates",
            "additional work"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Others mentioned this, but I think this essentially gates what you are\ndoing behind a full tree conversion over to ptdescs.\n\nThe most useful thing we can do with this series is look at it and\ndecide what _other_ things need to get done before the tree could\npossibly go in that direction, like ptdesc or a the disambiguation\nbetween PTE_SIZE and PG_SIZE that you've kicked off here.",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-19"
        },
        {
          "author": "Matthew Wilcox",
          "summary": "Reviewer suggested an alternative approach to implementing larger page sizes by allocating the larger size and using it for multiple entries, expressing skepticism about the slab approach",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "skeptical tone",
            "request for consideration of alternative approach"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Have you looked at the s390/ppc implementations (yes, they're different,\nno, that sucks)?  slab seems like the wrong approach to me.\n\nThere's a third approach that I've never looked at which is to allocate\nthe larger size, then just use it for N consecutive entries.",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-19"
        },
        {
          "author": "Pedro Falcato",
          "summary": "Reviewer Pedro Falcato noted that the proposed change would result in 90%+ of allocations being 64k, which he hopes would be achieved by combining this patch with another patch modifying slab_min_order.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "hopes",
            "depending"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Yep. That plus slab_min_order would hopefully yield a system where 90%+\n(depending on how your filesystem's buffer cache works) allocations are 64K.\n\n-- \nPedro",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-19"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "Author acknowledged that struct page memory consumption is a static cost, not reclaimed, and emphasized the importance of reclaimable page cache memory, suggesting userspace can control rounding overhead by handling data in large files.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "acknowledged",
            "emphasized"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "That's fair.\n\nThe problem with struct page memory consumption is that it is static and\ncannot be reclaimed. You pay the struct page tax no matter what.\n\nPage cache rounding overhead can be large, but a motivated userspace can\nkeep it under control by avoiding splitting a dataset into many small\nfiles. And this memory is reclaimable.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
          "reply_to": "Dave Hansen",
          "message_date": "2026-02-19"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "Author acknowledged that packing of page tables is not a requirement for correctness and plans to use the whole order-0 page for proof-of-concept",
          "sentiment": "neutral",
          "sentiment_signals": [
            "acknowledged need to catch up on ptdescs",
            "planned PoC approach"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "I have not followed ptdescs closely. Need to catch up.\n\nFor PoC, I will just waste full order-0 page for page table. Packing is\nnot required for correctness.",
          "reply_to": "Dave Hansen",
          "message_date": "2026-02-19"
        },
        {
          "author": "Dave Hansen",
          "summary": "Reviewer noted that the proposed change to use a larger page size would not cause issues with the KPTI pgd because its current allocation of 8k PGDs would fit within the new 128k allocation, but found this outcome 'weird' and questioned whether it was functional.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "questioning the functionality of a specific outcome"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Yeah, I guess padding it out is ugly but effective.\n\nI was trying to figure out how it would apply to the KPTI pgd because we\njust flip bit 12 to switch between user and kernel PGDs. But I guess the\n8k of PGDs in the current allocation will fit fine in 128k, so it's\nweird but functional.",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-19"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "Author acknowledges a need for more work on handling page faults and VMA alignment, but does not commit to a specific fix or timeline.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledges need for more work"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "I was the first thing that came to mind. I have not put much time into\nit",
          "reply_to": "Matthew Wilcox",
          "message_date": "2026-02-19"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author is addressing Matthew Wilcox's concern about fragmentation when using 16k base pages on x86, explaining that the parent page table would need to be populated with 16 entries but fragmentation within the page itself is not a concern.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Yeah, that's a possible way. We would need to populate 16 page table\nentries of the parent page table. But you don't need to care about\nfragmentation within the page.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
          "reply_to": "Matthew Wilcox",
          "message_date": "2026-02-19"
        },
        {
          "author": "Kalesh Singh",
          "summary": "Reviewer Kalesh Singh pointed out that the patch does not handle page faults into file mappings correctly when PTE_SIZE is less than PG_SIZE, and suggested that the page fault handler needs to be updated to handle misaligned cases.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Thu, Feb 19, 2026 at 7:39 AM David Hildenbrand (Arm)\n<david@kernel.org> wrote:",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-19"
        },
        {
          "author": "Kalesh Singh",
          "summary": "Reviewer Kalesh Singh noted that the current design does not enforce a larger granularity on VMAs to emulate a userspace page size, which is necessary for Android's use case of emulating 16KB devices on x86, and requested discussion on extending the design to cover this use case.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "interested in discussing"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Hi Kiryl,\n\nI'd be interested to discuss this at LSFMM.\n\nOn Android, we have a separate but related use case: we emulate the\nuserspace page size on x86, primarily to enable app developers to\nconduct compatibility testing of their apps for 16KB Android devices.\n[1]\n\nIt mainly works by enforcing a larger granularity on the VMAs to\nemulate a userspace page size, somewhat similar to what David\nmentioned, while the underlying kernel still operates on a 4KB\ngranularity. [2]\n\nIIUC the current design would not enfore the larger granularity /\nalignment for VMAs to avoid breaking ABI. However, I'd be interest to\ndiscuss whether it can be extended to cover this usecase as well.\n\n[1]  https://developer.android.com/guide/practices/page-sizes#16kb-emulator\n[2] https://source.android.com/docs/core/architecture/16kb-page-size/getting-started-cf-x86-64-pgagnostic\n\nThanks,\nKalesh",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-19"
        },
        {
          "author": "Zi Yan",
          "summary": "The reviewer suggests adding a super pageblock that consists of N consecutive pageblocks to enable anti-fragmentation at larger granularity, specifically 1GB, and questions whether these free pages should go into the buddy allocator.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "debatable"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Right. The idea is to add super pageblock (or whatever name), which consists of N consecutive\npageblocks, so that anti fragmentation can work at larger granularity, e.g., 1GB, to create\nfree pages. Whether 1GB free pages from memory compaction need to go into buddy allocator\nor not is debatable.\n\n--\nBest Regards,\nYan, Zi",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-19"
        },
        {
          "author": "Liam Howlett",
          "summary": "The reviewer expressed concern that increasing page size would increase memory pressure and degrade primary workloads on machines with multiple concurrent tasks, potentially leading to more frequent OOMs for secondary tasks.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "But we are in reclaim a lot more these days.  As I'm sure you are aware,\nwe are trying to maximize the resources (both cpu and ram) of any\nmachine powered on.  Entering reclaim will consume the cpu time and will\naffect other tasks.\n\nEspecially with multiple workload machines, the tendency is to have a\nprimary focus with the lower desired work being killed, if necessary.\nReducing the overhead just means more secondary tasks, or a bigger\nfootprint of the ones already executing.\n\nIncreasing the memory pressure will degrade the primary workload more\nfrequently, even if we recover enough to avoid OOMing the secondary.\n\nWhile in the struct page tax world, the secondary task would be killed\nafter a shorter (and less frequently executed) reclaim comes up short.\nSo, I would think that we would be degrading the primary workload in an\nattempt to keep the secondary alive?  Maybe I'm over-simplifying here?\n\nNear the other end of the spectrum, we have chromebooks that are\nconstantly in reclaim, even with 4k pages.  I guess these machines would\nbe destine to maintain the same page size they use today.  That is, this\nsolution for the struct page tax is only useful if you have a lot of\nmemory.  But then again, that's where the bookkeeping costs become hard\nto take.\n\nThanks,\nLiam",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-19"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-23": {
      "report_file": "2026-02-20_ollama_llama3.1-8b.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "David (Arm)",
          "summary": "Right, I assume that they will have to be thought about that, and possibly, some binaries/libraries recompiled.",
          "sentiment": "neutral",
          "sentiment_signals": [],
          "has_inline_review": true,
          "tags_given": [],
          "analysis_source": "heuristic",
          "raw_body": "On 2/20/26 20:33, Kalesh Singh wrote:\n> On Fri, Feb 20, 2026 at 8:30\\u202fAM David Hildenbrand (Arm)\n> <david@kernel.org> wrote:\n>>\n>> On 2/20/26 13:07, Kiryl Shutsemau wrote:\n>>>\n>>> Well, it will drastically limit the adoption. We have too much legacy\n>>> stuff on x86.\n>>\n>> I'd assume that many applications nowadays can deal with differing page\n>> sizes (thanks to some other architectures paving the way).\n>>\n>> But yes, some real legacy stuff, or stuff that ever only cared about\n>> intel still hardcodes pagesize=4k.\n> \n> I think most issues will stem from linkers setting the default ELF\n> segment alignment (max-page-size) for x86 to 4096. So those ELFs will\n> not load correctly or at all on the larger emulated granularity.\n\nRight, I assume that they will have to be thought about that, and \npossibly, some binaries/libraries recompiled.\n\n-- \nCheers,\n\nDavid\n",
          "reply_to": "",
          "message_date": "2026-02-23",
          "message_id": ""
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "I think backward compatibility is important and I believe we can get there without ABI break. And optimize from there. BTW, x86-64 SysV ABI allows for 64k page size: Systems are permitted to use any power-of-two page size between 4KB and 64KB, inclusive. But it doesn't work in practice.",
          "sentiment": "neutral",
          "sentiment_signals": [],
          "has_inline_review": true,
          "tags_given": [],
          "analysis_source": "heuristic",
          "raw_body": "On Mon, Feb 23, 2026 at 12:04:10PM +0100, David Hildenbrand (Arm) wrote:\n> On 2/20/26 20:33, Kalesh Singh wrote:\n> > On Fri, Feb 20, 2026 at 8:30\\u202fAM David Hildenbrand (Arm)\n> > <david@kernel.org> wrote:\n> > > \n> > > On 2/20/26 13:07, Kiryl Shutsemau wrote:\n> > > > \n> > > > Well, it will drastically limit the adoption. We have too much legacy\n> > > > stuff on x86.\n> > > \n> > > I'd assume that many applications nowadays can deal with differing page\n> > > sizes (thanks to some other architectures paving the way).\n> > > \n> > > But yes, some real legacy stuff, or stuff that ever only cared about\n> > > intel still hardcodes pagesize=4k.\n> > \n> > I think most issues will stem from linkers setting the default ELF\n> > segment alignment (max-page-size) for x86 to 4096. So those ELFs will\n> > not load correctly or at all on the larger emulated granularity.\n> \n> Right, I assume that they will have to be thought about that, and possibly,\n> some binaries/libraries recompiled.\n\nI think backward compatibility is important and I believe we can get\nthere without ABI break. And optimize from there.\n\nBTW, x86-64 SysV ABI allows for 64k page size:\n\n\tSystems are permitted to use any power-of-two page size between\n\t4KB and 64KB, inclusive.\n\nBut it doesn't work in practice.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n",
          "reply_to": "",
          "message_date": "2026-02-23",
          "message_id": ""
        },
        {
          "author": "David (Arm)",
          "summary": "Even in well controlled environments you would run in a hyperscaler?",
          "sentiment": "neutral",
          "sentiment_signals": [],
          "has_inline_review": true,
          "tags_given": [],
          "analysis_source": "heuristic",
          "raw_body": "On 2/23/26 12:13, Kiryl Shutsemau wrote:\n> On Mon, Feb 23, 2026 at 12:04:10PM +0100, David Hildenbrand (Arm) wrote:\n>> On 2/20/26 20:33, Kalesh Singh wrote:\n>>> On Fri, Feb 20, 2026 at 8:30\\u202fAM David Hildenbrand (Arm)\n>>> <david@kernel.org> wrote:\n>>>\n>>> I think most issues will stem from linkers setting the default ELF\n>>> segment alignment (max-page-size) for x86 to 4096. So those ELFs will\n>>> not load correctly or at all on the larger emulated granularity.\n>>\n>> Right, I assume that they will have to be thought about that, and possibly,\n>> some binaries/libraries recompiled.\n> \n> I think backward compatibility is important and I believe we can get\n> there without ABI break. And optimize from there.\n> \n> BTW, x86-64 SysV ABI allows for 64k page size:\n> \n> \tSystems are permitted to use any power-of-two page size between\n> \t4KB and 64KB, inclusive.\n> \n> But it doesn't work in practice.\n\nEven in well controlled environments you would run in a hyperscaler?\n\n-- \nCheers,\n\nDavid\n",
          "reply_to": "",
          "message_date": "2026-02-23",
          "message_id": ""
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "I have not invested much time into investigating this. I intentionally targeted compatible version assuming it will be better received by upstream. I want it to be usable outside specially cured userspace. 64k might not be good fit for a desktop, but 16k can be a different story.",
          "sentiment": "neutral",
          "sentiment_signals": [],
          "has_inline_review": true,
          "tags_given": [],
          "analysis_source": "heuristic",
          "raw_body": "On Mon, Feb 23, 2026 at 12:27:33PM +0100, David Hildenbrand (Arm) wrote:\n> On 2/23/26 12:13, Kiryl Shutsemau wrote:\n> > On Mon, Feb 23, 2026 at 12:04:10PM +0100, David Hildenbrand (Arm) wrote:\n> > > On 2/20/26 20:33, Kalesh Singh wrote:\n> > > > On Fri, Feb 20, 2026 at 8:30\\u202fAM David Hildenbrand (Arm)\n> > > > <david@kernel.org> wrote:\n> > > > \n> > > > I think most issues will stem from linkers setting the default ELF\n> > > > segment alignment (max-page-size) for x86 to 4096. So those ELFs will\n> > > > not load correctly or at all on the larger emulated granularity.\n> > > \n> > > Right, I assume that they will have to be thought about that, and possibly,\n> > > some binaries/libraries recompiled.\n> > \n> > I think backward compatibility is important and I believe we can get\n> > there without ABI break. And optimize from there.\n> > \n> > BTW, x86-64 SysV ABI allows for 64k page size:\n> > \n> > \tSystems are permitted to use any power-of-two page size between\n> > \t4KB and 64KB, inclusive.\n> > \n> > But it doesn't work in practice.\n> \n> Even in well controlled environments you would run in a hyperscaler?\n\nI have not invested much time into investigating this.\n\nI intentionally targeted compatible version assuming it will be better\nreceived by upstream. I want it to be usable outside specially cured\nuserspace. 64k might not be good fit for a desktop, but 16k can be a\ndifferent story.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n",
          "reply_to": "",
          "message_date": "2026-02-23",
          "message_id": ""
        },
        {
          "author": "Dave Hansen",
          "summary": "I think what Kirill is trying to say is that \"it breaks userspace\". ;) A hyperscaler (or other \"embedded\" environment) might be willing or able to go fix up userspace breakage. I would suspect our high frequency trading friends would be all over this if it shaved a microsecond off their receive times. The more important question is what it breaks and how badly it breaks things.",
          "sentiment": "neutral",
          "sentiment_signals": [],
          "has_inline_review": true,
          "tags_given": [],
          "analysis_source": "heuristic",
          "raw_body": "On 2/23/26 03:27, David Hildenbrand (Arm) wrote:\n...\n>> BTW, x86-64 SysV ABI allows for 64k page size:\n>>\n>> Systems are permitted to use any power-of-two page size between\n>> 4KB and 64KB, inclusive.\n>>\n>> But it doesn't work in practice.\n> \n> Even in well controlled environments you would run in a hyperscaler?\n\nI think what Kirill is trying to say is that \"it breaks userspace\". ;)\n\nA hyperscaler (or other \"embedded\" environment) might be willing or able\nto go fix up userspace breakage. I would suspect our high frequency\ntrading friends would be all over this if it shaved a microsecond off\ntheir receive times.\n\nThe more important question is what it breaks and how badly it breaks\nthings. 5-level paging, for instance, broke some JITs that historically\nused the new (>48) upper virtual address bits for metadata. The gains\nfrom 5-level paging were big enough and the userspace breakage was\nconfined and fixable enough that 5-level paging was viable.\n\nI'm not sure which side a larger base page side will fall on, though. Is\nit going to be an out-of-tree hack that a few folks use, or will it be\nmore like 5-level paging and be good enough that it goes into mainline?\n",
          "reply_to": "",
          "message_date": "2026-02-23",
          "message_id": ""
        },
        {
          "author": "David (Arm)",
          "summary": "Yes. Probably similar to Intel proposing an actual 64k page size. Expected. :) Just thinking about VMAs spanning partial pages makes me shiver. Or A single page spanning multiple VMAs. I haven't seen the code yet, but I am certain that I will not like it. I'm happy to be proven wrong :)",
          "sentiment": "neutral",
          "sentiment_signals": [],
          "has_inline_review": true,
          "tags_given": [],
          "analysis_source": "heuristic",
          "raw_body": "On 2/23/26 16:14, Dave Hansen wrote:\n> On 2/23/26 03:27, David Hildenbrand (Arm) wrote:\n> ...\n>>> BTW, x86-64 SysV ABI allows for 64k page size:\n>>>\n>>>  Systems are permitted to use any power-of-two page size between\n>>>  4KB and 64KB, inclusive.\n>>>\n>>> But it doesn't work in practice.\n>>\n>> Even in well controlled environments you would run in a hyperscaler?\n> \n> I think what Kirill is trying to say is that \"it breaks userspace\". ;)\n\nYes. Probably similar to Intel proposing an actual 64k page size. \nExpected. :)\n\n> \n> A hyperscaler (or other \"embedded\" environment) might be willing or able\n> to go fix up userspace breakage. I would suspect our high frequency\n> trading friends would be all over this if it shaved a microsecond off\n> their receive times.\n> \n> The more important question is what it breaks and how badly it breaks\n> things. 5-level paging, for instance, broke some JITs that historically\n> used the new (>48) upper virtual address bits for metadata. The gains\n> from 5-level paging were big enough and the userspace breakage was\n> confined and fixable enough that 5-level paging was viable.\n> \n> I'm not sure which side a larger base page side will fall on, though. Is\n> it going to be an out-of-tree hack that a few folks use, or will it be\n> more like 5-level paging and be good enough that it goes into mainline?\n\nJust thinking about VMAs spanning partial pages makes me shiver. Or A \nsingle page spanning multiple VMAs.\n\nI haven't seen the code yet, but I am certain that I will not like it.\n\nI'm happy to be proven wrong :)\n\n-- \nCheers,\n\nDavid\n",
          "reply_to": "",
          "message_date": "2026-02-23",
          "message_id": ""
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "Hate to break it to you, but we have it now upstream :P THP can span multiple VMAs. And can be partially mapped. The only new thing is that we allow this for order-0 page now. And you cannot realistically recover wasted memory -- no deferred split. I will do my best, but no promises :)",
          "sentiment": "neutral",
          "sentiment_signals": [],
          "has_inline_review": true,
          "tags_given": [],
          "analysis_source": "heuristic",
          "raw_body": "On Mon, Feb 23, 2026 at 04:31:56PM +0100, David Hildenbrand (Arm) wrote:\n> On 2/23/26 16:14, Dave Hansen wrote:\n> > On 2/23/26 03:27, David Hildenbrand (Arm) wrote:\n> > ...\n> > > > BTW, x86-64 SysV ABI allows for 64k page size:\n> > > > \n> > > >  Systems are permitted to use any power-of-two page size between\n> > > >  4KB and 64KB, inclusive.\n> > > > \n> > > > But it doesn't work in practice.\n> > > \n> > > Even in well controlled environments you would run in a hyperscaler?\n> > \n> > I think what Kirill is trying to say is that \"it breaks userspace\". ;)\n> \n> Yes. Probably similar to Intel proposing an actual 64k page size. Expected.\n> :)\n> \n> > \n> > A hyperscaler (or other \"embedded\" environment) might be willing or able\n> > to go fix up userspace breakage. I would suspect our high frequency\n> > trading friends would be all over this if it shaved a microsecond off\n> > their receive times.\n> > \n> > The more important question is what it breaks and how badly it breaks\n> > things. 5-level paging, for instance, broke some JITs that historically\n> > used the new (>48) upper virtual address bits for metadata. The gains\n> > from 5-level paging were big enough and the userspace breakage was\n> > confined and fixable enough that 5-level paging was viable.\n> > \n> > I'm not sure which side a larger base page side will fall on, though. Is\n> > it going to be an out-of-tree hack that a few folks use, or will it be\n> > more like 5-level paging and be good enough that it goes into mainline?\n> \n> Just thinking about VMAs spanning partial pages makes me shiver. Or A\n> single page spanning multiple VMAs.\n\nHate to break it to you, but we have it now upstream :P\n\nTHP can span multiple VMAs. And can be partially mapped.\n\nThe only new thing is that we allow this for order-0 page now. And you\ncannot realistically recover wasted memory -- no deferred split.\n\n> I haven't seen the code yet, but I am certain that I will not like it.\n> \n> I'm happy to be proven wrong :)\n\nI will do my best, but no promises :)\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n",
          "reply_to": "",
          "message_date": "2026-02-23",
          "message_id": ""
        },
        {
          "author": "David (Arm)",
          "summary": "Single mapcount, single anon-exclusive flag. Completely different story :P",
          "sentiment": "neutral",
          "sentiment_signals": [],
          "has_inline_review": true,
          "tags_given": [],
          "analysis_source": "heuristic",
          "raw_body": ">>\n>> Just thinking about VMAs spanning partial pages makes me shiver. Or A\n>> single page spanning multiple VMAs.\n> \n> Hate to break it to you, but we have it now upstream :P\n> \n> THP can span multiple VMAs. And can be partially mapped.\n\nSingle mapcount, single anon-exclusive flag.\n\nCompletely different story :P\n\n-- \nCheers,\n\nDavid\n",
          "reply_to": "",
          "message_date": "2026-02-23",
          "message_id": ""
        }
      ],
      "analysis_source": "heuristic"
    }
  }
}