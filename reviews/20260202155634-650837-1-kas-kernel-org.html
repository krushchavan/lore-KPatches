<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Review Comments: [PATCHv7 00/17] mm: Eliminate fake head pages from vmemmap optimization</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto,
                         "Helvetica Neue", Arial, sans-serif;
            background: #f5f5f5;
            color: #333;
            line-height: 1.6;
            padding: 20px;
            max-width: 900px;
            margin: 0 auto;
        }
        .home-link { margin-bottom: 12px; display: block; }
        .home-link a { color: #0366d6; text-decoration: none; font-size: 0.9em; }
        .home-link a:hover { text-decoration: underline; }

        h1 { font-size: 1.3em; margin-bottom: 2px; color: #1a1a1a; line-height: 1.3; }

        .lore-link { font-size: 0.85em; margin: 4px 0 6px; display: block; }
        .lore-link a { color: #0366d6; text-decoration: none; }
        .lore-link a:hover { text-decoration: underline; }

        .date-range {
            font-size: 0.8em;
            color: #888;
            margin-bottom: 16px;
        }
        .date-range a { color: #0366d6; text-decoration: none; }
        .date-range a:hover { text-decoration: underline; }

        /* thread-node scroll margin so the card isn't clipped at the top */
        .thread-node { scroll-margin-top: 8px; }

        /* ── Patch summary ──────────────────────────────────────────── */
        .patch-summary-block {
            background: #fff;
            border-radius: 8px;
            padding: 12px 16px;
            margin-bottom: 20px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            border-left: 3px solid #4a90d9;
        }
        .patch-summary-label {
            font-size: 0.72em;
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 0.06em;
            color: #4a90d9;
            margin-bottom: 4px;
        }
        .patch-summary-text {
            font-size: 0.88em;
            color: #444;
            line-height: 1.55;
        }

        /* ── Thread tree ────────────────────────────────────────────── */
        .thread-tree {
            display: flex;
            flex-direction: column;
            gap: 6px;
        }

        /* Depth indentation via left border */
        .thread-node { position: relative; }
        .thread-children {
            margin-left: 20px;
            padding-left: 12px;
            border-left: 2px solid #e0e0e0;
            margin-top: 6px;
            display: flex;
            flex-direction: column;
            gap: 6px;
        }

        /* ── Review comment card ────────────────────────────────────── */
        .review-comment {
            background: #fff;
            border-radius: 6px;
            padding: 10px 14px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            font-size: 0.88em;
        }
        .review-comment-header {
            display: flex;
            flex-wrap: wrap;
            align-items: center;
            gap: 6px;
            margin-bottom: 5px;
        }
        .review-author {
            font-weight: 700;
            color: #1a1a1a;
            font-size: 0.95em;
        }

        /* Date chip — links back to the daily report */
        .date-chip {
            font-size: 0.75em;
            color: #777;
            background: #f0f0f0;
            border-radius: 10px;
            padding: 1px 7px;
            text-decoration: none;
            white-space: nowrap;
        }
        a.date-chip:hover { background: #e0e8f5; color: #0366d6; }

        .badge {
            display: inline-block;
            padding: 1px 8px;
            border-radius: 10px;
            font-size: 0.75em;
            font-weight: 600;
        }
        .inline-review-badge {
            display: inline-block;
            padding: 0 6px;
            border-radius: 8px;
            font-size: 0.78em;
            font-weight: 500;
            background: #e3f2fd;
            color: #1565c0;
        }
        .review-tag-badge {
            display: inline-block;
            padding: 0 6px;
            border-radius: 8px;
            font-size: 0.78em;
            font-weight: 500;
            background: #e8f5e9;
            color: #2e7d32;
        }
        .analysis-source-badge {
            display: inline-block;
            padding: 1px 7px;
            border-radius: 10px;
            font-size: 0.72em;
            font-weight: 600;
            border: 1px solid rgba(0,0,0,0.1);
        }

        .review-comment-text {
            color: #444;
            line-height: 1.55;
            margin-bottom: 4px;
        }
        .review-comment-signals {
            margin-top: 3px;
            font-size: 0.85em;
            color: #aaa;
            font-style: italic;
        }

        /* ── Collapsible raw body ───────────────────────────────────── */
        .raw-body-toggle {
            margin-top: 5px;
            font-size: 0.85em;
        }
        .raw-body-toggle summary {
            cursor: pointer;
            color: #888;
            padding: 2px 0;
            font-weight: 500;
            font-size: 0.9em;
            list-style: none;
        }
        .raw-body-toggle summary::-webkit-details-marker { display: none; }
        .raw-body-toggle summary::before { content: "▶ "; font-size: 0.7em; }
        .raw-body-toggle[open] summary::before { content: "▼ "; }
        .raw-body-toggle summary:hover { color: #555; }
        .raw-body-text {
            white-space: pre-wrap;
            font-size: 0.95em;
            background: #f8f8f8;
            padding: 8px 10px;
            border-radius: 4px;
            max-height: 360px;
            overflow-y: auto;
            margin-top: 4px;
            line-height: 1.5;
            color: #444;
            border: 1px solid #e8e8e8;
        }
        .reply-to-label {
            font-size: 0.8em;
            color: #999;
            font-style: italic;
            margin-top: 3px;
        }
        .lore-link {
            display: inline-block;
            margin-top: 4px;
            font-size: 0.82em;
            color: #0366d6;
            text-decoration: none;
            font-weight: 500;
            white-space: nowrap;
        }
        .lore-link:hover { text-decoration: underline; color: #0056b3; }

        .no-reviews {
            color: #aaa;
            font-size: 0.85em;
            font-style: italic;
            padding: 8px 0;
        }

        footer {
            text-align: center;
            color: #bbb;
            font-size: 0.78em;
            margin-top: 36px;
            padding: 16px;
        }
    </style>
</head>
<body>
    <div class="home-link"><a href="../index.html">&larr; Back to reports</a></div>
    <h1>[PATCHv7 00/17] mm: Eliminate fake head pages from vmemmap optimization</h1>
    <div class="lore-link"><a href="https://lore.kernel.org/all/20260202155634.650837-1-kas@kernel.org/" target="_blank">View on lore.kernel.org &rarr;</a></div>
    <div class="date-range">Active on: <a href="#2026-02-27">2026-02-27</a> &bull; <a href="#2026-02-23">2026-02-23</a> &bull; <a href="#2026-02-15">2026-02-15</a> &bull; <a href="#2026-02-10">2026-02-10</a> &bull; <a href="#2026-02-07">2026-02-07</a> &bull; <a href="#2026-02-05">2026-02-05</a> &bull; <a href="#2026-02-04">2026-02-04</a> &bull; <a href="#2026-02-03">2026-02-03</a> &bull; <a href="#2026-02-02">2026-02-02</a></div>
    <div class="patch-summary-block"><div class="patch-summary-label">Patch summary</div><div class="patch-summary-text">This patch series removes &#x27;fake head pages&#x27; from the HugeTLB vmemmap optimization by changing how tail pages encode their relationship to the head page, simplifying compound_head() and page_ref_add_unless(). The new approach uses a mask-based encoding for architectures where sizeof(struct page) is a power of 2, allowing shared read-only tail pages across huge pages on a NUMA node. This reduces complexity and overhead in the hot path, but testing has shown either no change or only slight performance improvement.</div></div>
    <div class="thread-tree">
<div class="thread-node depth-0" id="2026-02-02">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Kiryl Shutsemau (author)</span>
<a class="date-chip" href="../2026-02-27_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-02">2026-02-02</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author is addressing a concern about passing down the head and tail page indices, instead proposing to pass the tail and head pages directly along with their order in preparation for changing how the head position is encoded in the tail page.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Move MAX_FOLIO_ORDER definition from mm.h to mmzone.h.

This is preparation for adding the vmemmap_tails array to struct
pglist_data, which requires MAX_FOLIO_ORDER to be available in mmzone.h.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Acked-by: David Hildenbrand (Red Hat) &lt;david@kernel.org&gt;
Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
Acked-by: Muchun Song &lt;muchun.song@linux.dev&gt;
---
 include/linux/mm.h     | 31 -------------------------------
 include/linux/mmzone.h | 31 +++++++++++++++++++++++++++++++
 2 files changed, 31 insertions(+), 31 deletions(-)

diff --git a/include/linux/mm.h b/include/linux/mm.h
index f8a8fd47399c..8d5fa655fea4 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -27,7 +27,6 @@
 #include &lt;linux/page-flags.h&gt;
 #include &lt;linux/page_ref.h&gt;
 #include &lt;linux/overflow.h&gt;
-#include &lt;linux/sizes.h&gt;
 #include &lt;linux/sched.h&gt;
 #include &lt;linux/pgtable.h&gt;
 #include &lt;linux/kasan.h&gt;
@@ -2477,36 +2476,6 @@ static inline unsigned long folio_nr_pages(const struct folio *folio)
 	return folio_large_nr_pages(folio);
 }
 
-#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)
-/*
- * We don&#x27;t expect any folios that exceed buddy sizes (and consequently
- * memory sections).
- */
-#define MAX_FOLIO_ORDER		MAX_PAGE_ORDER
-#elif defined(CONFIG_SPARSEMEM) &amp;&amp; !defined(CONFIG_SPARSEMEM_VMEMMAP)
-/*
- * Only pages within a single memory section are guaranteed to be
- * contiguous. By limiting folios to a single memory section, all folio
- * pages are guaranteed to be contiguous.
- */
-#define MAX_FOLIO_ORDER		PFN_SECTION_SHIFT
-#elif defined(CONFIG_HUGETLB_PAGE)
-/*
- * There is no real limit on the folio size. We limit them to the maximum we
- * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect
- * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.
- */
-#define MAX_FOLIO_ORDER		get_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)
-#else
-/*
- * Without hugetlb, gigantic folios that are bigger than a single PUD are
- * currently impossible.
- */
-#define MAX_FOLIO_ORDER		PUD_ORDER
-#endif
-
-#define MAX_FOLIO_NR_PAGES	(1UL &lt;&lt; MAX_FOLIO_ORDER)
-
 /*
  * compound_nr() returns the number of pages in this potentially compound
  * page.  compound_nr() can be called on a tail page, and is defined to
diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 3e51190a55e4..be8ce40b5638 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -23,6 +23,7 @@
 #include &lt;linux/page-flags.h&gt;
 #include &lt;linux/local_lock.h&gt;
 #include &lt;linux/zswap.h&gt;
+#include &lt;linux/sizes.h&gt;
 #include &lt;asm/page.h&gt;
 
 /* Free memory management - zoned buddy allocator.  */
@@ -61,6 +62,36 @@
  */
 #define PAGE_ALLOC_COSTLY_ORDER 3
 
+#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)
+/*
+ * We don&#x27;t expect any folios that exceed buddy sizes (and consequently
+ * memory sections).
+ */
+#define MAX_FOLIO_ORDER		MAX_PAGE_ORDER
+#elif defined(CONFIG_SPARSEMEM) &amp;&amp; !defined(CONFIG_SPARSEMEM_VMEMMAP)
+/*
+ * Only pages within a single memory section are guaranteed to be
+ * contiguous. By limiting folios to a single memory section, all folio
+ * pages are guaranteed to be contiguous.
+ */
+#define MAX_FOLIO_ORDER		PFN_SECTION_SHIFT
+#elif defined(CONFIG_HUGETLB_PAGE)
+/*
+ * There is no real limit on the folio size. We limit them to the maximum we
+ * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect
+ * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.
+ */
+#define MAX_FOLIO_ORDER		get_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)
+#else
+/*
+ * Without hugetlb, gigantic folios that are bigger than a single PUD are
+ * currently impossible.
+ */
+#define MAX_FOLIO_ORDER		PUD_ORDER
+#endif
+
+#define MAX_FOLIO_NR_PAGES	(1UL &lt;&lt; MAX_FOLIO_ORDER)
+
 enum migratetype {
 	MIGRATE_UNMOVABLE,
 	MIGRATE_MOVABLE,
-- 
2.51.2

---

Instead of passing down the head page and tail page index, pass the tail
and head pages directly, as well as the order of the compound page.

This is a preparation for changing how the head position is encoded in
the tail page.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
---
 include/linux/page-flags.h |  4 +++-
 mm/hugetlb.c               |  8 +++++---
 mm/internal.h              | 12 ++++++------
 mm/mm_init.c               |  2 +-
 mm/page_alloc.c            |  2 +-
 5 files changed, 16 insertions(+), 12 deletions(-)

diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
index f7a0e4af0c73..8a3694369e15 100644
--- a/include/linux/page-flags.h
+++ b/include/linux/page-flags.h
@@ -865,7 +865,9 @@ static inline bool folio_test_large(const struct folio *folio)
 	return folio_test_head(folio);
 }
 
-static __always_inline void set_compound_head(struct page *page, struct page *head)
+static __always_inline void set_compound_head(struct page *page,
+					      const struct page *head,
+					      unsigned int order)
 {
 	WRITE_ONCE(page-&gt;compound_head, (unsigned long)head + 1);
 }
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 6e855a32de3d..54ba7cd05a86 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -3168,6 +3168,7 @@ int __alloc_bootmem_huge_page(struct hstate *h, int nid)
 
 /* Initialize [start_page:end_page_number] tail struct pages of a hugepage */
 static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,
+					struct hstate *h,
 					unsigned long start_page_number,
 					unsigned long end_page_number)
 {
@@ -3176,6 +3177,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,
 	struct page *page = folio_page(folio, start_page_number);
 	unsigned long head_pfn = folio_pfn(folio);
 	unsigned long pfn, end_pfn = head_pfn + end_page_number;
+	unsigned int order = huge_page_order(h);
 
 	/*
 	 * As we marked all tail pages with memblock_reserved_mark_noinit(),
@@ -3183,7 +3185,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,
 	 */
 	for (pfn = head_pfn + start_page_number; pfn &lt; end_pfn; page++, pfn++) {
 		__init_single_page(page, pfn, zone, nid);
-		prep_compound_tail((struct page *)folio, pfn - head_pfn);
+		prep_compound_tail(page, &amp;folio-&gt;page, order);
 		set_page_count(page, 0);
 	}
 }
@@ -3203,7 +3205,7 @@ static void __init hugetlb_folio_init_vmemmap(struct folio *folio,
 	__folio_set_head(folio);
 	ret = folio_ref_freeze(folio, 1);
 	VM_BUG_ON(!ret);
-	hugetlb_folio_init_tail_vmemmap(folio, 1, nr_pages);
+	hugetlb_folio_init_tail_vmemmap(folio, h, 1, nr_pages);
 	prep_compound_head(&amp;folio-&gt;page, huge_page_order(h));
 }
 
@@ -3260,7 +3262,7 @@ static void __init prep_and_add_bootmem_folios(struct hstate *h,
 			 * time as this is early in boot and there should
 			 * be no contention.
 			 */
-			hugetlb_folio_init_tail_vmemmap(folio,
+			hugetlb_folio_init_tail_vmemmap(folio, h,
 					HUGETLB_VMEMMAP_RESERVE_PAGES,
 					pages_per_huge_page(h));
 		}
diff --git a/mm/internal.h b/mm/internal.h
index d67e8bb75734..037ddcda25ff 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -879,13 +879,13 @@ static inline void prep_compound_head(struct page *page, unsigned int order)
 		INIT_LIST_HEAD(&amp;folio-&gt;_deferred_list);
 }
 
-static inline void prep_compound_tail(struct page *head, int tail_idx)
+static inline void prep_compound_tail(struct page *tail,
+				      const struct page *head,
+				      unsigned int order)
 {
-	struct page *p = head + tail_idx;
-
-	p-&gt;mapping = TAIL_MAPPING;
-	set_compound_head(p, head);
-	set_page_private(p, 0);
+	tail-&gt;mapping = TAIL_MAPPING;
+	set_compound_head(tail, head, order);
+	set_page_private(tail, 0);
 }
 
 void post_alloc_hook(struct page *page, unsigned int order, gfp_t gfp_flags);
diff --git a/mm/mm_init.c b/mm/mm_init.c
index 1a29a719af58..ba50f4c4337b 100644
--- a/mm/mm_init.c
+++ b/mm/mm_init.c
@@ -1099,7 +1099,7 @@ static void __ref memmap_init_compound(struct page *head,
 		struct page *page = pfn_to_page(pfn);
 
 		__init_zone_device_page(page, pfn, zone_idx, nid, pgmap);
-		prep_compound_tail(head, pfn - head_pfn);
+		prep_compound_tail(page, head, order);
 		set_page_count(page, 0);
 	}
 	prep_compound_head(head, order);
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index e4104973e22f..00c7ea958767 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -744,7 +744,7 @@ void prep_compound_page(struct page *page, unsigned int order)
 
 	__SetPageHead(page);
 	for (i = 1; i &lt; nr_pages; i++)
-		prep_compound_tail(page, i);
+		prep_compound_tail(page + i, page, order);
 
 	prep_compound_head(page, order);
 }
-- 
2.51.2

---

The &#x27;compound_head&#x27; field in the &#x27;struct page&#x27; encodes whether the page
is a tail and where to locate the head page. Bit 0 is set if the page is
a tail, and the remaining bits in the field point to the head page.

As preparation for changing how the field encodes information about the
head page, rename the field to &#x27;compound_info&#x27;.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
---
 .../admin-guide/kdump/vmcoreinfo.rst          |  2 +-
 Documentation/mm/vmemmap_dedup.rst            |  6 +++---
 include/linux/mm_types.h                      | 20 +++++++++----------
 include/linux/page-flags.h                    | 18 ++++++++---------
 include/linux/types.h                         |  2 +-
 kernel/vmcore_info.c                          |  2 +-
 mm/page_alloc.c                               |  2 +-
 mm/slab.h                                     |  2 +-
 mm/util.c                                     |  2 +-
 9 files changed, 28 insertions(+), 28 deletions(-)

diff --git a/Documentation/admin-guide/kdump/vmcoreinfo.rst b/Documentation/admin-guide/kdump/vmcoreinfo.rst
index 404a15f6782c..7663c610fe90 100644
--- a/Documentation/admin-guide/kdump/vmcoreinfo.rst
+++ b/Documentation/admin-guide/kdump/vmcoreinfo.rst
@@ -141,7 +141,7 @@ nodemask_t
 The size of a nodemask_t type. Used to compute the number of online
 nodes.
 
-(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_head)
+(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_info)
 ----------------------------------------------------------------------------------
 
 User-space tools compute their values based on the offset of these
diff --git a/Documentation/mm/vmemmap_dedup.rst b/Documentation/mm/vmemmap_dedup.rst
index b4a55b6569fa..1863d88d2dcb 100644
--- a/Documentation/mm/vmemmap_dedup.rst
+++ b/Documentation/mm/vmemmap_dedup.rst
@@ -24,7 +24,7 @@ For each base page, there is a corresponding ``struct page``.
 Within the HugeTLB subsystem, only the first 4 ``struct page`` are used to
 contain unique information about a HugeTLB page. ``__NR_USED_SUBPAGE`` provides
 this upper limit. The only &#x27;useful&#x27; information in the remaining ``struct page``
-is the compound_head field, and this field is the same for all tail pages.
+is the compound_info field, and this field is the same for all tail pages.
 
 By removing redundant ``struct page`` for HugeTLB pages, memory can be returned
 to the buddy allocator for other uses.
@@ -124,10 +124,10 @@ Here is how things look before optimization::
  |           |
  +-----------+
 
-The value of page-&gt;compound_head is the same for all tail pages. The first
+The value of page-&gt;compound_info is the same for all tail pages. The first
 page of ``struct page`` (page 0) associated with the HugeTLB page contains the 4
 ``struct page`` necessary to describe the HugeTLB. The only use of the remaining
-pages of ``struct page`` (page 1 to page 7) is to point to page-&gt;compound_head.
+pages of ``struct page`` (page 1 to page 7) is to point to page-&gt;compound_info.
 Therefore, we can remap pages 1 to 7 to page 0. Only 1 page of ``struct page``
 will be used for each HugeTLB page. This will allow us to free the remaining
 7 pages to the buddy allocator.
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 3cc8ae722886..7bc82a2b889f 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -126,14 +126,14 @@ struct page {
 			atomic_long_t pp_ref_count;
 		};
 		struct {	/* Tail pages of compound page */
-			unsigned long compound_head;	/* Bit zero is set */
+			unsigned long compound_info;	/* Bit zero is set */
 		};
 		struct {	/* ZONE_DEVICE pages */
 			/*
-			 * The first word is used for compound_head or folio
+			 * The first word is used for compound_info or folio
 			 * pgmap
 			 */
-			void *_unused_pgmap_compound_head;
+			void *_unused_pgmap_compound_info;
 			void *zone_device_data;
 			/*
 			 * ZONE_DEVICE private pages are counted as being
@@ -409,7 +409,7 @@ struct folio {
 	/* private: avoid cluttering the output */
 				/* For the Unevictable &quot;LRU list&quot; slot */
 				struct {
-					/* Avoid compound_head */
+					/* Avoid compound_info */
 					void *__filler;
 	/* public: */
 					unsigned int mlock_count;
@@ -510,7 +510,7 @@ struct folio {
 FOLIO_MATCH(flags, flags);
 FOLIO_MATCH(lru, lru);
 FOLIO_MATCH(mapping, mapping);
-FOLIO_MATCH(compound_head, lru);
+FOLIO_MATCH(compound_info, lru);
 FOLIO_MATCH(__folio_index, index);
 FOLIO_MATCH(private, private);
 FOLIO_MATCH(_mapcount, _mapcount);
@@ -529,7 +529,7 @@ FOLIO_MATCH(_last_cpupid, _last_cpupid);
 	static_assert(offsetof(struct folio, fl) ==			\
 			offsetof(struct page, pg) + sizeof(struct page))
 FOLIO_MATCH(flags, _flags_1);
-FOLIO_MATCH(compound_head, _head_1);
+FOLIO_MATCH(compound_info, _head_1);
 FOLIO_MATCH(_mapcount, _mapcount_1);
 FOLIO_MATCH(_refcount, _refcount_1);
 #undef FOLIO_MATCH
@@ -537,13 +537,13 @@ FOLIO_MATCH(_refcount, _refcount_1);
 	static_assert(offsetof(struct folio, fl) ==			\
 			offsetof(struct page, pg) + 2 * sizeof(struct page))
 FOLIO_MATCH(flags, _flags_2);
-FOLIO_MATCH(compound_head, _head_2);
+FOLIO_MATCH(compound_info, _head_2);
 #undef FOLIO_MATCH
 #define FOLIO_MATCH(pg, fl)						\
 	static_assert(offsetof(struct folio, fl) ==			\
 			offsetof(struct page, pg) + 3 * sizeof(struct page))
 FOLIO_MATCH(flags, _flags_3);
-FOLIO_MATCH(compound_head, _head_3);
+FOLIO_MATCH(compound_info, _head_3);
 #undef FOLIO_MATCH
 
 /**
@@ -609,8 +609,8 @@ struct ptdesc {
 #define TABLE_MATCH(pg, pt)						\
 	static_assert(offsetof(struct page, pg) == offsetof(struct ptdesc, pt))
 TABLE_MATCH(flags, pt_flags);
-TABLE_MATCH(compound_head, pt_list);
-TABLE_MATCH(compound_head, _pt_pad_1);
+TABLE_MATCH(compound_info, pt_list);
+TABLE_MATCH(compound_info, _pt_pad_1);
 TABLE_MATCH(mapping, __page_mapping);
 TABLE_MATCH(__folio_index, pt_index);
 TABLE_MATCH(rcu_head, pt_rcu_head);
diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
index 8a3694369e15..aa46d49e82f7 100644
--- a/include/linux/page-flags.h
+++ b/include/linux/page-flags.h
@@ -213,7 +213,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page
 	/*
 	 * Only addresses aligned with PAGE_SIZE of struct page may be fake head
 	 * struct page. The alignment check aims to avoid access the fields (
-	 * e.g. compound_head) of the @page[1]. It can avoid touch a (possibly)
+	 * e.g. compound_info) of the @page[1]. It can avoid touch a (possibly)
 	 * cold cacheline in some cases.
 	 */
 	if (IS_ALIGNED((unsigned long)page, PAGE_SIZE) &amp;&amp;
@@ -223,7 +223,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page
 		 * because the @page is a compound page composed with at least
 		 * two contiguous pages.
 		 */
-		unsigned long head = READ_ONCE(page[1].compound_head);
+		unsigned long head = READ_ONCE(page[1].compound_info);
 
 		if (likely(head &amp; 1))
 			return (const struct page *)(head - 1);
@@ -281,7 +281,7 @@ static __always_inline int page_is_fake_head(const struct page *page)
 
 static __always_inline unsigned long _compound_head(const struct page *page)
 {
-	unsigned long head = READ_ONCE(page-&gt;compound_head);
+	unsigned long head = READ_ONCE(page-&gt;compound_info);
 
 	if (unlikely(head &amp; 1))
 		return head - 1;
@@ -320,13 +320,13 @@ static __always_inline unsigned long _compound_head(const struct page *page)
 
 static __always_inline int PageTail(const struct page *page)
 {
-	return READ_ONCE(page-&gt;compound_head) &amp; 1 || page_is_fake_head(page);
+	return READ_ONCE(page-&gt;compound_info) &amp; 1 || page_is_fake_head(page);
 }
 
 static __always_inline int PageCompound(const struct page *page)
 {
 	return test_bit(PG_head, &amp;page-&gt;flags.f) ||
-	       READ_ONCE(page-&gt;compound_head) &amp; 1;
+	       READ_ONCE(page-&gt;compound_info) &amp; 1;
 }
 
 #define	PAGE_POISON_PATTERN	-1l
@@ -348,7 +348,7 @@ static const unsigned long *const_folio_flags(const struct folio *folio,
 {
 	const struct page *page = &amp;folio-&gt;page;
 
-	VM_BUG_ON_PGFLAGS(page-&gt;compound_head &amp; 1, page);
+	VM_BUG_ON_PGFLAGS(page-&gt;compound_info &amp; 1, page);
 	VM_BUG_ON_PGFLAGS(n &gt; 0 &amp;&amp; !test_bit(PG_head, &amp;page-&gt;flags.f), page);
 	return &amp;page[n].flags.f;
 }
@@ -357,7 +357,7 @@ static unsigned long *folio_flags(struct folio *folio, unsigned n)
 {
 	struct page *page = &amp;folio-&gt;page;
 
-	VM_BUG_ON_PGFLAGS(page-&gt;compound_head &amp; 1, page);
+	VM_BUG_ON_PGFLAGS(page-&gt;compound_info &amp; 1, page);
 	VM_BUG_ON_PGFLAGS(n &gt; 0 &amp;&amp; !test_bit(PG_head, &amp;page-&gt;flags.f), page);
 	return &amp;page[n].flags.f;
 }
@@ -869,12 +869,12 @@ static __always_inline void set_compound_head(struct page *page,
 					      const struct page *head,
 					      unsigned int order)
 {
-	WRITE_ONCE(page-&gt;compound_head, (unsigned long)head + 1);
+	WRITE_ONCE(page-&gt;compound_info, (unsigned long)head + 1);
 }
 
 static __always_inline void clear_compound_head(struct page *page)
 {
-	WRITE_ONCE(page-&gt;compound_head, 0);
+	WRITE_ONCE(page-&gt;compound_info, 0);
 }
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
diff --git a/include/linux/types.h b/include/linux/types.h
index f69be881369f..604697abf151 100644
--- a/include/linux/types.h
+++ b/include/linux/types.h
@@ -234,7 +234,7 @@ struct ustat {
  *
  * This guarantee is important for few reasons:
  *  - future call_rcu_lazy() will make use of lower bits in the pointer;
- *  - the structure shares storage space in struct page with @compound_head,
+ *  - the structure shares storage space in struct page with @compound_info,
  *    which encode PageTail() in bit 0. The guarantee is needed to avoid
  *    false-positive PageTail().
  */
diff --git a/kernel/vmcore_info.c b/kernel/vmcore_info.c
index 46198580373a..0a46df3e3db9 100644
--- a/kernel/vmcore_info.c
+++ b/kernel/vmcore_info.c
@@ -198,7 +198,7 @@ static int __init crash_save_vmcoreinfo_init(void)
 	VMCOREINFO_OFFSET(page, lru);
 	VMCOREINFO_OFFSET(page, _mapcount);
 	VMCOREINFO_OFFSET(page, private);
-	VMCOREINFO_OFFSET(page, compound_head);
+	VMCOREINFO_OFFSET(page, compound_info);
 	VMCOREINFO_OFFSET(pglist_data, node_zones);
 	VMCOREINFO_OFFSET(pglist_data, nr_zones);
 #ifdef CONFIG_FLATMEM
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 00c7ea958767..cb7375eb1713 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -731,7 +731,7 @@ static inline bool pcp_allowed_order(unsigned int order)
  * The first PAGE_SIZE page is called the &quot;head page&quot; and have PG_head set.
  *
  * The remaining PAGE_SIZE pages are called &quot;tail pages&quot;. PageTail() is encoded
- * in bit 0 of page-&gt;compound_head. The rest of bits is pointer to head page.
+ * in bit 0 of page-&gt;compound_info. The rest of bits is pointer to head page.
  *
  * The first tail page&#x27;s -&gt;compound_order holds the order of allocation.
  * This usage means that zero-order pages may not be compound.
diff --git a/mm/slab.h b/mm/slab.h
index e767aa7e91b0..8a2a9c6c697b 100644
--- a/mm/slab.h
+++ b/mm/slab.h
@@ -100,7 +100,7 @@ struct slab {
 #define SLAB_MATCH(pg, sl)						\
 	static_assert(offsetof(struct page, pg) == offsetof(struct slab, sl))
 SLAB_MATCH(flags, flags);
-SLAB_MATCH(compound_head, slab_cache);	/* Ensure bit 0 is clear */
+SLAB_MATCH(compound_info, slab_cache);	/* Ensure bit 0 is clear */
 SLAB_MATCH(_refcount, __page_refcount);
 #ifdef CONFIG_MEMCG
 SLAB_MATCH(memcg_data, obj_exts);
diff --git a/mm/util.c b/mm/util.c
index b05ab6f97e11..3ebcb9e6035c 100644
--- a/mm/util.c
+++ b/mm/util.c
@@ -1247,7 +1247,7 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)
 again:
 	memset(&amp;ps-&gt;folio_snapshot, 0, sizeof(struct folio));
 	memcpy(&amp;ps-&gt;page_snapshot, page, sizeof(*page));
-	head = ps-&gt;page_snapshot.compound_head;
+	head = ps-&gt;page_snapshot.compound_info;
 	if ((head &amp; 1) == 0) {
 		ps-&gt;idx = 0;
 		foliop = (struct folio *)&amp;ps-&gt;page_snapshot;
-- 
2.51.2

---

Move set_compound_head() and clear_compound_head() to be adjacent to the
compound_head() function in page-flags.h.

These functions encode and decode the same compound_info field, so
keeping them together makes it easier to verify their logic is
consistent, especially when the encoding changes.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
---
 include/linux/page-flags.h | 24 ++++++++++++------------
 1 file changed, 12 insertions(+), 12 deletions(-)

diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
index aa46d49e82f7..d14a17ffb55b 100644
--- a/include/linux/page-flags.h
+++ b/include/linux/page-flags.h
@@ -290,6 +290,18 @@ static __always_inline unsigned long _compound_head(const struct page *page)
 
 #define compound_head(page)	((typeof(page))_compound_head(page))
 
+static __always_inline void set_compound_head(struct page *page,
+					      const struct page *head,
+					      unsigned int order)
+{
+	WRITE_ONCE(page-&gt;compound_info, (unsigned long)head + 1);
+}
+
+static __always_inline void clear_compound_head(struct page *page)
+{
+	WRITE_ONCE(page-&gt;compound_info, 0);
+}
+
 /**
  * page_folio - Converts from page to folio.
  * @p: The page.
@@ -865,18 +877,6 @@ static inline bool folio_test_large(const struct folio *folio)
 	return folio_test_head(folio);
 }
 
-static __always_inline void set_compound_head(struct page *page,
-					      const struct page *head,
-					      unsigned int order)
-{
-	WRITE_ONCE(page-&gt;compound_info, (unsigned long)head + 1);
-}
-
-static __always_inline void clear_compound_head(struct page *page)
-{
-	WRITE_ONCE(page-&gt;compound_info, 0);
-}
-
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 static inline void ClearPageCompound(struct page *page)
 {
-- 
2.51.2

---

The upcoming change to the HugeTLB vmemmap optimization (HVO) requires
struct pages of the head page to be naturally aligned with regard to the
folio size.

Align vmemmap to MAX_FOLIO_NR_PAGES.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
---
 arch/riscv/mm/init.c | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/arch/riscv/mm/init.c b/arch/riscv/mm/init.c
index 21d534824624..c555b9a4fdce 100644
--- a/arch/riscv/mm/init.c
+++ b/arch/riscv/mm/init.c
@@ -63,7 +63,8 @@ phys_addr_t phys_ram_base __ro_after_init;
 EXPORT_SYMBOL(phys_ram_base);
 
 #ifdef CONFIG_SPARSEMEM_VMEMMAP
-#define VMEMMAP_ADDR_ALIGN	(1ULL &lt;&lt; SECTION_SIZE_BITS)
+#define VMEMMAP_ADDR_ALIGN	max(1ULL &lt;&lt; SECTION_SIZE_BITS, \
+				    MAX_FOLIO_NR_PAGES * sizeof(struct page))
 
 unsigned long vmemmap_start_pfn __ro_after_init;
 EXPORT_SYMBOL(vmemmap_start_pfn);
-- 
2.51.2</pre>
</details>
<a href="https://lore.kernel.org/r/20260202155634.650837-2-kas@kernel.org" target="_blank" rel="noopener" class="lore-link">View on lore &#8599;</a>
<div class="review-comment-signals">Signals: preparation, change</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Kiryl Shutsemau (author)</span>
<a class="date-chip" href="../2026-02-27_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-02">2026-02-02</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed a concern about the alignment of struct pages in HugeTLB vmemmap optimization, explained that for cases where sizeof(struct page) is power-of-2, they plan to change the encoding of compound_info to store a mask that can be applied to the virtual address of the tail page to access the head page. This modification will allow all tail pages of the same order to have identical &#x27;compound_info&#x27;, regardless of the compound page they are associated with, paving the way for eliminating fake heads.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">The upcoming change to the HugeTLB vmemmap optimization (HVO) requires
struct pages of the head page to be naturally aligned with regard to the
folio size.

Align vmemmap to MAX_FOLIO_NR_PAGES.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
---
 arch/loongarch/include/asm/pgtable.h | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h
index c33b3bcb733e..f9416acb9156 100644
--- a/arch/loongarch/include/asm/pgtable.h
+++ b/arch/loongarch/include/asm/pgtable.h
@@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
 	 min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL &lt;&lt; cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)
 #endif
 
-#define vmemmap		((struct page *)((VMALLOC_END + PMD_SIZE) &amp; PMD_MASK))
+#define VMEMMAP_ALIGN	max(PMD_SIZE, MAX_FOLIO_NR_PAGES * sizeof(struct page))
+#define vmemmap		((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))
 #define VMEMMAP_END	((unsigned long)vmemmap + VMEMMAP_SIZE - 1)
 
 #define KFENCE_AREA_START	(VMEMMAP_END + 1)
-- 
2.51.2

---

For tail pages, the kernel uses the &#x27;compound_info&#x27; field to get to the
head page. The bit 0 of the field indicates whether the page is a
tail page, and if set, the remaining bits represent a pointer to the
head page.

For cases when size of struct page is power-of-2, change the encoding of
compound_info to store a mask that can be applied to the virtual address
of the tail page in order to access the head page. It is possible
because struct page of the head page is naturally aligned with regards
to order of the page.

The significant impact of this modification is that all tail pages of
the same order will now have identical &#x27;compound_info&#x27;, regardless of
the compound page they are associated with. This paves the way for
eliminating fake heads.

The HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only
applied when the sizeof(struct page) is power-of-2. Having identical
tail pages allows the same page to be mapped into the vmemmap of all
pages, maintaining memory savings without fake heads.

If sizeof(struct page) is not power-of-2, there is no functional
changes.

Limit mask usage to HugeTLB vmemmap optimization (HVO) where it makes
a difference. The approach with mask would work in the wider set of
conditions, but it requires validating that struct pages are naturally
aligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
---
 include/linux/page-flags.h | 81 ++++++++++++++++++++++++++++++++++----
 mm/slab.h                  | 16 ++++++--
 mm/util.c                  | 16 ++++++--
 3 files changed, 97 insertions(+), 16 deletions(-)

diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
index d14a17ffb55b..8f2c7fbc739b 100644
--- a/include/linux/page-flags.h
+++ b/include/linux/page-flags.h
@@ -198,6 +198,29 @@ enum pageflags {
 
 #ifndef __GENERATING_BOUNDS_H
 
+/*
+ * For tail pages, if the size of struct page is power-of-2 -&gt;compound_info
+ * encodes the mask that converts the address of the tail page address to
+ * the head page address.
+ *
+ * Otherwise, -&gt;compound_info has direct pointer to head pages.
+ */
+static __always_inline bool compound_info_has_mask(void)
+{
+	/*
+	 * Limit mask usage to HugeTLB vmemmap optimization (HVO) where it
+	 * makes a difference.
+	 *
+	 * The approach with mask would work in the wider set of conditions,
+	 * but it requires validating that struct pages are naturally aligned
+	 * for all orders up to the MAX_FOLIO_ORDER, which can be tricky.
+	 */
+	if (!IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP))
+		return false;
+
+	return is_power_of_2(sizeof(struct page));
+}
+
 #ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP
 DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);
 
@@ -210,6 +233,10 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page
 	if (!static_branch_unlikely(&amp;hugetlb_optimize_vmemmap_key))
 		return page;
 
+	/* Fake heads only exists if compound_info_has_mask() is true */
+	if (!compound_info_has_mask())
+		return page;
+
 	/*
 	 * Only addresses aligned with PAGE_SIZE of struct page may be fake head
 	 * struct page. The alignment check aims to avoid access the fields (
@@ -223,10 +250,14 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page
 		 * because the @page is a compound page composed with at least
 		 * two contiguous pages.
 		 */
-		unsigned long head = READ_ONCE(page[1].compound_info);
+		unsigned long info = READ_ONCE(page[1].compound_info);
 
-		if (likely(head &amp; 1))
-			return (const struct page *)(head - 1);
+		/* See set_compound_head() */
+		if (likely(info &amp; 1)) {
+			unsigned long p = (unsigned long)page;
+
+			return (const struct page *)(p &amp; info);
+		}
 	}
 	return page;
 }
@@ -281,11 +312,26 @@ static __always_inline int page_is_fake_head(const struct page *page)
 
 static __always_inline unsigned long _compound_head(const struct page *page)
 {
-	unsigned long head = READ_ONCE(page-&gt;compound_info);
+	unsigned long info = READ_ONCE(page-&gt;compound_info);
 
-	if (unlikely(head &amp; 1))
-		return head - 1;
-	return (unsigned long)page_fixed_fake_head(page);
+	/* Bit 0 encodes PageTail() */
+	if (!(info &amp; 1))
+		return (unsigned long)page_fixed_fake_head(page);
+
+	/*
+	 * If compound_info_has_mask() is false, the rest of compound_info is
+	 * the pointer to the head page.
+	 */
+	if (!compound_info_has_mask())
+		return info - 1;
+
+	/*
+	 * If compoun_info_has_mask() is true the rest of the info encodes
+	 * the mask that converts the address of the tail page to the head page.
+	 *
+	 * No need to clear bit 0 in the mask as &#x27;page&#x27; always has it clear.
+	 */
+	return (unsigned long)page &amp; info;
 }
 
 #define compound_head(page)	((typeof(page))_compound_head(page))
@@ -294,7 +340,26 @@ static __always_inline void set_compound_head(struct page *page,
 					      const struct page *head,
 					      unsigned int order)
 {
-	WRITE_ONCE(page-&gt;compound_info, (unsigned long)head + 1);
+	unsigned int shift;
+	unsigned long mask;
+
+	if (!compound_info_has_mask()) {
+		WRITE_ONCE(page-&gt;compound_info, (unsigned long)head | 1);
+		return;
+	}
+
+	/*
+	 * If the size of struct page is power-of-2, bits [shift:0] of the
+	 * virtual address of compound head are zero.
+	 *
+	 * Calculate mask that can be applied to the virtual address of
+	 * the tail page to get address of the head page.
+	 */
+	shift = order + order_base_2(sizeof(struct page));
+	mask = GENMASK(BITS_PER_LONG - 1, shift);
+
+	/* Bit 0 encodes PageTail() */
+	WRITE_ONCE(page-&gt;compound_info, mask | 1);
 }
 
 static __always_inline void clear_compound_head(struct page *page)
diff --git a/mm/slab.h b/mm/slab.h
index 8a2a9c6c697b..f68c3ac8126f 100644
--- a/mm/slab.h
+++ b/mm/slab.h
@@ -137,11 +137,19 @@ static_assert(IS_ALIGNED(offsetof(struct slab, freelist), sizeof(struct freelist
  */
 static inline struct slab *page_slab(const struct page *page)
 {
-	unsigned long head;
+	unsigned long info;
+
+	info = READ_ONCE(page-&gt;compound_info);
+	if (info &amp; 1) {
+		/* See compound_head() */
+		if (compound_info_has_mask()) {
+			unsigned long p = (unsigned long)page;
+			page = (struct page *)(p &amp; info);
+		} else {
+			page = (struct page *)(info - 1);
+		}
+	}
 
-	head = READ_ONCE(page-&gt;compound_head);
-	if (head &amp; 1)
-		page = (struct page *)(head - 1);
 	if (data_race(page-&gt;page_type &gt;&gt; 24) != PGTY_slab)
 		page = NULL;
 
diff --git a/mm/util.c b/mm/util.c
index 3ebcb9e6035c..20dccf2881d7 100644
--- a/mm/util.c
+++ b/mm/util.c
@@ -1237,7 +1237,7 @@ static void set_ps_flags(struct page_snapshot *ps, const struct folio *folio,
  */
 void snapshot_page(struct page_snapshot *ps, const struct page *page)
 {
-	unsigned long head, nr_pages = 1;
+	unsigned long info, nr_pages = 1;
 	struct folio *foliop;
 	int loops = 5;
 
@@ -1247,8 +1247,8 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)
 again:
 	memset(&amp;ps-&gt;folio_snapshot, 0, sizeof(struct folio));
 	memcpy(&amp;ps-&gt;page_snapshot, page, sizeof(*page));
-	head = ps-&gt;page_snapshot.compound_info;
-	if ((head &amp; 1) == 0) {
+	info = ps-&gt;page_snapshot.compound_info;
+	if (!(info &amp; 1)) {
 		ps-&gt;idx = 0;
 		foliop = (struct folio *)&amp;ps-&gt;page_snapshot;
 		if (!folio_test_large(foliop)) {
@@ -1259,7 +1259,15 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)
 		}
 		foliop = (struct folio *)page;
 	} else {
-		foliop = (struct folio *)(head - 1);
+		/* See compound_head() */
+		if (compound_info_has_mask()) {
+			unsigned long p = (unsigned long)page;
+
+			foliop = (struct folio *)(p &amp; info);
+		} else {
+			foliop = (struct folio *)(info - 1);
+		}
+
 		ps-&gt;idx = folio_page_idx(foliop, page);
 	}
 
-- 
2.51.2

---

With the upcoming changes to HVO, a single page of tail struct pages
will be shared across all huge pages of the same order on a node. Since
huge pages on the same node may belong to different zones, the zone
information stored in shared tail page flags would be incorrect.

Always fetch zone information from the head page, which has unique and
correct zone flags for each compound page.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
---
 include/linux/mmzone.h | 1 +
 1 file changed, 1 insertion(+)

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index be8ce40b5638..192143b5cdc0 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)
 
 static inline enum zone_type page_zonenum(const struct page *page)
 {
+	page = compound_head(page);
 	return memdesc_zonenum(page-&gt;flags);
 }
 
-- 
2.51.2

---

If page-&gt;compound_info encodes a mask, it is expected that vmemmap to be
naturally aligned to the maximum folio size.

Add a VM_BUG_ON() to check the alignment.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
---
 mm/sparse.c | 7 +++++++
 1 file changed, 7 insertions(+)

diff --git a/mm/sparse.c b/mm/sparse.c
index b5b2b6f7041b..6c9b62607f3f 100644
--- a/mm/sparse.c
+++ b/mm/sparse.c
@@ -600,6 +600,13 @@ void __init sparse_init(void)
 	BUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));
 	memblocks_present();
 
+	if (compound_info_has_mask()) {
+		unsigned long alignment;
+
+		alignment = MAX_FOLIO_NR_PAGES * sizeof(struct page);
+		VM_BUG_ON(!IS_ALIGNED((unsigned long) pfn_to_page(0), alignment));
+	}
+
 	pnum_begin = first_present_section_nr();
 	nid_begin = sparse_early_nid(__nr_to_section(pnum_begin));
 
-- 
2.51.2

---

This series removes &quot;fake head pages&quot; from the HugeTLB vmemmap
optimization (HVO) by changing how tail pages encode their relationship
to the head page.

It simplifies compound_head() and page_ref_add_unless(). Both are in the
hot path.

Background
==========

HVO reduces memory overhead by freeing vmemmap pages for HugeTLB pages
and remapping the freed virtual addresses to a single physical page.
Previously, all tail page vmemmap entries were remapped to the first
vmemmap page (containing the head struct page), creating &quot;fake heads&quot; -
tail pages that appear to have PG_head set when accessed through the
deduplicated vmemmap.

This required special handling in compound_head() to detect and work
around fake heads, adding complexity and overhead to a very hot path.

New Approach
============

For architectures/configs where sizeof(struct page) is a power of 2 (the
common case), this series changes how position of the head page is encoded
in the tail pages.

Instead of storing a pointer to the head page, the -&gt;compound_info
(renamed from -&gt;compound_head) now stores a mask.

The mask can be applied to any tail page&#x27;s virtual address to compute
the head page address. Critically, all tail pages of the same order now
have identical compound_info values, regardless of which compound page
they belong to.

The key insight is that all tail pages of the same order now have
identical compound_info values, regardless of which compound page they
belong to. This allows a single page of tail struct pages to be shared
across all huge pages of the same order on a NUMA node.

Benefits
========

1. Simplified compound_head(): No fake head detection needed, can be
   implemented in a branchless manner.

2. Simplified page_ref_add_unless(): RCU protection removed since there&#x27;s
   no race with fake head remapping.

3. Cleaner architecture: The shared tail pages are truly read-only and
   contain valid tail page metadata.

If sizeof(struct page) is not power-of-2, there are no functional changes.
HVO is not supported in this configuration.

I had hoped to see performance improvement, but my testing thus far has
shown either no change or only a slight improvement within the noise.

Series Organization
===================

Patch 1: Preparation - move MAX_FOLIO_ORDER to mmzone.h
Patches 2-4: Refactoring - interface changes, field rename, code movement
Patches 5-6: Arch fixes - align vmemmap for riscv and LoongArch
Patch 7: Core change - new mask-based compound_head() encoding
Patch 8: Correctness fix - page_zonenum() must use head page
Patch 9: Add memmap alignment check for compound_info_has_mask()
Patch 10: Refactor vmemmap_walk for new design
Patch 11: Eliminate fake heads with shared tail pages
Patches 12-15: Cleanup - remove fake head infrastructure
Patch 16: Documentation update
Patch 17: Get rid of opencoded compound_head() in page_slab()

Changes in v6:
==============
  - Simplify memmap alignment check in mm/sparse.c: use VM_BUG_ON()
    (Muchun)

  - Store struct page pointers in vmemmap_tails[] instead of PFNs.
    (Muchun)

  - Fix build error on powerpc due to negative NR_VMEMMAP_TAILS.

Changes in v5:
==============
  - Rebased to mm-everything-2026-01-27-04-35

  - Add arch-specific patches to align vmemmap to maximal folio size
    for riscv and LoongArch architectures.

  - Strengthen the memmap alignment check in mm/sparse.c: use BUG()
    for CONFIG_DEBUG_VM, WARN() otherwise. (Muchun)

  - Use cmpxchg() instead of hugetlb_lock to update vmemmap_tails
    array. (Muchun)

  - Update page_slab().

Changes in v4:
==============
  - Fix build issues due to linux/mmzone.h &lt;-&gt; linux/pgtable.h
    dependency loop by avoiding including linux/pgtable.h into
    linux/mmzone.h

  - Rework vmemmap_remap_alloc() interface. (Muchun)

  - Use &amp;folio-&gt;page instead of folio address for optimization
    target. (Muchun)

Changes in v3:
==============
  - Fixed error recovery path in vmemmap_remap_free() to pass correct start
    address for TLB flush. (Muchun)

  - Wrapped the mask-based compound_info encoding within CONFIG_SPARSEMEM_VMEMMAP
    check via compound_info_has_mask(). For other memory models, alignment
    guarantees are harder to verify. (Muchun)

  - Updated vmemmap_dedup.rst documentation wording: changed &quot;vmemmap_tail
    shared for the struct hstate&quot; to &quot;A single, per-node page frame shared
    among all hugepages of the same size&quot;. (Muchun)

  - Fixed build error with MAX_FOLIO_ORDER expanding to undefined PUD_ORDER
    in certain configurations. (kernel test robot)

Changes in v2:
==============

- Handle boot-allocated huge pages correctly. (Frank)

- Changed from per-hstate vmemmap_tail to per-node vmemmap_tails[] array
  in pglist_data. (Muchun)

- Added spin_lock(&amp;hugetlb_lock) protection in vmemmap_get_tail() to fix
  a race condition where two threads could both allocate tail pages.
  The losing thread now properly frees its allocated page. (Usama)

- Add warning if memmap is not aligned to MAX_FOLIO_SIZE, which is
  required for the mask approach. (Muchun)

- Make page_zonenum() use head page - correctness fix since shared
  tail pages cannot have valid zone information. (Muchun)

- Added &#x27;const&#x27; qualifier to head parameter in set_compound_head() and
  prep_compound_tail(). (Usama)

- Updated commit messages.

Kiryl Shutsemau (17):
  mm: Move MAX_FOLIO_ORDER definition to mmzone.h
  mm: Change the interface of prep_compound_tail()
  mm: Rename the &#x27;compound_head&#x27; field in the &#x27;struct page&#x27; to
    &#x27;compound_info&#x27;
  mm: Move set/clear_compound_head() next to compound_head()
  riscv/mm: Align vmemmap to maximal folio size
  LoongArch/mm: Align vmemmap to maximal folio size
  mm: Rework compound_head() for power-of-2 sizeof(struct page)
  mm: Make page_zonenum() use head page
  mm/sparse: Check memmap alignment for compound_info_has_mask()
  mm/hugetlb: Refactor code around vmemmap_walk
  mm/hugetlb: Remove fake head pages
  mm: Drop fake head checks
  hugetlb: Remove VMEMMAP_SYNCHRONIZE_RCU
  mm/hugetlb: Remove hugetlb_optimize_vmemmap_key static key
  mm: Remove the branch from compound_head()
  hugetlb: Update vmemmap_dedup.rst
  mm/slab: Use compound_head() in page_slab()

 .../admin-guide/kdump/vmcoreinfo.rst          |   2 +-
 Documentation/mm/vmemmap_dedup.rst            |  62 ++--
 arch/loongarch/include/asm/pgtable.h          |   3 +-
 arch/riscv/mm/init.c                          |   3 +-
 include/linux/mm.h                            |  31 --
 include/linux/mm_types.h                      |  20 +-
 include/linux/mmzone.h                        |  47 +++
 include/linux/page-flags.h                    | 167 +++++-----
 include/linux/page_ref.h                      |   8 +-
 include/linux/types.h                         |   2 +-
 kernel/vmcore_info.c                          |   2 +-
 mm/hugetlb.c                                  |   8 +-
 mm/hugetlb_vmemmap.c                          | 288 ++++++++----------
 mm/internal.h                                 |  12 +-
 mm/mm_init.c                                  |   2 +-
 mm/page_alloc.c                               |   4 +-
 mm/slab.h                                     |   8 +-
 mm/sparse-vmemmap.c                           |  43 ++-
 mm/sparse.c                                   |   7 +
 mm/util.c                                     |  16 +-
 20 files changed, 363 insertions(+), 372 deletions(-)

-- 
2.51.2


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

Move MAX_FOLIO_ORDER definition from mm.h to mmzone.h.

This is preparation for adding the vmemmap_tails array to struct
pglist_data, which requires MAX_FOLIO_ORDER to be available in mmzone.h.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Acked-by: David Hildenbrand (Red Hat) &lt;david@kernel.org&gt;
Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
Acked-by: Muchun Song &lt;muchun.song@linux.dev&gt;
---
 include/linux/mm.h     | 31 -------------------------------
 include/linux/mmzone.h | 31 +++++++++++++++++++++++++++++++
 2 files changed, 31 insertions(+), 31 deletions(-)

diff --git a/include/linux/mm.h b/include/linux/mm.h
index f8a8fd47399c..8d5fa655fea4 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -27,7 +27,6 @@
 #include &lt;linux/page-flags.h&gt;
 #include &lt;linux/page_ref.h&gt;
 #include &lt;linux/overflow.h&gt;
-#include &lt;linux/sizes.h&gt;
 #include &lt;linux/sched.h&gt;
 #include &lt;linux/pgtable.h&gt;
 #include &lt;linux/kasan.h&gt;
@@ -2477,36 +2476,6 @@ static inline unsigned long folio_nr_pages(const struct folio *folio)
 	return folio_large_nr_pages(folio);
 }
 
-#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)
-/*
- * We don&#x27;t expect any folios that exceed buddy sizes (and consequently
- * memory sections).
- */
-#define MAX_FOLIO_ORDER		MAX_PAGE_ORDER
-#elif defined(CONFIG_SPARSEMEM) &amp;&amp; !defined(CONFIG_SPARSEMEM_VMEMMAP)
-/*
- * Only pages within a single memory section are guaranteed to be
- * contiguous. By limiting folios to a single memory section, all folio
- * pages are guaranteed to be contiguous.
- */
-#define MAX_FOLIO_ORDER		PFN_SECTION_SHIFT
-#elif defined(CONFIG_HUGETLB_PAGE)
-/*
- * There is no real limit on the folio size. We limit them to the maximum we
- * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect
- * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.
- */
-#define MAX_FOLIO_ORDER		get_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)
-#else
-/*
- * Without hugetlb, gigantic folios that are bigger than a single PUD are
- * currently impossible.
- */
-#define MAX_FOLIO_ORDER		PUD_ORDER
-#endif
-
-#define MAX_FOLIO_NR_PAGES	(1UL &lt;&lt; MAX_FOLIO_ORDER)
-
 /*
  * compound_nr() returns the number of pages in this potentially compound
  * page.  compound_nr() can be called on a tail page, and is defined to
diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 3e51190a55e4..be8ce40b5638 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -23,6 +23,7 @@
 #include &lt;linux/page-flags.h&gt;
 #include &lt;linux/local_lock.h&gt;
 #include &lt;linux/zswap.h&gt;
+#include &lt;linux/sizes.h&gt;
 #include &lt;asm/page.h&gt;
 
 /* Free memory management - zoned buddy allocator.  */
@@ -61,6 +62,36 @@
  */
 #define PAGE_ALLOC_COSTLY_ORDER 3
 
+#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)
+/*
+ * We don&#x27;t expect any folios that exceed buddy sizes (and consequently
+ * memory sections).
+ */
+#define MAX_FOLIO_ORDER		MAX_PAGE_ORDER
+#elif defined(CONFIG_SPARSEMEM) &amp;&amp; !defined(CONFIG_SPARSEMEM_VMEMMAP)
+/*
+ * Only pages within a single memory section are guaranteed to be
+ * contiguous. By limiting folios to a single memory section, all folio
+ * pages are guaranteed to be contiguous.
+ */
+#define MAX_FOLIO_ORDER		PFN_SECTION_SHIFT
+#elif defined(CONFIG_HUGETLB_PAGE)
+/*
+ * There is no real limit on the folio size. We limit them to the maximum we
+ * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect
+ * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.
+ */
+#define MAX_FOLIO_ORDER		get_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)
+#else
+/*
+ * Without hugetlb, gigantic folios that are bigger than a single PUD are
+ * currently impossible.
+ */
+#define MAX_FOLIO_ORDER		PUD_ORDER
+#endif
+
+#define MAX_FOLIO_NR_PAGES	(1UL &lt;&lt; MAX_FOLIO_ORDER)
+
 enum migratetype {
 	MIGRATE_UNMOVABLE,
 	MIGRATE_MOVABLE,
-- 
2.51.2


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv</pre>
</details>
<a href="https://lore.kernel.org/r/20260202155634.650837-7-kas@kernel.org" target="_blank" rel="noopener" class="lore-link">View on lore &#8599;</a>
<div class="review-comment-signals">Signals: clarification, explanation</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Kiryl Shutsemau (author)</span>
<a class="date-chip" href="../2026-02-27_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-02">2026-02-02</a>
<span class="badge" style="color:#155724;background:#d4edda">Positive</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed a concern about the consistency of set_compound_head() and clear_compound_head() logic by moving these functions adjacent to compound_head() in page-flags.h, making it easier to verify their logic is consistent.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Move set_compound_head() and clear_compound_head() to be adjacent to the
compound_head() function in page-flags.h.

These functions encode and decode the same compound_info field, so
keeping them together makes it easier to verify their logic is
consistent, especially when the encoding changes.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
---
 include/linux/page-flags.h | 24 ++++++++++++------------
 1 file changed, 12 insertions(+), 12 deletions(-)

diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
index aa46d49e82f7..d14a17ffb55b 100644
--- a/include/linux/page-flags.h
+++ b/include/linux/page-flags.h
@@ -290,6 +290,18 @@ static __always_inline unsigned long _compound_head(const struct page *page)
 
 #define compound_head(page)	((typeof(page))_compound_head(page))
 
+static __always_inline void set_compound_head(struct page *page,
+					      const struct page *head,
+					      unsigned int order)
+{
+	WRITE_ONCE(page-&gt;compound_info, (unsigned long)head + 1);
+}
+
+static __always_inline void clear_compound_head(struct page *page)
+{
+	WRITE_ONCE(page-&gt;compound_info, 0);
+}
+
 /**
  * page_folio - Converts from page to folio.
  * @p: The page.
@@ -865,18 +877,6 @@ static inline bool folio_test_large(const struct folio *folio)
 	return folio_test_head(folio);
 }
 
-static __always_inline void set_compound_head(struct page *page,
-					      const struct page *head,
-					      unsigned int order)
-{
-	WRITE_ONCE(page-&gt;compound_info, (unsigned long)head + 1);
-}
-
-static __always_inline void clear_compound_head(struct page *page)
-{
-	WRITE_ONCE(page-&gt;compound_info, 0);
-}
-
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 static inline void ClearPageCompound(struct page *page)
 {
-- 
2.51.2


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

Instead of passing down the head page and tail page index, pass the tail
and head pages directly, as well as the order of the compound page.

This is a preparation for changing how the head position is encoded in
the tail page.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
---
 include/linux/page-flags.h |  4 +++-
 mm/hugetlb.c               |  8 +++++---
 mm/internal.h              | 12 ++++++------
 mm/mm_init.c               |  2 +-
 mm/page_alloc.c            |  2 +-
 5 files changed, 16 insertions(+), 12 deletions(-)

diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
index f7a0e4af0c73..8a3694369e15 100644
--- a/include/linux/page-flags.h
+++ b/include/linux/page-flags.h
@@ -865,7 +865,9 @@ static inline bool folio_test_large(const struct folio *folio)
 	return folio_test_head(folio);
 }
 
-static __always_inline void set_compound_head(struct page *page, struct page *head)
+static __always_inline void set_compound_head(struct page *page,
+					      const struct page *head,
+					      unsigned int order)
 {
 	WRITE_ONCE(page-&gt;compound_head, (unsigned long)head + 1);
 }
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 6e855a32de3d..54ba7cd05a86 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -3168,6 +3168,7 @@ int __alloc_bootmem_huge_page(struct hstate *h, int nid)
 
 /* Initialize [start_page:end_page_number] tail struct pages of a hugepage */
 static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,
+					struct hstate *h,
 					unsigned long start_page_number,
 					unsigned long end_page_number)
 {
@@ -3176,6 +3177,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,
 	struct page *page = folio_page(folio, start_page_number);
 	unsigned long head_pfn = folio_pfn(folio);
 	unsigned long pfn, end_pfn = head_pfn + end_page_number;
+	unsigned int order = huge_page_order(h);
 
 	/*
 	 * As we marked all tail pages with memblock_reserved_mark_noinit(),
@@ -3183,7 +3185,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,
 	 */
 	for (pfn = head_pfn + start_page_number; pfn &lt; end_pfn; page++, pfn++) {
 		__init_single_page(page, pfn, zone, nid);
-		prep_compound_tail((struct page *)folio, pfn - head_pfn);
+		prep_compound_tail(page, &amp;folio-&gt;page, order);
 		set_page_count(page, 0);
 	}
 }
@@ -3203,7 +3205,7 @@ static void __init hugetlb_folio_init_vmemmap(struct folio *folio,
 	__folio_set_head(folio);
 	ret = folio_ref_freeze(folio, 1);
 	VM_BUG_ON(!ret);
-	hugetlb_folio_init_tail_vmemmap(folio, 1, nr_pages);
+	hugetlb_folio_init_tail_vmemmap(folio, h, 1, nr_pages);
 	prep_compound_head(&amp;folio-&gt;page, huge_page_order(h));
 }
 
@@ -3260,7 +3262,7 @@ static void __init prep_and_add_bootmem_folios(struct hstate *h,
 			 * time as this is early in boot and there should
 			 * be no contention.
 			 */
-			hugetlb_folio_init_tail_vmemmap(folio,
+			hugetlb_folio_init_tail_vmemmap(folio, h,
 					HUGETLB_VMEMMAP_RESERVE_PAGES,
 					pages_per_huge_page(h));
 		}
diff --git a/mm/internal.h b/mm/internal.h
index d67e8bb75734..037ddcda25ff 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -879,13 +879,13 @@ static inline void prep_compound_head(struct page *page, unsigned int order)
 		INIT_LIST_HEAD(&amp;folio-&gt;_deferred_list);
 }
 
-static inline void prep_compound_tail(struct page *head, int tail_idx)
+static inline void prep_compound_tail(struct page *tail,
+				      const struct page *head,
+				      unsigned int order)
 {
-	struct page *p = head + tail_idx;
-
-	p-&gt;mapping = TAIL_MAPPING;
-	set_compound_head(p, head);
-	set_page_private(p, 0);
+	tail-&gt;mapping = TAIL_MAPPING;
+	set_compound_head(tail, head, order);
+	set_page_private(tail, 0);
 }
 
 void post_alloc_hook(struct page *page, unsigned int order, gfp_t gfp_flags);
diff --git a/mm/mm_init.c b/mm/mm_init.c
index 1a29a719af58..ba50f4c4337b 100644
--- a/mm/mm_init.c
+++ b/mm/mm_init.c
@@ -1099,7 +1099,7 @@ static void __ref memmap_init_compound(struct page *head,
 		struct page *page = pfn_to_page(pfn);
 
 		__init_zone_device_page(page, pfn, zone_idx, nid, pgmap);
-		prep_compound_tail(head, pfn - head_pfn);
+		prep_compound_tail(page, head, order);
 		set_page_count(page, 0);
 	}
 	prep_compound_head(head, order);
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index e4104973e22f..00c7ea958767 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -744,7 +744,7 @@ void prep_compound_page(struct page *page, unsigned int order)
 
 	__SetPageHead(page);
 	for (i = 1; i &lt; nr_pages; i++)
-		prep_compound_tail(page, i);
+		prep_compound_tail(page + i, page, order);
 
 	prep_compound_head(page, order);
 }
-- 
2.51.2


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

The &#x27;compound_head&#x27; field in the &#x27;struct page&#x27; encodes whether the page
is a tail and where to locate the head page. Bit 0 is set if the page is
a tail, and the remaining bits in the field point to the head page.

As preparation for changing how the field encodes information about the
head page, rename the field to &#x27;compound_info&#x27;.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
---
 .../admin-guide/kdump/vmcoreinfo.rst          |  2 +-
 Documentation/mm/vmemmap_dedup.rst            |  6 +++---
 include/linux/mm_types.h                      | 20 +++++++++----------
 include/linux/page-flags.h                    | 18 ++++++++---------
 include/linux/types.h                         |  2 +-
 kernel/vmcore_info.c                          |  2 +-
 mm/page_alloc.c                               |  2 +-
 mm/slab.h                                     |  2 +-
 mm/util.c                                     |  2 +-
 9 files changed, 28 insertions(+), 28 deletions(-)

diff --git a/Documentation/admin-guide/kdump/vmcoreinfo.rst b/Documentation/admin-guide/kdump/vmcoreinfo.rst
index 404a15f6782c..7663c610fe90 100644
--- a/Documentation/admin-guide/kdump/vmcoreinfo.rst
+++ b/Documentation/admin-guide/kdump/vmcoreinfo.rst
@@ -141,7 +141,7 @@ nodemask_t
 The size of a nodemask_t type. Used to compute the number of online
 nodes.
 
-(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_head)
+(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_info)
 ----------------------------------------------------------------------------------
 
 User-space tools compute their values based on the offset of these
diff --git a/Documentation/mm/vmemmap_dedup.rst b/Documentation/mm/vmemmap_dedup.rst
index b4a55b6569fa..1863d88d2dcb 100644
--- a/Documentation/mm/vmemmap_dedup.rst
+++ b/Documentation/mm/vmemmap_dedup.rst
@@ -24,7 +24,7 @@ For each base page, there is a corresponding ``struct page``.
 Within the HugeTLB subsystem, only the first 4 ``struct page`` are used to
 contain unique information about a HugeTLB page. ``__NR_USED_SUBPAGE`` provides
 this upper limit. The only &#x27;useful&#x27; information in the remaining ``struct page``
-is the compound_head field, and this field is the same for all tail pages.
+is the compound_info field, and this field is the same for all tail pages.
 
 By removing redundant ``struct page`` for HugeTLB pages, memory can be returned
 to the buddy allocator for other uses.
@@ -124,10 +124,10 @@ Here is how things look before optimization::
  |           |
  +-----------+
 
-The value of page-&gt;compound_head is the same for all tail pages. The first
+The value of page-&gt;compound_info is the same for all tail pages. The first
 page of ``struct page`` (page 0) associated with the HugeTLB page contains the 4
 ``struct page`` necessary to describe the HugeTLB. The only use of the remaining
-pages of ``struct page`` (page 1 to page 7) is to point to page-&gt;compound_head.
+pages of ``struct page`` (page 1 to page 7) is to point to page-&gt;compound_info.
 Therefore, we can remap pages 1 to 7 to page 0. Only 1 page of ``struct page``
 will be used for each HugeTLB page. This will allow us to free the remaining
 7 pages to the buddy allocator.
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 3cc8ae722886..7bc82a2b889f 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -126,14 +126,14 @@ struct page {
 			atomic_long_t pp_ref_count;
 		};
 		struct {	/* Tail pages of compound page */
-			unsigned long compound_head;	/* Bit zero is set */
+			unsigned long compound_info;	/* Bit zero is set */
 		};
 		struct {	/* ZONE_DEVICE pages */
 			/*
-			 * The first word is used for compound_head or folio
+			 * The first word is used for compound_info or folio
 			 * pgmap
 			 */
-			void *_unused_pgmap_compound_head;
+			void *_unused_pgmap_compound_info;
 			void *zone_device_data;
 			/*
 			 * ZONE_DEVICE private pages are counted as being
@@ -409,7 +409,7 @@ struct folio {
 	/* private: avoid cluttering the output */
 				/* For the Unevictable &quot;LRU list&quot; slot */
 				struct {
-					/* Avoid compound_head */
+					/* Avoid compound_info */
 					void *__filler;
 	/* public: */
 					unsigned int mlock_count;
@@ -510,7 +510,7 @@ struct folio {
 FOLIO_MATCH(flags, flags);
 FOLIO_MATCH(lru, lru);
 FOLIO_MATCH(mapping, mapping);
-FOLIO_MATCH(compound_head, lru);
+FOLIO_MATCH(compound_info, lru);
 FOLIO_MATCH(__folio_index, index);
 FOLIO_MATCH(private, private);
 FOLIO_MATCH(_mapcount, _mapcount);
@@ -529,7 +529,7 @@ FOLIO_MATCH(_last_cpupid, _last_cpupid);
 	static_assert(offsetof(struct folio, fl) ==			\
 			offsetof(struct page, pg) + sizeof(struct page))
 FOLIO_MATCH(flags, _flags_1);
-FOLIO_MATCH(compound_head, _head_1);
+FOLIO_MATCH(compound_info, _head_1);
 FOLIO_MATCH(_mapcount, _mapcount_1);
 FOLIO_MATCH(_refcount, _refcount_1);
 #undef FOLIO_MATCH
@@ -537,13 +537,13 @@ FOLIO_MATCH(_refcount, _refcount_1);
 	static_assert(offsetof(struct folio, fl) ==			\
 			offsetof(struct page, pg) + 2 * sizeof(struct page))
 FOLIO_MATCH(flags, _flags_2);
-FOLIO_MATCH(compound_head, _head_2);
+FOLIO_MATCH(compound_info, _head_2);
 #undef FOLIO_MATCH
 #define FOLIO_MATCH(pg, fl)						\
 	static_assert(offsetof(struct folio, fl) ==			\
 			offsetof(struct page, pg) + 3 * sizeof(struct page))
 FOLIO_MATCH(flags, _flags_3);
-FOLIO_MATCH(compound_head, _head_3);
+FOLIO_MATCH(compound_info, _head_3);
 #undef FOLIO_MATCH
 
 /**
@@ -609,8 +609,8 @@ struct ptdesc {
 #define TABLE_MATCH(pg, pt)						\
 	static_assert(offsetof(struct page, pg) == offsetof(struct ptdesc, pt))
 TABLE_MATCH(flags, pt_flags);
-TABLE_MATCH(compound_head, pt_list);
-TABLE_MATCH(compound_head, _pt_pad_1);
+TABLE_MATCH(compound_info, pt_list);
+TABLE_MATCH(compound_info, _pt_pad_1);
 TABLE_MATCH(mapping, __page_mapping);
 TABLE_MATCH(__folio_index, pt_index);
 TABLE_MATCH(rcu_head, pt_rcu_head);
diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
index 8a3694369e15..aa46d49e82f7 100644
--- a/include/linux/page-flags.h
+++ b/include/linux/page-flags.h
@@ -213,7 +213,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page
 	/*
 	 * Only addresses aligned with PAGE_SIZE of struct page may be fake head
 	 * struct page. The alignment check aims to avoid access the fields (
-	 * e.g. compound_head) of the @page[1]. It can avoid touch a (possibly)
+	 * e.g. compound_info) of the @page[1]. It can avoid touch a (possibly)
 	 * cold cacheline in some cases.
 	 */
 	if (IS_ALIGNED((unsigned long)page, PAGE_SIZE) &amp;&amp;
@@ -223,7 +223,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page
 		 * because the @page is a compound page composed with at least
 		 * two contiguous pages.
 		 */
-		unsigned long head = READ_ONCE(page[1].compound_head);
+		unsigned long head = READ_ONCE(page[1].compound_info);
 
 		if (likely(head &amp; 1))
 			return (const struct page *)(head - 1);
@@ -281,7 +281,7 @@ static __always_inline int page_is_fake_head(const struct page *page)
 
 static __always_inline unsigned long _compound_head(const struct page *page)
 {
-	unsigned long head = READ_ONCE(page-&gt;compound_head);
+	unsigned long head = READ_ONCE(page-&gt;compound_info);
 
 	if (unlikely(head &amp; 1))
 		return head - 1;
@@ -320,13 +320,13 @@ static __always_inline unsigned long _compound_head(const struct page *page)
 
 static __always_inline int PageTail(const struct page *page)
 {
-	return READ_ONCE(page-&gt;compound_head) &amp; 1 || page_is_fake_head(page);
+	return READ_ONCE(page-&gt;compound_info) &amp; 1 || page_is_fake_head(page);
 }
 
 static __always_inline int PageCompound(const struct page *page)
 {
 	return test_bit(PG_head, &amp;page-&gt;flags.f) ||
-	       READ_ONCE(page-&gt;compound_head) &amp; 1;
+	       READ_ONCE(page-&gt;compound_info) &amp; 1;
 }
 
 #define	PAGE_POISON_PATTERN	-1l
@@ -348,7 +348,7 @@ static const unsigned long *const_folio_flags(const struct folio *folio,
 {
 	const struct page *page = &amp;folio-&gt;page;
 
-	VM_BUG_ON_PGFLAGS(page-&gt;compound_head &amp; 1, page);
+	VM_BUG_ON_PGFLAGS(page-&gt;compound_info &amp; 1, page);
 	VM_BUG_ON_PGFLAGS(n &gt; 0 &amp;&amp; !test_bit(PG_head, &amp;page-&gt;flags.f), page);
 	return &amp;page[n].flags.f;
 }
@@ -357,7 +357,7 @@ static unsigned long *folio_flags(struct folio *folio, unsigned n)
 {
 	struct page *page = &amp;folio-&gt;page;
 
-	VM_BUG_ON_PGFLAGS(page-&gt;compound_head &amp; 1, page);
+	VM_BUG_ON_PGFLAGS(page-&gt;compound_info &amp; 1, page);
 	VM_BUG_ON_PGFLAGS(n &gt; 0 &amp;&amp; !test_bit(PG_head, &amp;page-&gt;flags.f), page);
 	return &amp;page[n].flags.f;
 }
@@ -869,12 +869,12 @@ static __always_inline void set_compound_head(struct page *page,
 					      const struct page *head,
 					      unsigned int order)
 {
-	WRITE_ONCE(page-&gt;compound_head, (unsigned long)head + 1);
+	WRITE_ONCE(page-&gt;compound_info, (unsigned long)head + 1);
 }
 
 static __always_inline void clear_compound_head(struct page *page)
 {
-	WRITE_ONCE(page-&gt;compound_head, 0);
+	WRITE_ONCE(page-&gt;compound_info, 0);
 }
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
diff --git a/include/linux/types.h b/include/linux/types.h
index f69be881369f..604697abf151 100644
--- a/include/linux/types.h
+++ b/include/linux/types.h
@@ -234,7 +234,7 @@ struct ustat {
  *
  * This guarantee is important for few reasons:
  *  - future call_rcu_lazy() will make use of lower bits in the pointer;
- *  - the structure shares storage space in struct page with @compound_head,
+ *  - the structure shares storage space in struct page with @compound_info,
  *    which encode PageTail() in bit 0. The guarantee is needed to avoid
  *    false-positive PageTail().
  */
diff --git a/kernel/vmcore_info.c b/kernel/vmcore_info.c
index 46198580373a..0a46df3e3db9 100644
--- a/kernel/vmcore_info.c
+++ b/kernel/vmcore_info.c
@@ -198,7 +198,7 @@ static int __init crash_save_vmcoreinfo_init(void)
 	VMCOREINFO_OFFSET(page, lru);
 	VMCOREINFO_OFFSET(page, _mapcount);
 	VMCOREINFO_OFFSET(page, private);
-	VMCOREINFO_OFFSET(page, compound_head);
+	VMCOREINFO_OFFSET(page, compound_info);
 	VMCOREINFO_OFFSET(pglist_data, node_zones);
 	VMCOREINFO_OFFSET(pglist_data, nr_zones);
 #ifdef CONFIG_FLATMEM
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 00c7ea958767..cb7375eb1713 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -731,7 +731,7 @@ static inline bool pcp_allowed_order(unsigned int order)
  * The first PAGE_SIZE page is called the &quot;head page&quot; and have PG_head set.
  *
  * The remaining PAGE_SIZE pages are called &quot;tail pages&quot;. PageTail() is encoded
- * in bit 0 of page-&gt;compound_head. The rest of bits is pointer to head page.
+ * in bit 0 of page-&gt;compound_info. The rest of bits is pointer to head page.
  *
  * The first tail page&#x27;s -&gt;compound_order holds the order of allocation.
  * This usage means that zero-order pages may not be compound.
diff --git a/mm/slab.h b/mm/slab.h
index e767aa7e91b0..8a2a9c6c697b 100644
--- a/mm/slab.h
+++ b/mm/slab.h
@@ -100,7 +100,7 @@ struct slab {
 #define SLAB_MATCH(pg, sl)						\
 	static_assert(offsetof(struct page, pg) == offsetof(struct slab, sl))
 SLAB_MATCH(flags, flags);
-SLAB_MATCH(compound_head, slab_cache);	/* Ensure bit 0 is clear */
+SLAB_MATCH(compound_info, slab_cache);	/* Ensure bit 0 is clear */
 SLAB_MATCH(_refcount, __page_refcount);
 #ifdef CONFIG_MEMCG
 SLAB_MATCH(memcg_data, obj_exts);
diff --git a/mm/util.c b/mm/util.c
index b05ab6f97e11..3ebcb9e6035c 100644
--- a/mm/util.c
+++ b/mm/util.c
@@ -1247,7 +1247,7 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)
 again:
 	memset(&amp;ps-&gt;folio_snapshot, 0, sizeof(struct folio));
 	memcpy(&amp;ps-&gt;page_snapshot, page, sizeof(*page));
-	head = ps-&gt;page_snapshot.compound_head;
+	head = ps-&gt;page_snapshot.compound_info;
 	if ((head &amp; 1) == 0) {
 		ps-&gt;idx = 0;
 		foliop = (struct folio *)&amp;ps-&gt;page_snapshot;
-- 
2.51.2


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

The upcoming change to the HugeTLB vmemmap optimization (HVO) requires
struct pages of the head page to be naturally aligned with regard to the
folio size.

Align vmemmap to MAX_FOLIO_NR_PAGES.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
---
 arch/riscv/mm/init.c | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/arch/riscv/mm/init.c b/arch/riscv/mm/init.c
index 21d534824624..c555b9a4fdce 100644
--- a/arch/riscv/mm/init.c
+++ b/arch/riscv/mm/init.c
@@ -63,7 +63,8 @@ phys_addr_t phys_ram_base __ro_after_init;
 EXPORT_SYMBOL(phys_ram_base);
 
 #ifdef CONFIG_SPARSEMEM_VMEMMAP
-#define VMEMMAP_ADDR_ALIGN	(1ULL &lt;&lt; SECTION_SIZE_BITS)
+#define VMEMMAP_ADDR_ALIGN	max(1ULL &lt;&lt; SECTION_SIZE_BITS, \
+				    MAX_FOLIO_NR_PAGES * sizeof(struct page))
 
 unsigned long vmemmap_start_pfn __ro_after_init;
 EXPORT_SYMBOL(vmemmap_start_pfn);
-- 
2.51.2


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

The upcoming change to the HugeTLB vmemmap optimization (HVO) requires
struct pages of the head page to be naturally aligned with regard to the
folio size.

Align vmemmap to MAX_FOLIO_NR_PAGES.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
---
 arch/loongarch/include/asm/pgtable.h | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h
index c33b3bcb733e..f9416acb9156 100644
--- a/arch/loongarch/include/asm/pgtable.h
+++ b/arch/loongarch/include/asm/pgtable.h
@@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
 	 min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL &lt;&lt; cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)
 #endif
 
-#define vmemmap		((struct page *)((VMALLOC_END + PMD_SIZE) &amp; PMD_MASK))
+#define VMEMMAP_ALIGN	max(PMD_SIZE, MAX_FOLIO_NR_PAGES * sizeof(struct page))
+#define vmemmap		((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))
 #define VMEMMAP_END	((unsigned long)vmemmap + VMEMMAP_SIZE - 1)
 
 #define KFENCE_AREA_START	(VMEMMAP_END + 1)
-- 
2.51.2


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv</pre>
</details>
<a href="https://lore.kernel.org/r/20260202155634.650837-5-kas@kernel.org" target="_blank" rel="noopener" class="lore-link">View on lore &#8599;</a>
<div class="review-comment-signals">Signals: acknowledged fix, improved code organization</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Kiryl Shutsemau (author)</span>
<a class="date-chip" href="../2026-02-27_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-02">2026-02-02</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed a concern about the mask-based compound_info encoding being too restrictive, agreeing that it should be limited to HugeTLB vmemmap optimization (HVO) where it makes a difference. They also acknowledged that validating struct pages are naturally aligned for all orders up to MAX_FOLIO_ORDER can be tricky and would require additional validation.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">For tail pages, the kernel uses the &#x27;compound_info&#x27; field to get to the
head page. The bit 0 of the field indicates whether the page is a
tail page, and if set, the remaining bits represent a pointer to the
head page.

For cases when size of struct page is power-of-2, change the encoding of
compound_info to store a mask that can be applied to the virtual address
of the tail page in order to access the head page. It is possible
because struct page of the head page is naturally aligned with regards
to order of the page.

The significant impact of this modification is that all tail pages of
the same order will now have identical &#x27;compound_info&#x27;, regardless of
the compound page they are associated with. This paves the way for
eliminating fake heads.

The HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only
applied when the sizeof(struct page) is power-of-2. Having identical
tail pages allows the same page to be mapped into the vmemmap of all
pages, maintaining memory savings without fake heads.

If sizeof(struct page) is not power-of-2, there is no functional
changes.

Limit mask usage to HugeTLB vmemmap optimization (HVO) where it makes
a difference. The approach with mask would work in the wider set of
conditions, but it requires validating that struct pages are naturally
aligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
---
 include/linux/page-flags.h | 81 ++++++++++++++++++++++++++++++++++----
 mm/slab.h                  | 16 ++++++--
 mm/util.c                  | 16 ++++++--
 3 files changed, 97 insertions(+), 16 deletions(-)

diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
index d14a17ffb55b..8f2c7fbc739b 100644
--- a/include/linux/page-flags.h
+++ b/include/linux/page-flags.h
@@ -198,6 +198,29 @@ enum pageflags {
 
 #ifndef __GENERATING_BOUNDS_H
 
+/*
+ * For tail pages, if the size of struct page is power-of-2 -&gt;compound_info
+ * encodes the mask that converts the address of the tail page address to
+ * the head page address.
+ *
+ * Otherwise, -&gt;compound_info has direct pointer to head pages.
+ */
+static __always_inline bool compound_info_has_mask(void)
+{
+	/*
+	 * Limit mask usage to HugeTLB vmemmap optimization (HVO) where it
+	 * makes a difference.
+	 *
+	 * The approach with mask would work in the wider set of conditions,
+	 * but it requires validating that struct pages are naturally aligned
+	 * for all orders up to the MAX_FOLIO_ORDER, which can be tricky.
+	 */
+	if (!IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP))
+		return false;
+
+	return is_power_of_2(sizeof(struct page));
+}
+
 #ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP
 DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);
 
@@ -210,6 +233,10 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page
 	if (!static_branch_unlikely(&amp;hugetlb_optimize_vmemmap_key))
 		return page;
 
+	/* Fake heads only exists if compound_info_has_mask() is true */
+	if (!compound_info_has_mask())
+		return page;
+
 	/*
 	 * Only addresses aligned with PAGE_SIZE of struct page may be fake head
 	 * struct page. The alignment check aims to avoid access the fields (
@@ -223,10 +250,14 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page
 		 * because the @page is a compound page composed with at least
 		 * two contiguous pages.
 		 */
-		unsigned long head = READ_ONCE(page[1].compound_info);
+		unsigned long info = READ_ONCE(page[1].compound_info);
 
-		if (likely(head &amp; 1))
-			return (const struct page *)(head - 1);
+		/* See set_compound_head() */
+		if (likely(info &amp; 1)) {
+			unsigned long p = (unsigned long)page;
+
+			return (const struct page *)(p &amp; info);
+		}
 	}
 	return page;
 }
@@ -281,11 +312,26 @@ static __always_inline int page_is_fake_head(const struct page *page)
 
 static __always_inline unsigned long _compound_head(const struct page *page)
 {
-	unsigned long head = READ_ONCE(page-&gt;compound_info);
+	unsigned long info = READ_ONCE(page-&gt;compound_info);
 
-	if (unlikely(head &amp; 1))
-		return head - 1;
-	return (unsigned long)page_fixed_fake_head(page);
+	/* Bit 0 encodes PageTail() */
+	if (!(info &amp; 1))
+		return (unsigned long)page_fixed_fake_head(page);
+
+	/*
+	 * If compound_info_has_mask() is false, the rest of compound_info is
+	 * the pointer to the head page.
+	 */
+	if (!compound_info_has_mask())
+		return info - 1;
+
+	/*
+	 * If compoun_info_has_mask() is true the rest of the info encodes
+	 * the mask that converts the address of the tail page to the head page.
+	 *
+	 * No need to clear bit 0 in the mask as &#x27;page&#x27; always has it clear.
+	 */
+	return (unsigned long)page &amp; info;
 }
 
 #define compound_head(page)	((typeof(page))_compound_head(page))
@@ -294,7 +340,26 @@ static __always_inline void set_compound_head(struct page *page,
 					      const struct page *head,
 					      unsigned int order)
 {
-	WRITE_ONCE(page-&gt;compound_info, (unsigned long)head + 1);
+	unsigned int shift;
+	unsigned long mask;
+
+	if (!compound_info_has_mask()) {
+		WRITE_ONCE(page-&gt;compound_info, (unsigned long)head | 1);
+		return;
+	}
+
+	/*
+	 * If the size of struct page is power-of-2, bits [shift:0] of the
+	 * virtual address of compound head are zero.
+	 *
+	 * Calculate mask that can be applied to the virtual address of
+	 * the tail page to get address of the head page.
+	 */
+	shift = order + order_base_2(sizeof(struct page));
+	mask = GENMASK(BITS_PER_LONG - 1, shift);
+
+	/* Bit 0 encodes PageTail() */
+	WRITE_ONCE(page-&gt;compound_info, mask | 1);
 }
 
 static __always_inline void clear_compound_head(struct page *page)
diff --git a/mm/slab.h b/mm/slab.h
index 8a2a9c6c697b..f68c3ac8126f 100644
--- a/mm/slab.h
+++ b/mm/slab.h
@@ -137,11 +137,19 @@ static_assert(IS_ALIGNED(offsetof(struct slab, freelist), sizeof(struct freelist
  */
 static inline struct slab *page_slab(const struct page *page)
 {
-	unsigned long head;
+	unsigned long info;
+
+	info = READ_ONCE(page-&gt;compound_info);
+	if (info &amp; 1) {
+		/* See compound_head() */
+		if (compound_info_has_mask()) {
+			unsigned long p = (unsigned long)page;
+			page = (struct page *)(p &amp; info);
+		} else {
+			page = (struct page *)(info - 1);
+		}
+	}
 
-	head = READ_ONCE(page-&gt;compound_head);
-	if (head &amp; 1)
-		page = (struct page *)(head - 1);
 	if (data_race(page-&gt;page_type &gt;&gt; 24) != PGTY_slab)
 		page = NULL;
 
diff --git a/mm/util.c b/mm/util.c
index 3ebcb9e6035c..20dccf2881d7 100644
--- a/mm/util.c
+++ b/mm/util.c
@@ -1237,7 +1237,7 @@ static void set_ps_flags(struct page_snapshot *ps, const struct folio *folio,
  */
 void snapshot_page(struct page_snapshot *ps, const struct page *page)
 {
-	unsigned long head, nr_pages = 1;
+	unsigned long info, nr_pages = 1;
 	struct folio *foliop;
 	int loops = 5;
 
@@ -1247,8 +1247,8 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)
 again:
 	memset(&amp;ps-&gt;folio_snapshot, 0, sizeof(struct folio));
 	memcpy(&amp;ps-&gt;page_snapshot, page, sizeof(*page));
-	head = ps-&gt;page_snapshot.compound_info;
-	if ((head &amp; 1) == 0) {
+	info = ps-&gt;page_snapshot.compound_info;
+	if (!(info &amp; 1)) {
 		ps-&gt;idx = 0;
 		foliop = (struct folio *)&amp;ps-&gt;page_snapshot;
 		if (!folio_test_large(foliop)) {
@@ -1259,7 +1259,15 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)
 		}
 		foliop = (struct folio *)page;
 	} else {
-		foliop = (struct folio *)(head - 1);
+		/* See compound_head() */
+		if (compound_info_has_mask()) {
+			unsigned long p = (unsigned long)page;
+
+			foliop = (struct folio *)(p &amp; info);
+		} else {
+			foliop = (struct folio *)(info - 1);
+		}
+
 		ps-&gt;idx = folio_page_idx(foliop, page);
 	}
 
-- 
2.51.2


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

With the upcoming changes to HVO, a single page of tail struct pages
will be shared across all huge pages of the same order on a node. Since
huge pages on the same node may belong to different zones, the zone
information stored in shared tail page flags would be incorrect.

Always fetch zone information from the head page, which has unique and
correct zone flags for each compound page.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
---
 include/linux/mmzone.h | 1 +
 1 file changed, 1 insertion(+)

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index be8ce40b5638..192143b5cdc0 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)
 
 static inline enum zone_type page_zonenum(const struct page *page)
 {
+	page = compound_head(page);
 	return memdesc_zonenum(page-&gt;flags);
 }
 
-- 
2.51.2


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

If page-&gt;compound_info encodes a mask, it is expected that vmemmap to be
naturally aligned to the maximum folio size.

Add a VM_BUG_ON() to check the alignment.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
---
 mm/sparse.c | 7 +++++++
 1 file changed, 7 insertions(+)

diff --git a/mm/sparse.c b/mm/sparse.c
index b5b2b6f7041b..6c9b62607f3f 100644
--- a/mm/sparse.c
+++ b/mm/sparse.c
@@ -600,6 +600,13 @@ void __init sparse_init(void)
 	BUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));
 	memblocks_present();
 
+	if (compound_info_has_mask()) {
+		unsigned long alignment;
+
+		alignment = MAX_FOLIO_NR_PAGES * sizeof(struct page);
+		VM_BUG_ON(!IS_ALIGNED((unsigned long) pfn_to_page(0), alignment));
+	}
+
 	pnum_begin = first_present_section_nr();
 	nid_begin = sparse_early_nid(__nr_to_section(pnum_begin));
 
-- 
2.51.2


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On Wed, Feb 04, 2026 at 05:14:12PM +0100, David Hildenbrand (arm) wrote:
&gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; &gt; Instead of passing down the head page and tail page index, pass the tail
&gt; &gt; and head pages directly, as well as the order of the compound page.
&gt; &gt; 
&gt; &gt; This is a preparation for changing how the head position is encoded in
&gt; &gt; the tail page.
&gt; &gt; 
&gt; &gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; &gt; Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
&gt; &gt; Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt; &gt; ---
&gt; &gt;   include/linux/page-flags.h |  4 +++-
&gt; &gt;   mm/hugetlb.c               |  8 +++++---
&gt; &gt;   mm/internal.h              | 12 ++++++------
&gt; &gt;   mm/mm_init.c               |  2 +-
&gt; &gt;   mm/page_alloc.c            |  2 +-
&gt; &gt;   5 files changed, 16 insertions(+), 12 deletions(-)
&gt; &gt; 
&gt; &gt; diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
&gt; &gt; index f7a0e4af0c73..8a3694369e15 100644
&gt; &gt; --- a/include/linux/page-flags.h
&gt; &gt; +++ b/include/linux/page-flags.h
&gt; &gt; @@ -865,7 +865,9 @@ static inline bool folio_test_large(const struct folio *folio)
&gt; &gt;   	return folio_test_head(folio);
&gt; &gt;   }
&gt; &gt; -static __always_inline void set_compound_head(struct page *page, struct page *head)
&gt; &gt; +static __always_inline void set_compound_head(struct page *page,
&gt; &gt; +					      const struct page *head,
&gt; &gt; +					      unsigned int order)
&gt; 
&gt; Two tab indents please on second+ parameter list whenever you touch code.

Do we have this coding style preference written down somewhere?

-tip tree wants the opposite. Documentation/process/maintainer-tip.rst:

	When splitting function declarations or function calls, then please align
	the first argument in the second line with the first argument in the first
	line::

I want the editor to do The Right Thing\u2122 without my brain involvement.
Having different coding styles in different corners of the kernel makes
it hard.

&gt; 
&gt; &gt;   {
&gt; &gt;   	WRITE_ONCE(page-&gt;compound_head, (unsigned long)head + 1);
&gt; &gt;   }
&gt; &gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c
&gt; &gt; index 6e855a32de3d..54ba7cd05a86 100644
&gt; 
&gt; 
&gt; [...]
&gt; 
&gt; &gt; diff --git a/mm/internal.h b/mm/internal.h
&gt; &gt; index d67e8bb75734..037ddcda25ff 100644
&gt; &gt; --- a/mm/internal.h
&gt; &gt; +++ b/mm/internal.h
&gt; &gt; @@ -879,13 +879,13 @@ static inline void prep_compound_head(struct page *page, unsigned int order)
&gt; &gt;   		INIT_LIST_HEAD(&amp;folio-&gt;_deferred_list);
&gt; &gt;   }
&gt; &gt; -static inline void prep_compound_tail(struct page *head, int tail_idx)
&gt; &gt; +static inline void prep_compound_tail(struct page *tail,
&gt; 
&gt; Just wondering whether we should call this &quot;struct page *page&quot; for
&gt; consistency with set_compound_head().
&gt; 
&gt; Or alternatively, call it also &quot;tail&quot; in set_compound_head().

I will take the alternative path :)

&gt; 
&gt; &gt; +				      const struct page *head,
&gt; &gt; +				      unsigned int order)
&gt; 
&gt; Two tab indent, then this fits into two lines in total.
&gt; 
&gt; &gt;   {
&gt; &gt; -	struct page *p = head + tail_idx;
&gt; &gt; -
&gt; &gt; -	p-&gt;mapping = TAIL_MAPPING;
&gt; &gt; -	set_compound_head(p, head);
&gt; &gt; -	set_page_private(p, 0);
&gt; &gt; +	tail-&gt;mapping = TAIL_MAPPING;
&gt; &gt; +	set_compound_head(tail, head, order);
&gt; &gt; +	set_page_private(tail, 0);
&gt; &gt;   }
&gt; Only nits, in general LGTM
&gt; 
&gt; Acked-by: David Hildenbrand (arm) &lt;david@kernel.org&gt;

Thanks!

-- 
  Kiryl Shutsemau / Kirill A. Shutemov

---

On Wed, Feb 04, 2026 at 05:14:12PM +0100, David Hildenbrand (arm) wrote:
&gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; &gt; Instead of passing down the head page and tail page index, pass the tail
&gt; &gt; and head pages directly, as well as the order of the compound page.
&gt; &gt; 
&gt; &gt; This is a preparation for changing how the head position is encoded in
&gt; &gt; the tail page.
&gt; &gt; 
&gt; &gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; &gt; Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
&gt; &gt; Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt; &gt; ---
&gt; &gt;   include/linux/page-flags.h |  4 +++-
&gt; &gt;   mm/hugetlb.c               |  8 +++++---
&gt; &gt;   mm/internal.h              | 12 ++++++------
&gt; &gt;   mm/mm_init.c               |  2 +-
&gt; &gt;   mm/page_alloc.c            |  2 +-
&gt; &gt;   5 files changed, 16 insertions(+), 12 deletions(-)
&gt; &gt; 
&gt; &gt; diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
&gt; &gt; index f7a0e4af0c73..8a3694369e15 100644
&gt; &gt; --- a/include/linux/page-flags.h
&gt; &gt; +++ b/include/linux/page-flags.h
&gt; &gt; @@ -865,7 +865,9 @@ static inline bool folio_test_large(const struct folio *folio)
&gt; &gt;   	return folio_test_head(folio);
&gt; &gt;   }
&gt; &gt; -static __always_inline void set_compound_head(struct page *page, struct page *head)
&gt; &gt; +static __always_inline void set_compound_head(struct page *page,
&gt; &gt; +					      const struct page *head,
&gt; &gt; +					      unsigned int order)
&gt; 
&gt; Two tab indents please on second+ parameter list whenever you touch code.

Do we have this coding style preference written down somewhere?

-tip tree wants the opposite. Documentation/process/maintainer-tip.rst:

	When splitting function declarations or function calls, then please align
	the first argument in the second line with the first argument in the first
	line::

I want the editor to do The Right Thing™ without my brain involvement.
Having different coding styles in different corners of the kernel makes
it hard.

&gt; 
&gt; &gt;   {
&gt; &gt;   	WRITE_ONCE(page-&gt;compound_head, (unsigned long)head + 1);
&gt; &gt;   }
&gt; &gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c
&gt; &gt; index 6e855a32de3d..54ba7cd05a86 100644
&gt; 
&gt; 
&gt; [...]
&gt; 
&gt; &gt; diff --git a/mm/internal.h b/mm/internal.h
&gt; &gt; index d67e8bb75734..037ddcda25ff 100644
&gt; &gt; --- a/mm/internal.h
&gt; &gt; +++ b/mm/internal.h
&gt; &gt; @@ -879,13 +879,13 @@ static inline void prep_compound_head(struct page *page, unsigned int order)
&gt; &gt;   		INIT_LIST_HEAD(&amp;folio-&gt;_deferred_list);
&gt; &gt;   }
&gt; &gt; -static inline void prep_compound_tail(struct page *head, int tail_idx)
&gt; &gt; +static inline void prep_compound_tail(struct page *tail,
&gt; 
&gt; Just wondering whether we should call this &quot;struct page *page&quot; for
&gt; consistency with set_compound_head().
&gt; 
&gt; Or alternatively, call it also &quot;tail&quot; in set_compound_head().

I will take the alternative path :)

&gt; 
&gt; &gt; +				      const struct page *head,
&gt; &gt; +				      unsigned int order)
&gt; 
&gt; Two tab indent, then this fits into two lines in total.
&gt; 
&gt; &gt;   {
&gt; &gt; -	struct page *p = head + tail_idx;
&gt; &gt; -
&gt; &gt; -	p-&gt;mapping = TAIL_MAPPING;
&gt; &gt; -	set_compound_head(p, head);
&gt; &gt; -	set_page_private(p, 0);
&gt; &gt; +	tail-&gt;mapping = TAIL_MAPPING;
&gt; &gt; +	set_compound_head(tail, head, order);
&gt; &gt; +	set_page_private(tail, 0);
&gt; &gt;   }
&gt; Only nits, in general LGTM
&gt; 
&gt; Acked-by: David Hildenbrand (arm) &lt;david@kernel.org&gt;

Thanks!

-- 
  Kiryl Shutsemau / Kirill A. Shutemov

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On Thu, Feb 05, 2026 at 01:56:36PM +0100, David Hildenbrand (Arm) wrote:
&gt; On 2/4/26 17:56, David Hildenbrand (arm) wrote:
&gt; &gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; &gt; &gt; The upcoming change to the HugeTLB vmemmap optimization (HVO) requires
&gt; &gt; &gt; struct pages of the head page to be naturally aligned with regard to the
&gt; &gt; &gt; folio size.
&gt; &gt; &gt; 
&gt; &gt; &gt; Align vmemmap to MAX_FOLIO_NR_PAGES.
&gt; &gt; &gt; 
&gt; &gt; &gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; &gt; &gt; ---
&gt; &gt; &gt;  arch/loongarch/include/asm/pgtable.h | 3 ++-
&gt; &gt; &gt;  1 file changed, 2 insertions(+), 1 deletion(-)
&gt; &gt; &gt; 
&gt; &gt; &gt; diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/
&gt; &gt; &gt; include/asm/pgtable.h
&gt; &gt; &gt; index c33b3bcb733e..f9416acb9156 100644
&gt; &gt; &gt; --- a/arch/loongarch/include/asm/pgtable.h
&gt; &gt; &gt; +++ b/arch/loongarch/include/asm/pgtable.h
&gt; &gt; &gt; @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE /
&gt; &gt; &gt; sizeof(unsigned long)];
&gt; &gt; &gt;  min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE
&gt; &gt; &gt; * PAGE_SIZE, (1UL &lt;&lt; cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE -
&gt; &gt; &gt; KFENCE_AREA_SIZE)
&gt; &gt; &gt;  #endif
&gt; &gt; &gt; -#define vmemmap ((struct page *)((VMALLOC_END + PMD_SIZE) &amp;
&gt; &gt; &gt; PMD_MASK))
&gt; &gt; &gt; +#define VMEMMAP_ALIGN max(PMD_SIZE, MAX_FOLIO_NR_PAGES *
&gt; &gt; &gt; sizeof(struct page))
&gt; &gt; &gt; +#define vmemmap ((struct page *)(ALIGN(VMALLOC_END,
&gt; &gt; &gt; VMEMMAP_ALIGN)))
&gt; &gt; 
&gt; &gt; 
&gt; &gt; Same comment, the &quot;MAX_FOLIO_NR_PAGES * sizeof(struct page)&quot; is just
&gt; &gt; black magic here
&gt; &gt; and the description of the situation is wrong.
&gt; &gt; 
&gt; &gt; Maybe you want to pull the magic &quot;MAX_FOLIO_NR_PAGES * sizeof(struct
&gt; &gt; page)&quot; into the core and call it
&gt; &gt; 
&gt; &gt; #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct
&gt; &gt; page))
&gt; &gt; 
&gt; &gt; But then special case it base on (a) HVO being configured in an (b) HVO
&gt; &gt; being possible
&gt; &gt; 
&gt; &gt; #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP &amp;&amp; is_power_of_2(sizeof(struct page)
&gt; &gt; /* A very helpful comment explaining the situation. */
&gt; &gt; #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct
&gt; &gt; page))
&gt; &gt; #else
&gt; &gt; #define MAX_FOLIO_VMEMMAP_ALIGN 0
&gt; &gt; #endif
&gt; &gt; 
&gt; &gt; Something like that.
&gt; &gt; 
&gt; 
&gt; Thinking about this ...
&gt; 
&gt; the vmemmap start is always struct-page-aligned. Otherwise we&#x27;d be in
&gt; trouble already.
&gt; 
&gt; Isn&#x27;t it then sufficient to just align the start to MAX_FOLIO_NR_PAGES?
&gt; 
&gt; Let&#x27;s assume sizeof(struct page) == 64 and MAX_FOLIO_NR_PAGES = 512 for
&gt; simplicity.
&gt; 
&gt; vmemmap start would be multiples of 512 (0x0010000000).
&gt; 
&gt; 512, 1024, 1536, 2048 ...
&gt; 
&gt; Assume we have an 256-pages folio at 1536+256 = 0x111000000

s/0x/0b/, but okay.

&gt; Assume we have the last page of that folio (0x011111111111), we would just
&gt; get to the start of that folio by AND-ing with ~(256-1).
&gt; 
&gt; Which case am I ignoring?

IIUC, you are ignoring the actual size of struct page. It is not 1 byte :P

The last page of this 256-page folio is at 1536+256 + (64 * 255) which
is 0b100011011000000. There&#x27;s no mask that you can AND that gets you to
0b11100000000.

-- 
  Kiryl Shutsemau / Kirill A. Shutemov

---

On Thu, Feb 05, 2026 at 01:56:36PM +0100, David Hildenbrand (Arm) wrote:
&gt; On 2/4/26 17:56, David Hildenbrand (arm) wrote:
&gt; &gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; &gt; &gt; The upcoming change to the HugeTLB vmemmap optimization (HVO) requires
&gt; &gt; &gt; struct pages of the head page to be naturally aligned with regard to the
&gt; &gt; &gt; folio size.
&gt; &gt; &gt; 
&gt; &gt; &gt; Align vmemmap to MAX_FOLIO_NR_PAGES.
&gt; &gt; &gt; 
&gt; &gt; &gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; &gt; &gt; ---
&gt; &gt; &gt;  arch/loongarch/include/asm/pgtable.h | 3 ++-
&gt; &gt; &gt;  1 file changed, 2 insertions(+), 1 deletion(-)
&gt; &gt; &gt; 
&gt; &gt; &gt; diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/
&gt; &gt; &gt; include/asm/pgtable.h
&gt; &gt; &gt; index c33b3bcb733e..f9416acb9156 100644
&gt; &gt; &gt; --- a/arch/loongarch/include/asm/pgtable.h
&gt; &gt; &gt; +++ b/arch/loongarch/include/asm/pgtable.h
&gt; &gt; &gt; @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE /
&gt; &gt; &gt; sizeof(unsigned long)];
&gt; &gt; &gt;  min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE
&gt; &gt; &gt; * PAGE_SIZE, (1UL &lt;&lt; cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE -
&gt; &gt; &gt; KFENCE_AREA_SIZE)
&gt; &gt; &gt;  #endif
&gt; &gt; &gt; -#define vmemmap ((struct page *)((VMALLOC_END + PMD_SIZE) &amp;
&gt; &gt; &gt; PMD_MASK))
&gt; &gt; &gt; +#define VMEMMAP_ALIGN max(PMD_SIZE, MAX_FOLIO_NR_PAGES *
&gt; &gt; &gt; sizeof(struct page))
&gt; &gt; &gt; +#define vmemmap ((struct page *)(ALIGN(VMALLOC_END,
&gt; &gt; &gt; VMEMMAP_ALIGN)))
&gt; &gt; 
&gt; &gt; 
&gt; &gt; Same comment, the &quot;MAX_FOLIO_NR_PAGES * sizeof(struct page)&quot; is just
&gt; &gt; black magic here
&gt; &gt; and the description of the situation is wrong.
&gt; &gt; 
&gt; &gt; Maybe you want to pull the magic &quot;MAX_FOLIO_NR_PAGES * sizeof(struct
&gt; &gt; page)&quot; into the core and call it
&gt; &gt; 
&gt; &gt; #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct
&gt; &gt; page))
&gt; &gt; 
&gt; &gt; But then special case it base on (a) HVO being configured in an (b) HVO
&gt; &gt; being possible
&gt; &gt; 
&gt; &gt; #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP &amp;&amp; is_power_of_2(sizeof(struct page)
&gt; &gt; /* A very helpful comment explaining the situation. */
&gt; &gt; #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct
&gt; &gt; page))
&gt; &gt; #else
&gt; &gt; #define MAX_FOLIO_VMEMMAP_ALIGN 0
&gt; &gt; #endif
&gt; &gt; 
&gt; &gt; Something like that.
&gt; &gt; 
&gt; 
&gt; Thinking about this ...
&gt; 
&gt; the vmemmap start is always struct-page-aligned. Otherwise we&#x27;d be in
&gt; trouble already.
&gt; 
&gt; Isn&#x27;t it then sufficient to just align the start to MAX_FOLIO_NR_PAGES?
&gt; 
&gt; Let&#x27;s assume sizeof(struct page) == 64 and MAX_FOLIO_NR_PAGES = 512 for
&gt; simplicity.
&gt; 
&gt; vmemmap start would be multiples of 512 (0x0010000000).
&gt; 
&gt; 512, 1024, 1536, 2048 ...
&gt; 
&gt; Assume we have an 256-pages folio at 1536+256 = 0x111000000

s/0x/0b/, but okay.

&gt; Assume we have the last page of that folio (0x011111111111), we would just
&gt; get to the start of that folio by AND-ing with ~(256-1).
&gt; 
&gt; Which case am I ignoring?

IIUC, you are ignoring the actual size of struct page. It is not 1 byte :P

The last page of this 256-page folio is at 1536+256 + (64 * 255) which
is 0b100011011000000. There&#x27;s no mask that you can AND that gets you to
0b11100000000.

-- 
  Kiryl Shutsemau / Kirill A. Shutemov

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv</pre>
</details>
<a href="https://lore.kernel.org/r/20260202155634.650837-8-kas@kernel.org" target="_blank" rel="noopener" class="lore-link">View on lore &#8599;</a>
<div class="review-comment-signals">Signals: acknowledged a limitation, agreed with a restriction</div>
</div>
</div>
<div class="thread-node depth-0" id="2026-02-03">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Muchun Song</span>
<a class="date-chip" href="../2026-02-27_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-03">2026-02-03</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Muchun Song pointed out that the shared tail pages will have incorrect zone information because huge pages on the same node may belong to different zones, and suggested always fetching zone information from the head page.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">&gt; On Feb 2, 2026, at 23:56, Kiryl Shutsemau &lt;kas@kernel.org&gt; wrote:
&gt; 
&gt; If page-&gt;compound_info encodes a mask, it is expected that vmemmap to be
&gt; naturally aligned to the maximum folio size.
&gt; 
&gt; Add a VM_BUG_ON() to check the alignment.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;

Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;

---

&gt; On Feb 2, 2026, at 23:56, Kiryl Shutsemau &lt;kas@kernel.org&gt; wrote:
&gt; 
&gt; If page-&gt;compound_info encodes a mask, it is expected that vmemmap to be
&gt; naturally aligned to the maximum folio size.
&gt; 
&gt; Add a VM_BUG_ON() to check the alignment.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;

Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;



_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

&gt; On Feb 2, 2026, at 23:56, Kiryl Shutsemau &lt;kas@kernel.org&gt; wrote:
&gt; 
&gt; With the upcoming changes to HVO, a single page of tail struct pages
&gt; will be shared across all huge pages of the same order on a node. Since
&gt; huge pages on the same node may belong to different zones, the zone
&gt; information stored in shared tail page flags would be incorrect.
&gt; 
&gt; Always fetch zone information from the head page, which has unique and
&gt; correct zone flags for each compound page.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;

Acked-by: Muchun Song &lt;muchun.song@linux.dev&gt;

---

&gt; On Feb 2, 2026, at 23:56, Kiryl Shutsemau &lt;kas@kernel.org&gt; wrote:
&gt; 
&gt; With the upcoming changes to HVO, a single page of tail struct pages
&gt; will be shared across all huge pages of the same order on a node. Since
&gt; huge pages on the same node may belong to different zones, the zone
&gt; information stored in shared tail page flags would be incorrect.
&gt; 
&gt; Always fetch zone information from the head page, which has unique and
&gt; correct zone flags for each compound page.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;

Acked-by: Muchun Song &lt;muchun.song@linux.dev&gt;


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv</pre>
</details>
<a href="https://lore.kernel.org/r/C310A603-DB7F-4140-B045-7F1E3CC98C05@linux.dev" target="_blank" rel="noopener" class="lore-link">View on lore &#8599;</a>
<div class="review-comment-signals">Signals: requested changes</div>
</div>
</div>
<div class="thread-node depth-0" id="2026-02-04">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">David (arm)</span>
<a class="date-chip" href="../2026-02-27_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-04">2026-02-04</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Acked-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer David (arm) suggested two minor code style improvements: adding two tab indents to the parameter list in set_compound_head() and renaming a function parameter from &#x27;page&#x27; to &#x27;tail&#x27; for consistency.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; Instead of passing down the head page and tail page index, pass the tail
&gt; and head pages directly, as well as the order of the compound page.
&gt; 
&gt; This is a preparation for changing how the head position is encoded in
&gt; the tail page.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
&gt; Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt; ---
&gt;   include/linux/page-flags.h |  4 +++-
&gt;   mm/hugetlb.c               |  8 +++++---
&gt;   mm/internal.h              | 12 ++++++------
&gt;   mm/mm_init.c               |  2 +-
&gt;   mm/page_alloc.c            |  2 +-
&gt;   5 files changed, 16 insertions(+), 12 deletions(-)
&gt; 
&gt; diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
&gt; index f7a0e4af0c73..8a3694369e15 100644
&gt; --- a/include/linux/page-flags.h
&gt; +++ b/include/linux/page-flags.h
&gt; @@ -865,7 +865,9 @@ static inline bool folio_test_large(const struct folio *folio)
&gt;   	return folio_test_head(folio);
&gt;   }
&gt;   
&gt; -static __always_inline void set_compound_head(struct page *page, struct page *head)
&gt; +static __always_inline void set_compound_head(struct page *page,
&gt; +					      const struct page *head,
&gt; +					      unsigned int order)

Two tab indents please on second+ parameter list whenever you touch code.

&gt;   {
&gt;   	WRITE_ONCE(page-&gt;compound_head, (unsigned long)head + 1);
&gt;   }
&gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c
&gt; index 6e855a32de3d..54ba7cd05a86 100644


[...]

&gt; diff --git a/mm/internal.h b/mm/internal.h
&gt; index d67e8bb75734..037ddcda25ff 100644
&gt; --- a/mm/internal.h
&gt; +++ b/mm/internal.h
&gt; @@ -879,13 +879,13 @@ static inline void prep_compound_head(struct page *page, unsigned int order)
&gt;   		INIT_LIST_HEAD(&amp;folio-&gt;_deferred_list);
&gt;   }
&gt;   
&gt; -static inline void prep_compound_tail(struct page *head, int tail_idx)
&gt; +static inline void prep_compound_tail(struct page *tail,

Just wondering whether we should call this &quot;struct page *page&quot; for 
consistency with set_compound_head().

Or alternatively, call it also &quot;tail&quot; in set_compound_head().

&gt; +				      const struct page *head,
&gt; +				      unsigned int order)

Two tab indent, then this fits into two lines in total.

&gt;   {
&gt; -	struct page *p = head + tail_idx;
&gt; -
&gt; -	p-&gt;mapping = TAIL_MAPPING;
&gt; -	set_compound_head(p, head);
&gt; -	set_page_private(p, 0);
&gt; +	tail-&gt;mapping = TAIL_MAPPING;
&gt; +	set_compound_head(tail, head, order);
&gt; +	set_page_private(tail, 0);
&gt;   }
Only nits, in general LGTM

Acked-by: David Hildenbrand (arm) &lt;david@kernel.org&gt;

-- 
Cheers,

David

---

On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; The &#x27;compound_head&#x27; field in the &#x27;struct page&#x27; encodes whether the page
&gt; is a tail and where to locate the head page. Bit 0 is set if the page is
&gt; a tail, and the remaining bits in the field point to the head page.
&gt; 
&gt; As preparation for changing how the field encodes information about the
&gt; head page, rename the field to &#x27;compound_info&#x27;.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
&gt; Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt; ---

Acked-by: David Hildenbrand (arm) &lt;david@kernel.org&gt;

-- 
Cheers,

David

---

On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; Instead of passing down the head page and tail page index, pass the tail
&gt; and head pages directly, as well as the order of the compound page.
&gt; 
&gt; This is a preparation for changing how the head position is encoded in
&gt; the tail page.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
&gt; Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt; ---
&gt;   include/linux/page-flags.h |  4 +++-
&gt;   mm/hugetlb.c               |  8 +++++---
&gt;   mm/internal.h              | 12 ++++++------
&gt;   mm/mm_init.c               |  2 +-
&gt;   mm/page_alloc.c            |  2 +-
&gt;   5 files changed, 16 insertions(+), 12 deletions(-)
&gt; 
&gt; diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
&gt; index f7a0e4af0c73..8a3694369e15 100644
&gt; --- a/include/linux/page-flags.h
&gt; +++ b/include/linux/page-flags.h
&gt; @@ -865,7 +865,9 @@ static inline bool folio_test_large(const struct folio *folio)
&gt;   	return folio_test_head(folio);
&gt;   }
&gt;   
&gt; -static __always_inline void set_compound_head(struct page *page, struct page *head)
&gt; +static __always_inline void set_compound_head(struct page *page,
&gt; +					      const struct page *head,
&gt; +					      unsigned int order)

Two tab indents please on second+ parameter list whenever you touch code.

&gt;   {
&gt;   	WRITE_ONCE(page-&gt;compound_head, (unsigned long)head + 1);
&gt;   }
&gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c
&gt; index 6e855a32de3d..54ba7cd05a86 100644


[...]

&gt; diff --git a/mm/internal.h b/mm/internal.h
&gt; index d67e8bb75734..037ddcda25ff 100644
&gt; --- a/mm/internal.h
&gt; +++ b/mm/internal.h
&gt; @@ -879,13 +879,13 @@ static inline void prep_compound_head(struct page *page, unsigned int order)
&gt;   		INIT_LIST_HEAD(&amp;folio-&gt;_deferred_list);
&gt;   }
&gt;   
&gt; -static inline void prep_compound_tail(struct page *head, int tail_idx)
&gt; +static inline void prep_compound_tail(struct page *tail,

Just wondering whether we should call this &quot;struct page *page&quot; for 
consistency with set_compound_head().

Or alternatively, call it also &quot;tail&quot; in set_compound_head().

&gt; +				      const struct page *head,
&gt; +				      unsigned int order)

Two tab indent, then this fits into two lines in total.

&gt;   {
&gt; -	struct page *p = head + tail_idx;
&gt; -
&gt; -	p-&gt;mapping = TAIL_MAPPING;
&gt; -	set_compound_head(p, head);
&gt; -	set_page_private(p, 0);
&gt; +	tail-&gt;mapping = TAIL_MAPPING;
&gt; +	set_compound_head(tail, head, order);
&gt; +	set_page_private(tail, 0);
&gt;   }
Only nits, in general LGTM

Acked-by: David Hildenbrand (arm) &lt;david@kernel.org&gt;

-- 
Cheers,

David

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; The &#x27;compound_head&#x27; field in the &#x27;struct page&#x27; encodes whether the page
&gt; is a tail and where to locate the head page. Bit 0 is set if the page is
&gt; a tail, and the remaining bits in the field point to the head page.
&gt; 
&gt; As preparation for changing how the field encodes information about the
&gt; head page, rename the field to &#x27;compound_info&#x27;.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
&gt; Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt; ---

Acked-by: David Hildenbrand (arm) &lt;david@kernel.org&gt;

-- 
Cheers,

David

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; Move set_compound_head() and clear_compound_head() to be adjacent to the
&gt; compound_head() function in page-flags.h.
&gt; 
&gt; These functions encode and decode the same compound_info field, so
&gt; keeping them together makes it easier to verify their logic is
&gt; consistent, especially when the encoding changes.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
&gt; Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt; ---
&gt;   include/linux/page-flags.h | 24 ++++++++++++------------
&gt;   1 file changed, 12 insertions(+), 12 deletions(-)
&gt; 
&gt; diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
&gt; index aa46d49e82f7..d14a17ffb55b 100644
&gt; --- a/include/linux/page-flags.h
&gt; +++ b/include/linux/page-flags.h
&gt; @@ -290,6 +290,18 @@ static __always_inline unsigned long _compound_head(const struct page *page)
&gt;   
&gt;   #define compound_head(page)	((typeof(page))_compound_head(page))
&gt;   
&gt; +static __always_inline void set_compound_head(struct page *page,
&gt; +					      const struct page *head,
&gt; +					      unsigned int order)

^ :)

Acked-by: David Hildenbrand (arm) &lt;david@kernel.org&gt;

-- 
Cheers,

David

---

On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; Move set_compound_head() and clear_compound_head() to be adjacent to the
&gt; compound_head() function in page-flags.h.
&gt; 
&gt; These functions encode and decode the same compound_info field, so
&gt; keeping them together makes it easier to verify their logic is
&gt; consistent, especially when the encoding changes.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
&gt; Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt; ---
&gt;   include/linux/page-flags.h | 24 ++++++++++++------------
&gt;   1 file changed, 12 insertions(+), 12 deletions(-)
&gt; 
&gt; diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
&gt; index aa46d49e82f7..d14a17ffb55b 100644
&gt; --- a/include/linux/page-flags.h
&gt; +++ b/include/linux/page-flags.h
&gt; @@ -290,6 +290,18 @@ static __always_inline unsigned long _compound_head(const struct page *page)
&gt;   
&gt;   #define compound_head(page)	((typeof(page))_compound_head(page))
&gt;   
&gt; +static __always_inline void set_compound_head(struct page *page,
&gt; +					      const struct page *head,
&gt; +					      unsigned int order)

^ :)

Acked-by: David Hildenbrand (arm) &lt;david@kernel.org&gt;

-- 
Cheers,

David

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; The upcoming change to the HugeTLB vmemmap optimization (HVO) requires
&gt; struct pages of the head page to be naturally aligned with regard to the
&gt; folio size.
&gt; 
&gt; Align vmemmap to MAX_FOLIO_NR_PAGES.

I think neither that statement nor the one in the patch description is 
correct?

&quot;MAX_FOLIO_NR_PAGES * sizeof(struct page)&quot; is neither the maximum folio 
size nor MAX_FOLIO_NR_PAGES.

It&#x27;s the size of the memmap that a large folio could span at maximum.


Assuming we have a 16 GiB folio, the calculation would give us

	4194304 * sizeof(struct page)

Which could be something like (assuming 80 bytes)

	335544320

-&gt; not even a power of 2, weird? (for HVO you wouldn&#x27;t care as HVO would 
be disabled, but that aliment is super weird?)


Assuming 64 bytes, it would be a power of two (as 64 is a power of two).

	268435456 (1&lt;&lt; 28)


Which makes me wonder whether there is a way to avoid sizeof(struct 
page) here completely.

Or limit the alignment to the case where HVO is actually active and 
sizeof(struct page) makes any sense?


&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; ---
&gt;   arch/riscv/mm/init.c | 3 ++-
&gt;   1 file changed, 2 insertions(+), 1 deletion(-)
&gt; 
&gt; diff --git a/arch/riscv/mm/init.c b/arch/riscv/mm/init.c
&gt; index 21d534824624..c555b9a4fdce 100644
&gt; --- a/arch/riscv/mm/init.c
&gt; +++ b/arch/riscv/mm/init.c
&gt; @@ -63,7 +63,8 @@ phys_addr_t phys_ram_base __ro_after_init;
&gt;   EXPORT_SYMBOL(phys_ram_base);
&gt;   
&gt;   #ifdef CONFIG_SPARSEMEM_VMEMMAP
&gt; -#define VMEMMAP_ADDR_ALIGN	(1ULL &lt;&lt; SECTION_SIZE_BITS)
&gt; +#define VMEMMAP_ADDR_ALIGN	max(1ULL &lt;&lt; SECTION_SIZE_BITS, \
&gt; +				    MAX_FOLIO_NR_PAGES * sizeof(struct page))
&gt;   
&gt;   unsigned long vmemmap_start_pfn __ro_after_init;
&gt;   EXPORT_SYMBOL(vmemmap_start_pfn);


-- 
Cheers,

David

---

On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; The upcoming change to the HugeTLB vmemmap optimization (HVO) requires
&gt; struct pages of the head page to be naturally aligned with regard to the
&gt; folio size.
&gt; 
&gt; Align vmemmap to MAX_FOLIO_NR_PAGES.

I think neither that statement nor the one in the patch description is 
correct?

&quot;MAX_FOLIO_NR_PAGES * sizeof(struct page)&quot; is neither the maximum folio 
size nor MAX_FOLIO_NR_PAGES.

It&#x27;s the size of the memmap that a large folio could span at maximum.


Assuming we have a 16 GiB folio, the calculation would give us

	4194304 * sizeof(struct page)

Which could be something like (assuming 80 bytes)

	335544320

-&gt; not even a power of 2, weird? (for HVO you wouldn&#x27;t care as HVO would 
be disabled, but that aliment is super weird?)


Assuming 64 bytes, it would be a power of two (as 64 is a power of two).

	268435456 (1&lt;&lt; 28)


Which makes me wonder whether there is a way to avoid sizeof(struct 
page) here completely.

Or limit the alignment to the case where HVO is actually active and 
sizeof(struct page) makes any sense?


&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; ---
&gt;   arch/riscv/mm/init.c | 3 ++-
&gt;   1 file changed, 2 insertions(+), 1 deletion(-)
&gt; 
&gt; diff --git a/arch/riscv/mm/init.c b/arch/riscv/mm/init.c
&gt; index 21d534824624..c555b9a4fdce 100644
&gt; --- a/arch/riscv/mm/init.c
&gt; +++ b/arch/riscv/mm/init.c
&gt; @@ -63,7 +63,8 @@ phys_addr_t phys_ram_base __ro_after_init;
&gt;   EXPORT_SYMBOL(phys_ram_base);
&gt;   
&gt;   #ifdef CONFIG_SPARSEMEM_VMEMMAP
&gt; -#define VMEMMAP_ADDR_ALIGN	(1ULL &lt;&lt; SECTION_SIZE_BITS)
&gt; +#define VMEMMAP_ADDR_ALIGN	max(1ULL &lt;&lt; SECTION_SIZE_BITS, \
&gt; +				    MAX_FOLIO_NR_PAGES * sizeof(struct page))
&gt;   
&gt;   unsigned long vmemmap_start_pfn __ro_after_init;
&gt;   EXPORT_SYMBOL(vmemmap_start_pfn);


-- 
Cheers,

David

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; The upcoming change to the HugeTLB vmemmap optimization (HVO) requires
&gt; struct pages of the head page to be naturally aligned with regard to the
&gt; folio size.
&gt; 
&gt; Align vmemmap to MAX_FOLIO_NR_PAGES.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; ---
&gt;   arch/loongarch/include/asm/pgtable.h | 3 ++-
&gt;   1 file changed, 2 insertions(+), 1 deletion(-)
&gt; 
&gt; diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h
&gt; index c33b3bcb733e..f9416acb9156 100644
&gt; --- a/arch/loongarch/include/asm/pgtable.h
&gt; +++ b/arch/loongarch/include/asm/pgtable.h
&gt; @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
&gt;   	 min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL &lt;&lt; cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)
&gt;   #endif
&gt;   
&gt; -#define vmemmap		((struct page *)((VMALLOC_END + PMD_SIZE) &amp; PMD_MASK))
&gt; +#define VMEMMAP_ALIGN	max(PMD_SIZE, MAX_FOLIO_NR_PAGES * sizeof(struct page))
&gt; +#define vmemmap		((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))


Same comment, the &quot;MAX_FOLIO_NR_PAGES * sizeof(struct page)&quot; is just black magic here
and the description of the situation is wrong.

Maybe you want to pull the magic &quot;MAX_FOLIO_NR_PAGES * sizeof(struct page)&quot; into the core and call it

#define MAX_FOLIO_VMEMMAP_ALIGN	(MAX_FOLIO_NR_PAGES * sizeof(struct page))

But then special case it base on (a) HVO being configured in an (b) HVO being possible

#ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP &amp;&amp; is_power_of_2(sizeof(struct page)
/* A very helpful comment explaining the situation. */
#define MAX_FOLIO_VMEMMAP_ALIGN	(MAX_FOLIO_NR_PAGES * sizeof(struct page))
#else
#define MAX_FOLIO_VMEMMAP_ALIGN	0
#endif

Something like that.

-- 
Cheers,

David

---

On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; The upcoming change to the HugeTLB vmemmap optimization (HVO) requires
&gt; struct pages of the head page to be naturally aligned with regard to the
&gt; folio size.
&gt; 
&gt; Align vmemmap to MAX_FOLIO_NR_PAGES.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; ---
&gt;   arch/loongarch/include/asm/pgtable.h | 3 ++-
&gt;   1 file changed, 2 insertions(+), 1 deletion(-)
&gt; 
&gt; diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h
&gt; index c33b3bcb733e..f9416acb9156 100644
&gt; --- a/arch/loongarch/include/asm/pgtable.h
&gt; +++ b/arch/loongarch/include/asm/pgtable.h
&gt; @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
&gt;   	 min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL &lt;&lt; cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)
&gt;   #endif
&gt;   
&gt; -#define vmemmap		((struct page *)((VMALLOC_END + PMD_SIZE) &amp; PMD_MASK))
&gt; +#define VMEMMAP_ALIGN	max(PMD_SIZE, MAX_FOLIO_NR_PAGES * sizeof(struct page))
&gt; +#define vmemmap		((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))


Same comment, the &quot;MAX_FOLIO_NR_PAGES * sizeof(struct page)&quot; is just black magic here
and the description of the situation is wrong.

Maybe you want to pull the magic &quot;MAX_FOLIO_NR_PAGES * sizeof(struct page)&quot; into the core and call it

#define MAX_FOLIO_VMEMMAP_ALIGN	(MAX_FOLIO_NR_PAGES * sizeof(struct page))

But then special case it base on (a) HVO being configured in an (b) HVO being possible

#ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP &amp;&amp; is_power_of_2(sizeof(struct page)
/* A very helpful comment explaining the situation. */
#define MAX_FOLIO_VMEMMAP_ALIGN	(MAX_FOLIO_NR_PAGES * sizeof(struct page))
#else
#define MAX_FOLIO_VMEMMAP_ALIGN	0
#endif

Something like that.

-- 
Cheers,

David

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On 2/5/26 12:35, Kiryl Shutsemau wrote:
&gt; On Wed, Feb 04, 2026 at 05:14:12PM +0100, David Hildenbrand (arm) wrote:
&gt;&gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt;&gt;&gt; Instead of passing down the head page and tail page index, pass the tail
&gt;&gt;&gt; and head pages directly, as well as the order of the compound page.
&gt;&gt;&gt;
&gt;&gt;&gt; This is a preparation for changing how the head position is encoded in
&gt;&gt;&gt; the tail page.
&gt;&gt;&gt;
&gt;&gt;&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt;&gt;&gt; Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
&gt;&gt;&gt; Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt;&gt;&gt; ---
&gt;&gt;&gt;    include/linux/page-flags.h |  4 +++-
&gt;&gt;&gt;    mm/hugetlb.c               |  8 +++++---
&gt;&gt;&gt;    mm/internal.h              | 12 ++++++------
&gt;&gt;&gt;    mm/mm_init.c               |  2 +-
&gt;&gt;&gt;    mm/page_alloc.c            |  2 +-
&gt;&gt;&gt;    5 files changed, 16 insertions(+), 12 deletions(-)
&gt;&gt;&gt;
&gt;&gt;&gt; diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
&gt;&gt;&gt; index f7a0e4af0c73..8a3694369e15 100644
&gt;&gt;&gt; --- a/include/linux/page-flags.h
&gt;&gt;&gt; +++ b/include/linux/page-flags.h
&gt;&gt;&gt; @@ -865,7 +865,9 @@ static inline bool folio_test_large(const struct folio *folio)
&gt;&gt;&gt;    	return folio_test_head(folio);
&gt;&gt;&gt;    }
&gt;&gt;&gt; -static __always_inline void set_compound_head(struct page *page, struct page *head)
&gt;&gt;&gt; +static __always_inline void set_compound_head(struct page *page,
&gt;&gt;&gt; +					      const struct page *head,
&gt;&gt;&gt; +					      unsigned int order)
&gt;&gt;
&gt;&gt; Two tab indents please on second+ parameter list whenever you touch code.
&gt; 
&gt; Do we have this coding style preference written down somewhere?

Good question. I assume not. But it&#x27;s what we do in MM :)

&gt; 
&gt; -tip tree wants the opposite. Documentation/process/maintainer-tip.rst:
&gt; 
&gt; 	When splitting function declarations or function calls, then please align
&gt; 	the first argument in the second line with the first argument in the first
&gt; 	line::
&gt; 
&gt; I want the editor to do The Right Thing\u2122 without my brain involvement.
&gt; Having different coding styles in different corners of the kernel makes
&gt; it hard.

Yeah, but unavoidable. :)

-- 
Cheers,

David

---

On 2/5/26 12:35, Kiryl Shutsemau wrote:
&gt; On Wed, Feb 04, 2026 at 05:14:12PM +0100, David Hildenbrand (arm) wrote:
&gt;&gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt;&gt;&gt; Instead of passing down the head page and tail page index, pass the tail
&gt;&gt;&gt; and head pages directly, as well as the order of the compound page.
&gt;&gt;&gt;
&gt;&gt;&gt; This is a preparation for changing how the head position is encoded in
&gt;&gt;&gt; the tail page.
&gt;&gt;&gt;
&gt;&gt;&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt;&gt;&gt; Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
&gt;&gt;&gt; Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt;&gt;&gt; ---
&gt;&gt;&gt;    include/linux/page-flags.h |  4 +++-
&gt;&gt;&gt;    mm/hugetlb.c               |  8 +++++---
&gt;&gt;&gt;    mm/internal.h              | 12 ++++++------
&gt;&gt;&gt;    mm/mm_init.c               |  2 +-
&gt;&gt;&gt;    mm/page_alloc.c            |  2 +-
&gt;&gt;&gt;    5 files changed, 16 insertions(+), 12 deletions(-)
&gt;&gt;&gt;
&gt;&gt;&gt; diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
&gt;&gt;&gt; index f7a0e4af0c73..8a3694369e15 100644
&gt;&gt;&gt; --- a/include/linux/page-flags.h
&gt;&gt;&gt; +++ b/include/linux/page-flags.h
&gt;&gt;&gt; @@ -865,7 +865,9 @@ static inline bool folio_test_large(const struct folio *folio)
&gt;&gt;&gt;    	return folio_test_head(folio);
&gt;&gt;&gt;    }
&gt;&gt;&gt; -static __always_inline void set_compound_head(struct page *page, struct page *head)
&gt;&gt;&gt; +static __always_inline void set_compound_head(struct page *page,
&gt;&gt;&gt; +					      const struct page *head,
&gt;&gt;&gt; +					      unsigned int order)
&gt;&gt;
&gt;&gt; Two tab indents please on second+ parameter list whenever you touch code.
&gt; 
&gt; Do we have this coding style preference written down somewhere?

Good question. I assume not. But it&#x27;s what we do in MM :)

&gt; 
&gt; -tip tree wants the opposite. Documentation/process/maintainer-tip.rst:
&gt; 
&gt; 	When splitting function declarations or function calls, then please align
&gt; 	the first argument in the second line with the first argument in the first
&gt; 	line::
&gt; 
&gt; I want the editor to do The Right Thing™ without my brain involvement.
&gt; Having different coding styles in different corners of the kernel makes
&gt; it hard.

Yeah, but unavoidable. :)

-- 
Cheers,

David

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On 2/4/26 17:56, David Hildenbrand (arm) wrote:
&gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt;&gt; The upcoming change to the HugeTLB vmemmap optimization (HVO) requires
&gt;&gt; struct pages of the head page to be naturally aligned with regard to the
&gt;&gt; folio size.
&gt;&gt;
&gt;&gt; Align vmemmap to MAX_FOLIO_NR_PAGES.
&gt;&gt;
&gt;&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt;&gt; ---
&gt;&gt;  arch/loongarch/include/asm/pgtable.h | 3 ++-
&gt;&gt;  1 file changed, 2 insertions(+), 1 deletion(-)
&gt;&gt;
&gt;&gt; diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/ 
&gt;&gt; include/asm/pgtable.h
&gt;&gt; index c33b3bcb733e..f9416acb9156 100644
&gt;&gt; --- a/arch/loongarch/include/asm/pgtable.h
&gt;&gt; +++ b/arch/loongarch/include/asm/pgtable.h
&gt;&gt; @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / 
&gt;&gt; sizeof(unsigned long)];
&gt;&gt;  min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * 
&gt;&gt; PAGE_SIZE, (1UL &lt;&lt; cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - 
&gt;&gt; KFENCE_AREA_SIZE)
&gt;&gt;  #endif
&gt;&gt; -#define vmemmap ((struct page *)((VMALLOC_END + PMD_SIZE) &amp; 
&gt;&gt; PMD_MASK))
&gt;&gt; +#define VMEMMAP_ALIGN max(PMD_SIZE, MAX_FOLIO_NR_PAGES * 
&gt;&gt; sizeof(struct page))
&gt;&gt; +#define vmemmap ((struct page *)(ALIGN(VMALLOC_END, 
&gt;&gt; VMEMMAP_ALIGN)))
&gt; 
&gt; 
&gt; Same comment, the &quot;MAX_FOLIO_NR_PAGES * sizeof(struct page)&quot; is just 
&gt; black magic here
&gt; and the description of the situation is wrong.
&gt; 
&gt; Maybe you want to pull the magic &quot;MAX_FOLIO_NR_PAGES * sizeof(struct 
&gt; page)&quot; into the core and call it
&gt; 
&gt; #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct 
&gt; page))
&gt; 
&gt; But then special case it base on (a) HVO being configured in an (b) HVO 
&gt; being possible
&gt; 
&gt; #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP &amp;&amp; is_power_of_2(sizeof(struct page)
&gt; /* A very helpful comment explaining the situation. */
&gt; #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct 
&gt; page))
&gt; #else
&gt; #define MAX_FOLIO_VMEMMAP_ALIGN 0
&gt; #endif
&gt; 
&gt; Something like that.
&gt; 

Thinking about this ...

the vmemmap start is always struct-page-aligned. Otherwise we&#x27;d be in 
trouble already.

Isn&#x27;t it then sufficient to just align the start to MAX_FOLIO_NR_PAGES?

Let&#x27;s assume sizeof(struct page) == 64 and MAX_FOLIO_NR_PAGES = 512 for 
simplicity.

vmemmap start would be multiples of 512 (0x0010000000).

512, 1024, 1536, 2048 ...

Assume we have an 256-pages folio at 1536+256 = 0x111000000

Assume we have the last page of that folio (0x011111111111), we would 
just get to the start of that folio by AND-ing with ~(256-1).

Which case am I ignoring?

-- 
Cheers,

David</pre>
</details>
<a href="https://lore.kernel.org/r/a61bc0a8-cf5a-418a-aeb4-96553b87f043@kernel.org" target="_blank" rel="noopener" class="lore-link">View on lore &#8599;</a>
<div class="review-comment-signals">Signals: LGTM, Only nits</div>
</div>
</div>
<div class="thread-node depth-0" id="2026-02-05">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Kiryl Shutsemau (author)</span>
<a class="date-chip" href="../2026-02-27_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-05">2026-02-05</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Author acknowledged that the calculation for MAX_FOLIO_NR_PAGES * sizeof(struct page) is incorrect and can result in a non-power-of-2 value, which would disable HVO even if it&#x27;s configured. He suggested using roundup_pow_of_two(sizeof(struct page)) instead, but also mentioned that avoiding sizeof(struct page) altogether might not be possible.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Wed, Feb 04, 2026 at 05:50:23PM +0100, David Hildenbrand (arm) wrote:
&gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; &gt; The upcoming change to the HugeTLB vmemmap optimization (HVO) requires
&gt; &gt; struct pages of the head page to be naturally aligned with regard to the
&gt; &gt; folio size.
&gt; &gt; 
&gt; &gt; Align vmemmap to MAX_FOLIO_NR_PAGES.
&gt; 
&gt; I think neither that statement nor the one in the patch description is
&gt; correct?
&gt; 
&gt; &quot;MAX_FOLIO_NR_PAGES * sizeof(struct page)&quot; is neither the maximum folio size
&gt; nor MAX_FOLIO_NR_PAGES.
&gt; 
&gt; It&#x27;s the size of the memmap that a large folio could span at maximum.
&gt; 
&gt; 
&gt; Assuming we have a 16 GiB folio, the calculation would give us
&gt; 
&gt; 	4194304 * sizeof(struct page)
&gt; 
&gt; Which could be something like (assuming 80 bytes)
&gt; 
&gt; 	335544320
&gt; 
&gt; -&gt; not even a power of 2, weird? (for HVO you wouldn&#x27;t care as HVO would be
&gt; disabled, but that aliment is super weird?)
&gt; 
&gt; 
&gt; Assuming 64 bytes, it would be a power of two (as 64 is a power of two).
&gt; 
&gt; 	268435456 (1&lt;&lt; 28)
&gt; 
&gt; 
&gt; Which makes me wonder whether there is a way to avoid sizeof(struct page)
&gt; here completely.

I don&#x27;t think we can. See the other thread.

What about using roundup_pow_of_two(sizeof(struct page)) here.

&gt; Or limit the alignment to the case where HVO is actually active and
&gt; sizeof(struct page) makes any sense?

The annoying part of HVO is that it is unknown at compile-time if it
will be used. You can compile kernel with HVO that will no be activated
due to non-power-of-2 sizeof(struct page) because of a debug config option.

-- 
  Kiryl Shutsemau / Kirill A. Shutemov

---

On Wed, Feb 04, 2026 at 05:50:23PM +0100, David Hildenbrand (arm) wrote:
&gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; &gt; The upcoming change to the HugeTLB vmemmap optimization (HVO) requires
&gt; &gt; struct pages of the head page to be naturally aligned with regard to the
&gt; &gt; folio size.
&gt; &gt; 
&gt; &gt; Align vmemmap to MAX_FOLIO_NR_PAGES.
&gt; 
&gt; I think neither that statement nor the one in the patch description is
&gt; correct?
&gt; 
&gt; &quot;MAX_FOLIO_NR_PAGES * sizeof(struct page)&quot; is neither the maximum folio size
&gt; nor MAX_FOLIO_NR_PAGES.
&gt; 
&gt; It&#x27;s the size of the memmap that a large folio could span at maximum.
&gt; 
&gt; 
&gt; Assuming we have a 16 GiB folio, the calculation would give us
&gt; 
&gt; 	4194304 * sizeof(struct page)
&gt; 
&gt; Which could be something like (assuming 80 bytes)
&gt; 
&gt; 	335544320
&gt; 
&gt; -&gt; not even a power of 2, weird? (for HVO you wouldn&#x27;t care as HVO would be
&gt; disabled, but that aliment is super weird?)
&gt; 
&gt; 
&gt; Assuming 64 bytes, it would be a power of two (as 64 is a power of two).
&gt; 
&gt; 	268435456 (1&lt;&lt; 28)
&gt; 
&gt; 
&gt; Which makes me wonder whether there is a way to avoid sizeof(struct page)
&gt; here completely.

I don&#x27;t think we can. See the other thread.

What about using roundup_pow_of_two(sizeof(struct page)) here.

&gt; Or limit the alignment to the case where HVO is actually active and
&gt; sizeof(struct page) makes any sense?

The annoying part of HVO is that it is unknown at compile-time if it
will be used. You can compile kernel with HVO that will no be activated
due to non-power-of-2 sizeof(struct page) because of a debug config option.

-- 
  Kiryl Shutsemau / Kirill A. Shutemov

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On Wed, Feb 04, 2026 at 05:56:45PM +0100, David Hildenbrand (arm) wrote:
&gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; &gt; The upcoming change to the HugeTLB vmemmap optimization (HVO) requires
&gt; &gt; struct pages of the head page to be naturally aligned with regard to the
&gt; &gt; folio size.
&gt; &gt; 
&gt; &gt; Align vmemmap to MAX_FOLIO_NR_PAGES.
&gt; &gt; 
&gt; &gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; &gt; ---
&gt; &gt;   arch/loongarch/include/asm/pgtable.h | 3 ++-
&gt; &gt;   1 file changed, 2 insertions(+), 1 deletion(-)
&gt; &gt; 
&gt; &gt; diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h
&gt; &gt; index c33b3bcb733e..f9416acb9156 100644
&gt; &gt; --- a/arch/loongarch/include/asm/pgtable.h
&gt; &gt; +++ b/arch/loongarch/include/asm/pgtable.h
&gt; &gt; @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
&gt; &gt;   	 min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL &lt;&lt; cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)
&gt; &gt;   #endif
&gt; &gt; -#define vmemmap		((struct page *)((VMALLOC_END + PMD_SIZE) &amp; PMD_MASK))
&gt; &gt; +#define VMEMMAP_ALIGN	max(PMD_SIZE, MAX_FOLIO_NR_PAGES * sizeof(struct page))
&gt; &gt; +#define vmemmap		((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))
&gt; 
&gt; 
&gt; Same comment, the &quot;MAX_FOLIO_NR_PAGES * sizeof(struct page)&quot; is just black magic here
&gt; and the description of the situation is wrong.
&gt; 
&gt; Maybe you want to pull the magic &quot;MAX_FOLIO_NR_PAGES * sizeof(struct page)&quot; into the core and call it
&gt; 
&gt; #define MAX_FOLIO_VMEMMAP_ALIGN	(MAX_FOLIO_NR_PAGES * sizeof(struct page))
&gt; 
&gt; But then special case it base on (a) HVO being configured in an (b) HVO being possible
&gt; 
&gt; #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP &amp;&amp; is_power_of_2(sizeof(struct page)

This would require some kind of asm-offsets.c/bounds.c magic to pull the
struct page size condition to the preprocessor level.

-- 
  Kiryl Shutsemau / Kirill A. Shutemov

---

On Wed, Feb 04, 2026 at 05:56:45PM +0100, David Hildenbrand (arm) wrote:
&gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; &gt; The upcoming change to the HugeTLB vmemmap optimization (HVO) requires
&gt; &gt; struct pages of the head page to be naturally aligned with regard to the
&gt; &gt; folio size.
&gt; &gt; 
&gt; &gt; Align vmemmap to MAX_FOLIO_NR_PAGES.
&gt; &gt; 
&gt; &gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; &gt; ---
&gt; &gt;   arch/loongarch/include/asm/pgtable.h | 3 ++-
&gt; &gt;   1 file changed, 2 insertions(+), 1 deletion(-)
&gt; &gt; 
&gt; &gt; diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h
&gt; &gt; index c33b3bcb733e..f9416acb9156 100644
&gt; &gt; --- a/arch/loongarch/include/asm/pgtable.h
&gt; &gt; +++ b/arch/loongarch/include/asm/pgtable.h
&gt; &gt; @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
&gt; &gt;   	 min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL &lt;&lt; cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)
&gt; &gt;   #endif
&gt; &gt; -#define vmemmap		((struct page *)((VMALLOC_END + PMD_SIZE) &amp; PMD_MASK))
&gt; &gt; +#define VMEMMAP_ALIGN	max(PMD_SIZE, MAX_FOLIO_NR_PAGES * sizeof(struct page))
&gt; &gt; +#define vmemmap		((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))
&gt; 
&gt; 
&gt; Same comment, the &quot;MAX_FOLIO_NR_PAGES * sizeof(struct page)&quot; is just black magic here
&gt; and the description of the situation is wrong.
&gt; 
&gt; Maybe you want to pull the magic &quot;MAX_FOLIO_NR_PAGES * sizeof(struct page)&quot; into the core and call it
&gt; 
&gt; #define MAX_FOLIO_VMEMMAP_ALIGN	(MAX_FOLIO_NR_PAGES * sizeof(struct page))
&gt; 
&gt; But then special case it base on (a) HVO being configured in an (b) HVO being possible
&gt; 
&gt; #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP &amp;&amp; is_power_of_2(sizeof(struct page)

This would require some kind of asm-offsets.c/bounds.c magic to pull the
struct page size condition to the preprocessor level.

-- 
  Kiryl Shutsemau / Kirill A. Shutemov

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:
&gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; &gt; With the upcoming changes to HVO, a single page of tail struct pages
&gt; &gt; will be shared across all huge pages of the same order on a node. Since
&gt; &gt; huge pages on the same node may belong to different zones, the zone
&gt; &gt; information stored in shared tail page flags would be incorrect.
&gt; &gt; 
&gt; &gt; Always fetch zone information from the head page, which has unique and
&gt; &gt; correct zone flags for each compound page.
&gt; &gt; 
&gt; &gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; &gt; Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt; &gt; ---
&gt; &gt;   include/linux/mmzone.h | 1 +
&gt; &gt;   1 file changed, 1 insertion(+)
&gt; &gt; 
&gt; &gt; diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
&gt; &gt; index be8ce40b5638..192143b5cdc0 100644
&gt; &gt; --- a/include/linux/mmzone.h
&gt; &gt; +++ b/include/linux/mmzone.h
&gt; &gt; @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)
&gt; &gt;   static inline enum zone_type page_zonenum(const struct page *page)
&gt; &gt;   {
&gt; &gt; +	page = compound_head(page);
&gt; &gt;   	return memdesc_zonenum(page-&gt;flags);
&gt; 
&gt; We end up calling page_zonenum() without holding a reference.
&gt; 
&gt; Given that _compound_head() does a READ_ONCE(), this should work even if we
&gt; see concurrent page freeing etc.
&gt; 
&gt; However, this change implies that we now perform a compound page lookup for
&gt; every PageHighMem() [meh], page_zone() [quite some users in the buddy,
&gt; including for pageblock access and page freeing].
&gt; 
&gt; That&#x27;s a nasty compromise for making HVO better? :)
&gt; 
&gt; We should likely limit that special casing to kernels that really rquire it
&gt; (HVO).

I will add compound_info_has_mask() check.

-- 
  Kiryl Shutsemau / Kirill A. Shutemov

---

On Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:
&gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; &gt; With the upcoming changes to HVO, a single page of tail struct pages
&gt; &gt; will be shared across all huge pages of the same order on a node. Since
&gt; &gt; huge pages on the same node may belong to different zones, the zone
&gt; &gt; information stored in shared tail page flags would be incorrect.
&gt; &gt; 
&gt; &gt; Always fetch zone information from the head page, which has unique and
&gt; &gt; correct zone flags for each compound page.
&gt; &gt; 
&gt; &gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; &gt; Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt; &gt; ---
&gt; &gt;   include/linux/mmzone.h | 1 +
&gt; &gt;   1 file changed, 1 insertion(+)
&gt; &gt; 
&gt; &gt; diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
&gt; &gt; index be8ce40b5638..192143b5cdc0 100644
&gt; &gt; --- a/include/linux/mmzone.h
&gt; &gt; +++ b/include/linux/mmzone.h
&gt; &gt; @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)
&gt; &gt;   static inline enum zone_type page_zonenum(const struct page *page)
&gt; &gt;   {
&gt; &gt; +	page = compound_head(page);
&gt; &gt;   	return memdesc_zonenum(page-&gt;flags);
&gt; 
&gt; We end up calling page_zonenum() without holding a reference.
&gt; 
&gt; Given that _compound_head() does a READ_ONCE(), this should work even if we
&gt; see concurrent page freeing etc.
&gt; 
&gt; However, this change implies that we now perform a compound page lookup for
&gt; every PageHighMem() [meh], page_zone() [quite some users in the buddy,
&gt; including for pageblock access and page freeing].
&gt; 
&gt; That&#x27;s a nasty compromise for making HVO better? :)
&gt; 
&gt; We should likely limit that special casing to kernels that really rquire it
&gt; (HVO).

I will add compound_info_has_mask() check.

-- 
  Kiryl Shutsemau / Kirill A. Shutemov

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On Tue, Feb 10, 2026 at 04:57:55PM +0100, Vlastimil Babka wrote:
&gt; On 2/9/26 12:52, Kiryl Shutsemau wrote:
&gt; &gt; On Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:
&gt; &gt;&gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; &gt;&gt; &gt; With the upcoming changes to HVO, a single page of tail struct pages
&gt; &gt;&gt; &gt; will be shared across all huge pages of the same order on a node. Since
&gt; &gt;&gt; &gt; huge pages on the same node may belong to different zones, the zone
&gt; &gt;&gt; &gt; information stored in shared tail page flags would be incorrect.
&gt; &gt;&gt; &gt; 
&gt; &gt;&gt; &gt; Always fetch zone information from the head page, which has unique and
&gt; &gt;&gt; &gt; correct zone flags for each compound page.
&gt; &gt;&gt; &gt; 
&gt; &gt;&gt; &gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; &gt;&gt; &gt; Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt; &gt;&gt; &gt; ---
&gt; &gt;&gt; &gt;   include/linux/mmzone.h | 1 +
&gt; &gt;&gt; &gt;   1 file changed, 1 insertion(+)
&gt; &gt;&gt; &gt; 
&gt; &gt;&gt; &gt; diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
&gt; &gt;&gt; &gt; index be8ce40b5638..192143b5cdc0 100644
&gt; &gt;&gt; &gt; --- a/include/linux/mmzone.h
&gt; &gt;&gt; &gt; +++ b/include/linux/mmzone.h
&gt; &gt;&gt; &gt; @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)
&gt; &gt;&gt; &gt;   static inline enum zone_type page_zonenum(const struct page *page)
&gt; &gt;&gt; &gt;   {
&gt; &gt;&gt; &gt; +	page = compound_head(page);
&gt; &gt;&gt; &gt;   	return memdesc_zonenum(page-&gt;flags);
&gt; &gt;&gt; 
&gt; &gt;&gt; We end up calling page_zonenum() without holding a reference.
&gt; &gt;&gt; 
&gt; &gt;&gt; Given that _compound_head() does a READ_ONCE(), this should work even if we
&gt; &gt;&gt; see concurrent page freeing etc.
&gt; &gt;&gt; 
&gt; &gt;&gt; However, this change implies that we now perform a compound page lookup for
&gt; &gt;&gt; every PageHighMem() [meh], page_zone() [quite some users in the buddy,
&gt; &gt;&gt; including for pageblock access and page freeing].
&gt; &gt;&gt; 
&gt; &gt;&gt; That&#x27;s a nasty compromise for making HVO better? :)
&gt; &gt;&gt; 
&gt; &gt;&gt; We should likely limit that special casing to kernels that really rquire it
&gt; &gt;&gt; (HVO).
&gt; &gt; 
&gt; &gt; I will add compound_info_has_mask() check.
&gt; 
&gt; Not thrilled by this indeed. Would it be a problem to have the shared tail
&gt; pages per node+zone instead of just per node?

I thought it would be overkill. It likely is going to be unused for most
nodes. But sure, move it to per-zone.

-- 
  Kiryl Shutsemau / Kirill A. Shutemov

---

On Tue, Feb 10, 2026 at 04:57:55PM +0100, Vlastimil Babka wrote:
&gt; On 2/9/26 12:52, Kiryl Shutsemau wrote:
&gt; &gt; On Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:
&gt; &gt;&gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; &gt;&gt; &gt; With the upcoming changes to HVO, a single page of tail struct pages
&gt; &gt;&gt; &gt; will be shared across all huge pages of the same order on a node. Since
&gt; &gt;&gt; &gt; huge pages on the same node may belong to different zones, the zone
&gt; &gt;&gt; &gt; information stored in shared tail page flags would be incorrect.
&gt; &gt;&gt; &gt; 
&gt; &gt;&gt; &gt; Always fetch zone information from the head page, which has unique and
&gt; &gt;&gt; &gt; correct zone flags for each compound page.
&gt; &gt;&gt; &gt; 
&gt; &gt;&gt; &gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; &gt;&gt; &gt; Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt; &gt;&gt; &gt; ---
&gt; &gt;&gt; &gt;   include/linux/mmzone.h | 1 +
&gt; &gt;&gt; &gt;   1 file changed, 1 insertion(+)
&gt; &gt;&gt; &gt; 
&gt; &gt;&gt; &gt; diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
&gt; &gt;&gt; &gt; index be8ce40b5638..192143b5cdc0 100644
&gt; &gt;&gt; &gt; --- a/include/linux/mmzone.h
&gt; &gt;&gt; &gt; +++ b/include/linux/mmzone.h
&gt; &gt;&gt; &gt; @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)
&gt; &gt;&gt; &gt;   static inline enum zone_type page_zonenum(const struct page *page)
&gt; &gt;&gt; &gt;   {
&gt; &gt;&gt; &gt; +	page = compound_head(page);
&gt; &gt;&gt; &gt;   	return memdesc_zonenum(page-&gt;flags);
&gt; &gt;&gt; 
&gt; &gt;&gt; We end up calling page_zonenum() without holding a reference.
&gt; &gt;&gt; 
&gt; &gt;&gt; Given that _compound_head() does a READ_ONCE(), this should work even if we
&gt; &gt;&gt; see concurrent page freeing etc.
&gt; &gt;&gt; 
&gt; &gt;&gt; However, this change implies that we now perform a compound page lookup for
&gt; &gt;&gt; every PageHighMem() [meh], page_zone() [quite some users in the buddy,
&gt; &gt;&gt; including for pageblock access and page freeing].
&gt; &gt;&gt; 
&gt; &gt;&gt; That&#x27;s a nasty compromise for making HVO better? :)
&gt; &gt;&gt; 
&gt; &gt;&gt; We should likely limit that special casing to kernels that really rquire it
&gt; &gt;&gt; (HVO).
&gt; &gt; 
&gt; &gt; I will add compound_info_has_mask() check.
&gt; 
&gt; Not thrilled by this indeed. Would it be a problem to have the shared tail
&gt; pages per node+zone instead of just per node?

I thought it would be overkill. It likely is going to be unused for most
nodes. But sure, move it to per-zone.

-- 
  Kiryl Shutsemau / Kirill A. Shutemov

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On Mon, Feb 16, 2026 at 11:30:22AM +0000, Kiryl Shutsemau wrote:
&gt; On Tue, Feb 10, 2026 at 04:57:55PM +0100, Vlastimil Babka wrote:
&gt; &gt; On 2/9/26 12:52, Kiryl Shutsemau wrote:
&gt; &gt; &gt; On Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:
&gt; &gt; &gt;&gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; &gt; &gt;&gt; &gt; With the upcoming changes to HVO, a single page of tail struct pages
&gt; &gt; &gt;&gt; &gt; will be shared across all huge pages of the same order on a node. Since
&gt; &gt; &gt;&gt; &gt; huge pages on the same node may belong to different zones, the zone
&gt; &gt; &gt;&gt; &gt; information stored in shared tail page flags would be incorrect.
&gt; &gt; &gt;&gt; &gt; 
&gt; &gt; &gt;&gt; &gt; Always fetch zone information from the head page, which has unique and
&gt; &gt; &gt;&gt; &gt; correct zone flags for each compound page.
&gt; &gt; &gt;&gt; &gt; 
&gt; &gt; &gt;&gt; &gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; &gt; &gt;&gt; &gt; Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt; &gt; &gt;&gt; &gt; ---
&gt; &gt; &gt;&gt; &gt;   include/linux/mmzone.h | 1 +
&gt; &gt; &gt;&gt; &gt;   1 file changed, 1 insertion(+)
&gt; &gt; &gt;&gt; &gt; 
&gt; &gt; &gt;&gt; &gt; diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
&gt; &gt; &gt;&gt; &gt; index be8ce40b5638..192143b5cdc0 100644
&gt; &gt; &gt;&gt; &gt; --- a/include/linux/mmzone.h
&gt; &gt; &gt;&gt; &gt; +++ b/include/linux/mmzone.h
&gt; &gt; &gt;&gt; &gt; @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)
&gt; &gt; &gt;&gt; &gt;   static inline enum zone_type page_zonenum(const struct page *page)
&gt; &gt; &gt;&gt; &gt;   {
&gt; &gt; &gt;&gt; &gt; +	page = compound_head(page);
&gt; &gt; &gt;&gt; &gt;   	return memdesc_zonenum(page-&gt;flags);
&gt; &gt; &gt;&gt; 
&gt; &gt; &gt;&gt; We end up calling page_zonenum() without holding a reference.
&gt; &gt; &gt;&gt; 
&gt; &gt; &gt;&gt; Given that _compound_head() does a READ_ONCE(), this should work even if we
&gt; &gt; &gt;&gt; see concurrent page freeing etc.
&gt; &gt; &gt;&gt; 
&gt; &gt; &gt;&gt; However, this change implies that we now perform a compound page lookup for
&gt; &gt; &gt;&gt; every PageHighMem() [meh], page_zone() [quite some users in the buddy,
&gt; &gt; &gt;&gt; including for pageblock access and page freeing].
&gt; &gt; &gt;&gt; 
&gt; &gt; &gt;&gt; That&#x27;s a nasty compromise for making HVO better? :)
&gt; &gt; &gt;&gt; 
&gt; &gt; &gt;&gt; We should likely limit that special casing to kernels that really rquire it
&gt; &gt; &gt;&gt; (HVO).
&gt; &gt; &gt; 
&gt; &gt; &gt; I will add compound_info_has_mask() check.
&gt; &gt; 
&gt; &gt; Not thrilled by this indeed. Would it be a problem to have the shared tail
&gt; &gt; pages per node+zone instead of just per node?
&gt; 
&gt; I thought it would be overkill. It likely is going to be unused for most
&gt; nodes. But sure, move it to per-zone.

I gave it a try, but stumbled on a problem.

We need to know the zone in hugetlb_vmemmap_init_early(), but zones are
not yet defined.

hugetlb_vmemmap_init_early() is called from within sparse_init(), but
span of zones is defined in free_area_init() after sparse_init().

Any ideas, how get past this? :/

-- 
  Kiryl Shutsemau / Kirill A. Shutemov

---

On Mon, Feb 16, 2026 at 11:30:22AM +0000, Kiryl Shutsemau wrote:
&gt; On Tue, Feb 10, 2026 at 04:57:55PM +0100, Vlastimil Babka wrote:
&gt; &gt; On 2/9/26 12:52, Kiryl Shutsemau wrote:
&gt; &gt; &gt; On Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:
&gt; &gt; &gt;&gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; &gt; &gt;&gt; &gt; With the upcoming changes to HVO, a single page of tail struct pages
&gt; &gt; &gt;&gt; &gt; will be shared across all huge pages of the same order on a node. Since
&gt; &gt; &gt;&gt; &gt; huge pages on the same node may belong to different zones, the zone
&gt; &gt; &gt;&gt; &gt; information stored in shared tail page flags would be incorrect.
&gt; &gt; &gt;&gt; &gt; 
&gt; &gt; &gt;&gt; &gt; Always fetch zone information from the head page, which has unique and
&gt; &gt; &gt;&gt; &gt; correct zone flags for each compound page.
&gt; &gt; &gt;&gt; &gt; 
&gt; &gt; &gt;&gt; &gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; &gt; &gt;&gt; &gt; Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt; &gt; &gt;&gt; &gt; ---
&gt; &gt; &gt;&gt; &gt;   include/linux/mmzone.h | 1 +
&gt; &gt; &gt;&gt; &gt;   1 file changed, 1 insertion(+)
&gt; &gt; &gt;&gt; &gt; 
&gt; &gt; &gt;&gt; &gt; diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
&gt; &gt; &gt;&gt; &gt; index be8ce40b5638..192143b5cdc0 100644
&gt; &gt; &gt;&gt; &gt; --- a/include/linux/mmzone.h
&gt; &gt; &gt;&gt; &gt; +++ b/include/linux/mmzone.h
&gt; &gt; &gt;&gt; &gt; @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)
&gt; &gt; &gt;&gt; &gt;   static inline enum zone_type page_zonenum(const struct page *page)
&gt; &gt; &gt;&gt; &gt;   {
&gt; &gt; &gt;&gt; &gt; +	page = compound_head(page);
&gt; &gt; &gt;&gt; &gt;   	return memdesc_zonenum(page-&gt;flags);
&gt; &gt; &gt;&gt; 
&gt; &gt; &gt;&gt; We end up calling page_zonenum() without holding a reference.
&gt; &gt; &gt;&gt; 
&gt; &gt; &gt;&gt; Given that _compound_head() does a READ_ONCE(), this should work even if we
&gt; &gt; &gt;&gt; see concurrent page freeing etc.
&gt; &gt; &gt;&gt; 
&gt; &gt; &gt;&gt; However, this change implies that we now perform a compound page lookup for
&gt; &gt; &gt;&gt; every PageHighMem() [meh], page_zone() [quite some users in the buddy,
&gt; &gt; &gt;&gt; including for pageblock access and page freeing].
&gt; &gt; &gt;&gt; 
&gt; &gt; &gt;&gt; That&#x27;s a nasty compromise for making HVO better? :)
&gt; &gt; &gt;&gt; 
&gt; &gt; &gt;&gt; We should likely limit that special casing to kernels that really rquire it
&gt; &gt; &gt;&gt; (HVO).
&gt; &gt; &gt; 
&gt; &gt; &gt; I will add compound_info_has_mask() check.
&gt; &gt; 
&gt; &gt; Not thrilled by this indeed. Would it be a problem to have the shared tail
&gt; &gt; pages per node+zone instead of just per node?
&gt; 
&gt; I thought it would be overkill. It likely is going to be unused for most
&gt; nodes. But sure, move it to per-zone.

I gave it a try, but stumbled on a problem.

We need to know the zone in hugetlb_vmemmap_init_early(), but zones are
not yet defined.

hugetlb_vmemmap_init_early() is called from within sparse_init(), but
span of zones is defined in free_area_init() after sparse_init().

Any ideas, how get past this? :/

-- 
  Kiryl Shutsemau / Kirill A. Shutemov

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv</pre>
</details>
<a href="https://lore.kernel.org/r/aYSe0TAIzxJ9i1Wy@thinkstation" target="_blank" rel="noopener" class="lore-link">View on lore &#8599;</a>
<div class="review-comment-signals">Signals: acknowledged a technical issue, no clear resolution signal</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">David (arm)</span>
<a class="date-chip" href="../2026-02-27_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-05">2026-02-05</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer David (arm) noted that the patch introduces a new alignment requirement for vmemmap, which is not strictly necessary because vmemmap start is always struct-page-aligned. He suggested pulling the magic number &#x27;MAX_FOLIO_NR_PAGES * sizeof(struct page)&#x27; into the core and defining it conditionally based on HVO configuration and the power-of-2 property of sizeof(struct page).</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On 2/4/26 17:56, David Hildenbrand (arm) wrote:
&gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt;&gt; The upcoming change to the HugeTLB vmemmap optimization (HVO) requires
&gt;&gt; struct pages of the head page to be naturally aligned with regard to the
&gt;&gt; folio size.
&gt;&gt;
&gt;&gt; Align vmemmap to MAX_FOLIO_NR_PAGES.
&gt;&gt;
&gt;&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt;&gt; ---
&gt;&gt;   arch/loongarch/include/asm/pgtable.h | 3 ++-
&gt;&gt;   1 file changed, 2 insertions(+), 1 deletion(-)
&gt;&gt;
&gt;&gt; diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/ 
&gt;&gt; include/asm/pgtable.h
&gt;&gt; index c33b3bcb733e..f9416acb9156 100644
&gt;&gt; --- a/arch/loongarch/include/asm/pgtable.h
&gt;&gt; +++ b/arch/loongarch/include/asm/pgtable.h
&gt;&gt; @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / 
&gt;&gt; sizeof(unsigned long)];
&gt;&gt;        min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * 
&gt;&gt; PAGE_SIZE, (1UL &lt;&lt; cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - 
&gt;&gt; KFENCE_AREA_SIZE)
&gt;&gt;   #endif
&gt;&gt; -#define vmemmap        ((struct page *)((VMALLOC_END + PMD_SIZE) &amp; 
&gt;&gt; PMD_MASK))
&gt;&gt; +#define VMEMMAP_ALIGN    max(PMD_SIZE, MAX_FOLIO_NR_PAGES * 
&gt;&gt; sizeof(struct page))
&gt;&gt; +#define vmemmap        ((struct page *)(ALIGN(VMALLOC_END, 
&gt;&gt; VMEMMAP_ALIGN)))
&gt; 
&gt; 
&gt; Same comment, the &quot;MAX_FOLIO_NR_PAGES * sizeof(struct page)&quot; is just 
&gt; black magic here
&gt; and the description of the situation is wrong.
&gt; 
&gt; Maybe you want to pull the magic &quot;MAX_FOLIO_NR_PAGES * sizeof(struct 
&gt; page)&quot; into the core and call it
&gt; 
&gt; #define MAX_FOLIO_VMEMMAP_ALIGN    (MAX_FOLIO_NR_PAGES * sizeof(struct 
&gt; page))
&gt; 
&gt; But then special case it base on (a) HVO being configured in an (b) HVO 
&gt; being possible
&gt; 
&gt; #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP &amp;&amp; is_power_of_2(sizeof(struct page)
&gt; /* A very helpful comment explaining the situation. */
&gt; #define MAX_FOLIO_VMEMMAP_ALIGN    (MAX_FOLIO_NR_PAGES * sizeof(struct 
&gt; page))
&gt; #else
&gt; #define MAX_FOLIO_VMEMMAP_ALIGN    0
&gt; #endif
&gt; 
&gt; Something like that.
&gt; 

Thinking about this ...

the vmemmap start is always struct-page-aligned. Otherwise we&#x27;d be in 
trouble already.

Isn&#x27;t it then sufficient to just align the start to MAX_FOLIO_NR_PAGES?

Let&#x27;s assume sizeof(struct page) == 64 and MAX_FOLIO_NR_PAGES = 512 for 
simplicity.

vmemmap start would be multiples of 512 (0x0010000000).

512, 1024, 1536, 2048 ...

Assume we have an 256-pages folio at 1536+256 = 0x111000000

Assume we have the last page of that folio (0x011111111111), we would 
just get to the start of that folio by AND-ing with ~(256-1).

Which case am I ignoring?

-- 
Cheers,

David

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; With the upcoming changes to HVO, a single page of tail struct pages
&gt; will be shared across all huge pages of the same order on a node. Since
&gt; huge pages on the same node may belong to different zones, the zone
&gt; information stored in shared tail page flags would be incorrect.
&gt; 
&gt; Always fetch zone information from the head page, which has unique and
&gt; correct zone flags for each compound page.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt; ---
&gt;   include/linux/mmzone.h | 1 +
&gt;   1 file changed, 1 insertion(+)
&gt; 
&gt; diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
&gt; index be8ce40b5638..192143b5cdc0 100644
&gt; --- a/include/linux/mmzone.h
&gt; +++ b/include/linux/mmzone.h
&gt; @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)
&gt;   
&gt;   static inline enum zone_type page_zonenum(const struct page *page)
&gt;   {
&gt; +	page = compound_head(page);
&gt;   	return memdesc_zonenum(page-&gt;flags);

We end up calling page_zonenum() without holding a reference.

Given that _compound_head() does a READ_ONCE(), this should work even if 
we see concurrent page freeing etc.

However, this change implies that we now perform a compound page lookup 
for every PageHighMem() [meh], page_zone() [quite some users in the 
buddy, including for pageblock access and page freeing].

That&#x27;s a nasty compromise for making HVO better? :)

We should likely limit that special casing to kernels that really rquire 
it (HVO).

-- 
Cheers,

David

---

On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; With the upcoming changes to HVO, a single page of tail struct pages
&gt; will be shared across all huge pages of the same order on a node. Since
&gt; huge pages on the same node may belong to different zones, the zone
&gt; information stored in shared tail page flags would be incorrect.
&gt; 
&gt; Always fetch zone information from the head page, which has unique and
&gt; correct zone flags for each compound page.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt; ---
&gt;   include/linux/mmzone.h | 1 +
&gt;   1 file changed, 1 insertion(+)
&gt; 
&gt; diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
&gt; index be8ce40b5638..192143b5cdc0 100644
&gt; --- a/include/linux/mmzone.h
&gt; +++ b/include/linux/mmzone.h
&gt; @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)
&gt;   
&gt;   static inline enum zone_type page_zonenum(const struct page *page)
&gt;   {
&gt; +	page = compound_head(page);
&gt;   	return memdesc_zonenum(page-&gt;flags);

We end up calling page_zonenum() without holding a reference.

Given that _compound_head() does a READ_ONCE(), this should work even if 
we see concurrent page freeing etc.

However, this change implies that we now perform a compound page lookup 
for every PageHighMem() [meh], page_zone() [quite some users in the 
buddy, including for pageblock access and page freeing].

That&#x27;s a nasty compromise for making HVO better? :)

We should likely limit that special casing to kernels that really rquire 
it (HVO).

-- 
Cheers,

David

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; If page-&gt;compound_info encodes a mask, it is expected that vmemmap to be
&gt; naturally aligned to the maximum folio size.
&gt; 
&gt; Add a VM_BUG_ON() to check the alignment.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt; ---
&gt;   mm/sparse.c | 7 +++++++
&gt;   1 file changed, 7 insertions(+)
&gt; 
&gt; diff --git a/mm/sparse.c b/mm/sparse.c
&gt; index b5b2b6f7041b..6c9b62607f3f 100644
&gt; --- a/mm/sparse.c
&gt; +++ b/mm/sparse.c
&gt; @@ -600,6 +600,13 @@ void __init sparse_init(void)
&gt;   	BUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));
&gt;   	memblocks_present();
&gt;   
&gt; +	if (compound_info_has_mask()) {
&gt; +		unsigned long alignment;
&gt; +
&gt; +		alignment = MAX_FOLIO_NR_PAGES * sizeof(struct page);
&gt; +		VM_BUG_ON(!IS_ALIGNED((unsigned long) pfn_to_page(0), alignment));

No VM_BUG_ON. VM_WARN_ON_ONCE() should be good enough, no?

As discussed in the other thread, is checking for MAX_FOLIO_NR_PAGES 
alignment sufficient?

-- 
Cheers,

David

---

On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; If page-&gt;compound_info encodes a mask, it is expected that vmemmap to be
&gt; naturally aligned to the maximum folio size.
&gt; 
&gt; Add a VM_BUG_ON() to check the alignment.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt; ---
&gt;   mm/sparse.c | 7 +++++++
&gt;   1 file changed, 7 insertions(+)
&gt; 
&gt; diff --git a/mm/sparse.c b/mm/sparse.c
&gt; index b5b2b6f7041b..6c9b62607f3f 100644
&gt; --- a/mm/sparse.c
&gt; +++ b/mm/sparse.c
&gt; @@ -600,6 +600,13 @@ void __init sparse_init(void)
&gt;   	BUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));
&gt;   	memblocks_present();
&gt;   
&gt; +	if (compound_info_has_mask()) {
&gt; +		unsigned long alignment;
&gt; +
&gt; +		alignment = MAX_FOLIO_NR_PAGES * sizeof(struct page);
&gt; +		VM_BUG_ON(!IS_ALIGNED((unsigned long) pfn_to_page(0), alignment));

No VM_BUG_ON. VM_WARN_ON_ONCE() should be good enough, no?

As discussed in the other thread, is checking for MAX_FOLIO_NR_PAGES 
alignment sufficient?

-- 
Cheers,

David

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On 2/5/26 14:43, Kiryl Shutsemau wrote:
&gt; On Thu, Feb 05, 2026 at 01:56:36PM +0100, David Hildenbrand (Arm) wrote:
&gt;&gt; On 2/4/26 17:56, David Hildenbrand (arm) wrote:
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; Same comment, the &quot;MAX_FOLIO_NR_PAGES * sizeof(struct page)&quot; is just
&gt;&gt;&gt; black magic here
&gt;&gt;&gt; and the description of the situation is wrong.
&gt;&gt;&gt;
&gt;&gt;&gt; Maybe you want to pull the magic &quot;MAX_FOLIO_NR_PAGES * sizeof(struct
&gt;&gt;&gt; page)&quot; into the core and call it
&gt;&gt;&gt;
&gt;&gt;&gt; #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct
&gt;&gt;&gt; page))
&gt;&gt;&gt;
&gt;&gt;&gt; But then special case it base on (a) HVO being configured in an (b) HVO
&gt;&gt;&gt; being possible
&gt;&gt;&gt;
&gt;&gt;&gt; #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP &amp;&amp; is_power_of_2(sizeof(struct page)
&gt;&gt;&gt; /* A very helpful comment explaining the situation. */
&gt;&gt;&gt; #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct
&gt;&gt;&gt; page))
&gt;&gt;&gt; #else
&gt;&gt;&gt; #define MAX_FOLIO_VMEMMAP_ALIGN 0
&gt;&gt;&gt; #endif
&gt;&gt;&gt;
&gt;&gt;&gt; Something like that.
&gt;&gt;&gt;
&gt;&gt;
&gt;&gt; Thinking about this ...
&gt;&gt;
&gt;&gt; the vmemmap start is always struct-page-aligned. Otherwise we&#x27;d be in
&gt;&gt; trouble already.
&gt;&gt;
&gt;&gt; Isn&#x27;t it then sufficient to just align the start to MAX_FOLIO_NR_PAGES?
&gt;&gt;
&gt;&gt; Let&#x27;s assume sizeof(struct page) == 64 and MAX_FOLIO_NR_PAGES = 512 for
&gt;&gt; simplicity.
&gt;&gt;
&gt;&gt; vmemmap start would be multiples of 512 (0x0010000000).
&gt;&gt;
&gt;&gt; 512, 1024, 1536, 2048 ...
&gt;&gt;
&gt;&gt; Assume we have an 256-pages folio at 1536+256 = 0x111000000
&gt; 
&gt; s/0x/0b/, but okay.

:)

&gt; 
&gt;&gt; Assume we have the last page of that folio (0x011111111111), we would just
&gt;&gt; get to the start of that folio by AND-ing with ~(256-1).
&gt;&gt;
&gt;&gt; Which case am I ignoring?
&gt; 
&gt; IIUC, you are ignoring the actual size of struct page. It is not 1 byte :P

I thought it wouldn&#x27;t matter but, yeah, that&#x27;s it.

&quot;Align the vmemmap to the maximum folio metadata size&quot; it is.

Then you can explain the situation also alongside 
MAX_FOLIO_VMEMMAP_ALIGN, and that we expect this to be a power of 2.

-- 
Cheers,

David

---

On 2/5/26 14:43, Kiryl Shutsemau wrote:
&gt; On Thu, Feb 05, 2026 at 01:56:36PM +0100, David Hildenbrand (Arm) wrote:
&gt;&gt; On 2/4/26 17:56, David Hildenbrand (arm) wrote:
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; Same comment, the &quot;MAX_FOLIO_NR_PAGES * sizeof(struct page)&quot; is just
&gt;&gt;&gt; black magic here
&gt;&gt;&gt; and the description of the situation is wrong.
&gt;&gt;&gt;
&gt;&gt;&gt; Maybe you want to pull the magic &quot;MAX_FOLIO_NR_PAGES * sizeof(struct
&gt;&gt;&gt; page)&quot; into the core and call it
&gt;&gt;&gt;
&gt;&gt;&gt; #define MAX_FOLIO_VMEMMAP_ALIGN    (MAX_FOLIO_NR_PAGES * sizeof(struct
&gt;&gt;&gt; page))
&gt;&gt;&gt;
&gt;&gt;&gt; But then special case it base on (a) HVO being configured in an (b) HVO
&gt;&gt;&gt; being possible
&gt;&gt;&gt;
&gt;&gt;&gt; #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP &amp;&amp; is_power_of_2(sizeof(struct page)
&gt;&gt;&gt; /* A very helpful comment explaining the situation. */
&gt;&gt;&gt; #define MAX_FOLIO_VMEMMAP_ALIGN    (MAX_FOLIO_NR_PAGES * sizeof(struct
&gt;&gt;&gt; page))
&gt;&gt;&gt; #else
&gt;&gt;&gt; #define MAX_FOLIO_VMEMMAP_ALIGN    0
&gt;&gt;&gt; #endif
&gt;&gt;&gt;
&gt;&gt;&gt; Something like that.
&gt;&gt;&gt;
&gt;&gt;
&gt;&gt; Thinking about this ...
&gt;&gt;
&gt;&gt; the vmemmap start is always struct-page-aligned. Otherwise we&#x27;d be in
&gt;&gt; trouble already.
&gt;&gt;
&gt;&gt; Isn&#x27;t it then sufficient to just align the start to MAX_FOLIO_NR_PAGES?
&gt;&gt;
&gt;&gt; Let&#x27;s assume sizeof(struct page) == 64 and MAX_FOLIO_NR_PAGES = 512 for
&gt;&gt; simplicity.
&gt;&gt;
&gt;&gt; vmemmap start would be multiples of 512 (0x0010000000).
&gt;&gt;
&gt;&gt; 512, 1024, 1536, 2048 ...
&gt;&gt;
&gt;&gt; Assume we have an 256-pages folio at 1536+256 = 0x111000000
&gt; 
&gt; s/0x/0b/, but okay.

:)

&gt; 
&gt;&gt; Assume we have the last page of that folio (0x011111111111), we would just
&gt;&gt; get to the start of that folio by AND-ing with ~(256-1).
&gt;&gt;
&gt;&gt; Which case am I ignoring?
&gt; 
&gt; IIUC, you are ignoring the actual size of struct page. It is not 1 byte :P

I thought it wouldn&#x27;t matter but, yeah, that&#x27;s it.

&quot;Align the vmemmap to the maximum folio metadata size&quot; it is.

Then you can explain the situation also alongside 
MAX_FOLIO_VMEMMAP_ALIGN, and that we expect this to be a power of 2.

-- 
Cheers,

David

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On 2/5/26 14:50, Kiryl Shutsemau wrote:
&gt; On Wed, Feb 04, 2026 at 05:50:23PM +0100, David Hildenbrand (arm) wrote:
&gt;&gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt;&gt;&gt; The upcoming change to the HugeTLB vmemmap optimization (HVO) requires
&gt;&gt;&gt; struct pages of the head page to be naturally aligned with regard to the
&gt;&gt;&gt; folio size.
&gt;&gt;&gt;
&gt;&gt;&gt; Align vmemmap to MAX_FOLIO_NR_PAGES.
&gt;&gt;
&gt;&gt; I think neither that statement nor the one in the patch description is
&gt;&gt; correct?
&gt;&gt;
&gt;&gt; &quot;MAX_FOLIO_NR_PAGES * sizeof(struct page)&quot; is neither the maximum folio size
&gt;&gt; nor MAX_FOLIO_NR_PAGES.
&gt;&gt;
&gt;&gt; It&#x27;s the size of the memmap that a large folio could span at maximum.
&gt;&gt;
&gt;&gt;
&gt;&gt; Assuming we have a 16 GiB folio, the calculation would give us
&gt;&gt;
&gt;&gt; 	4194304 * sizeof(struct page)
&gt;&gt;
&gt;&gt; Which could be something like (assuming 80 bytes)
&gt;&gt;
&gt;&gt; 	335544320
&gt;&gt;
&gt;&gt; -&gt; not even a power of 2, weird? (for HVO you wouldn&#x27;t care as HVO would be
&gt;&gt; disabled, but that aliment is super weird?)
&gt;&gt;
&gt;&gt;
&gt;&gt; Assuming 64 bytes, it would be a power of two (as 64 is a power of two).
&gt;&gt;
&gt;&gt; 	268435456 (1&lt;&lt; 28)
&gt;&gt;
&gt;&gt;
&gt;&gt; Which makes me wonder whether there is a way to avoid sizeof(struct page)
&gt;&gt; here completely.
&gt; 
&gt; I don&#x27;t think we can. See the other thread.

Agreed. You could only go for something larger (like PAGE_SIZE).

&gt; 
&gt; What about using roundup_pow_of_two(sizeof(struct page)) here.

Better I think.

&gt; 
&gt;&gt; Or limit the alignment to the case where HVO is actually active and
&gt;&gt; sizeof(struct page) makes any sense?
&gt; 
&gt; The annoying part of HVO is that it is unknown at compile-time if it
&gt; will be used. You can compile kernel with HVO that will no be activated
&gt; due to non-power-of-2 sizeof(struct page) because of a debug config option.
Ah, and now I remember that sizeof cannot be used in macros, damnit.

-- 
Cheers,

David

---

On 2/5/26 14:50, Kiryl Shutsemau wrote:
&gt; On Wed, Feb 04, 2026 at 05:50:23PM +0100, David Hildenbrand (arm) wrote:
&gt;&gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt;&gt;&gt; The upcoming change to the HugeTLB vmemmap optimization (HVO) requires
&gt;&gt;&gt; struct pages of the head page to be naturally aligned with regard to the
&gt;&gt;&gt; folio size.
&gt;&gt;&gt;
&gt;&gt;&gt; Align vmemmap to MAX_FOLIO_NR_PAGES.
&gt;&gt;
&gt;&gt; I think neither that statement nor the one in the patch description is
&gt;&gt; correct?
&gt;&gt;
&gt;&gt; &quot;MAX_FOLIO_NR_PAGES * sizeof(struct page)&quot; is neither the maximum folio size
&gt;&gt; nor MAX_FOLIO_NR_PAGES.
&gt;&gt;
&gt;&gt; It&#x27;s the size of the memmap that a large folio could span at maximum.
&gt;&gt;
&gt;&gt;
&gt;&gt; Assuming we have a 16 GiB folio, the calculation would give us
&gt;&gt;
&gt;&gt; 	4194304 * sizeof(struct page)
&gt;&gt;
&gt;&gt; Which could be something like (assuming 80 bytes)
&gt;&gt;
&gt;&gt; 	335544320
&gt;&gt;
&gt;&gt; -&gt; not even a power of 2, weird? (for HVO you wouldn&#x27;t care as HVO would be
&gt;&gt; disabled, but that aliment is super weird?)
&gt;&gt;
&gt;&gt;
&gt;&gt; Assuming 64 bytes, it would be a power of two (as 64 is a power of two).
&gt;&gt;
&gt;&gt; 	268435456 (1&lt;&lt; 28)
&gt;&gt;
&gt;&gt;
&gt;&gt; Which makes me wonder whether there is a way to avoid sizeof(struct page)
&gt;&gt; here completely.
&gt; 
&gt; I don&#x27;t think we can. See the other thread.

Agreed. You could only go for something larger (like PAGE_SIZE).

&gt; 
&gt; What about using roundup_pow_of_two(sizeof(struct page)) here.

Better I think.

&gt; 
&gt;&gt; Or limit the alignment to the case where HVO is actually active and
&gt;&gt; sizeof(struct page) makes any sense?
&gt; 
&gt; The annoying part of HVO is that it is unknown at compile-time if it
&gt; will be used. You can compile kernel with HVO that will no be activated
&gt; due to non-power-of-2 sizeof(struct page) because of a debug config option.
Ah, and now I remember that sizeof cannot be used in macros, damnit.

-- 
Cheers,

David

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On 2/5/26 14:52, Kiryl Shutsemau wrote:
&gt; On Wed, Feb 04, 2026 at 05:56:45PM +0100, David Hildenbrand (arm) wrote:
&gt;&gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt;&gt;&gt; The upcoming change to the HugeTLB vmemmap optimization (HVO) requires
&gt;&gt;&gt; struct pages of the head page to be naturally aligned with regard to the
&gt;&gt;&gt; folio size.
&gt;&gt;&gt;
&gt;&gt;&gt; Align vmemmap to MAX_FOLIO_NR_PAGES.
&gt;&gt;&gt;
&gt;&gt;&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt;&gt;&gt; ---
&gt;&gt;&gt;    arch/loongarch/include/asm/pgtable.h | 3 ++-
&gt;&gt;&gt;    1 file changed, 2 insertions(+), 1 deletion(-)
&gt;&gt;&gt;
&gt;&gt;&gt; diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h
&gt;&gt;&gt; index c33b3bcb733e..f9416acb9156 100644
&gt;&gt;&gt; --- a/arch/loongarch/include/asm/pgtable.h
&gt;&gt;&gt; +++ b/arch/loongarch/include/asm/pgtable.h
&gt;&gt;&gt; @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
&gt;&gt;&gt;    	 min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL &lt;&lt; cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)
&gt;&gt;&gt;    #endif
&gt;&gt;&gt; -#define vmemmap		((struct page *)((VMALLOC_END + PMD_SIZE) &amp; PMD_MASK))
&gt;&gt;&gt; +#define VMEMMAP_ALIGN	max(PMD_SIZE, MAX_FOLIO_NR_PAGES * sizeof(struct page))
&gt;&gt;&gt; +#define vmemmap		((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))
&gt;&gt;
&gt;&gt;
&gt;&gt; Same comment, the &quot;MAX_FOLIO_NR_PAGES * sizeof(struct page)&quot; is just black magic here
&gt;&gt; and the description of the situation is wrong.
&gt;&gt;
&gt;&gt; Maybe you want to pull the magic &quot;MAX_FOLIO_NR_PAGES * sizeof(struct page)&quot; into the core and call it
&gt;&gt;
&gt;&gt; #define MAX_FOLIO_VMEMMAP_ALIGN	(MAX_FOLIO_NR_PAGES * sizeof(struct page))
&gt;&gt;
&gt;&gt; But then special case it base on (a) HVO being configured in an (b) HVO being possible
&gt;&gt;
&gt;&gt; #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP &amp;&amp; is_power_of_2(sizeof(struct page)
&gt; 
&gt; This would require some kind of asm-offsets.c/bounds.c magic to pull the
&gt; struct page size condition to the preprocessor level.
&gt; 

Right.

I guess you could move that into the macro and let the compiler handle it.

-- 
Cheers,

David

---

On 2/5/26 14:52, Kiryl Shutsemau wrote:
&gt; On Wed, Feb 04, 2026 at 05:56:45PM +0100, David Hildenbrand (arm) wrote:
&gt;&gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt;&gt;&gt; The upcoming change to the HugeTLB vmemmap optimization (HVO) requires
&gt;&gt;&gt; struct pages of the head page to be naturally aligned with regard to the
&gt;&gt;&gt; folio size.
&gt;&gt;&gt;
&gt;&gt;&gt; Align vmemmap to MAX_FOLIO_NR_PAGES.
&gt;&gt;&gt;
&gt;&gt;&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt;&gt;&gt; ---
&gt;&gt;&gt;    arch/loongarch/include/asm/pgtable.h | 3 ++-
&gt;&gt;&gt;    1 file changed, 2 insertions(+), 1 deletion(-)
&gt;&gt;&gt;
&gt;&gt;&gt; diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h
&gt;&gt;&gt; index c33b3bcb733e..f9416acb9156 100644
&gt;&gt;&gt; --- a/arch/loongarch/include/asm/pgtable.h
&gt;&gt;&gt; +++ b/arch/loongarch/include/asm/pgtable.h
&gt;&gt;&gt; @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
&gt;&gt;&gt;    	 min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL &lt;&lt; cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)
&gt;&gt;&gt;    #endif
&gt;&gt;&gt; -#define vmemmap		((struct page *)((VMALLOC_END + PMD_SIZE) &amp; PMD_MASK))
&gt;&gt;&gt; +#define VMEMMAP_ALIGN	max(PMD_SIZE, MAX_FOLIO_NR_PAGES * sizeof(struct page))
&gt;&gt;&gt; +#define vmemmap		((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))
&gt;&gt;
&gt;&gt;
&gt;&gt; Same comment, the &quot;MAX_FOLIO_NR_PAGES * sizeof(struct page)&quot; is just black magic here
&gt;&gt; and the description of the situation is wrong.
&gt;&gt;
&gt;&gt; Maybe you want to pull the magic &quot;MAX_FOLIO_NR_PAGES * sizeof(struct page)&quot; into the core and call it
&gt;&gt;
&gt;&gt; #define MAX_FOLIO_VMEMMAP_ALIGN	(MAX_FOLIO_NR_PAGES * sizeof(struct page))
&gt;&gt;
&gt;&gt; But then special case it base on (a) HVO being configured in an (b) HVO being possible
&gt;&gt;
&gt;&gt; #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP &amp;&amp; is_power_of_2(sizeof(struct page)
&gt; 
&gt; This would require some kind of asm-offsets.c/bounds.c magic to pull the
&gt; struct page size condition to the preprocessor level.
&gt; 

Right.

I guess you could move that into the macro and let the compiler handle it.

-- 
Cheers,

David

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On 2/5/26 14:31, David Hildenbrand (Arm) wrote:
&gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt;&gt; If page-&gt;compound_info encodes a mask, it is expected that vmemmap to be
&gt;&gt; naturally aligned to the maximum folio size.
&gt;&gt;
&gt;&gt; Add a VM_BUG_ON() to check the alignment.
&gt;&gt;
&gt;&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt;&gt; Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt;&gt; ---
&gt;&gt;  mm/sparse.c | 7 +++++++
&gt;&gt;  1 file changed, 7 insertions(+)
&gt;&gt;
&gt;&gt; diff --git a/mm/sparse.c b/mm/sparse.c
&gt;&gt; index b5b2b6f7041b..6c9b62607f3f 100644
&gt;&gt; --- a/mm/sparse.c
&gt;&gt; +++ b/mm/sparse.c
&gt;&gt; @@ -600,6 +600,13 @@ void __init sparse_init(void)
&gt;&gt;  BUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));
&gt;&gt;  memblocks_present();
&gt;&gt; + if (compound_info_has_mask()) {
&gt;&gt; + unsigned long alignment;
&gt;&gt; +
&gt;&gt; + alignment = MAX_FOLIO_NR_PAGES * sizeof(struct page);
&gt;&gt; + VM_BUG_ON(!IS_ALIGNED((unsigned long) pfn_to_page(0), 
&gt;&gt; alignment));
&gt; 
&gt; No VM_BUG_ON. VM_WARN_ON_ONCE() should be good enough, no?
&gt; 
&gt; As discussed in the other thread, is checking for MAX_FOLIO_NR_PAGES 
&gt; alignment sufficient?

And after further discussions, we could use MAX_FOLIO_VMEMMAP_ALIGN 
macro once we have that.

-- 
Cheers,

David

---

On 2/5/26 14:31, David Hildenbrand (Arm) wrote:
&gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt;&gt; If page-&gt;compound_info encodes a mask, it is expected that vmemmap to be
&gt;&gt; naturally aligned to the maximum folio size.
&gt;&gt;
&gt;&gt; Add a VM_BUG_ON() to check the alignment.
&gt;&gt;
&gt;&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt;&gt; Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt;&gt; ---
&gt;&gt;   mm/sparse.c | 7 +++++++
&gt;&gt;   1 file changed, 7 insertions(+)
&gt;&gt;
&gt;&gt; diff --git a/mm/sparse.c b/mm/sparse.c
&gt;&gt; index b5b2b6f7041b..6c9b62607f3f 100644
&gt;&gt; --- a/mm/sparse.c
&gt;&gt; +++ b/mm/sparse.c
&gt;&gt; @@ -600,6 +600,13 @@ void __init sparse_init(void)
&gt;&gt;       BUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));
&gt;&gt;       memblocks_present();
&gt;&gt; +    if (compound_info_has_mask()) {
&gt;&gt; +        unsigned long alignment;
&gt;&gt; +
&gt;&gt; +        alignment = MAX_FOLIO_NR_PAGES * sizeof(struct page);
&gt;&gt; +        VM_BUG_ON(!IS_ALIGNED((unsigned long) pfn_to_page(0), 
&gt;&gt; alignment));
&gt; 
&gt; No VM_BUG_ON. VM_WARN_ON_ONCE() should be good enough, no?
&gt; 
&gt; As discussed in the other thread, is checking for MAX_FOLIO_NR_PAGES 
&gt; alignment sufficient?

And after further discussions, we could use MAX_FOLIO_VMEMMAP_ALIGN 
macro once we have that.

-- 
Cheers,

David

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv</pre>
</details>
<a href="https://lore.kernel.org/r/062900fa-6419-4748-81d1-9128ce6c46d0@kernel.org" target="_blank" rel="noopener" class="lore-link">View on lore &#8599;</a>
<div class="review-comment-signals">Signals: requested changes, suggested improvement</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">David (arm)</span>
<a class="date-chip" href="../2026-02-27_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-05">2026-02-05</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Acked-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer David noted that clearing bit 0 before applying the mask to get the head page address is unnecessary, as the page pointer should not have set it in the first place.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; For tail pages, the kernel uses the &#x27;compound_info&#x27; field to get to the
&gt; head page. The bit 0 of the field indicates whether the page is a
&gt; tail page, and if set, the remaining bits represent a pointer to the
&gt; head page.
&gt; 
&gt; For cases when size of struct page is power-of-2, change the encoding of
&gt; compound_info to store a mask that can be applied to the virtual address
&gt; of the tail page in order to access the head page. It is possible
&gt; because struct page of the head page is naturally aligned with regards
&gt; to order of the page.
&gt; 
&gt; The significant impact of this modification is that all tail pages of
&gt; the same order will now have identical &#x27;compound_info&#x27;, regardless of
&gt; the compound page they are associated with. This paves the way for
&gt; eliminating fake heads.
&gt; 
&gt; The HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only
&gt; applied when the sizeof(struct page) is power-of-2. Having identical
&gt; tail pages allows the same page to be mapped into the vmemmap of all
&gt; pages, maintaining memory savings without fake heads.
&gt; 
&gt; If sizeof(struct page) is not power-of-2, there is no functional
&gt; changes.
&gt; 
&gt; Limit mask usage to HugeTLB vmemmap optimization (HVO) where it makes
&gt; a difference. The approach with mask would work in the wider set of
&gt; conditions, but it requires validating that struct pages are naturally
&gt; aligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
&gt; Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;

[...]

&gt;   	struct folio *foliop;
&gt;   	int loops = 5;
&gt;   
&gt; @@ -1247,8 +1247,8 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)
&gt;   again:
&gt;   	memset(&amp;ps-&gt;folio_snapshot, 0, sizeof(struct folio));
&gt;   	memcpy(&amp;ps-&gt;page_snapshot, page, sizeof(*page));
&gt; -	head = ps-&gt;page_snapshot.compound_info;
&gt; -	if ((head &amp; 1) == 0) {
&gt; +	info = ps-&gt;page_snapshot.compound_info;
&gt; +	if (!(info &amp; 1)) {
&gt;   		ps-&gt;idx = 0;
&gt;   		foliop = (struct folio *)&amp;ps-&gt;page_snapshot;
&gt;   		if (!folio_test_large(foliop)) {
&gt; @@ -1259,7 +1259,15 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)
&gt;   		}
&gt;   		foliop = (struct folio *)page;
&gt;   	} else {
&gt; -		foliop = (struct folio *)(head - 1);
&gt; +		/* See compound_head() */
&gt; +		if (compound_info_has_mask()) {
&gt; +			unsigned long p = (unsigned long)page;
&gt; +
&gt; +			foliop = (struct folio *)(p &amp; info);

IIUC, we don&#x27;t care about clearing bit0 before the &amp; as the page pointer 
shouldn&#x27;t have set it in the first page.

Pretty neat

Acked-by: David Hildenbrand (Arm) &lt;david@kernel.org&gt;

-- 
Cheers,

David

---

On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; For tail pages, the kernel uses the &#x27;compound_info&#x27; field to get to the
&gt; head page. The bit 0 of the field indicates whether the page is a
&gt; tail page, and if set, the remaining bits represent a pointer to the
&gt; head page.
&gt; 
&gt; For cases when size of struct page is power-of-2, change the encoding of
&gt; compound_info to store a mask that can be applied to the virtual address
&gt; of the tail page in order to access the head page. It is possible
&gt; because struct page of the head page is naturally aligned with regards
&gt; to order of the page.
&gt; 
&gt; The significant impact of this modification is that all tail pages of
&gt; the same order will now have identical &#x27;compound_info&#x27;, regardless of
&gt; the compound page they are associated with. This paves the way for
&gt; eliminating fake heads.
&gt; 
&gt; The HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only
&gt; applied when the sizeof(struct page) is power-of-2. Having identical
&gt; tail pages allows the same page to be mapped into the vmemmap of all
&gt; pages, maintaining memory savings without fake heads.
&gt; 
&gt; If sizeof(struct page) is not power-of-2, there is no functional
&gt; changes.
&gt; 
&gt; Limit mask usage to HugeTLB vmemmap optimization (HVO) where it makes
&gt; a difference. The approach with mask would work in the wider set of
&gt; conditions, but it requires validating that struct pages are naturally
&gt; aligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
&gt; Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;

[...]

&gt;   	struct folio *foliop;
&gt;   	int loops = 5;
&gt;   
&gt; @@ -1247,8 +1247,8 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)
&gt;   again:
&gt;   	memset(&amp;ps-&gt;folio_snapshot, 0, sizeof(struct folio));
&gt;   	memcpy(&amp;ps-&gt;page_snapshot, page, sizeof(*page));
&gt; -	head = ps-&gt;page_snapshot.compound_info;
&gt; -	if ((head &amp; 1) == 0) {
&gt; +	info = ps-&gt;page_snapshot.compound_info;
&gt; +	if (!(info &amp; 1)) {
&gt;   		ps-&gt;idx = 0;
&gt;   		foliop = (struct folio *)&amp;ps-&gt;page_snapshot;
&gt;   		if (!folio_test_large(foliop)) {
&gt; @@ -1259,7 +1259,15 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)
&gt;   		}
&gt;   		foliop = (struct folio *)page;
&gt;   	} else {
&gt; -		foliop = (struct folio *)(head - 1);
&gt; +		/* See compound_head() */
&gt; +		if (compound_info_has_mask()) {
&gt; +			unsigned long p = (unsigned long)page;
&gt; +
&gt; +			foliop = (struct folio *)(p &amp; info);

IIUC, we don&#x27;t care about clearing bit0 before the &amp; as the page pointer 
shouldn&#x27;t have set it in the first page.

Pretty neat

Acked-by: David Hildenbrand (Arm) &lt;david@kernel.org&gt;

-- 
Cheers,

David

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On 2/16/26 00:13, Matthew Wilcox wrote:
&gt; On Mon, Feb 02, 2026 at 03:56:24PM +0000, Kiryl Shutsemau wrote:
&gt;&gt; With the upcoming changes to HVO, a single page of tail struct pages
&gt;&gt; will be shared across all huge pages of the same order on a node. Since
&gt;&gt; huge pages on the same node may belong to different zones, the zone
&gt;&gt; information stored in shared tail page flags would be incorrect.
&gt;&gt;
&gt;&gt; Always fetch zone information from the head page, which has unique and
&gt;&gt; correct zone flags for each compound page.
&gt; 
&gt; You&#x27;re right that different pages in the same folio can have different
&gt; zone number.  But does it matter ... or to put it another way, why is
&gt; returning the zone number of the head page the correct way to resolve
&gt; this?

How can a folio cross zones?

Runtime allocated hugetlb folios from the CMA/buddy (alloc_contig_range) 
definitely fall into a single zone.

So is it about ones allocated early during boot, where, by chance, we 
manage to cross ZONE_NORMAL + ZONE_MOVABLE etc?

I thought that it&#x27;s also not allowed there, and I wonder whether we 
should disallow it if it&#x27;s possible.

&gt; 
&gt; Arguably, the caller is asking for the zone number of _this page_, and
&gt; does not care about the zone number of the head page.  It would be good
&gt; to have a short discussion of this in the commit message (but probably
&gt; not worth putting this in a comment).

Agreed, in particular, if there would be a functional change. So far I 
assumed there would be no such change.

Things like shrink_zone_span() really need to know the zone of that 
page, not the one of the head; unless both fall into the same zone.

-- 
Cheers,

David

---

On 2/16/26 00:13, Matthew Wilcox wrote:
&gt; On Mon, Feb 02, 2026 at 03:56:24PM +0000, Kiryl Shutsemau wrote:
&gt;&gt; With the upcoming changes to HVO, a single page of tail struct pages
&gt;&gt; will be shared across all huge pages of the same order on a node. Since
&gt;&gt; huge pages on the same node may belong to different zones, the zone
&gt;&gt; information stored in shared tail page flags would be incorrect.
&gt;&gt;
&gt;&gt; Always fetch zone information from the head page, which has unique and
&gt;&gt; correct zone flags for each compound page.
&gt; 
&gt; You&#x27;re right that different pages in the same folio can have different
&gt; zone number.  But does it matter ... or to put it another way, why is
&gt; returning the zone number of the head page the correct way to resolve
&gt; this?

How can a folio cross zones?

Runtime allocated hugetlb folios from the CMA/buddy (alloc_contig_range) 
definitely fall into a single zone.

So is it about ones allocated early during boot, where, by chance, we 
manage to cross ZONE_NORMAL + ZONE_MOVABLE etc?

I thought that it&#x27;s also not allowed there, and I wonder whether we 
should disallow it if it&#x27;s possible.

&gt; 
&gt; Arguably, the caller is asking for the zone number of _this page_, and
&gt; does not care about the zone number of the head page.  It would be good
&gt; to have a short discussion of this in the commit message (but probably
&gt; not worth putting this in a comment).

Agreed, in particular, if there would be a functional change. So far I 
assumed there would be no such change.

Things like shrink_zone_span() really need to know the zone of that 
page, not the one of the head; unless both fall into the same zone.

-- 
Cheers,

David

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On 2/23/26 19:18, Matthew Wilcox wrote:
&gt; On Mon, Feb 16, 2026 at 10:06:57AM +0100, David Hildenbrand (Arm) wrote:
&gt;&gt; On 2/16/26 00:13, Matthew Wilcox wrote:
&gt;&gt;&gt;
&gt;&gt;&gt; You&#x27;re right that different pages in the same folio can have different
&gt;&gt;&gt; zone number.  But does it matter ... or to put it another way, why is
&gt;&gt;&gt; returning the zone number of the head page the correct way to resolve
&gt;&gt;&gt; this?
&gt;&gt;
&gt;&gt; How can a folio cross zones?
&gt; 
&gt; I thought 1GB pages in hugetlb could cross zones?  Maybe that used to be
&gt; true and isn&#x27;t any more, or maybe it was never true and I was just
&gt; confused.

I recall that 1G folios could end up in ZONE_MOVABLE (comment in
page_is_unmovable()), but my memory is fuzzy when it comes to crossing
zones (ZONE_NORMAL -&gt; ZONE_MOVABLE).

Freeing+reinitializing the vmemmap for HVO with such folios would
already be problematic I suppose: we would silently switch the zone for
some of these pages.

When freeing such (boottime) hugetlb folios to the buddy, we use
free_frozen_pages(). In there we lookup the zone once.

Likely also problematic :)

-- 
Cheers,

David

---

On 2/23/26 19:18, Matthew Wilcox wrote:
&gt; On Mon, Feb 16, 2026 at 10:06:57AM +0100, David Hildenbrand (Arm) wrote:
&gt;&gt; On 2/16/26 00:13, Matthew Wilcox wrote:
&gt;&gt;&gt;
&gt;&gt;&gt; You&#x27;re right that different pages in the same folio can have different
&gt;&gt;&gt; zone number.  But does it matter ... or to put it another way, why is
&gt;&gt;&gt; returning the zone number of the head page the correct way to resolve
&gt;&gt;&gt; this?
&gt;&gt;
&gt;&gt; How can a folio cross zones?
&gt; 
&gt; I thought 1GB pages in hugetlb could cross zones?  Maybe that used to be
&gt; true and isn&#x27;t any more, or maybe it was never true and I was just
&gt; confused.

I recall that 1G folios could end up in ZONE_MOVABLE (comment in
page_is_unmovable()), but my memory is fuzzy when it comes to crossing
zones (ZONE_NORMAL -&gt; ZONE_MOVABLE).

Freeing+reinitializing the vmemmap for HVO with such folios would
already be problematic I suppose: we would silently switch the zone for
some of these pages.

When freeing such (boottime) hugetlb folios to the buddy, we use
free_frozen_pages(). In there we lookup the zone once.

Likely also problematic :)

-- 
Cheers,

David

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv</pre>
</details>
<a href="https://lore.kernel.org/r/6e182cb7-6f32-41c0-ba86-520728b161c7@kernel.org" target="_blank" rel="noopener" class="lore-link">View on lore &#8599;</a>
<div class="review-comment-signals">Signals: signal1, signal2</div>
</div>
</div>
<div class="thread-node depth-0" id="2026-02-07">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Usama Arif</span>
<a class="date-chip" href="../2026-02-27_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-07">2026-02-07</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Acked-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Usama Arif pointed out that the new mask-based compound_head() encoding has a potential issue: it requires validating that struct pages are naturally aligned for all orders up to MAX_FOLIO_ORDER, which can be tricky. He suggested limiting the usage of this approach to HugeTLB vmemmap optimization (HVO) where it makes a difference.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On 02/02/2026 15:56, Kiryl Shutsemau wrote:
&gt; For tail pages, the kernel uses the &#x27;compound_info&#x27; field to get to the
&gt; head page. The bit 0 of the field indicates whether the page is a
&gt; tail page, and if set, the remaining bits represent a pointer to the
&gt; head page.
&gt; 
&gt; For cases when size of struct page is power-of-2, change the encoding of
&gt; compound_info to store a mask that can be applied to the virtual address
&gt; of the tail page in order to access the head page. It is possible
&gt; because struct page of the head page is naturally aligned with regards
&gt; to order of the page.
&gt; 
&gt; The significant impact of this modification is that all tail pages of
&gt; the same order will now have identical &#x27;compound_info&#x27;, regardless of
&gt; the compound page they are associated with. This paves the way for
&gt; eliminating fake heads.
&gt; 
&gt; The HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only
&gt; applied when the sizeof(struct page) is power-of-2. Having identical
&gt; tail pages allows the same page to be mapped into the vmemmap of all
&gt; pages, maintaining memory savings without fake heads.
&gt; 
&gt; If sizeof(struct page) is not power-of-2, there is no functional
&gt; changes.
&gt; 
&gt; Limit mask usage to HugeTLB vmemmap optimization (HVO) where it makes
&gt; a difference. The approach with mask would work in the wider set of
&gt; conditions, but it requires validating that struct pages are naturally
&gt; aligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
&gt; Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt; ---

Acked-by: Usama Arif &lt;usamaarif642@gmail.com&gt;

Small nit below:

&gt;  include/linux/page-flags.h | 81 ++++++++++++++++++++++++++++++++++----
&gt;  mm/slab.h                  | 16 ++++++--
&gt;  mm/util.c                  | 16 ++++++--
&gt;  3 files changed, 97 insertions(+), 16 deletions(-)
&gt; 
&gt; diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
&gt; index d14a17ffb55b..8f2c7fbc739b 100644
&gt; --- a/include/linux/page-flags.h
&gt; +++ b/include/linux/page-flags.h
&gt; @@ -198,6 +198,29 @@ enum pageflags {
&gt;  
&gt;  #ifndef __GENERATING_BOUNDS_H
&gt;  
&gt; +/*
&gt; + * For tail pages, if the size of struct page is power-of-2 -&gt;compound_info
&gt; + * encodes the mask that converts the address of the tail page address to
&gt; + * the head page address.
&gt; + *
&gt; + * Otherwise, -&gt;compound_info has direct pointer to head pages.
&gt; + */
&gt; +static __always_inline bool compound_info_has_mask(void)
&gt; +{
&gt; +	/*
&gt; +	 * Limit mask usage to HugeTLB vmemmap optimization (HVO) where it
&gt; +	 * makes a difference.
&gt; +	 *
&gt; +	 * The approach with mask would work in the wider set of conditions,
&gt; +	 * but it requires validating that struct pages are naturally aligned
&gt; +	 * for all orders up to the MAX_FOLIO_ORDER, which can be tricky.
&gt; +	 */
&gt; +	if (!IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP))
&gt; +		return false;
&gt; +
&gt; +	return is_power_of_2(sizeof(struct page));
&gt; +}
&gt; +
&gt;  #ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP
&gt;  DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);
&gt;  
&gt; @@ -210,6 +233,10 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page
&gt;  	if (!static_branch_unlikely(&amp;hugetlb_optimize_vmemmap_key))
&gt;  		return page;
&gt;  
&gt; +	/* Fake heads only exists if compound_info_has_mask() is true */
&gt; +	if (!compound_info_has_mask())
&gt; +		return page;
&gt; +
&gt;  	/*
&gt;  	 * Only addresses aligned with PAGE_SIZE of struct page may be fake head
&gt;  	 * struct page. The alignment check aims to avoid access the fields (
&gt; @@ -223,10 +250,14 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page
&gt;  		 * because the @page is a compound page composed with at least
&gt;  		 * two contiguous pages.
&gt;  		 */
&gt; -		unsigned long head = READ_ONCE(page[1].compound_info);
&gt; +		unsigned long info = READ_ONCE(page[1].compound_info);
&gt;  
&gt; -		if (likely(head &amp; 1))
&gt; -			return (const struct page *)(head - 1);
&gt; +		/* See set_compound_head() */
&gt; +		if (likely(info &amp; 1)) {
&gt; +			unsigned long p = (unsigned long)page;
&gt; +
&gt; +			return (const struct page *)(p &amp; info);
&gt; +		}
&gt;  	}
&gt;  	return page;
&gt;  }
&gt; @@ -281,11 +312,26 @@ static __always_inline int page_is_fake_head(const struct page *page)
&gt;  
&gt;  static __always_inline unsigned long _compound_head(const struct page *page)
&gt;  {
&gt; -	unsigned long head = READ_ONCE(page-&gt;compound_info);
&gt; +	unsigned long info = READ_ONCE(page-&gt;compound_info);
&gt;  
&gt; -	if (unlikely(head &amp; 1))
&gt; -		return head - 1;
&gt; -	return (unsigned long)page_fixed_fake_head(page);
&gt; +	/* Bit 0 encodes PageTail() */
&gt; +	if (!(info &amp; 1))
&gt; +		return (unsigned long)page_fixed_fake_head(page);
&gt; +
&gt; +	/*
&gt; +	 * If compound_info_has_mask() is false, the rest of compound_info is
&gt; +	 * the pointer to the head page.
&gt; +	 */
&gt; +	if (!compound_info_has_mask())
&gt; +		return info - 1;
&gt; +
&gt; +	/*
&gt; +	 * If compoun_info_has_mask() is true the rest of the info encodes

s/compoun_info_has_mask/compound_info_has_mask/

---

On 02/02/2026 15:56, Kiryl Shutsemau wrote:
&gt; For tail pages, the kernel uses the &#x27;compound_info&#x27; field to get to the
&gt; head page. The bit 0 of the field indicates whether the page is a
&gt; tail page, and if set, the remaining bits represent a pointer to the
&gt; head page.
&gt; 
&gt; For cases when size of struct page is power-of-2, change the encoding of
&gt; compound_info to store a mask that can be applied to the virtual address
&gt; of the tail page in order to access the head page. It is possible
&gt; because struct page of the head page is naturally aligned with regards
&gt; to order of the page.
&gt; 
&gt; The significant impact of this modification is that all tail pages of
&gt; the same order will now have identical &#x27;compound_info&#x27;, regardless of
&gt; the compound page they are associated with. This paves the way for
&gt; eliminating fake heads.
&gt; 
&gt; The HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only
&gt; applied when the sizeof(struct page) is power-of-2. Having identical
&gt; tail pages allows the same page to be mapped into the vmemmap of all
&gt; pages, maintaining memory savings without fake heads.
&gt; 
&gt; If sizeof(struct page) is not power-of-2, there is no functional
&gt; changes.
&gt; 
&gt; Limit mask usage to HugeTLB vmemmap optimization (HVO) where it makes
&gt; a difference. The approach with mask would work in the wider set of
&gt; conditions, but it requires validating that struct pages are naturally
&gt; aligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
&gt; Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt; ---

Acked-by: Usama Arif &lt;usamaarif642@gmail.com&gt;

Small nit below:

&gt;  include/linux/page-flags.h | 81 ++++++++++++++++++++++++++++++++++----
&gt;  mm/slab.h                  | 16 ++++++--
&gt;  mm/util.c                  | 16 ++++++--
&gt;  3 files changed, 97 insertions(+), 16 deletions(-)
&gt; 
&gt; diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
&gt; index d14a17ffb55b..8f2c7fbc739b 100644
&gt; --- a/include/linux/page-flags.h
&gt; +++ b/include/linux/page-flags.h
&gt; @@ -198,6 +198,29 @@ enum pageflags {
&gt;  
&gt;  #ifndef __GENERATING_BOUNDS_H
&gt;  
&gt; +/*
&gt; + * For tail pages, if the size of struct page is power-of-2 -&gt;compound_info
&gt; + * encodes the mask that converts the address of the tail page address to
&gt; + * the head page address.
&gt; + *
&gt; + * Otherwise, -&gt;compound_info has direct pointer to head pages.
&gt; + */
&gt; +static __always_inline bool compound_info_has_mask(void)
&gt; +{
&gt; +	/*
&gt; +	 * Limit mask usage to HugeTLB vmemmap optimization (HVO) where it
&gt; +	 * makes a difference.
&gt; +	 *
&gt; +	 * The approach with mask would work in the wider set of conditions,
&gt; +	 * but it requires validating that struct pages are naturally aligned
&gt; +	 * for all orders up to the MAX_FOLIO_ORDER, which can be tricky.
&gt; +	 */
&gt; +	if (!IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP))
&gt; +		return false;
&gt; +
&gt; +	return is_power_of_2(sizeof(struct page));
&gt; +}
&gt; +
&gt;  #ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP
&gt;  DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);
&gt;  
&gt; @@ -210,6 +233,10 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page
&gt;  	if (!static_branch_unlikely(&amp;hugetlb_optimize_vmemmap_key))
&gt;  		return page;
&gt;  
&gt; +	/* Fake heads only exists if compound_info_has_mask() is true */
&gt; +	if (!compound_info_has_mask())
&gt; +		return page;
&gt; +
&gt;  	/*
&gt;  	 * Only addresses aligned with PAGE_SIZE of struct page may be fake head
&gt;  	 * struct page. The alignment check aims to avoid access the fields (
&gt; @@ -223,10 +250,14 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page
&gt;  		 * because the @page is a compound page composed with at least
&gt;  		 * two contiguous pages.
&gt;  		 */
&gt; -		unsigned long head = READ_ONCE(page[1].compound_info);
&gt; +		unsigned long info = READ_ONCE(page[1].compound_info);
&gt;  
&gt; -		if (likely(head &amp; 1))
&gt; -			return (const struct page *)(head - 1);
&gt; +		/* See set_compound_head() */
&gt; +		if (likely(info &amp; 1)) {
&gt; +			unsigned long p = (unsigned long)page;
&gt; +
&gt; +			return (const struct page *)(p &amp; info);
&gt; +		}
&gt;  	}
&gt;  	return page;
&gt;  }
&gt; @@ -281,11 +312,26 @@ static __always_inline int page_is_fake_head(const struct page *page)
&gt;  
&gt;  static __always_inline unsigned long _compound_head(const struct page *page)
&gt;  {
&gt; -	unsigned long head = READ_ONCE(page-&gt;compound_info);
&gt; +	unsigned long info = READ_ONCE(page-&gt;compound_info);
&gt;  
&gt; -	if (unlikely(head &amp; 1))
&gt; -		return head - 1;
&gt; -	return (unsigned long)page_fixed_fake_head(page);
&gt; +	/* Bit 0 encodes PageTail() */
&gt; +	if (!(info &amp; 1))
&gt; +		return (unsigned long)page_fixed_fake_head(page);
&gt; +
&gt; +	/*
&gt; +	 * If compound_info_has_mask() is false, the rest of compound_info is
&gt; +	 * the pointer to the head page.
&gt; +	 */
&gt; +	if (!compound_info_has_mask())
&gt; +		return info - 1;
&gt; +
&gt; +	/*
&gt; +	 * If compoun_info_has_mask() is true the rest of the info encodes

s/compoun_info_has_mask/compound_info_has_mask/

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On 02/02/2026 15:56, Kiryl Shutsemau wrote:
&gt; Move MAX_FOLIO_ORDER definition from mm.h to mmzone.h.
&gt; 
&gt; This is preparation for adding the vmemmap_tails array to struct
&gt; pglist_data, which requires MAX_FOLIO_ORDER to be available in mmzone.h.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Acked-by: David Hildenbrand (Red Hat) &lt;david@kernel.org&gt;
&gt; Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt; Acked-by: Muchun Song &lt;muchun.song@linux.dev&gt;

Acked-by: Usama Arif &lt;usamaarif642@gmail.com&gt;

---

On 02/02/2026 15:56, Kiryl Shutsemau wrote:
&gt; Move MAX_FOLIO_ORDER definition from mm.h to mmzone.h.
&gt; 
&gt; This is preparation for adding the vmemmap_tails array to struct
&gt; pglist_data, which requires MAX_FOLIO_ORDER to be available in mmzone.h.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Acked-by: David Hildenbrand (Red Hat) &lt;david@kernel.org&gt;
&gt; Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt; Acked-by: Muchun Song &lt;muchun.song@linux.dev&gt;

Acked-by: Usama Arif &lt;usamaarif642@gmail.com&gt;

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv</pre>
</details>
<a href="https://lore.kernel.org/r/fd80736b-7b2a-4675-82a7-1902705c6361@gmail.com" target="_blank" rel="noopener" class="lore-link">View on lore &#8599;</a>
<div class="review-comment-signals">Signals: limiting mask usage, potential issue with alignment</div>
</div>
</div>
<div class="thread-node depth-0" id="2026-02-10">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Vlastimil Babka</span>
<a class="date-chip" href="../2026-02-27_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-10">2026-02-10</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Vlastimil Babka noted that for tail pages, the kernel uses the &#x27;compound_info&#x27; field to get to the head page, and requested that the bit 0 of the field be checked before accessing it to prevent potential issues.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; Move MAX_FOLIO_ORDER definition from mm.h to mmzone.h.
&gt; 
&gt; This is preparation for adding the vmemmap_tails array to struct
&gt; pglist_data, which requires MAX_FOLIO_ORDER to be available in mmzone.h.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Acked-by: David Hildenbrand (Red Hat) &lt;david@kernel.org&gt;
&gt; Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt; Acked-by: Muchun Song &lt;muchun.song@linux.dev&gt;

Reviewed-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;

---

On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; Move MAX_FOLIO_ORDER definition from mm.h to mmzone.h.
&gt; 
&gt; This is preparation for adding the vmemmap_tails array to struct
&gt; pglist_data, which requires MAX_FOLIO_ORDER to be available in mmzone.h.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Acked-by: David Hildenbrand (Red Hat) &lt;david@kernel.org&gt;
&gt; Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt; Acked-by: Muchun Song &lt;muchun.song@linux.dev&gt;

Reviewed-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; Instead of passing down the head page and tail page index, pass the tail
&gt; and head pages directly, as well as the order of the compound page.
&gt; 
&gt; This is a preparation for changing how the head position is encoded in
&gt; the tail page.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
&gt; Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;

Reviewed-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;

---

On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; Instead of passing down the head page and tail page index, pass the tail
&gt; and head pages directly, as well as the order of the compound page.
&gt; 
&gt; This is a preparation for changing how the head position is encoded in
&gt; the tail page.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
&gt; Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;

Reviewed-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; The &#x27;compound_head&#x27; field in the &#x27;struct page&#x27; encodes whether the page
&gt; is a tail and where to locate the head page. Bit 0 is set if the page is
&gt; a tail, and the remaining bits in the field point to the head page.
&gt; 
&gt; As preparation for changing how the field encodes information about the
&gt; head page, rename the field to &#x27;compound_info&#x27;.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
&gt; Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;

Reviewed-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;

---

On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; The &#x27;compound_head&#x27; field in the &#x27;struct page&#x27; encodes whether the page
&gt; is a tail and where to locate the head page. Bit 0 is set if the page is
&gt; a tail, and the remaining bits in the field point to the head page.
&gt; 
&gt; As preparation for changing how the field encodes information about the
&gt; head page, rename the field to &#x27;compound_info&#x27;.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
&gt; Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;

Reviewed-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; Move set_compound_head() and clear_compound_head() to be adjacent to the
&gt; compound_head() function in page-flags.h.
&gt; 
&gt; These functions encode and decode the same compound_info field, so
&gt; keeping them together makes it easier to verify their logic is
&gt; consistent, especially when the encoding changes.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
&gt; Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;

Reviewed-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;

---

On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; Move set_compound_head() and clear_compound_head() to be adjacent to the
&gt; compound_head() function in page-flags.h.
&gt; 
&gt; These functions encode and decode the same compound_info field, so
&gt; keeping them together makes it easier to verify their logic is
&gt; consistent, especially when the encoding changes.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
&gt; Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;

Reviewed-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;



_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; For tail pages, the kernel uses the &#x27;compound_info&#x27; field to get to the
&gt; head page. The bit 0 of the field indicates whether the page is a
&gt; tail page, and if set, the remaining bits represent a pointer to the
&gt; head page.
&gt; 
&gt; For cases when size of struct page is power-of-2, change the encoding of
&gt; compound_info to store a mask that can be applied to the virtual address
&gt; of the tail page in order to access the head page. It is possible
&gt; because struct page of the head page is naturally aligned with regards
&gt; to order of the page.
&gt; 
&gt; The significant impact of this modification is that all tail pages of
&gt; the same order will now have identical &#x27;compound_info&#x27;, regardless of
&gt; the compound page they are associated with. This paves the way for
&gt; eliminating fake heads.
&gt; 
&gt; The HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only
&gt; applied when the sizeof(struct page) is power-of-2. Having identical
&gt; tail pages allows the same page to be mapped into the vmemmap of all
&gt; pages, maintaining memory savings without fake heads.
&gt; 
&gt; If sizeof(struct page) is not power-of-2, there is no functional
&gt; changes.
&gt; 
&gt; Limit mask usage to HugeTLB vmemmap optimization (HVO) where it makes
&gt; a difference. The approach with mask would work in the wider set of
&gt; conditions, but it requires validating that struct pages are naturally
&gt; aligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
&gt; Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;

Reviewed-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;

nit:

&gt; ---
&gt;  include/linux/page-flags.h | 81 ++++++++++++++++++++++++++++++++++----
&gt;  mm/slab.h                  | 16 ++++++--
&gt;  mm/util.c                  | 16 ++++++--
&gt;  3 files changed, 97 insertions(+), 16 deletions(-)
&gt; 
&gt; diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
&gt; index d14a17ffb55b..8f2c7fbc739b 100644
&gt; --- a/include/linux/page-flags.h
&gt; +++ b/include/linux/page-flags.h
&gt; @@ -198,6 +198,29 @@ enum pageflags {
&gt;  
&gt;  #ifndef __GENERATING_BOUNDS_H
&gt;  
&gt; +/*
&gt; + * For tail pages, if the size of struct page is power-of-2 -&gt;compound_info
&gt; + * encodes the mask that converts the address of the tail page address to
&gt; + * the head page address.
&gt; + *
&gt; + * Otherwise, -&gt;compound_info has direct pointer to head pages.
&gt; + */
&gt; +static __always_inline bool compound_info_has_mask(void)
&gt; +{
&gt; +	/*
&gt; +	 * Limit mask usage to HugeTLB vmemmap optimization (HVO) where it
&gt; +	 * makes a difference.
&gt; +	 *
&gt; +	 * The approach with mask would work in the wider set of conditions,
&gt; +	 * but it requires validating that struct pages are naturally aligned
&gt; +	 * for all orders up to the MAX_FOLIO_ORDER, which can be tricky.
&gt; +	 */
&gt; +	if (!IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP))
&gt; +		return false;
&gt; +
&gt; +	return is_power_of_2(sizeof(struct page));
&gt; +}
&gt; +
&gt;  #ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP
&gt;  DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);
&gt;  
&gt; @@ -210,6 +233,10 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page
&gt;  	if (!static_branch_unlikely(&amp;hugetlb_optimize_vmemmap_key))
&gt;  		return page;
&gt;  
&gt; +	/* Fake heads only exists if compound_info_has_mask() is true */
&gt; +	if (!compound_info_has_mask())
&gt; +		return page;
&gt; +

Could we move this compile-time-constant test above the static branch test?

&gt;  	/*
&gt;  	 * Only addresses aligned with PAGE_SIZE of struct page may be fake head
&gt;  	 * struct page. The alignment check aims to avoid access the fields (
&gt; @@ -223,10 +250,14 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page
&gt;  		 * because the @page is a compound page composed with at least
&gt;  		 * two contiguous pages.
&gt;  		 */
&gt; -		unsigned long head = READ_ONCE(page[1].compound_info);
&gt; +		unsigned long info = READ_ONCE(page[1].compound_info);
&gt;  
&gt; -		if (likely(head &amp; 1))
&gt; -			return (const struct page *)(head - 1);
&gt; +		/* See set_compound_head() */
&gt; +		if (likely(info &amp; 1)) {
&gt; +			unsigned long p = (unsigned long)page;
&gt; +
&gt; +			return (const struct page *)(p &amp; info);
&gt; +		}
&gt;  	}
&gt;  	return page;
&gt;  }

---

On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt; For tail pages, the kernel uses the &#x27;compound_info&#x27; field to get to the
&gt; head page. The bit 0 of the field indicates whether the page is a
&gt; tail page, and if set, the remaining bits represent a pointer to the
&gt; head page.
&gt; 
&gt; For cases when size of struct page is power-of-2, change the encoding of
&gt; compound_info to store a mask that can be applied to the virtual address
&gt; of the tail page in order to access the head page. It is possible
&gt; because struct page of the head page is naturally aligned with regards
&gt; to order of the page.
&gt; 
&gt; The significant impact of this modification is that all tail pages of
&gt; the same order will now have identical &#x27;compound_info&#x27;, regardless of
&gt; the compound page they are associated with. This paves the way for
&gt; eliminating fake heads.
&gt; 
&gt; The HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only
&gt; applied when the sizeof(struct page) is power-of-2. Having identical
&gt; tail pages allows the same page to be mapped into the vmemmap of all
&gt; pages, maintaining memory savings without fake heads.
&gt; 
&gt; If sizeof(struct page) is not power-of-2, there is no functional
&gt; changes.
&gt; 
&gt; Limit mask usage to HugeTLB vmemmap optimization (HVO) where it makes
&gt; a difference. The approach with mask would work in the wider set of
&gt; conditions, but it requires validating that struct pages are naturally
&gt; aligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.
&gt; 
&gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt; Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
&gt; Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;

Reviewed-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;

nit:

&gt; ---
&gt;  include/linux/page-flags.h | 81 ++++++++++++++++++++++++++++++++++----
&gt;  mm/slab.h                  | 16 ++++++--
&gt;  mm/util.c                  | 16 ++++++--
&gt;  3 files changed, 97 insertions(+), 16 deletions(-)
&gt; 
&gt; diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
&gt; index d14a17ffb55b..8f2c7fbc739b 100644
&gt; --- a/include/linux/page-flags.h
&gt; +++ b/include/linux/page-flags.h
&gt; @@ -198,6 +198,29 @@ enum pageflags {
&gt;  
&gt;  #ifndef __GENERATING_BOUNDS_H
&gt;  
&gt; +/*
&gt; + * For tail pages, if the size of struct page is power-of-2 -&gt;compound_info
&gt; + * encodes the mask that converts the address of the tail page address to
&gt; + * the head page address.
&gt; + *
&gt; + * Otherwise, -&gt;compound_info has direct pointer to head pages.
&gt; + */
&gt; +static __always_inline bool compound_info_has_mask(void)
&gt; +{
&gt; +	/*
&gt; +	 * Limit mask usage to HugeTLB vmemmap optimization (HVO) where it
&gt; +	 * makes a difference.
&gt; +	 *
&gt; +	 * The approach with mask would work in the wider set of conditions,
&gt; +	 * but it requires validating that struct pages are naturally aligned
&gt; +	 * for all orders up to the MAX_FOLIO_ORDER, which can be tricky.
&gt; +	 */
&gt; +	if (!IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP))
&gt; +		return false;
&gt; +
&gt; +	return is_power_of_2(sizeof(struct page));
&gt; +}
&gt; +
&gt;  #ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP
&gt;  DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);
&gt;  
&gt; @@ -210,6 +233,10 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page
&gt;  	if (!static_branch_unlikely(&amp;hugetlb_optimize_vmemmap_key))
&gt;  		return page;
&gt;  
&gt; +	/* Fake heads only exists if compound_info_has_mask() is true */
&gt; +	if (!compound_info_has_mask())
&gt; +		return page;
&gt; +

Could we move this compile-time-constant test above the static branch test?

&gt;  	/*
&gt;  	 * Only addresses aligned with PAGE_SIZE of struct page may be fake head
&gt;  	 * struct page. The alignment check aims to avoid access the fields (
&gt; @@ -223,10 +250,14 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page
&gt;  		 * because the @page is a compound page composed with at least
&gt;  		 * two contiguous pages.
&gt;  		 */
&gt; -		unsigned long head = READ_ONCE(page[1].compound_info);
&gt; +		unsigned long info = READ_ONCE(page[1].compound_info);
&gt;  
&gt; -		if (likely(head &amp; 1))
&gt; -			return (const struct page *)(head - 1);
&gt; +		/* See set_compound_head() */
&gt; +		if (likely(info &amp; 1)) {
&gt; +			unsigned long p = (unsigned long)page;
&gt; +
&gt; +			return (const struct page *)(p &amp; info);
&gt; +		}
&gt;  	}
&gt;  	return page;
&gt;  }

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On 2/9/26 12:52, Kiryl Shutsemau wrote:
&gt; On Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:
&gt;&gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt;&gt; &gt; With the upcoming changes to HVO, a single page of tail struct pages
&gt;&gt; &gt; will be shared across all huge pages of the same order on a node. Since
&gt;&gt; &gt; huge pages on the same node may belong to different zones, the zone
&gt;&gt; &gt; information stored in shared tail page flags would be incorrect.
&gt;&gt; &gt; 
&gt;&gt; &gt; Always fetch zone information from the head page, which has unique and
&gt;&gt; &gt; correct zone flags for each compound page.
&gt;&gt; &gt; 
&gt;&gt; &gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt;&gt; &gt; Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt;&gt; &gt; ---
&gt;&gt; &gt;   include/linux/mmzone.h | 1 +
&gt;&gt; &gt;   1 file changed, 1 insertion(+)
&gt;&gt; &gt; 
&gt;&gt; &gt; diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
&gt;&gt; &gt; index be8ce40b5638..192143b5cdc0 100644
&gt;&gt; &gt; --- a/include/linux/mmzone.h
&gt;&gt; &gt; +++ b/include/linux/mmzone.h
&gt;&gt; &gt; @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)
&gt;&gt; &gt;   static inline enum zone_type page_zonenum(const struct page *page)
&gt;&gt; &gt;   {
&gt;&gt; &gt; +	page = compound_head(page);
&gt;&gt; &gt;   	return memdesc_zonenum(page-&gt;flags);
&gt;&gt; 
&gt;&gt; We end up calling page_zonenum() without holding a reference.
&gt;&gt; 
&gt;&gt; Given that _compound_head() does a READ_ONCE(), this should work even if we
&gt;&gt; see concurrent page freeing etc.
&gt;&gt; 
&gt;&gt; However, this change implies that we now perform a compound page lookup for
&gt;&gt; every PageHighMem() [meh], page_zone() [quite some users in the buddy,
&gt;&gt; including for pageblock access and page freeing].
&gt;&gt; 
&gt;&gt; That&#x27;s a nasty compromise for making HVO better? :)
&gt;&gt; 
&gt;&gt; We should likely limit that special casing to kernels that really rquire it
&gt;&gt; (HVO).
&gt; 
&gt; I will add compound_info_has_mask() check.

Not thrilled by this indeed. Would it be a problem to have the shared tail
pages per node+zone instead of just per node?

---

On 2/9/26 12:52, Kiryl Shutsemau wrote:
&gt; On Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:
&gt;&gt; On 2/2/26 16:56, Kiryl Shutsemau wrote:
&gt;&gt; &gt; With the upcoming changes to HVO, a single page of tail struct pages
&gt;&gt; &gt; will be shared across all huge pages of the same order on a node. Since
&gt;&gt; &gt; huge pages on the same node may belong to different zones, the zone
&gt;&gt; &gt; information stored in shared tail page flags would be incorrect.
&gt;&gt; &gt; 
&gt;&gt; &gt; Always fetch zone information from the head page, which has unique and
&gt;&gt; &gt; correct zone flags for each compound page.
&gt;&gt; &gt; 
&gt;&gt; &gt; Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
&gt;&gt; &gt; Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
&gt;&gt; &gt; ---
&gt;&gt; &gt;   include/linux/mmzone.h | 1 +
&gt;&gt; &gt;   1 file changed, 1 insertion(+)
&gt;&gt; &gt; 
&gt;&gt; &gt; diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
&gt;&gt; &gt; index be8ce40b5638..192143b5cdc0 100644
&gt;&gt; &gt; --- a/include/linux/mmzone.h
&gt;&gt; &gt; +++ b/include/linux/mmzone.h
&gt;&gt; &gt; @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)
&gt;&gt; &gt;   static inline enum zone_type page_zonenum(const struct page *page)
&gt;&gt; &gt;   {
&gt;&gt; &gt; +	page = compound_head(page);
&gt;&gt; &gt;   	return memdesc_zonenum(page-&gt;flags);
&gt;&gt; 
&gt;&gt; We end up calling page_zonenum() without holding a reference.
&gt;&gt; 
&gt;&gt; Given that _compound_head() does a READ_ONCE(), this should work even if we
&gt;&gt; see concurrent page freeing etc.
&gt;&gt; 
&gt;&gt; However, this change implies that we now perform a compound page lookup for
&gt;&gt; every PageHighMem() [meh], page_zone() [quite some users in the buddy,
&gt;&gt; including for pageblock access and page freeing].
&gt;&gt; 
&gt;&gt; That&#x27;s a nasty compromise for making HVO better? :)
&gt;&gt; 
&gt;&gt; We should likely limit that special casing to kernels that really rquire it
&gt;&gt; (HVO).
&gt; 
&gt; I will add compound_info_has_mask() check.

Not thrilled by this indeed. Would it be a problem to have the shared tail
pages per node+zone instead of just per node?




_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On 2/16/26 10:06, David Hildenbrand (Arm) wrote:
&gt; On 2/16/26 00:13, Matthew Wilcox wrote:
&gt;&gt; On Mon, Feb 02, 2026 at 03:56:24PM +0000, Kiryl Shutsemau wrote:
&gt;&gt;&gt; With the upcoming changes to HVO, a single page of tail struct pages
&gt;&gt;&gt; will be shared across all huge pages of the same order on a node. Since
&gt;&gt;&gt; huge pages on the same node may belong to different zones, the zone
&gt;&gt;&gt; information stored in shared tail page flags would be incorrect.
&gt;&gt;&gt;
&gt;&gt;&gt; Always fetch zone information from the head page, which has unique and
&gt;&gt;&gt; correct zone flags for each compound page.
&gt;&gt; 
&gt;&gt; You&#x27;re right that different pages in the same folio can have different
&gt;&gt; zone number.  But does it matter ... or to put it another way, why is
&gt;&gt; returning the zone number of the head page the correct way to resolve
&gt;&gt; this?
&gt; 
&gt; How can a folio cross zones?
&gt; 
&gt; Runtime allocated hugetlb folios from the CMA/buddy (alloc_contig_range) 
&gt; definitely fall into a single zone.
&gt; 
&gt; So is it about ones allocated early during boot, where, by chance, we 
&gt; manage to cross ZONE_NORMAL + ZONE_MOVABLE etc?
&gt; 
&gt; I thought that it&#x27;s also not allowed there, and I wonder whether we 
&gt; should disallow it if it&#x27;s possible.

I would be surprised if things didn&#x27;t break horribly if we allowed crossing
zones in a single folio. I&#x27;d rather not allow it.

(And I still don&#x27;t like how this patch solves the issue)

&gt;&gt; 
&gt;&gt; Arguably, the caller is asking for the zone number of _this page_, and
&gt;&gt; does not care about the zone number of the head page.  It would be good
&gt;&gt; to have a short discussion of this in the commit message (but probably
&gt;&gt; not worth putting this in a comment).
&gt; 
&gt; Agreed, in particular, if there would be a functional change. So far I 
&gt; assumed there would be no such change.
&gt; 
&gt; Things like shrink_zone_span() really need to know the zone of that 
&gt; page, not the one of the head; unless both fall into the same zone.
&gt;

---

On 2/16/26 10:06, David Hildenbrand (Arm) wrote:
&gt; On 2/16/26 00:13, Matthew Wilcox wrote:
&gt;&gt; On Mon, Feb 02, 2026 at 03:56:24PM +0000, Kiryl Shutsemau wrote:
&gt;&gt;&gt; With the upcoming changes to HVO, a single page of tail struct pages
&gt;&gt;&gt; will be shared across all huge pages of the same order on a node. Since
&gt;&gt;&gt; huge pages on the same node may belong to different zones, the zone
&gt;&gt;&gt; information stored in shared tail page flags would be incorrect.
&gt;&gt;&gt;
&gt;&gt;&gt; Always fetch zone information from the head page, which has unique and
&gt;&gt;&gt; correct zone flags for each compound page.
&gt;&gt; 
&gt;&gt; You&#x27;re right that different pages in the same folio can have different
&gt;&gt; zone number.  But does it matter ... or to put it another way, why is
&gt;&gt; returning the zone number of the head page the correct way to resolve
&gt;&gt; this?
&gt; 
&gt; How can a folio cross zones?
&gt; 
&gt; Runtime allocated hugetlb folios from the CMA/buddy (alloc_contig_range) 
&gt; definitely fall into a single zone.
&gt; 
&gt; So is it about ones allocated early during boot, where, by chance, we 
&gt; manage to cross ZONE_NORMAL + ZONE_MOVABLE etc?
&gt; 
&gt; I thought that it&#x27;s also not allowed there, and I wonder whether we 
&gt; should disallow it if it&#x27;s possible.

I would be surprised if things didn&#x27;t break horribly if we allowed crossing
zones in a single folio. I&#x27;d rather not allow it.

(And I still don&#x27;t like how this patch solves the issue)

&gt;&gt; 
&gt;&gt; Arguably, the caller is asking for the zone number of _this page_, and
&gt;&gt; does not care about the zone number of the head page.  It would be good
&gt;&gt; to have a short discussion of this in the commit message (but probably
&gt;&gt; not worth putting this in a comment).
&gt; 
&gt; Agreed, in particular, if there would be a functional change. So far I 
&gt; assumed there would be no such change.
&gt; 
&gt; Things like shrink_zone_span() really need to know the zone of that 
&gt; page, not the one of the head; unless both fall into the same zone.
&gt; 


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv</pre>
</details>
<a href="https://lore.kernel.org/r/ae2be3d3-57a2-44ed-9a3d-c7de2ea79970@suse.cz" target="_blank" rel="noopener" class="lore-link">View on lore &#8599;</a>
<div class="review-comment-signals">Signals: potential issue, requested change</div>
</div>
</div>
<div class="thread-node depth-0" id="2026-02-15">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Matthew Wilcox</span>
<a class="date-chip" href="../2026-02-27_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-15">2026-02-15</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Matthew Wilcox questioned the correctness of always returning the zone number of the head page, suggesting that the caller may be asking for the zone number of &#x27;this page&#x27;, and proposed a discussion in the commit message to clarify this.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Mon, Feb 02, 2026 at 03:56:24PM +0000, Kiryl Shutsemau wrote:
&gt; With the upcoming changes to HVO, a single page of tail struct pages
&gt; will be shared across all huge pages of the same order on a node. Since
&gt; huge pages on the same node may belong to different zones, the zone
&gt; information stored in shared tail page flags would be incorrect.
&gt; 
&gt; Always fetch zone information from the head page, which has unique and
&gt; correct zone flags for each compound page.

You&#x27;re right that different pages in the same folio can have different
zone number.  But does it matter ... or to put it another way, why is
returning the zone number of the head page the correct way to resolve
this?

Arguably, the caller is asking for the zone number of _this page_, and
does not care about the zone number of the head page.  It would be good
to have a short discussion of this in the commit message (but probably
not worth putting this in a comment).

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

On Mon, Feb 02, 2026 at 03:56:24PM +0000, Kiryl Shutsemau wrote:
&gt; With the upcoming changes to HVO, a single page of tail struct pages
&gt; will be shared across all huge pages of the same order on a node. Since
&gt; huge pages on the same node may belong to different zones, the zone
&gt; information stored in shared tail page flags would be incorrect.
&gt; 
&gt; Always fetch zone information from the head page, which has unique and
&gt; correct zone flags for each compound page.

You&#x27;re right that different pages in the same folio can have different
zone number.  But does it matter ... or to put it another way, why is
returning the zone number of the head page the correct way to resolve
this?

Arguably, the caller is asking for the zone number of _this page_, and
does not care about the zone number of the head page.  It would be good
to have a short discussion of this in the commit message (but probably
not worth putting this in a comment).

---

On Mon, Feb 16, 2026 at 10:06:57AM +0100, David Hildenbrand (Arm) wrote:
&gt; On 2/16/26 00:13, Matthew Wilcox wrote:
&gt; &gt; On Mon, Feb 02, 2026 at 03:56:24PM +0000, Kiryl Shutsemau wrote:
&gt; &gt; &gt; With the upcoming changes to HVO, a single page of tail struct pages
&gt; &gt; &gt; will be shared across all huge pages of the same order on a node. Since
&gt; &gt; &gt; huge pages on the same node may belong to different zones, the zone
&gt; &gt; &gt; information stored in shared tail page flags would be incorrect.
&gt; &gt; &gt; 
&gt; &gt; &gt; Always fetch zone information from the head page, which has unique and
&gt; &gt; &gt; correct zone flags for each compound page.
&gt; &gt; 
&gt; &gt; You&#x27;re right that different pages in the same folio can have different
&gt; &gt; zone number.  But does it matter ... or to put it another way, why is
&gt; &gt; returning the zone number of the head page the correct way to resolve
&gt; &gt; this?
&gt; 
&gt; How can a folio cross zones?

I thought 1GB pages in hugetlb could cross zones?  Maybe that used to be
true and isn&#x27;t any more, or maybe it was never true and I was just
confused.

---

On Mon, Feb 16, 2026 at 10:06:57AM +0100, David Hildenbrand (Arm) wrote:
&gt; On 2/16/26 00:13, Matthew Wilcox wrote:
&gt; &gt; On Mon, Feb 02, 2026 at 03:56:24PM +0000, Kiryl Shutsemau wrote:
&gt; &gt; &gt; With the upcoming changes to HVO, a single page of tail struct pages
&gt; &gt; &gt; will be shared across all huge pages of the same order on a node. Since
&gt; &gt; &gt; huge pages on the same node may belong to different zones, the zone
&gt; &gt; &gt; information stored in shared tail page flags would be incorrect.
&gt; &gt; &gt; 
&gt; &gt; &gt; Always fetch zone information from the head page, which has unique and
&gt; &gt; &gt; correct zone flags for each compound page.
&gt; &gt; 
&gt; &gt; You&#x27;re right that different pages in the same folio can have different
&gt; &gt; zone number.  But does it matter ... or to put it another way, why is
&gt; &gt; returning the zone number of the head page the correct way to resolve
&gt; &gt; this?
&gt; 
&gt; How can a folio cross zones?

I thought 1GB pages in hugetlb could cross zones?  Maybe that used to be
true and isn&#x27;t any more, or maybe it was never true and I was just
confused.


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv</pre>
</details>
<a href="https://lore.kernel.org/r/aZJTLwV2SaaKu1k_@casper.infradead.org" target="_blank" rel="noopener" class="lore-link">View on lore &#8599;</a>
<div class="review-comment-signals">Signals: questioning correctness, requested clarification</div>
</div>
</div>
<div class="thread-node depth-0" id="2026-02-23">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Frank Linden</span>
<a class="date-chip" href="../2026-02-27_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-23">2026-02-23</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Frank Linden noted that HugeTLB folios could cross zones due to bootmem (memblock) allocated pages, which would cause issues when freeing and reinitializing the vmemmap for HVO. He recalled a patch (14ed3a595fa4) that fixed this issue by checking for zone intersections in hugetlb allocation.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Mon, Feb 23, 2026 at 11:32 AM David Hildenbrand (Arm)
&lt;david@kernel.org&gt; wrote:
&gt;
&gt; On 2/23/26 19:18, Matthew Wilcox wrote:
&gt; &gt; On Mon, Feb 16, 2026 at 10:06:57AM +0100, David Hildenbrand (Arm) wrote:
&gt; &gt;&gt; On 2/16/26 00:13, Matthew Wilcox wrote:
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt; You&#x27;re right that different pages in the same folio can have different
&gt; &gt;&gt;&gt; zone number.  But does it matter ... or to put it another way, why is
&gt; &gt;&gt;&gt; returning the zone number of the head page the correct way to resolve
&gt; &gt;&gt;&gt; this?
&gt; &gt;&gt;
&gt; &gt;&gt; How can a folio cross zones?
&gt; &gt;
&gt; &gt; I thought 1GB pages in hugetlb could cross zones?  Maybe that used to be
&gt; &gt; true and isn&#x27;t any more, or maybe it was never true and I was just
&gt; &gt; confused.
&gt;
&gt; I recall that 1G folios could end up in ZONE_MOVABLE (comment in
&gt; page_is_unmovable()), but my memory is fuzzy when it comes to crossing
&gt; zones (ZONE_NORMAL -&gt; ZONE_MOVABLE).
&gt;
&gt; Freeing+reinitializing the vmemmap for HVO with such folios would
&gt; already be problematic I suppose: we would silently switch the zone for
&gt; some of these pages.
&gt;
&gt; When freeing such (boottime) hugetlb folios to the buddy, we use
&gt; free_frozen_pages(). In there we lookup the zone once.
&gt;
&gt; Likely also problematic :)

HugeTLB folios weren&#x27;t supposed to cross zones, but they could do that
in some cases for bootmem (memblock) allocated pages, causing the
issue you describe.

I fixed that with 14ed3a595fa4 (&quot;mm/hugetlb: check bootmem pages for
zone intersections&quot;), so they won&#x27;t cross zones anymore. The other
allocation methods used for HugeTLB folios, alloc_contig_pages() and
cma_alloc_folio, won&#x27;t return anything that crosses a zone boundary by
their nature.

So I think that&#x27;s all good.

- Frank

---

On Mon, Feb 23, 2026 at 11:32 AM David Hildenbrand (Arm)
&lt;david@kernel.org&gt; wrote:
&gt;
&gt; On 2/23/26 19:18, Matthew Wilcox wrote:
&gt; &gt; On Mon, Feb 16, 2026 at 10:06:57AM +0100, David Hildenbrand (Arm) wrote:
&gt; &gt;&gt; On 2/16/26 00:13, Matthew Wilcox wrote:
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt; You&#x27;re right that different pages in the same folio can have different
&gt; &gt;&gt;&gt; zone number.  But does it matter ... or to put it another way, why is
&gt; &gt;&gt;&gt; returning the zone number of the head page the correct way to resolve
&gt; &gt;&gt;&gt; this?
&gt; &gt;&gt;
&gt; &gt;&gt; How can a folio cross zones?
&gt; &gt;
&gt; &gt; I thought 1GB pages in hugetlb could cross zones?  Maybe that used to be
&gt; &gt; true and isn&#x27;t any more, or maybe it was never true and I was just
&gt; &gt; confused.
&gt;
&gt; I recall that 1G folios could end up in ZONE_MOVABLE (comment in
&gt; page_is_unmovable()), but my memory is fuzzy when it comes to crossing
&gt; zones (ZONE_NORMAL -&gt; ZONE_MOVABLE).
&gt;
&gt; Freeing+reinitializing the vmemmap for HVO with such folios would
&gt; already be problematic I suppose: we would silently switch the zone for
&gt; some of these pages.
&gt;
&gt; When freeing such (boottime) hugetlb folios to the buddy, we use
&gt; free_frozen_pages(). In there we lookup the zone once.
&gt;
&gt; Likely also problematic :)

HugeTLB folios weren&#x27;t supposed to cross zones, but they could do that
in some cases for bootmem (memblock) allocated pages, causing the
issue you describe.

I fixed that with 14ed3a595fa4 (&quot;mm/hugetlb: check bootmem pages for
zone intersections&quot;), so they won&#x27;t cross zones anymore. The other
allocation methods used for HugeTLB folios, alloc_contig_pages() and
cma_alloc_folio, won&#x27;t return anything that crosses a zone boundary by
their nature.

So I think that&#x27;s all good.

- Frank

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv</pre>
</details>
<a href="https://lore.kernel.org/r/CAPTztWbr7y0myXB17Vz5HEZTw8a3PJ4qaxRKgtZmt-qXx1ofeA@mail.gmail.com" target="_blank" rel="noopener" class="lore-link">View on lore &#8599;</a>
<div class="review-comment-signals">Signals: clarification, acknowledgment</div>
</div>
</div>
<div class="thread-node depth-0" id="2026-02-27">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Kiryl Shutsemau (author)</span>
<a class="date-chip" href="../2026-02-27_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-27">2026-02-27</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author is addressing a concern about passing down the head page and tail page index, which was deemed too complex. The author instead proposes to pass the tail and head pages directly, along with the compound page order, as a preparation for changing how the head position is encoded in the tail page.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">From: Kiryl Shutsemau &lt;kas@kernel.org&gt;

Move MAX_FOLIO_ORDER definition from mm.h to mmzone.h.

This is preparation for adding the vmemmap_tails array to struct
zone, which requires MAX_FOLIO_ORDER to be available in mmzone.h.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Acked-by: David Hildenbrand (Red Hat) &lt;david@kernel.org&gt;
Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
Acked-by: Muchun Song &lt;muchun.song@linux.dev&gt;
Acked-by: Usama Arif &lt;usamaarif642@gmail.com&gt;
Reviewed-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;
---
 include/linux/mm.h     | 31 -------------------------------
 include/linux/mmzone.h | 31 +++++++++++++++++++++++++++++++
 2 files changed, 31 insertions(+), 31 deletions(-)

diff --git a/include/linux/mm.h b/include/linux/mm.h
index 5be3d8a8f806..7f4dbbb9d783 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -27,7 +27,6 @@
 #include &lt;linux/page-flags.h&gt;
 #include &lt;linux/page_ref.h&gt;
 #include &lt;linux/overflow.h&gt;
-#include &lt;linux/sizes.h&gt;
 #include &lt;linux/sched.h&gt;
 #include &lt;linux/pgtable.h&gt;
 #include &lt;linux/kasan.h&gt;
@@ -2479,36 +2478,6 @@ static inline unsigned long folio_nr_pages(const struct folio *folio)
 	return folio_large_nr_pages(folio);
 }
 
-#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)
-/*
- * We don&#x27;t expect any folios that exceed buddy sizes (and consequently
- * memory sections).
- */
-#define MAX_FOLIO_ORDER		MAX_PAGE_ORDER
-#elif defined(CONFIG_SPARSEMEM) &amp;&amp; !defined(CONFIG_SPARSEMEM_VMEMMAP)
-/*
- * Only pages within a single memory section are guaranteed to be
- * contiguous. By limiting folios to a single memory section, all folio
- * pages are guaranteed to be contiguous.
- */
-#define MAX_FOLIO_ORDER		PFN_SECTION_SHIFT
-#elif defined(CONFIG_HUGETLB_PAGE)
-/*
- * There is no real limit on the folio size. We limit them to the maximum we
- * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect
- * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.
- */
-#define MAX_FOLIO_ORDER		get_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)
-#else
-/*
- * Without hugetlb, gigantic folios that are bigger than a single PUD are
- * currently impossible.
- */
-#define MAX_FOLIO_ORDER		PUD_ORDER
-#endif
-
-#define MAX_FOLIO_NR_PAGES	(1UL &lt;&lt; MAX_FOLIO_ORDER)
-
 /*
  * compound_nr() returns the number of pages in this potentially compound
  * page.  compound_nr() can be called on a tail page, and is defined to
diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 3e51190a55e4..be8ce40b5638 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -23,6 +23,7 @@
 #include &lt;linux/page-flags.h&gt;
 #include &lt;linux/local_lock.h&gt;
 #include &lt;linux/zswap.h&gt;
+#include &lt;linux/sizes.h&gt;
 #include &lt;asm/page.h&gt;
 
 /* Free memory management - zoned buddy allocator.  */
@@ -61,6 +62,36 @@
  */
 #define PAGE_ALLOC_COSTLY_ORDER 3
 
+#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)
+/*
+ * We don&#x27;t expect any folios that exceed buddy sizes (and consequently
+ * memory sections).
+ */
+#define MAX_FOLIO_ORDER		MAX_PAGE_ORDER
+#elif defined(CONFIG_SPARSEMEM) &amp;&amp; !defined(CONFIG_SPARSEMEM_VMEMMAP)
+/*
+ * Only pages within a single memory section are guaranteed to be
+ * contiguous. By limiting folios to a single memory section, all folio
+ * pages are guaranteed to be contiguous.
+ */
+#define MAX_FOLIO_ORDER		PFN_SECTION_SHIFT
+#elif defined(CONFIG_HUGETLB_PAGE)
+/*
+ * There is no real limit on the folio size. We limit them to the maximum we
+ * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect
+ * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.
+ */
+#define MAX_FOLIO_ORDER		get_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)
+#else
+/*
+ * Without hugetlb, gigantic folios that are bigger than a single PUD are
+ * currently impossible.
+ */
+#define MAX_FOLIO_ORDER		PUD_ORDER
+#endif
+
+#define MAX_FOLIO_NR_PAGES	(1UL &lt;&lt; MAX_FOLIO_ORDER)
+
 enum migratetype {
 	MIGRATE_UNMOVABLE,
 	MIGRATE_MOVABLE,
-- 
2.51.2

---

From: Kiryl Shutsemau &lt;kas@kernel.org&gt;

Instead of passing down the head page and tail page index, pass the tail
and head pages directly, as well as the order of the compound page.

This is a preparation for changing how the head position is encoded in
the tail page.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
Acked-by: David Hildenbrand (arm) &lt;david@kernel.org&gt;
Reviewed-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;
---
 include/linux/page-flags.h |  5 +++--
 mm/hugetlb.c               |  8 +++++---
 mm/internal.h              | 11 +++++------
 mm/mm_init.c               |  2 +-
 mm/page_alloc.c            |  2 +-
 5 files changed, 15 insertions(+), 13 deletions(-)

diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
index f7a0e4af0c73..5e7687ccccf8 100644
--- a/include/linux/page-flags.h
+++ b/include/linux/page-flags.h
@@ -865,9 +865,10 @@ static inline bool folio_test_large(const struct folio *folio)
 	return folio_test_head(folio);
 }
 
-static __always_inline void set_compound_head(struct page *page, struct page *head)
+static __always_inline void set_compound_head(struct page *tail,
+		const struct page *head, unsigned int order)
 {
-	WRITE_ONCE(page-&gt;compound_head, (unsigned long)head + 1);
+	WRITE_ONCE(tail-&gt;compound_head, (unsigned long)head + 1);
 }
 
 static __always_inline void clear_compound_head(struct page *page)
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 0beb6e22bc26..fc55f22c9e41 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -3168,6 +3168,7 @@ int __alloc_bootmem_huge_page(struct hstate *h, int nid)
 
 /* Initialize [start_page:end_page_number] tail struct pages of a hugepage */
 static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,
+					struct hstate *h,
 					unsigned long start_page_number,
 					unsigned long end_page_number)
 {
@@ -3176,6 +3177,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,
 	struct page *page = folio_page(folio, start_page_number);
 	unsigned long head_pfn = folio_pfn(folio);
 	unsigned long pfn, end_pfn = head_pfn + end_page_number;
+	unsigned int order = huge_page_order(h);
 
 	/*
 	 * As we marked all tail pages with memblock_reserved_mark_noinit(),
@@ -3183,7 +3185,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,
 	 */
 	for (pfn = head_pfn + start_page_number; pfn &lt; end_pfn; page++, pfn++) {
 		__init_single_page(page, pfn, zone, nid);
-		prep_compound_tail((struct page *)folio, pfn - head_pfn);
+		prep_compound_tail(page, &amp;folio-&gt;page, order);
 		set_page_count(page, 0);
 	}
 }
@@ -3203,7 +3205,7 @@ static void __init hugetlb_folio_init_vmemmap(struct folio *folio,
 	__folio_set_head(folio);
 	ret = folio_ref_freeze(folio, 1);
 	VM_BUG_ON(!ret);
-	hugetlb_folio_init_tail_vmemmap(folio, 1, nr_pages);
+	hugetlb_folio_init_tail_vmemmap(folio, h, 1, nr_pages);
 	prep_compound_head(&amp;folio-&gt;page, huge_page_order(h));
 }
 
@@ -3260,7 +3262,7 @@ static void __init prep_and_add_bootmem_folios(struct hstate *h,
 			 * time as this is early in boot and there should
 			 * be no contention.
 			 */
-			hugetlb_folio_init_tail_vmemmap(folio,
+			hugetlb_folio_init_tail_vmemmap(folio, h,
 					HUGETLB_VMEMMAP_RESERVE_PAGES,
 					pages_per_huge_page(h));
 		}
diff --git a/mm/internal.h b/mm/internal.h
index cb0af847d7d9..c76122f22294 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -878,13 +878,12 @@ static inline void prep_compound_head(struct page *page, unsigned int order)
 		INIT_LIST_HEAD(&amp;folio-&gt;_deferred_list);
 }
 
-static inline void prep_compound_tail(struct page *head, int tail_idx)
+static inline void prep_compound_tail(struct page *tail,
+		const struct page *head, unsigned int order)
 {
-	struct page *p = head + tail_idx;
-
-	p-&gt;mapping = TAIL_MAPPING;
-	set_compound_head(p, head);
-	set_page_private(p, 0);
+	tail-&gt;mapping = TAIL_MAPPING;
+	set_compound_head(tail, head, order);
+	set_page_private(tail, 0);
 }
 
 void post_alloc_hook(struct page *page, unsigned int order, gfp_t gfp_flags);
diff --git a/mm/mm_init.c b/mm/mm_init.c
index 61d983d23f55..0a12a9be0bcc 100644
--- a/mm/mm_init.c
+++ b/mm/mm_init.c
@@ -1099,7 +1099,7 @@ static void __ref memmap_init_compound(struct page *head,
 		struct page *page = pfn_to_page(pfn);
 
 		__init_zone_device_page(page, pfn, zone_idx, nid, pgmap);
-		prep_compound_tail(head, pfn - head_pfn);
+		prep_compound_tail(page, head, order);
 		set_page_count(page, 0);
 	}
 	prep_compound_head(head, order);
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index fcc32737f451..aa657e4a99e8 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -744,7 +744,7 @@ void prep_compound_page(struct page *page, unsigned int order)
 
 	__SetPageHead(page);
 	for (i = 1; i &lt; nr_pages; i++)
-		prep_compound_tail(page, i);
+		prep_compound_tail(page + i, page, order);
 
 	prep_compound_head(page, order);
 }
-- 
2.51.2

---

From: Kiryl Shutsemau &lt;kas@kernel.org&gt;

The &#x27;compound_head&#x27; field in the &#x27;struct page&#x27; encodes whether the page
is a tail and where to locate the head page. Bit 0 is set if the page is
a tail, and the remaining bits in the field point to the head page.

As preparation for changing how the field encodes information about the
head page, rename the field to &#x27;compound_info&#x27;.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
Acked-by: David Hildenbrand (arm) &lt;david@kernel.org&gt;
Reviewed-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;
---
 .../admin-guide/kdump/vmcoreinfo.rst          |  2 +-
 Documentation/mm/vmemmap_dedup.rst            |  6 +++---
 include/linux/mm_types.h                      | 20 +++++++++----------
 include/linux/page-flags.h                    | 18 ++++++++---------
 include/linux/types.h                         |  2 +-
 kernel/vmcore_info.c                          |  2 +-
 mm/page_alloc.c                               |  2 +-
 mm/slab.h                                     |  2 +-
 mm/util.c                                     |  2 +-
 9 files changed, 28 insertions(+), 28 deletions(-)

diff --git a/Documentation/admin-guide/kdump/vmcoreinfo.rst b/Documentation/admin-guide/kdump/vmcoreinfo.rst
index 404a15f6782c..7663c610fe90 100644
--- a/Documentation/admin-guide/kdump/vmcoreinfo.rst
+++ b/Documentation/admin-guide/kdump/vmcoreinfo.rst
@@ -141,7 +141,7 @@ nodemask_t
 The size of a nodemask_t type. Used to compute the number of online
 nodes.
 
-(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_head)
+(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_info)
 ----------------------------------------------------------------------------------
 
 User-space tools compute their values based on the offset of these
diff --git a/Documentation/mm/vmemmap_dedup.rst b/Documentation/mm/vmemmap_dedup.rst
index b4a55b6569fa..1863d88d2dcb 100644
--- a/Documentation/mm/vmemmap_dedup.rst
+++ b/Documentation/mm/vmemmap_dedup.rst
@@ -24,7 +24,7 @@ For each base page, there is a corresponding ``struct page``.
 Within the HugeTLB subsystem, only the first 4 ``struct page`` are used to
 contain unique information about a HugeTLB page. ``__NR_USED_SUBPAGE`` provides
 this upper limit. The only &#x27;useful&#x27; information in the remaining ``struct page``
-is the compound_head field, and this field is the same for all tail pages.
+is the compound_info field, and this field is the same for all tail pages.
 
 By removing redundant ``struct page`` for HugeTLB pages, memory can be returned
 to the buddy allocator for other uses.
@@ -124,10 +124,10 @@ Here is how things look before optimization::
  |           |
  +-----------+
 
-The value of page-&gt;compound_head is the same for all tail pages. The first
+The value of page-&gt;compound_info is the same for all tail pages. The first
 page of ``struct page`` (page 0) associated with the HugeTLB page contains the 4
 ``struct page`` necessary to describe the HugeTLB. The only use of the remaining
-pages of ``struct page`` (page 1 to page 7) is to point to page-&gt;compound_head.
+pages of ``struct page`` (page 1 to page 7) is to point to page-&gt;compound_info.
 Therefore, we can remap pages 1 to 7 to page 0. Only 1 page of ``struct page``
 will be used for each HugeTLB page. This will allow us to free the remaining
 7 pages to the buddy allocator.
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 3cc8ae722886..7bc82a2b889f 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -126,14 +126,14 @@ struct page {
 			atomic_long_t pp_ref_count;
 		};
 		struct {	/* Tail pages of compound page */
-			unsigned long compound_head;	/* Bit zero is set */
+			unsigned long compound_info;	/* Bit zero is set */
 		};
 		struct {	/* ZONE_DEVICE pages */
 			/*
-			 * The first word is used for compound_head or folio
+			 * The first word is used for compound_info or folio
 			 * pgmap
 			 */
-			void *_unused_pgmap_compound_head;
+			void *_unused_pgmap_compound_info;
 			void *zone_device_data;
 			/*
 			 * ZONE_DEVICE private pages are counted as being
@@ -409,7 +409,7 @@ struct folio {
 	/* private: avoid cluttering the output */
 				/* For the Unevictable &quot;LRU list&quot; slot */
 				struct {
-					/* Avoid compound_head */
+					/* Avoid compound_info */
 					void *__filler;
 	/* public: */
 					unsigned int mlock_count;
@@ -510,7 +510,7 @@ struct folio {
 FOLIO_MATCH(flags, flags);
 FOLIO_MATCH(lru, lru);
 FOLIO_MATCH(mapping, mapping);
-FOLIO_MATCH(compound_head, lru);
+FOLIO_MATCH(compound_info, lru);
 FOLIO_MATCH(__folio_index, index);
 FOLIO_MATCH(private, private);
 FOLIO_MATCH(_mapcount, _mapcount);
@@ -529,7 +529,7 @@ FOLIO_MATCH(_last_cpupid, _last_cpupid);
 	static_assert(offsetof(struct folio, fl) ==			\
 			offsetof(struct page, pg) + sizeof(struct page))
 FOLIO_MATCH(flags, _flags_1);
-FOLIO_MATCH(compound_head, _head_1);
+FOLIO_MATCH(compound_info, _head_1);
 FOLIO_MATCH(_mapcount, _mapcount_1);
 FOLIO_MATCH(_refcount, _refcount_1);
 #undef FOLIO_MATCH
@@ -537,13 +537,13 @@ FOLIO_MATCH(_refcount, _refcount_1);
 	static_assert(offsetof(struct folio, fl) ==			\
 			offsetof(struct page, pg) + 2 * sizeof(struct page))
 FOLIO_MATCH(flags, _flags_2);
-FOLIO_MATCH(compound_head, _head_2);
+FOLIO_MATCH(compound_info, _head_2);
 #undef FOLIO_MATCH
 #define FOLIO_MATCH(pg, fl)						\
 	static_assert(offsetof(struct folio, fl) ==			\
 			offsetof(struct page, pg) + 3 * sizeof(struct page))
 FOLIO_MATCH(flags, _flags_3);
-FOLIO_MATCH(compound_head, _head_3);
+FOLIO_MATCH(compound_info, _head_3);
 #undef FOLIO_MATCH
 
 /**
@@ -609,8 +609,8 @@ struct ptdesc {
 #define TABLE_MATCH(pg, pt)						\
 	static_assert(offsetof(struct page, pg) == offsetof(struct ptdesc, pt))
 TABLE_MATCH(flags, pt_flags);
-TABLE_MATCH(compound_head, pt_list);
-TABLE_MATCH(compound_head, _pt_pad_1);
+TABLE_MATCH(compound_info, pt_list);
+TABLE_MATCH(compound_info, _pt_pad_1);
 TABLE_MATCH(mapping, __page_mapping);
 TABLE_MATCH(__folio_index, pt_index);
 TABLE_MATCH(rcu_head, pt_rcu_head);
diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
index 5e7687ccccf8..70c4e43f2d9a 100644
--- a/include/linux/page-flags.h
+++ b/include/linux/page-flags.h
@@ -213,7 +213,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page
 	/*
 	 * Only addresses aligned with PAGE_SIZE of struct page may be fake head
 	 * struct page. The alignment check aims to avoid access the fields (
-	 * e.g. compound_head) of the @page[1]. It can avoid touch a (possibly)
+	 * e.g. compound_info) of the @page[1]. It can avoid touch a (possibly)
 	 * cold cacheline in some cases.
 	 */
 	if (IS_ALIGNED((unsigned long)page, PAGE_SIZE) &amp;&amp;
@@ -223,7 +223,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page
 		 * because the @page is a compound page composed with at least
 		 * two contiguous pages.
 		 */
-		unsigned long head = READ_ONCE(page[1].compound_head);
+		unsigned long head = READ_ONCE(page[1].compound_info);
 
 		if (likely(head &amp; 1))
 			return (const struct page *)(head - 1);
@@ -281,7 +281,7 @@ static __always_inline int page_is_fake_head(const struct page *page)
 
 static __always_inline unsigned long _compound_head(const struct page *page)
 {
-	unsigned long head = READ_ONCE(page-&gt;compound_head);
+	unsigned long head = READ_ONCE(page-&gt;compound_info);
 
 	if (unlikely(head &amp; 1))
 		return head - 1;
@@ -320,13 +320,13 @@ static __always_inline unsigned long _compound_head(const struct page *page)
 
 static __always_inline int PageTail(const struct page *page)
 {
-	return READ_ONCE(page-&gt;compound_head) &amp; 1 || page_is_fake_head(page);
+	return READ_ONCE(page-&gt;compound_info) &amp; 1 || page_is_fake_head(page);
 }
 
 static __always_inline int PageCompound(const struct page *page)
 {
 	return test_bit(PG_head, &amp;page-&gt;flags.f) ||
-	       READ_ONCE(page-&gt;compound_head) &amp; 1;
+	       READ_ONCE(page-&gt;compound_info) &amp; 1;
 }
 
 #define	PAGE_POISON_PATTERN	-1l
@@ -348,7 +348,7 @@ static const unsigned long *const_folio_flags(const struct folio *folio,
 {
 	const struct page *page = &amp;folio-&gt;page;
 
-	VM_BUG_ON_PGFLAGS(page-&gt;compound_head &amp; 1, page);
+	VM_BUG_ON_PGFLAGS(page-&gt;compound_info &amp; 1, page);
 	VM_BUG_ON_PGFLAGS(n &gt; 0 &amp;&amp; !test_bit(PG_head, &amp;page-&gt;flags.f), page);
 	return &amp;page[n].flags.f;
 }
@@ -357,7 +357,7 @@ static unsigned long *folio_flags(struct folio *folio, unsigned n)
 {
 	struct page *page = &amp;folio-&gt;page;
 
-	VM_BUG_ON_PGFLAGS(page-&gt;compound_head &amp; 1, page);
+	VM_BUG_ON_PGFLAGS(page-&gt;compound_info &amp; 1, page);
 	VM_BUG_ON_PGFLAGS(n &gt; 0 &amp;&amp; !test_bit(PG_head, &amp;page-&gt;flags.f), page);
 	return &amp;page[n].flags.f;
 }
@@ -868,12 +868,12 @@ static inline bool folio_test_large(const struct folio *folio)
 static __always_inline void set_compound_head(struct page *tail,
 		const struct page *head, unsigned int order)
 {
-	WRITE_ONCE(tail-&gt;compound_head, (unsigned long)head + 1);
+	WRITE_ONCE(tail-&gt;compound_info, (unsigned long)head + 1);
 }
 
 static __always_inline void clear_compound_head(struct page *page)
 {
-	WRITE_ONCE(page-&gt;compound_head, 0);
+	WRITE_ONCE(page-&gt;compound_info, 0);
 }
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
diff --git a/include/linux/types.h b/include/linux/types.h
index 7e71d260763c..608050dbca6a 100644
--- a/include/linux/types.h
+++ b/include/linux/types.h
@@ -239,7 +239,7 @@ struct ustat {
  *
  * This guarantee is important for few reasons:
  *  - future call_rcu_lazy() will make use of lower bits in the pointer;
- *  - the structure shares storage space in struct page with @compound_head,
+ *  - the structure shares storage space in struct page with @compound_info,
  *    which encode PageTail() in bit 0. The guarantee is needed to avoid
  *    false-positive PageTail().
  */
diff --git a/kernel/vmcore_info.c b/kernel/vmcore_info.c
index 8d82913223a1..94e4ef75b1b2 100644
--- a/kernel/vmcore_info.c
+++ b/kernel/vmcore_info.c
@@ -198,7 +198,7 @@ static int __init crash_save_vmcoreinfo_init(void)
 	VMCOREINFO_OFFSET(page, lru);
 	VMCOREINFO_OFFSET(page, _mapcount);
 	VMCOREINFO_OFFSET(page, private);
-	VMCOREINFO_OFFSET(page, compound_head);
+	VMCOREINFO_OFFSET(page, compound_info);
 	VMCOREINFO_OFFSET(pglist_data, node_zones);
 	VMCOREINFO_OFFSET(pglist_data, nr_zones);
 #ifdef CONFIG_FLATMEM
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index aa657e4a99e8..e83f67fbbf07 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -731,7 +731,7 @@ static inline bool pcp_allowed_order(unsigned int order)
  * The first PAGE_SIZE page is called the &quot;head page&quot; and have PG_head set.
  *
  * The remaining PAGE_SIZE pages are called &quot;tail pages&quot;. PageTail() is encoded
- * in bit 0 of page-&gt;compound_head. The rest of bits is pointer to head page.
+ * in bit 0 of page-&gt;compound_info. The rest of bits is pointer to head page.
  *
  * The first tail page&#x27;s -&gt;compound_order holds the order of allocation.
  * This usage means that zero-order pages may not be compound.
diff --git a/mm/slab.h b/mm/slab.h
index 71c7261bf822..62dfa50c1f01 100644
--- a/mm/slab.h
+++ b/mm/slab.h
@@ -94,7 +94,7 @@ struct slab {
 #define SLAB_MATCH(pg, sl)						\
 	static_assert(offsetof(struct page, pg) == offsetof(struct slab, sl))
 SLAB_MATCH(flags, flags);
-SLAB_MATCH(compound_head, slab_cache);	/* Ensure bit 0 is clear */
+SLAB_MATCH(compound_info, slab_cache);	/* Ensure bit 0 is clear */
 SLAB_MATCH(_refcount, __page_refcount);
 #ifdef CONFIG_MEMCG
 SLAB_MATCH(memcg_data, obj_exts);
diff --git a/mm/util.c b/mm/util.c
index b05ab6f97e11..3ebcb9e6035c 100644
--- a/mm/util.c
+++ b/mm/util.c
@@ -1247,7 +1247,7 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)
 again:
 	memset(&amp;ps-&gt;folio_snapshot, 0, sizeof(struct folio));
 	memcpy(&amp;ps-&gt;page_snapshot, page, sizeof(*page));
-	head = ps-&gt;page_snapshot.compound_head;
+	head = ps-&gt;page_snapshot.compound_info;
 	if ((head &amp; 1) == 0) {
 		ps-&gt;idx = 0;
 		foliop = (struct folio *)&amp;ps-&gt;page_snapshot;
-- 
2.51.2

---

From: Kiryl Shutsemau &lt;kas@kernel.org&gt;

Move set_compound_head() and clear_compound_head() to be adjacent to the
compound_head() function in page-flags.h.

These functions encode and decode the same compound_info field, so
keeping them together makes it easier to verify their logic is
consistent, especially when the encoding changes.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
Acked-by: David Hildenbrand (arm) &lt;david@kernel.org&gt;
Reviewed-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;
---
 include/linux/page-flags.h | 22 +++++++++++-----------
 1 file changed, 11 insertions(+), 11 deletions(-)

diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
index 70c4e43f2d9a..42bf8ed02a29 100644
--- a/include/linux/page-flags.h
+++ b/include/linux/page-flags.h
@@ -290,6 +290,17 @@ static __always_inline unsigned long _compound_head(const struct page *page)
 
 #define compound_head(page)	((typeof(page))_compound_head(page))
 
+static __always_inline void set_compound_head(struct page *tail,
+		const struct page *head, unsigned int order)
+{
+	WRITE_ONCE(tail-&gt;compound_info, (unsigned long)head + 1);
+}
+
+static __always_inline void clear_compound_head(struct page *page)
+{
+	WRITE_ONCE(page-&gt;compound_info, 0);
+}
+
 /**
  * page_folio - Converts from page to folio.
  * @p: The page.
@@ -865,17 +876,6 @@ static inline bool folio_test_large(const struct folio *folio)
 	return folio_test_head(folio);
 }
 
-static __always_inline void set_compound_head(struct page *tail,
-		const struct page *head, unsigned int order)
-{
-	WRITE_ONCE(tail-&gt;compound_info, (unsigned long)head + 1);
-}
-
-static __always_inline void clear_compound_head(struct page *page)
-{
-	WRITE_ONCE(page-&gt;compound_info, 0);
-}
-
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 static inline void ClearPageCompound(struct page *page)
 {
-- 
2.51.2</pre>
</details>
<a href="https://lore.kernel.org/r/20260227193030.272078-1-kas@kernel.org" target="_blank" rel="noopener" class="lore-link">View on lore &#8599;</a>
<div class="review-comment-signals">Signals: clarification, preparation</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Kiryl Shutsemau (author)</span>
<a class="date-chip" href="../2026-02-27_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-27">2026-02-27</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed a concern about aligning vmemmap to the newly introduced MAX_FOLIO_VMEMMAP_ALIGN, explaining that it&#x27;s required for HugeTLB Vmemmap Optimization (HVO) and providing patch updates for riscv and loongarch architectures. The author also described how changing the encoding of compound_info from a pointer to a mask will allow identical tail pages to be mapped into the vmemmap of all pages, eliminating fake heads.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">From: Kiryl Shutsemau &lt;kas@kernel.org&gt;

The upcoming change to the HugeTLB vmemmap optimization (HVO) requires
struct pages of the head page to be naturally aligned with regard to the
folio size.

Align vmemmap to the newly introduced MAX_FOLIO_VMEMMAP_ALIGN.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
---
 arch/riscv/mm/init.c   |  3 ++-
 include/linux/mmzone.h | 11 +++++++++++
 2 files changed, 13 insertions(+), 1 deletion(-)

diff --git a/arch/riscv/mm/init.c b/arch/riscv/mm/init.c
index 811e03786c56..e8fb2239a0b5 100644
--- a/arch/riscv/mm/init.c
+++ b/arch/riscv/mm/init.c
@@ -63,7 +63,8 @@ phys_addr_t phys_ram_base __ro_after_init;
 EXPORT_SYMBOL(phys_ram_base);
 
 #ifdef CONFIG_SPARSEMEM_VMEMMAP
-#define VMEMMAP_ADDR_ALIGN	(1ULL &lt;&lt; SECTION_SIZE_BITS)
+#define VMEMMAP_ADDR_ALIGN	max(1ULL &lt;&lt; SECTION_SIZE_BITS, \
+				    MAX_FOLIO_VMEMMAP_ALIGN)
 
 unsigned long vmemmap_start_pfn __ro_after_init;
 EXPORT_SYMBOL(vmemmap_start_pfn);
diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index be8ce40b5638..492a5be1090f 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -92,6 +92,17 @@
 
 #define MAX_FOLIO_NR_PAGES	(1UL &lt;&lt; MAX_FOLIO_ORDER)
 
+/*
+ * HugeTLB Vmemmap Optimization (HVO) requires struct pages of the head page to
+ * be naturally aligned with regard to the folio size.
+ *
+ * HVO which is only active if the size of struct page is a power of 2.
+ */
+#define MAX_FOLIO_VMEMMAP_ALIGN \
+	(IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP) &amp;&amp; \
+	 is_power_of_2(sizeof(struct page)) ? \
+	 MAX_FOLIO_NR_PAGES * sizeof(struct page) : 0)
+
 enum migratetype {
 	MIGRATE_UNMOVABLE,
 	MIGRATE_MOVABLE,
-- 
2.51.2

---

From: Kiryl Shutsemau &lt;kas@kernel.org&gt;

The upcoming change to the HugeTLB vmemmap optimization (HVO) requires
struct pages of the head page to be naturally aligned with regard to the
folio size.

Align vmemmap to MAX_FOLIO_VMEMMAP_ALIGN.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
---
 arch/loongarch/include/asm/pgtable.h | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h
index c33b3bcb733e..ea6c09eed2e7 100644
--- a/arch/loongarch/include/asm/pgtable.h
+++ b/arch/loongarch/include/asm/pgtable.h
@@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
 	 min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL &lt;&lt; cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)
 #endif
 
-#define vmemmap		((struct page *)((VMALLOC_END + PMD_SIZE) &amp; PMD_MASK))
+#define VMEMMAP_ALIGN	max(PMD_SIZE, MAX_FOLIO_VMEMMAP_ALIGN)
+#define vmemmap		((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))
 #define VMEMMAP_END	((unsigned long)vmemmap + VMEMMAP_SIZE - 1)
 
 #define KFENCE_AREA_START	(VMEMMAP_END + 1)
-- 
2.51.2

---

From: Kiryl Shutsemau &lt;kas@kernel.org&gt;

For tail pages, the kernel uses the &#x27;compound_info&#x27; field to get to the
head page. The bit 0 of the field indicates whether the page is a
tail page, and if set, the remaining bits represent a pointer to the
head page.

For cases when size of struct page is power-of-2, change the encoding of
compound_info to store a mask that can be applied to the virtual address
of the tail page in order to access the head page. It is possible
because struct page of the head page is naturally aligned with regards
to order of the page.

The significant impact of this modification is that all tail pages of
the same order will now have identical &#x27;compound_info&#x27;, regardless of
the compound page they are associated with. This paves the way for
eliminating fake heads.

The HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only
applied when the sizeof(struct page) is power-of-2. Having identical
tail pages allows the same page to be mapped into the vmemmap of all
pages, maintaining memory savings without fake heads.

If sizeof(struct page) is not power-of-2, there is no functional
changes.

Limit mask usage to HugeTLB vmemmap optimization (HVO) where it makes
a difference. The approach with mask would work in the wider set of
conditions, but it requires validating that struct pages are naturally
aligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
Acked-by: David Hildenbrand (Arm) &lt;david@kernel.org&gt;
Acked-by: Usama Arif &lt;usamaarif642@gmail.com&gt;
Reviewed-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;
---
 include/linux/page-flags.h | 81 ++++++++++++++++++++++++++++++++++----
 mm/slab.h                  | 16 ++++++--
 mm/util.c                  | 16 ++++++--
 3 files changed, 97 insertions(+), 16 deletions(-)

diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
index 42bf8ed02a29..01970bd38bff 100644
--- a/include/linux/page-flags.h
+++ b/include/linux/page-flags.h
@@ -198,6 +198,29 @@ enum pageflags {
 
 #ifndef __GENERATING_BOUNDS_H
 
+/*
+ * For tail pages, if the size of struct page is power-of-2 -&gt;compound_info
+ * encodes the mask that converts the address of the tail page address to
+ * the head page address.
+ *
+ * Otherwise, -&gt;compound_info has direct pointer to head pages.
+ */
+static __always_inline bool compound_info_has_mask(void)
+{
+	/*
+	 * Limit mask usage to HugeTLB vmemmap optimization (HVO) where it
+	 * makes a difference.
+	 *
+	 * The approach with mask would work in the wider set of conditions,
+	 * but it requires validating that struct pages are naturally aligned
+	 * for all orders up to the MAX_FOLIO_ORDER, which can be tricky.
+	 */
+	if (!IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP))
+		return false;
+
+	return is_power_of_2(sizeof(struct page));
+}
+
 #ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP
 DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);
 
@@ -207,6 +230,10 @@ DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);
  */
 static __always_inline const struct page *page_fixed_fake_head(const struct page *page)
 {
+	/* Fake heads only exists if compound_info_has_mask() is true */
+	if (!compound_info_has_mask())
+		return page;
+
 	if (!static_branch_unlikely(&amp;hugetlb_optimize_vmemmap_key))
 		return page;
 
@@ -223,10 +250,14 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page
 		 * because the @page is a compound page composed with at least
 		 * two contiguous pages.
 		 */
-		unsigned long head = READ_ONCE(page[1].compound_info);
+		unsigned long info = READ_ONCE(page[1].compound_info);
 
-		if (likely(head &amp; 1))
-			return (const struct page *)(head - 1);
+		/* See set_compound_head() */
+		if (likely(info &amp; 1)) {
+			unsigned long p = (unsigned long)page;
+
+			return (const struct page *)(p &amp; info);
+		}
 	}
 	return page;
 }
@@ -281,11 +312,26 @@ static __always_inline int page_is_fake_head(const struct page *page)
 
 static __always_inline unsigned long _compound_head(const struct page *page)
 {
-	unsigned long head = READ_ONCE(page-&gt;compound_info);
+	unsigned long info = READ_ONCE(page-&gt;compound_info);
 
-	if (unlikely(head &amp; 1))
-		return head - 1;
-	return (unsigned long)page_fixed_fake_head(page);
+	/* Bit 0 encodes PageTail() */
+	if (!(info &amp; 1))
+		return (unsigned long)page_fixed_fake_head(page);
+
+	/*
+	 * If compound_info_has_mask() is false, the rest of compound_info is
+	 * the pointer to the head page.
+	 */
+	if (!compound_info_has_mask())
+		return info - 1;
+
+	/*
+	 * If compound_info_has_mask() is true the rest of the info encodes
+	 * the mask that converts the address of the tail page to the head page.
+	 *
+	 * No need to clear bit 0 in the mask as &#x27;page&#x27; always has it clear.
+	 */
+	return (unsigned long)page &amp; info;
 }
 
 #define compound_head(page)	((typeof(page))_compound_head(page))
@@ -293,7 +339,26 @@ static __always_inline unsigned long _compound_head(const struct page *page)
 static __always_inline void set_compound_head(struct page *tail,
 		const struct page *head, unsigned int order)
 {
-	WRITE_ONCE(tail-&gt;compound_info, (unsigned long)head + 1);
+	unsigned int shift;
+	unsigned long mask;
+
+	if (!compound_info_has_mask()) {
+		WRITE_ONCE(tail-&gt;compound_info, (unsigned long)head | 1);
+		return;
+	}
+
+	/*
+	 * If the size of struct page is power-of-2, bits [shift:0] of the
+	 * virtual address of compound head are zero.
+	 *
+	 * Calculate mask that can be applied to the virtual address of
+	 * the tail page to get address of the head page.
+	 */
+	shift = order + order_base_2(sizeof(struct page));
+	mask = GENMASK(BITS_PER_LONG - 1, shift);
+
+	/* Bit 0 encodes PageTail() */
+	WRITE_ONCE(tail-&gt;compound_info, mask | 1);
 }
 
 static __always_inline void clear_compound_head(struct page *page)
diff --git a/mm/slab.h b/mm/slab.h
index 62dfa50c1f01..1a1b3758df05 100644
--- a/mm/slab.h
+++ b/mm/slab.h
@@ -131,11 +131,19 @@ static_assert(IS_ALIGNED(offsetof(struct slab, freelist), sizeof(struct freelist
  */
 static inline struct slab *page_slab(const struct page *page)
 {
-	unsigned long head;
+	unsigned long info;
+
+	info = READ_ONCE(page-&gt;compound_info);
+	if (info &amp; 1) {
+		/* See compound_head() */
+		if (compound_info_has_mask()) {
+			unsigned long p = (unsigned long)page;
+			page = (struct page *)(p &amp; info);
+		} else {
+			page = (struct page *)(info - 1);
+		}
+	}
 
-	head = READ_ONCE(page-&gt;compound_head);
-	if (head &amp; 1)
-		page = (struct page *)(head - 1);
 	if (data_race(page-&gt;page_type &gt;&gt; 24) != PGTY_slab)
 		page = NULL;
 
diff --git a/mm/util.c b/mm/util.c
index 3ebcb9e6035c..20dccf2881d7 100644
--- a/mm/util.c
+++ b/mm/util.c
@@ -1237,7 +1237,7 @@ static void set_ps_flags(struct page_snapshot *ps, const struct folio *folio,
  */
 void snapshot_page(struct page_snapshot *ps, const struct page *page)
 {
-	unsigned long head, nr_pages = 1;
+	unsigned long info, nr_pages = 1;
 	struct folio *foliop;
 	int loops = 5;
 
@@ -1247,8 +1247,8 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)
 again:
 	memset(&amp;ps-&gt;folio_snapshot, 0, sizeof(struct folio));
 	memcpy(&amp;ps-&gt;page_snapshot, page, sizeof(*page));
-	head = ps-&gt;page_snapshot.compound_info;
-	if ((head &amp; 1) == 0) {
+	info = ps-&gt;page_snapshot.compound_info;
+	if (!(info &amp; 1)) {
 		ps-&gt;idx = 0;
 		foliop = (struct folio *)&amp;ps-&gt;page_snapshot;
 		if (!folio_test_large(foliop)) {
@@ -1259,7 +1259,15 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)
 		}
 		foliop = (struct folio *)page;
 	} else {
-		foliop = (struct folio *)(head - 1);
+		/* See compound_head() */
+		if (compound_info_has_mask()) {
+			unsigned long p = (unsigned long)page;
+
+			foliop = (struct folio *)(p &amp; info);
+		} else {
+			foliop = (struct folio *)(info - 1);
+		}
+
 		ps-&gt;idx = folio_page_idx(foliop, page);
 	}
 
-- 
2.51.2

---

From: Kiryl Shutsemau &lt;kas@kernel.org&gt;

If page-&gt;compound_info encodes a mask, it is expected that vmemmap to be
naturally aligned to the maximum folio size.

Add a VM_WARN_ON_ONCE() to check the alignment.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
---
 mm/sparse.c | 5 +++++
 1 file changed, 5 insertions(+)

diff --git a/mm/sparse.c b/mm/sparse.c
index b5b2b6f7041b..dfabe554adf8 100644
--- a/mm/sparse.c
+++ b/mm/sparse.c
@@ -600,6 +600,11 @@ void __init sparse_init(void)
 	BUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));
 	memblocks_present();
 
+	if (compound_info_has_mask()) {
+		VM_WARN_ON_ONCE(!IS_ALIGNED((unsigned long) pfn_to_page(0),
+				    MAX_FOLIO_VMEMMAP_ALIGN));
+	}
+
 	pnum_begin = first_present_section_nr();
 	nid_begin = sparse_early_nid(__nr_to_section(pnum_begin));
 
-- 
2.51.2

---

Currently, the vmemmap for bootmem-allocated gigantic pages is populated
early in hugetlb_vmemmap_init_early(). However, the zone information is
only available after zones are initialized. If it is later discovered
that a page spans multiple zones, the HVO mapping must be undone and
replaced with a normal mapping using vmemmap_undo_hvo().

Defer the actual vmemmap population to hugetlb_vmemmap_init_late(). At
this stage, zones are already initialized, so it can be checked if the
page is valid for HVO before deciding how to populate the vmemmap.

This allows us to remove vmemmap_undo_hvo() and the complex logic
required to rollback HVO mappings.

In hugetlb_vmemmap_init_late(), if HVO population fails or if the zones
are invalid, fall back to a normal vmemmap population.

Postponing population until hugetlb_vmemmap_init_late() also makes zone
information available from within vmemmap_populate_hvo().

Signed-off-by: Kiryl Shutsemau (Meta) &lt;kas@kernel.org&gt;
---
 include/linux/mm.h   |  2 --
 mm/hugetlb_vmemmap.c | 37 +++++++++++++++----------------
 mm/sparse-vmemmap.c  | 53 --------------------------------------------
 3 files changed, 18 insertions(+), 74 deletions(-)

diff --git a/include/linux/mm.h b/include/linux/mm.h
index 7f4dbbb9d783..0e2d45008ff4 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -4484,8 +4484,6 @@ int vmemmap_populate(unsigned long start, unsigned long end, int node,
 		struct vmem_altmap *altmap);
 int vmemmap_populate_hvo(unsigned long start, unsigned long end, int node,
 			 unsigned long headsize);
-int vmemmap_undo_hvo(unsigned long start, unsigned long end, int node,
-		     unsigned long headsize);
 void vmemmap_wrprotect_hvo(unsigned long start, unsigned long end, int node,
 			  unsigned long headsize);
 void vmemmap_populate_print_last(void);
diff --git a/mm/hugetlb_vmemmap.c b/mm/hugetlb_vmemmap.c
index a9280259e12a..935ec5829be9 100644
--- a/mm/hugetlb_vmemmap.c
+++ b/mm/hugetlb_vmemmap.c
@@ -790,7 +790,6 @@ void __init hugetlb_vmemmap_init_early(int nid)
 {
 	unsigned long psize, paddr, section_size;
 	unsigned long ns, i, pnum, pfn, nr_pages;
-	unsigned long start, end;
 	struct huge_bootmem_page *m = NULL;
 	void *map;
 
@@ -808,14 +807,6 @@ void __init hugetlb_vmemmap_init_early(int nid)
 		paddr = virt_to_phys(m);
 		pfn = PHYS_PFN(paddr);
 		map = pfn_to_page(pfn);
-		start = (unsigned long)map;
-		end = start + nr_pages * sizeof(struct page);
-
-		if (vmemmap_populate_hvo(start, end, nid,
-					HUGETLB_VMEMMAP_RESERVE_SIZE) &lt; 0)
-			continue;
-
-		memmap_boot_pages_add(HUGETLB_VMEMMAP_RESERVE_SIZE / PAGE_SIZE);
 
 		pnum = pfn_to_section_nr(pfn);
 		ns = psize / section_size;
@@ -850,28 +841,36 @@ void __init hugetlb_vmemmap_init_late(int nid)
 		h = m-&gt;hstate;
 		pfn = PHYS_PFN(phys);
 		nr_pages = pages_per_huge_page(h);
+		map = pfn_to_page(pfn);
+		start = (unsigned long)map;
+		end = start + nr_pages * sizeof(struct page);
 
 		if (!hugetlb_bootmem_page_zones_valid(nid, m)) {
 			/*
 			 * Oops, the hugetlb page spans multiple zones.
-			 * Remove it from the list, and undo HVO.
+			 * Remove it from the list, and populate it normally.
 			 */
 			list_del(&amp;m-&gt;list);
 
-			map = pfn_to_page(pfn);
-
-			start = (unsigned long)map;
-			end = start + nr_pages * sizeof(struct page);
-
-			vmemmap_undo_hvo(start, end, nid,
-					 HUGETLB_VMEMMAP_RESERVE_SIZE);
-			nr_mmap = end - start - HUGETLB_VMEMMAP_RESERVE_SIZE;
+			vmemmap_populate(start, end, nid, NULL);
+			nr_mmap = end - start;
 			memmap_boot_pages_add(DIV_ROUND_UP(nr_mmap, PAGE_SIZE));
 
 			memblock_phys_free(phys, huge_page_size(h));
 			continue;
-		} else
+		}
+
+		if (vmemmap_populate_hvo(start, end, nid,
+					 HUGETLB_VMEMMAP_RESERVE_SIZE) &lt; 0) {
+			/* Fallback if HVO population fails */
+			vmemmap_populate(start, end, nid, NULL);
+			nr_mmap = end - start;
+		} else {
 			m-&gt;flags |= HUGE_BOOTMEM_ZONES_VALID;
+			nr_mmap = HUGETLB_VMEMMAP_RESERVE_SIZE;
+		}
+
+		memmap_boot_pages_add(DIV_ROUND_UP(nr_mmap, PAGE_SIZE));
 	}
 }
 #endif
diff --git a/mm/sparse-vmemmap.c b/mm/sparse-vmemmap.c
index 37522d6cb398..032a81450838 100644
--- a/mm/sparse-vmemmap.c
+++ b/mm/sparse-vmemmap.c
@@ -302,59 +302,6 @@ int __meminit vmemmap_populate_basepages(unsigned long start, unsigned long end,
 	return vmemmap_populate_range(start, end, node, altmap, -1, 0);
 }
 
-/*
- * Undo populate_hvo, and replace it with a normal base page mapping.
- * Used in memory init in case a HVO mapping needs to be undone.
- *
- * This can happen when it is discovered that a memblock allocated
- * hugetlb page spans multiple zones, which can only be verified
- * after zones have been initialized.
- *
- * We know that:
- * 1) The first @headsize / PAGE_SIZE vmemmap pages were individually
- *    allocated through memblock, and mapped.
- *
- * 2) The rest of the vmemmap pages are mirrors of the last head page.
- */
-int __meminit vmemmap_undo_hvo(unsigned long addr, unsigned long end,
-				      int node, unsigned long headsize)
-{
-	unsigned long maddr, pfn;
-	pte_t *pte;
-	int headpages;
-
-	/*
-	 * Should only be called early in boot, so nothing will
-	 * be accessing these page structures.
-	 */
-	WARN_ON(!early_boot_irqs_disabled);
-
-	headpages = headsize &gt;&gt; PAGE_SHIFT;
-
-	/*
-	 * Clear mirrored mappings for tail page structs.
-	 */
-	for (maddr = addr + headsize; maddr &lt; end; maddr += PAGE_SIZE) {
-		pte = virt_to_kpte(maddr);
-		pte_clear(&amp;init_mm, maddr, pte);
-	}
-
-	/*
-	 * Clear and free mappings for head page and first tail page
-	 * structs.
-	 */
-	for (maddr = addr; headpages-- &gt; 0; maddr += PAGE_SIZE) {
-		pte = virt_to_kpte(maddr);
-		pfn = pte_pfn(ptep_get(pte));
-		pte_clear(&amp;init_mm, maddr, pte);
-		memblock_phys_free(PFN_PHYS(pfn), PAGE_SIZE);
-	}
-
-	flush_tlb_kernel_range(addr, end);
-
-	return vmemmap_populate(addr, end, node, NULL);
-}
-
 /*
  * Write protect the mirrored tail page structs for HVO. This will be
  * called from the hugetlb code when gathering and initializing the
-- 
2.51.2</pre>
</details>
<a href="https://lore.kernel.org/r/20260227193030.272078-5-kas@kernel.org" target="_blank" rel="noopener" class="lore-link">View on lore &#8599;</a>
<div class="review-comment-signals">Signals: addressed_concern, provided_explanation</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Kiryl Shutsemau (author)</span>
<a class="date-chip" href="../2026-02-27_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-27">2026-02-27</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed a concern about zone information being correct for shared tail pages, explaining that in v7, these pages are allocated per-zone and the vmemmap population is deferred until zones are initialized.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">This series removes &quot;fake head pages&quot; from the HugeTLB vmemmap
optimization (HVO) by changing how tail pages encode their relationship
to the head page.

It simplifies compound_head() and page_ref_add_unless(). Both are in the
hot path.

Background
==========

HVO reduces memory overhead by freeing vmemmap pages for HugeTLB pages
and remapping the freed virtual addresses to a single physical page.
Previously, all tail page vmemmap entries were remapped to the first
vmemmap page (containing the head struct page), creating &quot;fake heads&quot; -
tail pages that appear to have PG_head set when accessed through the
deduplicated vmemmap.

This required special handling in compound_head() to detect and work
around fake heads, adding complexity and overhead to a very hot path.

New Approach
============

For architectures/configs where sizeof(struct page) is a power of 2 (the
common case), this series changes how position of the head page is encoded
in the tail pages.

Instead of storing a pointer to the head page, the -&gt;compound_info
(renamed from -&gt;compound_head) now stores a mask.

The mask can be applied to any tail page&#x27;s virtual address to compute
the head page address. Critically, all tail pages of the same order now
have identical compound_info values, regardless of which compound page
they belong to.

The key insight is that all tail pages of the same order now have
identical compound_info values, regardless of which compound page they
belong to.

In v7, these shared tail pages are allocated per-zone. This ensures 
that zone information (stored in page-&gt;flags) is correct even for 
shared tail pages, removing the need for the special-casing in 
page_zonenum() proposed in earlier versions.

To support per-zone shared pages for boot-allocated gigantic pages, 
the vmemmap population is deferred until zones are initialized. This 
simplifies the logic significantly and allows the removal of 
vmemmap_undo_hvo().

Benefits
========

1. Simplified compound_head(): No fake head detection needed, can be
   implemented in a branchless manner.

2. Simplified page_ref_add_unless(): RCU protection removed since there&#x27;s
   no race with fake head remapping.

3. Cleaner architecture: The shared tail pages are truly read-only and
   contain valid tail page metadata.

If sizeof(struct page) is not power-of-2, there are no functional changes.
HVO is not supported in this configuration.

I had hoped to see performance improvement, but my testing thus far has
shown either no change or only a slight improvement within the noise.

Series Organization
===================

Patch 1: Move MAX_FOLIO_ORDER definition to mmzone.h.
Patches 2-4: Refactoring of field names and interfaces.
Patches 5-6: Architecture alignment for LoongArch and RISC-V.
Patch 7: Mask-based compound_head() implementation.
Patch 8: Add memmap alignment checks.
Patch 9: Branchless compound_head() optimization.
Patch 10: Defer vmemmap population for bootmem hugepages.
Patch 11: Refactor vmemmap_walk.
Patch 12: x86 vDSO build fix.
Patch 13: Eliminate fake heads with per-zone shared tail pages.
Patches 14-16: Cleanup of fake head infrastructure.
Patch 17: Documentation update.
Patch 18: Use compound_head() in page_slab().

Changes in v7:
==============

  - Move vmemmap_tails from per-node to per-zone. This ensures tail
    pages have correct zone information.

  - Defer vmemmap population for boot-allocated huge pages to 
    hugetlb_vmemmap_init_late(). This makes zone information available 
    during population and allows removing vmemmap_undo_hvo().

  - Undefine CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP for x86 vdso32 to 
    fix build issues.

  - Remove the patch that modified page_zonenum(), as per-zone 
    shared pages make it unnecessary.

Changes in v6:
==============
  - Simplify memmap alignment check in mm/sparse.c: use VM_BUG_ON()
    (Muchun)

  - Store struct page pointers in vmemmap_tails[] instead of PFNs.
    (Muchun)

  - Fix build error on powerpc due to negative NR_VMEMMAP_TAILS.

Changes in v5:
==============
  - Rebased to mm-everything-2026-01-27-04-35

  - Add arch-specific patches to align vmemmap to maximal folio size
    for riscv and LoongArch architectures.

  - Strengthen the memmap alignment check in mm/sparse.c: use BUG()
    for CONFIG_DEBUG_VM, WARN() otherwise. (Muchun)

  - Use cmpxchg() instead of hugetlb_lock to update vmemmap_tails
    array. (Muchun)

  - Update page_slab().

Changes in v4:
==============
  - Fix build issues due to linux/mmzone.h &lt;-&gt; linux/pgtable.h
    dependency loop by avoiding including linux/pgtable.h into
    linux/mmzone.h

  - Rework vmemmap_remap_alloc() interface. (Muchun)

  - Use &amp;folio-&gt;page instead of folio address for optimization
    target. (Muchun)

Changes in v3:
==============
  - Fixed error recovery path in vmemmap_remap_free() to pass correct start
    address for TLB flush. (Muchun)

  - Wrapped the mask-based compound_info encoding within CONFIG_SPARSEMEM_VMEMMAP
    check via compound_info_has_mask(). For other memory models, alignment
    guarantees are harder to verify. (Muchun)

  - Updated vmemmap_dedup.rst documentation wording: changed &quot;vmemmap_tail
    shared for the struct hstate&quot; to &quot;A single, per-node page frame shared
    among all hugepages of the same size&quot;. (Muchun)

  - Fixed build error with MAX_FOLIO_ORDER expanding to undefined PUD_ORDER
    in certain configurations. (kernel test robot)

Changes in v2:
==============

- Handle boot-allocated huge pages correctly. (Frank)

- Changed from per-hstate vmemmap_tail to per-node vmemmap_tails[] array
  in pglist_data. (Muchun)

- Added spin_lock(&amp;hugetlb_lock) protection in vmemmap_get_tail() to fix
  a race condition where two threads could both allocate tail pages.
  The losing thread now properly frees its allocated page. (Usama)

- Add warning if memmap is not aligned to MAX_FOLIO_SIZE, which is
  required for the mask approach. (Muchun)

- Make page_zonenum() use head page - correctness fix since shared
  tail pages cannot have valid zone information. (Muchun)

- Added &#x27;const&#x27; qualifier to head parameter in set_compound_head() and
  prep_compound_tail(). (Usama)

- Updated commit messages.

Kiryl Shutsemau (16):
  mm: Move MAX_FOLIO_ORDER definition to mmzone.h
  mm: Change the interface of prep_compound_tail()
  mm: Rename the &#x27;compound_head&#x27; field in the &#x27;struct page&#x27; to
    &#x27;compound_info&#x27;
  mm: Move set/clear_compound_head() next to compound_head()
  riscv/mm: Align vmemmap to maximal folio size
  LoongArch/mm: Align vmemmap to maximal folio size
  mm: Rework compound_head() for power-of-2 sizeof(struct page)
  mm/sparse: Check memmap alignment for compound_info_has_mask()
  mm/hugetlb: Refactor code around vmemmap_walk
  mm/hugetlb: Remove fake head pages
  mm: Drop fake head checks
  hugetlb: Remove VMEMMAP_SYNCHRONIZE_RCU
  mm/hugetlb: Remove hugetlb_optimize_vmemmap_key static key
  mm: Remove the branch from compound_head()
  hugetlb: Update vmemmap_dedup.rst
  mm/slab: Use compound_head() in page_slab()

Kiryl Shutsemau (Meta) (2):
  mm/hugetlb: Defer vmemmap population for bootmem hugepages
  x86/vdso: Undefine CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP for vdso32

 .../admin-guide/kdump/vmcoreinfo.rst          |   2 +-
 Documentation/mm/vmemmap_dedup.rst            |  62 ++-
 arch/loongarch/include/asm/pgtable.h          |   3 +-
 arch/riscv/mm/init.c                          |   3 +-
 arch/x86/entry/vdso/vdso32/fake_32bit_build.h |   1 +
 include/linux/mm.h                            |  36 +-
 include/linux/mm_types.h                      |  20 +-
 include/linux/mmzone.h                        |  57 +++
 include/linux/page-flags.h                    | 166 ++++----
 include/linux/page_ref.h                      |   8 +-
 include/linux/types.h                         |   2 +-
 kernel/vmcore_info.c                          |   2 +-
 mm/hugetlb.c                                  |   8 +-
 mm/hugetlb_vmemmap.c                          | 362 +++++++++---------
 mm/internal.h                                 |  18 +-
 mm/mm_init.c                                  |   2 +-
 mm/page_alloc.c                               |   4 +-
 mm/slab.h                                     |   8 +-
 mm/sparse-vmemmap.c                           | 110 +++---
 mm/sparse.c                                   |   5 +
 mm/util.c                                     |  16 +-
 21 files changed, 448 insertions(+), 447 deletions(-)

-- 
2.51.2

---

From: Kiryl Shutsemau &lt;kas@kernel.org&gt;

Instead of passing down the head page and tail page index, pass the tail
and head pages directly, as well as the order of the compound page.

This is a preparation for changing how the head position is encoded in
the tail page.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
Acked-by: David Hildenbrand (arm) &lt;david@kernel.org&gt;
Reviewed-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;
---
 include/linux/page-flags.h |  5 +++--
 mm/hugetlb.c               |  8 +++++---
 mm/internal.h              | 11 +++++------
 mm/mm_init.c               |  2 +-
 mm/page_alloc.c            |  2 +-
 5 files changed, 15 insertions(+), 13 deletions(-)

diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
index f7a0e4af0c73..5e7687ccccf8 100644
--- a/include/linux/page-flags.h
+++ b/include/linux/page-flags.h
@@ -865,9 +865,10 @@ static inline bool folio_test_large(const struct folio *folio)
 	return folio_test_head(folio);
 }
 
-static __always_inline void set_compound_head(struct page *page, struct page *head)
+static __always_inline void set_compound_head(struct page *tail,
+		const struct page *head, unsigned int order)
 {
-	WRITE_ONCE(page-&gt;compound_head, (unsigned long)head + 1);
+	WRITE_ONCE(tail-&gt;compound_head, (unsigned long)head + 1);
 }
 
 static __always_inline void clear_compound_head(struct page *page)
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 0beb6e22bc26..fc55f22c9e41 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -3168,6 +3168,7 @@ int __alloc_bootmem_huge_page(struct hstate *h, int nid)
 
 /* Initialize [start_page:end_page_number] tail struct pages of a hugepage */
 static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,
+					struct hstate *h,
 					unsigned long start_page_number,
 					unsigned long end_page_number)
 {
@@ -3176,6 +3177,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,
 	struct page *page = folio_page(folio, start_page_number);
 	unsigned long head_pfn = folio_pfn(folio);
 	unsigned long pfn, end_pfn = head_pfn + end_page_number;
+	unsigned int order = huge_page_order(h);
 
 	/*
 	 * As we marked all tail pages with memblock_reserved_mark_noinit(),
@@ -3183,7 +3185,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,
 	 */
 	for (pfn = head_pfn + start_page_number; pfn &lt; end_pfn; page++, pfn++) {
 		__init_single_page(page, pfn, zone, nid);
-		prep_compound_tail((struct page *)folio, pfn - head_pfn);
+		prep_compound_tail(page, &amp;folio-&gt;page, order);
 		set_page_count(page, 0);
 	}
 }
@@ -3203,7 +3205,7 @@ static void __init hugetlb_folio_init_vmemmap(struct folio *folio,
 	__folio_set_head(folio);
 	ret = folio_ref_freeze(folio, 1);
 	VM_BUG_ON(!ret);
-	hugetlb_folio_init_tail_vmemmap(folio, 1, nr_pages);
+	hugetlb_folio_init_tail_vmemmap(folio, h, 1, nr_pages);
 	prep_compound_head(&amp;folio-&gt;page, huge_page_order(h));
 }
 
@@ -3260,7 +3262,7 @@ static void __init prep_and_add_bootmem_folios(struct hstate *h,
 			 * time as this is early in boot and there should
 			 * be no contention.
 			 */
-			hugetlb_folio_init_tail_vmemmap(folio,
+			hugetlb_folio_init_tail_vmemmap(folio, h,
 					HUGETLB_VMEMMAP_RESERVE_PAGES,
 					pages_per_huge_page(h));
 		}
diff --git a/mm/internal.h b/mm/internal.h
index cb0af847d7d9..c76122f22294 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -878,13 +878,12 @@ static inline void prep_compound_head(struct page *page, unsigned int order)
 		INIT_LIST_HEAD(&amp;folio-&gt;_deferred_list);
 }
 
-static inline void prep_compound_tail(struct page *head, int tail_idx)
+static inline void prep_compound_tail(struct page *tail,
+		const struct page *head, unsigned int order)
 {
-	struct page *p = head + tail_idx;
-
-	p-&gt;mapping = TAIL_MAPPING;
-	set_compound_head(p, head);
-	set_page_private(p, 0);
+	tail-&gt;mapping = TAIL_MAPPING;
+	set_compound_head(tail, head, order);
+	set_page_private(tail, 0);
 }
 
 void post_alloc_hook(struct page *page, unsigned int order, gfp_t gfp_flags);
diff --git a/mm/mm_init.c b/mm/mm_init.c
index 61d983d23f55..0a12a9be0bcc 100644
--- a/mm/mm_init.c
+++ b/mm/mm_init.c
@@ -1099,7 +1099,7 @@ static void __ref memmap_init_compound(struct page *head,
 		struct page *page = pfn_to_page(pfn);
 
 		__init_zone_device_page(page, pfn, zone_idx, nid, pgmap);
-		prep_compound_tail(head, pfn - head_pfn);
+		prep_compound_tail(page, head, order);
 		set_page_count(page, 0);
 	}
 	prep_compound_head(head, order);
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index fcc32737f451..aa657e4a99e8 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -744,7 +744,7 @@ void prep_compound_page(struct page *page, unsigned int order)
 
 	__SetPageHead(page);
 	for (i = 1; i &lt; nr_pages; i++)
-		prep_compound_tail(page, i);
+		prep_compound_tail(page + i, page, order);
 
 	prep_compound_head(page, order);
 }
-- 
2.51.2


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv</pre>
</details>
<a href="https://lore.kernel.org/r/20260202155634.650837-1-kas@kernel.org" target="_blank" rel="noopener" class="lore-link">View on lore &#8599;</a>
<div class="review-comment-signals">Signals: clarification, explanation</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Kiryl Shutsemau (author)</span>
<a class="date-chip" href="../2026-02-27_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-27">2026-02-27</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed a concern about the naming of the &#x27;compound_head&#x27; field in struct page, explaining that it encodes whether the page is a tail and where to locate the head page. The author renamed the field to &#x27;compound_info&#x27;, which will be used for the new mask-based encoding.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">From: Kiryl Shutsemau &lt;kas@kernel.org&gt;

The &#x27;compound_head&#x27; field in the &#x27;struct page&#x27; encodes whether the page
is a tail and where to locate the head page. Bit 0 is set if the page is
a tail, and the remaining bits in the field point to the head page.

As preparation for changing how the field encodes information about the
head page, rename the field to &#x27;compound_info&#x27;.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
Acked-by: David Hildenbrand (arm) &lt;david@kernel.org&gt;
Reviewed-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;
---
 .../admin-guide/kdump/vmcoreinfo.rst          |  2 +-
 Documentation/mm/vmemmap_dedup.rst            |  6 +++---
 include/linux/mm_types.h                      | 20 +++++++++----------
 include/linux/page-flags.h                    | 18 ++++++++---------
 include/linux/types.h                         |  2 +-
 kernel/vmcore_info.c                          |  2 +-
 mm/page_alloc.c                               |  2 +-
 mm/slab.h                                     |  2 +-
 mm/util.c                                     |  2 +-
 9 files changed, 28 insertions(+), 28 deletions(-)

diff --git a/Documentation/admin-guide/kdump/vmcoreinfo.rst b/Documentation/admin-guide/kdump/vmcoreinfo.rst
index 404a15f6782c..7663c610fe90 100644
--- a/Documentation/admin-guide/kdump/vmcoreinfo.rst
+++ b/Documentation/admin-guide/kdump/vmcoreinfo.rst
@@ -141,7 +141,7 @@ nodemask_t
 The size of a nodemask_t type. Used to compute the number of online
 nodes.
 
-(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_head)
+(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_info)
 ----------------------------------------------------------------------------------
 
 User-space tools compute their values based on the offset of these
diff --git a/Documentation/mm/vmemmap_dedup.rst b/Documentation/mm/vmemmap_dedup.rst
index b4a55b6569fa..1863d88d2dcb 100644
--- a/Documentation/mm/vmemmap_dedup.rst
+++ b/Documentation/mm/vmemmap_dedup.rst
@@ -24,7 +24,7 @@ For each base page, there is a corresponding ``struct page``.
 Within the HugeTLB subsystem, only the first 4 ``struct page`` are used to
 contain unique information about a HugeTLB page. ``__NR_USED_SUBPAGE`` provides
 this upper limit. The only &#x27;useful&#x27; information in the remaining ``struct page``
-is the compound_head field, and this field is the same for all tail pages.
+is the compound_info field, and this field is the same for all tail pages.
 
 By removing redundant ``struct page`` for HugeTLB pages, memory can be returned
 to the buddy allocator for other uses.
@@ -124,10 +124,10 @@ Here is how things look before optimization::
  |           |
  +-----------+
 
-The value of page-&gt;compound_head is the same for all tail pages. The first
+The value of page-&gt;compound_info is the same for all tail pages. The first
 page of ``struct page`` (page 0) associated with the HugeTLB page contains the 4
 ``struct page`` necessary to describe the HugeTLB. The only use of the remaining
-pages of ``struct page`` (page 1 to page 7) is to point to page-&gt;compound_head.
+pages of ``struct page`` (page 1 to page 7) is to point to page-&gt;compound_info.
 Therefore, we can remap pages 1 to 7 to page 0. Only 1 page of ``struct page``
 will be used for each HugeTLB page. This will allow us to free the remaining
 7 pages to the buddy allocator.
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 3cc8ae722886..7bc82a2b889f 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -126,14 +126,14 @@ struct page {
 			atomic_long_t pp_ref_count;
 		};
 		struct {	/* Tail pages of compound page */
-			unsigned long compound_head;	/* Bit zero is set */
+			unsigned long compound_info;	/* Bit zero is set */
 		};
 		struct {	/* ZONE_DEVICE pages */
 			/*
-			 * The first word is used for compound_head or folio
+			 * The first word is used for compound_info or folio
 			 * pgmap
 			 */
-			void *_unused_pgmap_compound_head;
+			void *_unused_pgmap_compound_info;
 			void *zone_device_data;
 			/*
 			 * ZONE_DEVICE private pages are counted as being
@@ -409,7 +409,7 @@ struct folio {
 	/* private: avoid cluttering the output */
 				/* For the Unevictable &quot;LRU list&quot; slot */
 				struct {
-					/* Avoid compound_head */
+					/* Avoid compound_info */
 					void *__filler;
 	/* public: */
 					unsigned int mlock_count;
@@ -510,7 +510,7 @@ struct folio {
 FOLIO_MATCH(flags, flags);
 FOLIO_MATCH(lru, lru);
 FOLIO_MATCH(mapping, mapping);
-FOLIO_MATCH(compound_head, lru);
+FOLIO_MATCH(compound_info, lru);
 FOLIO_MATCH(__folio_index, index);
 FOLIO_MATCH(private, private);
 FOLIO_MATCH(_mapcount, _mapcount);
@@ -529,7 +529,7 @@ FOLIO_MATCH(_last_cpupid, _last_cpupid);
 	static_assert(offsetof(struct folio, fl) ==			\
 			offsetof(struct page, pg) + sizeof(struct page))
 FOLIO_MATCH(flags, _flags_1);
-FOLIO_MATCH(compound_head, _head_1);
+FOLIO_MATCH(compound_info, _head_1);
 FOLIO_MATCH(_mapcount, _mapcount_1);
 FOLIO_MATCH(_refcount, _refcount_1);
 #undef FOLIO_MATCH
@@ -537,13 +537,13 @@ FOLIO_MATCH(_refcount, _refcount_1);
 	static_assert(offsetof(struct folio, fl) ==			\
 			offsetof(struct page, pg) + 2 * sizeof(struct page))
 FOLIO_MATCH(flags, _flags_2);
-FOLIO_MATCH(compound_head, _head_2);
+FOLIO_MATCH(compound_info, _head_2);
 #undef FOLIO_MATCH
 #define FOLIO_MATCH(pg, fl)						\
 	static_assert(offsetof(struct folio, fl) ==			\
 			offsetof(struct page, pg) + 3 * sizeof(struct page))
 FOLIO_MATCH(flags, _flags_3);
-FOLIO_MATCH(compound_head, _head_3);
+FOLIO_MATCH(compound_info, _head_3);
 #undef FOLIO_MATCH
 
 /**
@@ -609,8 +609,8 @@ struct ptdesc {
 #define TABLE_MATCH(pg, pt)						\
 	static_assert(offsetof(struct page, pg) == offsetof(struct ptdesc, pt))
 TABLE_MATCH(flags, pt_flags);
-TABLE_MATCH(compound_head, pt_list);
-TABLE_MATCH(compound_head, _pt_pad_1);
+TABLE_MATCH(compound_info, pt_list);
+TABLE_MATCH(compound_info, _pt_pad_1);
 TABLE_MATCH(mapping, __page_mapping);
 TABLE_MATCH(__folio_index, pt_index);
 TABLE_MATCH(rcu_head, pt_rcu_head);
diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
index 5e7687ccccf8..70c4e43f2d9a 100644
--- a/include/linux/page-flags.h
+++ b/include/linux/page-flags.h
@@ -213,7 +213,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page
 	/*
 	 * Only addresses aligned with PAGE_SIZE of struct page may be fake head
 	 * struct page. The alignment check aims to avoid access the fields (
-	 * e.g. compound_head) of the @page[1]. It can avoid touch a (possibly)
+	 * e.g. compound_info) of the @page[1]. It can avoid touch a (possibly)
 	 * cold cacheline in some cases.
 	 */
 	if (IS_ALIGNED((unsigned long)page, PAGE_SIZE) &amp;&amp;
@@ -223,7 +223,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page
 		 * because the @page is a compound page composed with at least
 		 * two contiguous pages.
 		 */
-		unsigned long head = READ_ONCE(page[1].compound_head);
+		unsigned long head = READ_ONCE(page[1].compound_info);
 
 		if (likely(head &amp; 1))
 			return (const struct page *)(head - 1);
@@ -281,7 +281,7 @@ static __always_inline int page_is_fake_head(const struct page *page)
 
 static __always_inline unsigned long _compound_head(const struct page *page)
 {
-	unsigned long head = READ_ONCE(page-&gt;compound_head);
+	unsigned long head = READ_ONCE(page-&gt;compound_info);
 
 	if (unlikely(head &amp; 1))
 		return head - 1;
@@ -320,13 +320,13 @@ static __always_inline unsigned long _compound_head(const struct page *page)
 
 static __always_inline int PageTail(const struct page *page)
 {
-	return READ_ONCE(page-&gt;compound_head) &amp; 1 || page_is_fake_head(page);
+	return READ_ONCE(page-&gt;compound_info) &amp; 1 || page_is_fake_head(page);
 }
 
 static __always_inline int PageCompound(const struct page *page)
 {
 	return test_bit(PG_head, &amp;page-&gt;flags.f) ||
-	       READ_ONCE(page-&gt;compound_head) &amp; 1;
+	       READ_ONCE(page-&gt;compound_info) &amp; 1;
 }
 
 #define	PAGE_POISON_PATTERN	-1l
@@ -348,7 +348,7 @@ static const unsigned long *const_folio_flags(const struct folio *folio,
 {
 	const struct page *page = &amp;folio-&gt;page;
 
-	VM_BUG_ON_PGFLAGS(page-&gt;compound_head &amp; 1, page);
+	VM_BUG_ON_PGFLAGS(page-&gt;compound_info &amp; 1, page);
 	VM_BUG_ON_PGFLAGS(n &gt; 0 &amp;&amp; !test_bit(PG_head, &amp;page-&gt;flags.f), page);
 	return &amp;page[n].flags.f;
 }
@@ -357,7 +357,7 @@ static unsigned long *folio_flags(struct folio *folio, unsigned n)
 {
 	struct page *page = &amp;folio-&gt;page;
 
-	VM_BUG_ON_PGFLAGS(page-&gt;compound_head &amp; 1, page);
+	VM_BUG_ON_PGFLAGS(page-&gt;compound_info &amp; 1, page);
 	VM_BUG_ON_PGFLAGS(n &gt; 0 &amp;&amp; !test_bit(PG_head, &amp;page-&gt;flags.f), page);
 	return &amp;page[n].flags.f;
 }
@@ -868,12 +868,12 @@ static inline bool folio_test_large(const struct folio *folio)
 static __always_inline void set_compound_head(struct page *tail,
 		const struct page *head, unsigned int order)
 {
-	WRITE_ONCE(tail-&gt;compound_head, (unsigned long)head + 1);
+	WRITE_ONCE(tail-&gt;compound_info, (unsigned long)head + 1);
 }
 
 static __always_inline void clear_compound_head(struct page *page)
 {
-	WRITE_ONCE(page-&gt;compound_head, 0);
+	WRITE_ONCE(page-&gt;compound_info, 0);
 }
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
diff --git a/include/linux/types.h b/include/linux/types.h
index 7e71d260763c..608050dbca6a 100644
--- a/include/linux/types.h
+++ b/include/linux/types.h
@@ -239,7 +239,7 @@ struct ustat {
  *
  * This guarantee is important for few reasons:
  *  - future call_rcu_lazy() will make use of lower bits in the pointer;
- *  - the structure shares storage space in struct page with @compound_head,
+ *  - the structure shares storage space in struct page with @compound_info,
  *    which encode PageTail() in bit 0. The guarantee is needed to avoid
  *    false-positive PageTail().
  */
diff --git a/kernel/vmcore_info.c b/kernel/vmcore_info.c
index 8d82913223a1..94e4ef75b1b2 100644
--- a/kernel/vmcore_info.c
+++ b/kernel/vmcore_info.c
@@ -198,7 +198,7 @@ static int __init crash_save_vmcoreinfo_init(void)
 	VMCOREINFO_OFFSET(page, lru);
 	VMCOREINFO_OFFSET(page, _mapcount);
 	VMCOREINFO_OFFSET(page, private);
-	VMCOREINFO_OFFSET(page, compound_head);
+	VMCOREINFO_OFFSET(page, compound_info);
 	VMCOREINFO_OFFSET(pglist_data, node_zones);
 	VMCOREINFO_OFFSET(pglist_data, nr_zones);
 #ifdef CONFIG_FLATMEM
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index aa657e4a99e8..e83f67fbbf07 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -731,7 +731,7 @@ static inline bool pcp_allowed_order(unsigned int order)
  * The first PAGE_SIZE page is called the &quot;head page&quot; and have PG_head set.
  *
  * The remaining PAGE_SIZE pages are called &quot;tail pages&quot;. PageTail() is encoded
- * in bit 0 of page-&gt;compound_head. The rest of bits is pointer to head page.
+ * in bit 0 of page-&gt;compound_info. The rest of bits is pointer to head page.
  *
  * The first tail page&#x27;s -&gt;compound_order holds the order of allocation.
  * This usage means that zero-order pages may not be compound.
diff --git a/mm/slab.h b/mm/slab.h
index 71c7261bf822..62dfa50c1f01 100644
--- a/mm/slab.h
+++ b/mm/slab.h
@@ -94,7 +94,7 @@ struct slab {
 #define SLAB_MATCH(pg, sl)						\
 	static_assert(offsetof(struct page, pg) == offsetof(struct slab, sl))
 SLAB_MATCH(flags, flags);
-SLAB_MATCH(compound_head, slab_cache);	/* Ensure bit 0 is clear */
+SLAB_MATCH(compound_info, slab_cache);	/* Ensure bit 0 is clear */
 SLAB_MATCH(_refcount, __page_refcount);
 #ifdef CONFIG_MEMCG
 SLAB_MATCH(memcg_data, obj_exts);
diff --git a/mm/util.c b/mm/util.c
index b05ab6f97e11..3ebcb9e6035c 100644
--- a/mm/util.c
+++ b/mm/util.c
@@ -1247,7 +1247,7 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)
 again:
 	memset(&amp;ps-&gt;folio_snapshot, 0, sizeof(struct folio));
 	memcpy(&amp;ps-&gt;page_snapshot, page, sizeof(*page));
-	head = ps-&gt;page_snapshot.compound_head;
+	head = ps-&gt;page_snapshot.compound_info;
 	if ((head &amp; 1) == 0) {
 		ps-&gt;idx = 0;
 		foliop = (struct folio *)&amp;ps-&gt;page_snapshot;
-- 
2.51.2


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

From: Kiryl Shutsemau &lt;kas@kernel.org&gt;

Move set_compound_head() and clear_compound_head() to be adjacent to the
compound_head() function in page-flags.h.

These functions encode and decode the same compound_info field, so
keeping them together makes it easier to verify their logic is
consistent, especially when the encoding changes.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
Acked-by: David Hildenbrand (arm) &lt;david@kernel.org&gt;
Reviewed-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;
---
 include/linux/page-flags.h | 22 +++++++++++-----------
 1 file changed, 11 insertions(+), 11 deletions(-)

diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
index 70c4e43f2d9a..42bf8ed02a29 100644
--- a/include/linux/page-flags.h
+++ b/include/linux/page-flags.h
@@ -290,6 +290,17 @@ static __always_inline unsigned long _compound_head(const struct page *page)
 
 #define compound_head(page)	((typeof(page))_compound_head(page))
 
+static __always_inline void set_compound_head(struct page *tail,
+		const struct page *head, unsigned int order)
+{
+	WRITE_ONCE(tail-&gt;compound_info, (unsigned long)head + 1);
+}
+
+static __always_inline void clear_compound_head(struct page *page)
+{
+	WRITE_ONCE(page-&gt;compound_info, 0);
+}
+
 /**
  * page_folio - Converts from page to folio.
  * @p: The page.
@@ -865,17 +876,6 @@ static inline bool folio_test_large(const struct folio *folio)
 	return folio_test_head(folio);
 }
 
-static __always_inline void set_compound_head(struct page *tail,
-		const struct page *head, unsigned int order)
-{
-	WRITE_ONCE(tail-&gt;compound_info, (unsigned long)head + 1);
-}
-
-static __always_inline void clear_compound_head(struct page *page)
-{
-	WRITE_ONCE(page-&gt;compound_info, 0);
-}
-
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 static inline void ClearPageCompound(struct page *page)
 {
-- 
2.51.2


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

From: Kiryl Shutsemau &lt;kas@kernel.org&gt;

The upcoming change to the HugeTLB vmemmap optimization (HVO) requires
struct pages of the head page to be naturally aligned with regard to the
folio size.

Align vmemmap to the newly introduced MAX_FOLIO_VMEMMAP_ALIGN.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
---
 arch/riscv/mm/init.c   |  3 ++-
 include/linux/mmzone.h | 11 +++++++++++
 2 files changed, 13 insertions(+), 1 deletion(-)

diff --git a/arch/riscv/mm/init.c b/arch/riscv/mm/init.c
index 811e03786c56..e8fb2239a0b5 100644
--- a/arch/riscv/mm/init.c
+++ b/arch/riscv/mm/init.c
@@ -63,7 +63,8 @@ phys_addr_t phys_ram_base __ro_after_init;
 EXPORT_SYMBOL(phys_ram_base);
 
 #ifdef CONFIG_SPARSEMEM_VMEMMAP
-#define VMEMMAP_ADDR_ALIGN	(1ULL &lt;&lt; SECTION_SIZE_BITS)
+#define VMEMMAP_ADDR_ALIGN	max(1ULL &lt;&lt; SECTION_SIZE_BITS, \
+				    MAX_FOLIO_VMEMMAP_ALIGN)
 
 unsigned long vmemmap_start_pfn __ro_after_init;
 EXPORT_SYMBOL(vmemmap_start_pfn);
diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index be8ce40b5638..492a5be1090f 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -92,6 +92,17 @@
 
 #define MAX_FOLIO_NR_PAGES	(1UL &lt;&lt; MAX_FOLIO_ORDER)
 
+/*
+ * HugeTLB Vmemmap Optimization (HVO) requires struct pages of the head page to
+ * be naturally aligned with regard to the folio size.
+ *
+ * HVO which is only active if the size of struct page is a power of 2.
+ */
+#define MAX_FOLIO_VMEMMAP_ALIGN \
+	(IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP) &amp;&amp; \
+	 is_power_of_2(sizeof(struct page)) ? \
+	 MAX_FOLIO_NR_PAGES * sizeof(struct page) : 0)
+
 enum migratetype {
 	MIGRATE_UNMOVABLE,
 	MIGRATE_MOVABLE,
-- 
2.51.2


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

From: Kiryl Shutsemau &lt;kas@kernel.org&gt;

The upcoming change to the HugeTLB vmemmap optimization (HVO) requires
struct pages of the head page to be naturally aligned with regard to the
folio size.

Align vmemmap to MAX_FOLIO_VMEMMAP_ALIGN.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
---
 arch/loongarch/include/asm/pgtable.h | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h
index c33b3bcb733e..ea6c09eed2e7 100644
--- a/arch/loongarch/include/asm/pgtable.h
+++ b/arch/loongarch/include/asm/pgtable.h
@@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
 	 min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL &lt;&lt; cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)
 #endif
 
-#define vmemmap		((struct page *)((VMALLOC_END + PMD_SIZE) &amp; PMD_MASK))
+#define VMEMMAP_ALIGN	max(PMD_SIZE, MAX_FOLIO_VMEMMAP_ALIGN)
+#define vmemmap		((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))
 #define VMEMMAP_END	((unsigned long)vmemmap + VMEMMAP_SIZE - 1)
 
 #define KFENCE_AREA_START	(VMEMMAP_END + 1)
-- 
2.51.2


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

From: Kiryl Shutsemau &lt;kas@kernel.org&gt;

Move MAX_FOLIO_ORDER definition from mm.h to mmzone.h.

This is preparation for adding the vmemmap_tails array to struct
zone, which requires MAX_FOLIO_ORDER to be available in mmzone.h.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Acked-by: David Hildenbrand (Red Hat) &lt;david@kernel.org&gt;
Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
Acked-by: Muchun Song &lt;muchun.song@linux.dev&gt;
Acked-by: Usama Arif &lt;usamaarif642@gmail.com&gt;
Reviewed-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;
---
 include/linux/mm.h     | 31 -------------------------------
 include/linux/mmzone.h | 31 +++++++++++++++++++++++++++++++
 2 files changed, 31 insertions(+), 31 deletions(-)

diff --git a/include/linux/mm.h b/include/linux/mm.h
index 5be3d8a8f806..7f4dbbb9d783 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -27,7 +27,6 @@
 #include &lt;linux/page-flags.h&gt;
 #include &lt;linux/page_ref.h&gt;
 #include &lt;linux/overflow.h&gt;
-#include &lt;linux/sizes.h&gt;
 #include &lt;linux/sched.h&gt;
 #include &lt;linux/pgtable.h&gt;
 #include &lt;linux/kasan.h&gt;
@@ -2479,36 +2478,6 @@ static inline unsigned long folio_nr_pages(const struct folio *folio)
 	return folio_large_nr_pages(folio);
 }
 
-#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)
-/*
- * We don&#x27;t expect any folios that exceed buddy sizes (and consequently
- * memory sections).
- */
-#define MAX_FOLIO_ORDER		MAX_PAGE_ORDER
-#elif defined(CONFIG_SPARSEMEM) &amp;&amp; !defined(CONFIG_SPARSEMEM_VMEMMAP)
-/*
- * Only pages within a single memory section are guaranteed to be
- * contiguous. By limiting folios to a single memory section, all folio
- * pages are guaranteed to be contiguous.
- */
-#define MAX_FOLIO_ORDER		PFN_SECTION_SHIFT
-#elif defined(CONFIG_HUGETLB_PAGE)
-/*
- * There is no real limit on the folio size. We limit them to the maximum we
- * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect
- * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.
- */
-#define MAX_FOLIO_ORDER		get_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)
-#else
-/*
- * Without hugetlb, gigantic folios that are bigger than a single PUD are
- * currently impossible.
- */
-#define MAX_FOLIO_ORDER		PUD_ORDER
-#endif
-
-#define MAX_FOLIO_NR_PAGES	(1UL &lt;&lt; MAX_FOLIO_ORDER)
-
 /*
  * compound_nr() returns the number of pages in this potentially compound
  * page.  compound_nr() can be called on a tail page, and is defined to
diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 3e51190a55e4..be8ce40b5638 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -23,6 +23,7 @@
 #include &lt;linux/page-flags.h&gt;
 #include &lt;linux/local_lock.h&gt;
 #include &lt;linux/zswap.h&gt;
+#include &lt;linux/sizes.h&gt;
 #include &lt;asm/page.h&gt;
 
 /* Free memory management - zoned buddy allocator.  */
@@ -61,6 +62,36 @@
  */
 #define PAGE_ALLOC_COSTLY_ORDER 3
 
+#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)
+/*
+ * We don&#x27;t expect any folios that exceed buddy sizes (and consequently
+ * memory sections).
+ */
+#define MAX_FOLIO_ORDER		MAX_PAGE_ORDER
+#elif defined(CONFIG_SPARSEMEM) &amp;&amp; !defined(CONFIG_SPARSEMEM_VMEMMAP)
+/*
+ * Only pages within a single memory section are guaranteed to be
+ * contiguous. By limiting folios to a single memory section, all folio
+ * pages are guaranteed to be contiguous.
+ */
+#define MAX_FOLIO_ORDER		PFN_SECTION_SHIFT
+#elif defined(CONFIG_HUGETLB_PAGE)
+/*
+ * There is no real limit on the folio size. We limit them to the maximum we
+ * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect
+ * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.
+ */
+#define MAX_FOLIO_ORDER		get_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)
+#else
+/*
+ * Without hugetlb, gigantic folios that are bigger than a single PUD are
+ * currently impossible.
+ */
+#define MAX_FOLIO_ORDER		PUD_ORDER
+#endif
+
+#define MAX_FOLIO_NR_PAGES	(1UL &lt;&lt; MAX_FOLIO_ORDER)
+
 enum migratetype {
 	MIGRATE_UNMOVABLE,
 	MIGRATE_MOVABLE,
-- 
2.51.2


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv</pre>
</details>
<a href="https://lore.kernel.org/r/20260227193030.272078-3-kas@kernel.org" target="_blank" rel="noopener" class="lore-link">View on lore &#8599;</a>
<div class="review-comment-signals">Signals: clarification, explanation</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Kiryl Shutsemau (author)</span>
<a class="date-chip" href="../2026-02-27_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-27">2026-02-27</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed a concern about the mask-based encoding of compound_info for tail pages, explaining that it only works when sizeof(struct page) is power-of-2 and struct pages are naturally aligned for all orders up to MAX_FOLIO_ORDER. The author acknowledged that this approach would work in more conditions but requires additional validation.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">From: Kiryl Shutsemau &lt;kas@kernel.org&gt;

For tail pages, the kernel uses the &#x27;compound_info&#x27; field to get to the
head page. The bit 0 of the field indicates whether the page is a
tail page, and if set, the remaining bits represent a pointer to the
head page.

For cases when size of struct page is power-of-2, change the encoding of
compound_info to store a mask that can be applied to the virtual address
of the tail page in order to access the head page. It is possible
because struct page of the head page is naturally aligned with regards
to order of the page.

The significant impact of this modification is that all tail pages of
the same order will now have identical &#x27;compound_info&#x27;, regardless of
the compound page they are associated with. This paves the way for
eliminating fake heads.

The HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only
applied when the sizeof(struct page) is power-of-2. Having identical
tail pages allows the same page to be mapped into the vmemmap of all
pages, maintaining memory savings without fake heads.

If sizeof(struct page) is not power-of-2, there is no functional
changes.

Limit mask usage to HugeTLB vmemmap optimization (HVO) where it makes
a difference. The approach with mask would work in the wider set of
conditions, but it requires validating that struct pages are naturally
aligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Reviewed-by: Muchun Song &lt;muchun.song@linux.dev&gt;
Reviewed-by: Zi Yan &lt;ziy@nvidia.com&gt;
Acked-by: David Hildenbrand (Arm) &lt;david@kernel.org&gt;
Acked-by: Usama Arif &lt;usamaarif642@gmail.com&gt;
Reviewed-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;
---
 include/linux/page-flags.h | 81 ++++++++++++++++++++++++++++++++++----
 mm/slab.h                  | 16 ++++++--
 mm/util.c                  | 16 ++++++--
 3 files changed, 97 insertions(+), 16 deletions(-)

diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
index 42bf8ed02a29..01970bd38bff 100644
--- a/include/linux/page-flags.h
+++ b/include/linux/page-flags.h
@@ -198,6 +198,29 @@ enum pageflags {
 
 #ifndef __GENERATING_BOUNDS_H
 
+/*
+ * For tail pages, if the size of struct page is power-of-2 -&gt;compound_info
+ * encodes the mask that converts the address of the tail page address to
+ * the head page address.
+ *
+ * Otherwise, -&gt;compound_info has direct pointer to head pages.
+ */
+static __always_inline bool compound_info_has_mask(void)
+{
+	/*
+	 * Limit mask usage to HugeTLB vmemmap optimization (HVO) where it
+	 * makes a difference.
+	 *
+	 * The approach with mask would work in the wider set of conditions,
+	 * but it requires validating that struct pages are naturally aligned
+	 * for all orders up to the MAX_FOLIO_ORDER, which can be tricky.
+	 */
+	if (!IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP))
+		return false;
+
+	return is_power_of_2(sizeof(struct page));
+}
+
 #ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP
 DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);
 
@@ -207,6 +230,10 @@ DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);
  */
 static __always_inline const struct page *page_fixed_fake_head(const struct page *page)
 {
+	/* Fake heads only exists if compound_info_has_mask() is true */
+	if (!compound_info_has_mask())
+		return page;
+
 	if (!static_branch_unlikely(&amp;hugetlb_optimize_vmemmap_key))
 		return page;
 
@@ -223,10 +250,14 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page
 		 * because the @page is a compound page composed with at least
 		 * two contiguous pages.
 		 */
-		unsigned long head = READ_ONCE(page[1].compound_info);
+		unsigned long info = READ_ONCE(page[1].compound_info);
 
-		if (likely(head &amp; 1))
-			return (const struct page *)(head - 1);
+		/* See set_compound_head() */
+		if (likely(info &amp; 1)) {
+			unsigned long p = (unsigned long)page;
+
+			return (const struct page *)(p &amp; info);
+		}
 	}
 	return page;
 }
@@ -281,11 +312,26 @@ static __always_inline int page_is_fake_head(const struct page *page)
 
 static __always_inline unsigned long _compound_head(const struct page *page)
 {
-	unsigned long head = READ_ONCE(page-&gt;compound_info);
+	unsigned long info = READ_ONCE(page-&gt;compound_info);
 
-	if (unlikely(head &amp; 1))
-		return head - 1;
-	return (unsigned long)page_fixed_fake_head(page);
+	/* Bit 0 encodes PageTail() */
+	if (!(info &amp; 1))
+		return (unsigned long)page_fixed_fake_head(page);
+
+	/*
+	 * If compound_info_has_mask() is false, the rest of compound_info is
+	 * the pointer to the head page.
+	 */
+	if (!compound_info_has_mask())
+		return info - 1;
+
+	/*
+	 * If compound_info_has_mask() is true the rest of the info encodes
+	 * the mask that converts the address of the tail page to the head page.
+	 *
+	 * No need to clear bit 0 in the mask as &#x27;page&#x27; always has it clear.
+	 */
+	return (unsigned long)page &amp; info;
 }
 
 #define compound_head(page)	((typeof(page))_compound_head(page))
@@ -293,7 +339,26 @@ static __always_inline unsigned long _compound_head(const struct page *page)
 static __always_inline void set_compound_head(struct page *tail,
 		const struct page *head, unsigned int order)
 {
-	WRITE_ONCE(tail-&gt;compound_info, (unsigned long)head + 1);
+	unsigned int shift;
+	unsigned long mask;
+
+	if (!compound_info_has_mask()) {
+		WRITE_ONCE(tail-&gt;compound_info, (unsigned long)head | 1);
+		return;
+	}
+
+	/*
+	 * If the size of struct page is power-of-2, bits [shift:0] of the
+	 * virtual address of compound head are zero.
+	 *
+	 * Calculate mask that can be applied to the virtual address of
+	 * the tail page to get address of the head page.
+	 */
+	shift = order + order_base_2(sizeof(struct page));
+	mask = GENMASK(BITS_PER_LONG - 1, shift);
+
+	/* Bit 0 encodes PageTail() */
+	WRITE_ONCE(tail-&gt;compound_info, mask | 1);
 }
 
 static __always_inline void clear_compound_head(struct page *page)
diff --git a/mm/slab.h b/mm/slab.h
index 62dfa50c1f01..1a1b3758df05 100644
--- a/mm/slab.h
+++ b/mm/slab.h
@@ -131,11 +131,19 @@ static_assert(IS_ALIGNED(offsetof(struct slab, freelist), sizeof(struct freelist
  */
 static inline struct slab *page_slab(const struct page *page)
 {
-	unsigned long head;
+	unsigned long info;
+
+	info = READ_ONCE(page-&gt;compound_info);
+	if (info &amp; 1) {
+		/* See compound_head() */
+		if (compound_info_has_mask()) {
+			unsigned long p = (unsigned long)page;
+			page = (struct page *)(p &amp; info);
+		} else {
+			page = (struct page *)(info - 1);
+		}
+	}
 
-	head = READ_ONCE(page-&gt;compound_head);
-	if (head &amp; 1)
-		page = (struct page *)(head - 1);
 	if (data_race(page-&gt;page_type &gt;&gt; 24) != PGTY_slab)
 		page = NULL;
 
diff --git a/mm/util.c b/mm/util.c
index 3ebcb9e6035c..20dccf2881d7 100644
--- a/mm/util.c
+++ b/mm/util.c
@@ -1237,7 +1237,7 @@ static void set_ps_flags(struct page_snapshot *ps, const struct folio *folio,
  */
 void snapshot_page(struct page_snapshot *ps, const struct page *page)
 {
-	unsigned long head, nr_pages = 1;
+	unsigned long info, nr_pages = 1;
 	struct folio *foliop;
 	int loops = 5;
 
@@ -1247,8 +1247,8 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)
 again:
 	memset(&amp;ps-&gt;folio_snapshot, 0, sizeof(struct folio));
 	memcpy(&amp;ps-&gt;page_snapshot, page, sizeof(*page));
-	head = ps-&gt;page_snapshot.compound_info;
-	if ((head &amp; 1) == 0) {
+	info = ps-&gt;page_snapshot.compound_info;
+	if (!(info &amp; 1)) {
 		ps-&gt;idx = 0;
 		foliop = (struct folio *)&amp;ps-&gt;page_snapshot;
 		if (!folio_test_large(foliop)) {
@@ -1259,7 +1259,15 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)
 		}
 		foliop = (struct folio *)page;
 	} else {
-		foliop = (struct folio *)(head - 1);
+		/* See compound_head() */
+		if (compound_info_has_mask()) {
+			unsigned long p = (unsigned long)page;
+
+			foliop = (struct folio *)(p &amp; info);
+		} else {
+			foliop = (struct folio *)(info - 1);
+		}
+
 		ps-&gt;idx = folio_page_idx(foliop, page);
 	}
 
-- 
2.51.2


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

From: Kiryl Shutsemau &lt;kas@kernel.org&gt;

If page-&gt;compound_info encodes a mask, it is expected that vmemmap to be
naturally aligned to the maximum folio size.

Add a VM_WARN_ON_ONCE() to check the alignment.

Signed-off-by: Kiryl Shutsemau &lt;kas@kernel.org&gt;
Acked-by: Zi Yan &lt;ziy@nvidia.com&gt;
---
 mm/sparse.c | 5 +++++
 1 file changed, 5 insertions(+)

diff --git a/mm/sparse.c b/mm/sparse.c
index b5b2b6f7041b..dfabe554adf8 100644
--- a/mm/sparse.c
+++ b/mm/sparse.c
@@ -600,6 +600,11 @@ void __init sparse_init(void)
 	BUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));
 	memblocks_present();
 
+	if (compound_info_has_mask()) {
+		VM_WARN_ON_ONCE(!IS_ALIGNED((unsigned long) pfn_to_page(0),
+				    MAX_FOLIO_VMEMMAP_ALIGN));
+	}
+
 	pnum_begin = first_present_section_nr();
 	nid_begin = sparse_early_nid(__nr_to_section(pnum_begin));
 
-- 
2.51.2


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

Currently, the vmemmap for bootmem-allocated gigantic pages is populated
early in hugetlb_vmemmap_init_early(). However, the zone information is
only available after zones are initialized. If it is later discovered
that a page spans multiple zones, the HVO mapping must be undone and
replaced with a normal mapping using vmemmap_undo_hvo().

Defer the actual vmemmap population to hugetlb_vmemmap_init_late(). At
this stage, zones are already initialized, so it can be checked if the
page is valid for HVO before deciding how to populate the vmemmap.

This allows us to remove vmemmap_undo_hvo() and the complex logic
required to rollback HVO mappings.

In hugetlb_vmemmap_init_late(), if HVO population fails or if the zones
are invalid, fall back to a normal vmemmap population.

Postponing population until hugetlb_vmemmap_init_late() also makes zone
information available from within vmemmap_populate_hvo().

Signed-off-by: Kiryl Shutsemau (Meta) &lt;kas@kernel.org&gt;
---
 include/linux/mm.h   |  2 --
 mm/hugetlb_vmemmap.c | 37 +++++++++++++++----------------
 mm/sparse-vmemmap.c  | 53 --------------------------------------------
 3 files changed, 18 insertions(+), 74 deletions(-)

diff --git a/include/linux/mm.h b/include/linux/mm.h
index 7f4dbbb9d783..0e2d45008ff4 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -4484,8 +4484,6 @@ int vmemmap_populate(unsigned long start, unsigned long end, int node,
 		struct vmem_altmap *altmap);
 int vmemmap_populate_hvo(unsigned long start, unsigned long end, int node,
 			 unsigned long headsize);
-int vmemmap_undo_hvo(unsigned long start, unsigned long end, int node,
-		     unsigned long headsize);
 void vmemmap_wrprotect_hvo(unsigned long start, unsigned long end, int node,
 			  unsigned long headsize);
 void vmemmap_populate_print_last(void);
diff --git a/mm/hugetlb_vmemmap.c b/mm/hugetlb_vmemmap.c
index a9280259e12a..935ec5829be9 100644
--- a/mm/hugetlb_vmemmap.c
+++ b/mm/hugetlb_vmemmap.c
@@ -790,7 +790,6 @@ void __init hugetlb_vmemmap_init_early(int nid)
 {
 	unsigned long psize, paddr, section_size;
 	unsigned long ns, i, pnum, pfn, nr_pages;
-	unsigned long start, end;
 	struct huge_bootmem_page *m = NULL;
 	void *map;
 
@@ -808,14 +807,6 @@ void __init hugetlb_vmemmap_init_early(int nid)
 		paddr = virt_to_phys(m);
 		pfn = PHYS_PFN(paddr);
 		map = pfn_to_page(pfn);
-		start = (unsigned long)map;
-		end = start + nr_pages * sizeof(struct page);
-
-		if (vmemmap_populate_hvo(start, end, nid,
-					HUGETLB_VMEMMAP_RESERVE_SIZE) &lt; 0)
-			continue;
-
-		memmap_boot_pages_add(HUGETLB_VMEMMAP_RESERVE_SIZE / PAGE_SIZE);
 
 		pnum = pfn_to_section_nr(pfn);
 		ns = psize / section_size;
@@ -850,28 +841,36 @@ void __init hugetlb_vmemmap_init_late(int nid)
 		h = m-&gt;hstate;
 		pfn = PHYS_PFN(phys);
 		nr_pages = pages_per_huge_page(h);
+		map = pfn_to_page(pfn);
+		start = (unsigned long)map;
+		end = start + nr_pages * sizeof(struct page);
 
 		if (!hugetlb_bootmem_page_zones_valid(nid, m)) {
 			/*
 			 * Oops, the hugetlb page spans multiple zones.
-			 * Remove it from the list, and undo HVO.
+			 * Remove it from the list, and populate it normally.
 			 */
 			list_del(&amp;m-&gt;list);
 
-			map = pfn_to_page(pfn);
-
-			start = (unsigned long)map;
-			end = start + nr_pages * sizeof(struct page);
-
-			vmemmap_undo_hvo(start, end, nid,
-					 HUGETLB_VMEMMAP_RESERVE_SIZE);
-			nr_mmap = end - start - HUGETLB_VMEMMAP_RESERVE_SIZE;
+			vmemmap_populate(start, end, nid, NULL);
+			nr_mmap = end - start;
 			memmap_boot_pages_add(DIV_ROUND_UP(nr_mmap, PAGE_SIZE));
 
 			memblock_phys_free(phys, huge_page_size(h));
 			continue;
-		} else
+		}
+
+		if (vmemmap_populate_hvo(start, end, nid,
+					 HUGETLB_VMEMMAP_RESERVE_SIZE) &lt; 0) {
+			/* Fallback if HVO population fails */
+			vmemmap_populate(start, end, nid, NULL);
+			nr_mmap = end - start;
+		} else {
 			m-&gt;flags |= HUGE_BOOTMEM_ZONES_VALID;
+			nr_mmap = HUGETLB_VMEMMAP_RESERVE_SIZE;
+		}
+
+		memmap_boot_pages_add(DIV_ROUND_UP(nr_mmap, PAGE_SIZE));
 	}
 }
 #endif
diff --git a/mm/sparse-vmemmap.c b/mm/sparse-vmemmap.c
index 37522d6cb398..032a81450838 100644
--- a/mm/sparse-vmemmap.c
+++ b/mm/sparse-vmemmap.c
@@ -302,59 +302,6 @@ int __meminit vmemmap_populate_basepages(unsigned long start, unsigned long end,
 	return vmemmap_populate_range(start, end, node, altmap, -1, 0);
 }
 
-/*
- * Undo populate_hvo, and replace it with a normal base page mapping.
- * Used in memory init in case a HVO mapping needs to be undone.
- *
- * This can happen when it is discovered that a memblock allocated
- * hugetlb page spans multiple zones, which can only be verified
- * after zones have been initialized.
- *
- * We know that:
- * 1) The first @headsize / PAGE_SIZE vmemmap pages were individually
- *    allocated through memblock, and mapped.
- *
- * 2) The rest of the vmemmap pages are mirrors of the last head page.
- */
-int __meminit vmemmap_undo_hvo(unsigned long addr, unsigned long end,
-				      int node, unsigned long headsize)
-{
-	unsigned long maddr, pfn;
-	pte_t *pte;
-	int headpages;
-
-	/*
-	 * Should only be called early in boot, so nothing will
-	 * be accessing these page structures.
-	 */
-	WARN_ON(!early_boot_irqs_disabled);
-
-	headpages = headsize &gt;&gt; PAGE_SHIFT;
-
-	/*
-	 * Clear mirrored mappings for tail page structs.
-	 */
-	for (maddr = addr + headsize; maddr &lt; end; maddr += PAGE_SIZE) {
-		pte = virt_to_kpte(maddr);
-		pte_clear(&amp;init_mm, maddr, pte);
-	}
-
-	/*
-	 * Clear and free mappings for head page and first tail page
-	 * structs.
-	 */
-	for (maddr = addr; headpages-- &gt; 0; maddr += PAGE_SIZE) {
-		pte = virt_to_kpte(maddr);
-		pfn = pte_pfn(ptep_get(pte));
-		pte_clear(&amp;init_mm, maddr, pte);
-		memblock_phys_free(PFN_PHYS(pfn), PAGE_SIZE);
-	}
-
-	flush_tlb_kernel_range(addr, end);
-
-	return vmemmap_populate(addr, end, node, NULL);
-}
-
 /*
  * Write protect the mirrored tail page structs for HVO. This will be
  * called from the hugetlb code when gathering and initializing the
-- 
2.51.2


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv

---

This series removes &quot;fake head pages&quot; from the HugeTLB vmemmap
optimization (HVO) by changing how tail pages encode their relationship
to the head page.

It simplifies compound_head() and page_ref_add_unless(). Both are in the
hot path.

Background
==========

HVO reduces memory overhead by freeing vmemmap pages for HugeTLB pages
and remapping the freed virtual addresses to a single physical page.
Previously, all tail page vmemmap entries were remapped to the first
vmemmap page (containing the head struct page), creating &quot;fake heads&quot; -
tail pages that appear to have PG_head set when accessed through the
deduplicated vmemmap.

This required special handling in compound_head() to detect and work
around fake heads, adding complexity and overhead to a very hot path.

New Approach
============

For architectures/configs where sizeof(struct page) is a power of 2 (the
common case), this series changes how position of the head page is encoded
in the tail pages.

Instead of storing a pointer to the head page, the -&gt;compound_info
(renamed from -&gt;compound_head) now stores a mask.

The mask can be applied to any tail page&#x27;s virtual address to compute
the head page address. Critically, all tail pages of the same order now
have identical compound_info values, regardless of which compound page
they belong to.

The key insight is that all tail pages of the same order now have
identical compound_info values, regardless of which compound page they
belong to.

In v7, these shared tail pages are allocated per-zone. This ensures 
that zone information (stored in page-&gt;flags) is correct even for 
shared tail pages, removing the need for the special-casing in 
page_zonenum() proposed in earlier versions.

To support per-zone shared pages for boot-allocated gigantic pages, 
the vmemmap population is deferred until zones are initialized. This 
simplifies the logic significantly and allows the removal of 
vmemmap_undo_hvo().

Benefits
========

1. Simplified compound_head(): No fake head detection needed, can be
   implemented in a branchless manner.

2. Simplified page_ref_add_unless(): RCU protection removed since there&#x27;s
   no race with fake head remapping.

3. Cleaner architecture: The shared tail pages are truly read-only and
   contain valid tail page metadata.

If sizeof(struct page) is not power-of-2, there are no functional changes.
HVO is not supported in this configuration.

I had hoped to see performance improvement, but my testing thus far has
shown either no change or only a slight improvement within the noise.

Series Organization
===================

Patch 1: Move MAX_FOLIO_ORDER definition to mmzone.h.
Patches 2-4: Refactoring of field names and interfaces.
Patches 5-6: Architecture alignment for LoongArch and RISC-V.
Patch 7: Mask-based compound_head() implementation.
Patch 8: Add memmap alignment checks.
Patch 9: Branchless compound_head() optimization.
Patch 10: Defer vmemmap population for bootmem hugepages.
Patch 11: Refactor vmemmap_walk.
Patch 12: x86 vDSO build fix.
Patch 13: Eliminate fake heads with per-zone shared tail pages.
Patches 14-16: Cleanup of fake head infrastructure.
Patch 17: Documentation update.
Patch 18: Use compound_head() in page_slab().

Changes in v7:
==============

  - Move vmemmap_tails from per-node to per-zone. This ensures tail
    pages have correct zone information.

  - Defer vmemmap population for boot-allocated huge pages to 
    hugetlb_vmemmap_init_late(). This makes zone information available 
    during population and allows removing vmemmap_undo_hvo().

  - Undefine CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP for x86 vdso32 to 
    fix build issues.

  - Remove the patch that modified page_zonenum(), as per-zone 
    shared pages make it unnecessary.

Changes in v6:
==============
  - Simplify memmap alignment check in mm/sparse.c: use VM_BUG_ON()
    (Muchun)

  - Store struct page pointers in vmemmap_tails[] instead of PFNs.
    (Muchun)

  - Fix build error on powerpc due to negative NR_VMEMMAP_TAILS.

Changes in v5:
==============
  - Rebased to mm-everything-2026-01-27-04-35

  - Add arch-specific patches to align vmemmap to maximal folio size
    for riscv and LoongArch architectures.

  - Strengthen the memmap alignment check in mm/sparse.c: use BUG()
    for CONFIG_DEBUG_VM, WARN() otherwise. (Muchun)

  - Use cmpxchg() instead of hugetlb_lock to update vmemmap_tails
    array. (Muchun)

  - Update page_slab().

Changes in v4:
==============
  - Fix build issues due to linux/mmzone.h &lt;-&gt; linux/pgtable.h
    dependency loop by avoiding including linux/pgtable.h into
    linux/mmzone.h

  - Rework vmemmap_remap_alloc() interface. (Muchun)

  - Use &amp;folio-&gt;page instead of folio address for optimization
    target. (Muchun)

Changes in v3:
==============
  - Fixed error recovery path in vmemmap_remap_free() to pass correct start
    address for TLB flush. (Muchun)

  - Wrapped the mask-based compound_info encoding within CONFIG_SPARSEMEM_VMEMMAP
    check via compound_info_has_mask(). For other memory models, alignment
    guarantees are harder to verify. (Muchun)

  - Updated vmemmap_dedup.rst documentation wording: changed &quot;vmemmap_tail
    shared for the struct hstate&quot; to &quot;A single, per-node page frame shared
    among all hugepages of the same size&quot;. (Muchun)

  - Fixed build error with MAX_FOLIO_ORDER expanding to undefined PUD_ORDER
    in certain configurations. (kernel test robot)

Changes in v2:
==============

- Handle boot-allocated huge pages correctly. (Frank)

- Changed from per-hstate vmemmap_tail to per-node vmemmap_tails[] array
  in pglist_data. (Muchun)

- Added spin_lock(&amp;hugetlb_lock) protection in vmemmap_get_tail() to fix
  a race condition where two threads could both allocate tail pages.
  The losing thread now properly frees its allocated page. (Usama)

- Add warning if memmap is not aligned to MAX_FOLIO_SIZE, which is
  required for the mask approach. (Muchun)

- Make page_zonenum() use head page - correctness fix since shared
  tail pages cannot have valid zone information. (Muchun)

- Added &#x27;const&#x27; qualifier to head parameter in set_compound_head() and
  prep_compound_tail(). (Usama)

- Updated commit messages.

Kiryl Shutsemau (16):
  mm: Move MAX_FOLIO_ORDER definition to mmzone.h
  mm: Change the interface of prep_compound_tail()
  mm: Rename the &#x27;compound_head&#x27; field in the &#x27;struct page&#x27; to
    &#x27;compound_info&#x27;
  mm: Move set/clear_compound_head() next to compound_head()
  riscv/mm: Align vmemmap to maximal folio size
  LoongArch/mm: Align vmemmap to maximal folio size
  mm: Rework compound_head() for power-of-2 sizeof(struct page)
  mm/sparse: Check memmap alignment for compound_info_has_mask()
  mm/hugetlb: Refactor code around vmemmap_walk
  mm/hugetlb: Remove fake head pages
  mm: Drop fake head checks
  hugetlb: Remove VMEMMAP_SYNCHRONIZE_RCU
  mm/hugetlb: Remove hugetlb_optimize_vmemmap_key static key
  mm: Remove the branch from compound_head()
  hugetlb: Update vmemmap_dedup.rst
  mm/slab: Use compound_head() in page_slab()

Kiryl Shutsemau (Meta) (2):
  mm/hugetlb: Defer vmemmap population for bootmem hugepages
  x86/vdso: Undefine CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP for vdso32

 .../admin-guide/kdump/vmcoreinfo.rst          |   2 +-
 Documentation/mm/vmemmap_dedup.rst            |  62 ++-
 arch/loongarch/include/asm/pgtable.h          |   3 +-
 arch/riscv/mm/init.c                          |   3 +-
 arch/x86/entry/vdso/vdso32/fake_32bit_build.h |   1 +
 include/linux/mm.h                            |  36 +-
 include/linux/mm_types.h                      |  20 +-
 include/linux/mmzone.h                        |  57 +++
 include/linux/page-flags.h                    | 166 ++++----
 include/linux/page_ref.h                      |   8 +-
 include/linux/types.h                         |   2 +-
 kernel/vmcore_info.c                          |   2 +-
 mm/hugetlb.c                                  |   8 +-
 mm/hugetlb_vmemmap.c                          | 362 +++++++++---------
 mm/internal.h                                 |  18 +-
 mm/mm_init.c                                  |   2 +-
 mm/page_alloc.c                               |   4 +-
 mm/slab.h                                     |   8 +-
 mm/sparse-vmemmap.c                           | 110 +++---
 mm/sparse.c                                   |   5 +
 mm/util.c                                     |  16 +-
 21 files changed, 448 insertions(+), 447 deletions(-)

-- 
2.51.2


_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv</pre>
</details>
<a href="https://lore.kernel.org/r/20260227193030.272078-7-kas@kernel.org" target="_blank" rel="noopener" class="lore-link">View on lore &#8599;</a>
<div class="review-comment-signals">Signals: acknowledged a limitation, explained the condition under which the mask-based encoding works</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Kiryl Shutsemau (author)</span>
<a class="date-chip" href="../2026-02-27_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-27">2026-02-27</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author is apologizing for a mistake in the submission thread, stating that they will resend the patches properly and acknowledging their own error.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">I&#x27;ve screwed up threading in the submission, please ignore.
I will resend properly.

Sorry for this mess. I should have figured out mail by now :/

-- 
  Kiryl Shutsemau / Kirill A. Shutemov

---

I&#x27;ve screwed up threading in the submission, please ignore.
I will resend properly.

Sorry for this mess. I should have figured out mail by now :/

-- 
  Kiryl Shutsemau / Kirill A. Shutemov

_______________________________________________
linux-riscv mailing list
linux-riscv@lists.infradead.org
http://lists.infradead.org/mailman/listinfo/linux-riscv</pre>
</details>
<a href="https://lore.kernel.org/r/aaHzKykadJwN7tF1@thinkstation" target="_blank" rel="noopener" class="lore-link">View on lore &#8599;</a>
<div class="review-comment-signals">Signals: apology, acknowledgment of error</div>
</div>
</div>
</div>

    <footer>LKML Daily Activity Tracker</footer>
    <script>
    // When arriving via a date anchor (e.g. #2026-02-15 from a daily report),
    // scroll the anchor into view after a brief delay so layout is complete.
    (function () {
        var hash = window.location.hash;
        if (!hash) return;
        var target = document.getElementById(hash.slice(1));
        if (!target) return;
        setTimeout(function () {
            target.scrollIntoView({behavior: 'smooth', block: 'start'});
        }, 80);
    })();
    </script>
</body>
</html>