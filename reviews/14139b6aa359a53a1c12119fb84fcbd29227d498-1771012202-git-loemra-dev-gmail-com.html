<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Review Comments: [PATCH v2 2/3] btrfs: inhibit extent buffer writeback to prevent COW amplification</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto,
                         "Helvetica Neue", Arial, sans-serif;
            background: #f5f5f5;
            color: #333;
            line-height: 1.6;
            padding: 20px;
            max-width: 900px;
            margin: 0 auto;
        }
        .home-link { margin-bottom: 12px; display: block; }
        .home-link a { color: #0366d6; text-decoration: none; font-size: 0.9em; }
        .home-link a:hover { text-decoration: underline; }

        h1 { font-size: 1.3em; margin-bottom: 2px; color: #1a1a1a; line-height: 1.3; }

        .lore-link { font-size: 0.85em; margin: 4px 0 6px; display: block; }
        .lore-link a { color: #0366d6; text-decoration: none; }
        .lore-link a:hover { text-decoration: underline; }

        .date-range {
            font-size: 0.8em;
            color: #888;
            margin-bottom: 16px;
        }
        .date-range a { color: #0366d6; text-decoration: none; }
        .date-range a:hover { text-decoration: underline; }

        /* thread-node scroll margin so the card isn't clipped at the top */
        .thread-node { scroll-margin-top: 8px; }

        /* ── Patch summary ──────────────────────────────────────────── */
        .patch-summary-block {
            background: #fff;
            border-radius: 8px;
            padding: 12px 16px;
            margin-bottom: 20px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            border-left: 3px solid #4a90d9;
        }
        .patch-summary-label {
            font-size: 0.72em;
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 0.06em;
            color: #4a90d9;
            margin-bottom: 4px;
        }
        .patch-summary-text {
            font-size: 0.88em;
            color: #444;
            line-height: 1.55;
        }

        /* ── Thread tree ────────────────────────────────────────────── */
        .thread-tree {
            display: flex;
            flex-direction: column;
            gap: 6px;
        }

        /* Depth indentation via left border */
        .thread-node { position: relative; }
        .thread-children {
            margin-left: 20px;
            padding-left: 12px;
            border-left: 2px solid #e0e0e0;
            margin-top: 6px;
            display: flex;
            flex-direction: column;
            gap: 6px;
        }

        /* ── Review comment card ────────────────────────────────────── */
        .review-comment {
            background: #fff;
            border-radius: 6px;
            padding: 10px 14px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            font-size: 0.88em;
        }
        .review-comment-header {
            display: flex;
            flex-wrap: wrap;
            align-items: center;
            gap: 6px;
            margin-bottom: 5px;
        }
        .review-author {
            font-weight: 700;
            color: #1a1a1a;
            font-size: 0.95em;
        }

        /* Date chip — links back to the daily report */
        .date-chip {
            font-size: 0.75em;
            color: #777;
            background: #f0f0f0;
            border-radius: 10px;
            padding: 1px 7px;
            text-decoration: none;
            white-space: nowrap;
        }
        a.date-chip:hover { background: #e0e8f5; color: #0366d6; }

        .badge {
            display: inline-block;
            padding: 1px 8px;
            border-radius: 10px;
            font-size: 0.75em;
            font-weight: 600;
        }
        .inline-review-badge {
            display: inline-block;
            padding: 0 6px;
            border-radius: 8px;
            font-size: 0.78em;
            font-weight: 500;
            background: #e3f2fd;
            color: #1565c0;
        }
        .review-tag-badge {
            display: inline-block;
            padding: 0 6px;
            border-radius: 8px;
            font-size: 0.78em;
            font-weight: 500;
            background: #e8f5e9;
            color: #2e7d32;
        }
        .analysis-source-badge {
            display: inline-block;
            padding: 1px 7px;
            border-radius: 10px;
            font-size: 0.72em;
            font-weight: 600;
            border: 1px solid rgba(0,0,0,0.1);
        }

        .review-comment-text {
            color: #444;
            line-height: 1.55;
            margin-bottom: 4px;
        }
        .review-comment-signals {
            margin-top: 3px;
            font-size: 0.85em;
            color: #aaa;
            font-style: italic;
        }

        /* ── Collapsible raw body ───────────────────────────────────── */
        .raw-body-toggle {
            margin-top: 5px;
            font-size: 0.85em;
        }
        .raw-body-toggle summary {
            cursor: pointer;
            color: #888;
            padding: 2px 0;
            font-weight: 500;
            font-size: 0.9em;
            list-style: none;
        }
        .raw-body-toggle summary::-webkit-details-marker { display: none; }
        .raw-body-toggle summary::before { content: "▶ "; font-size: 0.7em; }
        .raw-body-toggle[open] summary::before { content: "▼ "; }
        .raw-body-toggle summary:hover { color: #555; }
        .raw-body-text {
            white-space: pre-wrap;
            font-size: 0.95em;
            background: #f8f8f8;
            padding: 8px 10px;
            border-radius: 4px;
            max-height: 360px;
            overflow-y: auto;
            margin-top: 4px;
            line-height: 1.5;
            color: #444;
            border: 1px solid #e8e8e8;
        }

        .no-reviews {
            color: #aaa;
            font-size: 0.85em;
            font-style: italic;
            padding: 8px 0;
        }

        footer {
            text-align: center;
            color: #bbb;
            font-size: 0.78em;
            margin-top: 36px;
            padding: 16px;
        }
    </style>
</head>
<body>
    <div class="home-link"><a href="../">&larr; Back to reports</a></div>
    <h1>[PATCH v2 2/3] btrfs: inhibit extent buffer writeback to prevent COW amplification</h1>
    <div class="lore-link"><a href="https://lore.kernel.org/all/14139b6aa359a53a1c12119fb84fcbd29227d498.1771012202.git.loemra.dev@gmail.com/" target="_blank">View on lore.kernel.org &rarr;</a></div>
    <div class="date-range">Active on: <a href="#2026-02-17">2026-02-17</a> &bull; <a href="#2026-02-13">2026-02-13</a></div>
    
    <div class="thread-tree">
<div class="thread-node depth-0" id="2026-02-13">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Leo Martins (author)</span>
<a class="date-chip" href="../2026-02-13_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-13">2026-02-13</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The reviewer, Leo Martins, raised no specific technical concerns or objections to the patch. He acknowledged corrections and changes made in response to previous reviews.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">When memory pressure causes writeback of a recently COW&#x27;d buffer,
btrfs sets BTRFS_HEADER_FLAG_WRITTEN on it. Subsequent
btrfs_search_slot() restarts then see the WRITTEN flag and re-COW
the buffer unnecessarily, causing COW amplification that can exhaust
block reservations and degrade throughput.

Overwriting in place is crash-safe because the committed superblock
does not reference buffers allocated in the current (uncommitted)
transaction, so no on-disk tree points to this block yet.

When should_cow_block() encounters a WRITTEN buffer whose generation
matches the current transaction, instead of requesting a COW, re-dirty
the buffer and re-register its range in the transaction&#x27;s dirty_pages.

Both are necessary because btrfs tracks dirty metadata through two
independent mechanisms. set_extent_buffer_dirty() sets the
EXTENT_BUFFER_DIRTY flag and the buffer_tree xarray PAGECACHE_TAG_DIRTY
mark, which is what background writeback (btree_write_cache_pages) uses
to find and write dirty buffers. The transaction&#x27;s dirty_pages io tree
is a separate structure used by btrfs_write_and_wait_transaction() at
commit time to ensure all buffers allocated during the transaction are
persisted. The dirty_pages range was originally registered in
btrfs_init_new_buffer() when the block was first allocated, but
background writeback may have already written and cleared it.

Keep BTRFS_HEADER_FLAG_WRITTEN set so that btrfs_free_tree_block()
correctly pins the block if it is freed later.

Exclude cases where in-place overwrite is not safe:
 - EXTENT_BUFFER_WRITEBACK: buffer is mid-I/O
 - Zoned devices: require sequential writes
 - Log trees: log blocks are immediately referenced by a committed
   superblock via btrfs_sync_log(), so overwriting could corrupt the
   committed log
 - BTRFS_ROOT_FORCE_COW: snapshot in progress
 - BTRFS_HEADER_FLAG_RELOC: block being relocated

Signed-off-by: Leo Martins &lt;loemra.dev@gmail.com&gt;
---
 fs/btrfs/ctree.c | 53 +++++++++++++++++++++++++++++++++++++++++++++---
 1 file changed, 50 insertions(+), 3 deletions(-)

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 7267b2502665..a345e1be24d8 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -599,9 +599,9 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,
 	return ret;
 }
 
-static inline bool should_cow_block(const struct btrfs_trans_handle *trans,
+static inline bool should_cow_block(struct btrfs_trans_handle *trans,
 				    const struct btrfs_root *root,
-				    const struct extent_buffer *buf)
+				    struct extent_buffer *buf)
 {
 	if (btrfs_is_testing(root-&gt;fs_info))
 		return false;
@@ -621,8 +621,55 @@ static inline bool should_cow_block(const struct btrfs_trans_handle *trans,
 	if (btrfs_header_generation(buf) != trans-&gt;transid)
 		return true;
 
-	if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN))
+	if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {
+		/*
+		 * The buffer was allocated in this transaction and has been
+		 * written back to disk (WRITTEN is set). Normally we&#x27;d COW
+		 * it again, but since the committed superblock doesn&#x27;t
+		 * reference this buffer (it was allocated this transaction),
+		 * we can safely overwrite it in place.
+		 *
+		 * We keep BTRFS_HEADER_FLAG_WRITTEN set. The block has been
+		 * persisted at this bytenr and will be again after the
+		 * in-place update. This is important so that
+		 * btrfs_free_tree_block() correctly pins the block if it is
+		 * freed later (e.g., during tree rebalancing or FORCE_COW).
+		 *
+		 * We re-dirty the buffer to ensure the in-place modifications
+		 * will be written back to disk.
+		 *
+		 * Exclusions:
+		 * - Log trees: log blocks are written and immediately
+		 *   referenced by a committed superblock via
+		 *   btrfs_sync_log(), bypassing the normal transaction
+		 *   commit. Overwriting in place could corrupt the
+		 *   committed log.
+		 * - Zoned devices: require sequential writes
+		 * - FORCE_COW: snapshot in progress
+		 * - RELOC flag: block being relocated
+		 */
+		if (!test_bit(EXTENT_BUFFER_WRITEBACK, &amp;buf-&gt;bflags) &amp;&amp;
+		    !btrfs_is_zoned(root-&gt;fs_info) &amp;&amp;
+		    btrfs_root_id(root) != BTRFS_TREE_LOG_OBJECTID &amp;&amp;
+		    !test_bit(BTRFS_ROOT_FORCE_COW, &amp;root-&gt;state) &amp;&amp;
+		    !btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC)) {
+			/*
+			 * Re-register this block&#x27;s range in the current
+			 * transaction&#x27;s dirty_pages so that
+			 * btrfs_write_and_wait_transaction() writes it.
+			 * The range was originally registered when the block
+			 * was allocated, but that transaction&#x27;s dirty_pages
+			 * may have already been released.
+			 */
+			btrfs_set_extent_bit(&amp;trans-&gt;transaction-&gt;dirty_pages,
+					     buf-&gt;start,
+					     buf-&gt;start + buf-&gt;len - 1,
+					     EXTENT_DIRTY, NULL);
+			set_extent_buffer_dirty(buf);
+			return false;
+		}
 		return true;
+	}
 
 	/* Ensure we can see the FORCE_COW bit. */
 	smp_mb__before_atomic();
-- 
2.47.3



---

Inhibit writeback on COW&#x27;d extent buffers for the lifetime of the
transaction handle, preventing background writeback from setting
BTRFS_HEADER_FLAG_WRITTEN and causing unnecessary re-COW.

COW amplification occurs when background writeback flushes an extent
buffer that a transaction handle is still actively modifying. When
lock_extent_buffer_for_io() transitions a buffer from dirty to
writeback, it sets BTRFS_HEADER_FLAG_WRITTEN, marking the block as
having been persisted to disk at its current bytenr. Once WRITTEN is
set, should_cow_block() must either COW the block again or overwrite
it in place, both of which are unnecessary overhead when the buffer
is still being modified by the same handle that allocated it. By
inhibiting background writeback on actively-used buffers, WRITTEN is
never set while a transaction handle holds a reference to the buffer,
avoiding this overhead entirely.

Add an atomic_t writeback_inhibitors counter to struct extent_buffer,
which fits in an existing 6-byte hole without increasing struct size.
When a buffer is COW&#x27;d in btrfs_force_cow_block(), call
btrfs_inhibit_eb_writeback() to store the eb in the transaction
handle&#x27;s writeback_inhibited_ebs xarray (keyed by eb-&gt;start), take a
reference, and increment writeback_inhibitors. The function handles
dedup (same eb inhibited twice by the same handle) and replacement
(different eb at the same logical address). Allocation failure is
graceful: the buffer simply falls back to the pre-existing behavior
where it may be written back and re-COW&#x27;d.

In lock_extent_buffer_for_io(), when writeback_inhibitors is non-zero
and the writeback mode is WB_SYNC_NONE, skip the buffer. WB_SYNC_NONE
is used by the VM flusher threads for background and periodic
writeback, which are the only paths that cause COW amplification by
opportunistically writing out dirty extent buffers mid-transaction.
Skipping these is safe because the buffers remain dirty in the page
cache and will be written out at transaction commit time.

WB_SYNC_ALL must always proceed regardless of writeback_inhibitors.
This is required for correctness in the fsync path: btrfs_sync_log()
writes log tree blocks via filemap_fdatawrite_range() (WB_SYNC_ALL)
while the transaction handle that inhibited those same blocks is still
active. Without the WB_SYNC_ALL bypass, those inhibited log tree
blocks would be silently skipped, resulting in an incomplete log on
disk and corruption on replay. btrfs_write_and_wait_transaction()
also uses WB_SYNC_ALL via filemap_fdatawrite_range(); for that path,
inhibitors are already cleared beforehand, but the bypass ensures
correctness regardless.

Uninhibit in __btrfs_end_transaction() before atomic_dec(num_writers)
to prevent a race where the committer proceeds while buffers are still
inhibited. Also uninhibit in btrfs_commit_transaction() before writing
and in cleanup_transaction() for the error path.

Signed-off-by: Leo Martins &lt;loemra.dev@gmail.com&gt;
---
 fs/btrfs/ctree.c       |  4 +++
 fs/btrfs/extent_io.c   | 62 +++++++++++++++++++++++++++++++++++++++++-
 fs/btrfs/extent_io.h   |  5 ++++
 fs/btrfs/transaction.c | 19 +++++++++++++
 fs/btrfs/transaction.h |  2 ++
 5 files changed, 91 insertions(+), 1 deletion(-)

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index a345e1be24d8..55187ba59cc0 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -590,6 +590,10 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,
 		btrfs_tree_unlock(buf);
 	free_extent_buffer_stale(buf);
 	btrfs_mark_buffer_dirty(trans, cow);
+
+	/* Inhibit writeback on the COW&#x27;d buffer for this transaction handle */
+	btrfs_inhibit_eb_writeback(trans, cow);
+
 	*cow_ret = cow;
 	return 0;
 
diff --git a/fs/btrfs/extent_io.c b/fs/btrfs/extent_io.c
index dfc17c292217..0c9276cff299 100644
--- a/fs/btrfs/extent_io.c
+++ b/fs/btrfs/extent_io.c
@@ -1940,7 +1940,9 @@ static noinline_for_stack bool lock_extent_buffer_for_io(struct extent_buffer *e
 	 * of time.
 	 */
 	spin_lock(&amp;eb-&gt;refs_lock);
-	if (test_and_clear_bit(EXTENT_BUFFER_DIRTY, &amp;eb-&gt;bflags)) {
+	if ((wbc-&gt;sync_mode == WB_SYNC_ALL ||
+	     atomic_read(&amp;eb-&gt;writeback_inhibitors) == 0) &amp;&amp;
+	    test_and_clear_bit(EXTENT_BUFFER_DIRTY, &amp;eb-&gt;bflags)) {
 		XA_STATE(xas, &amp;fs_info-&gt;buffer_tree, eb-&gt;start &gt;&gt; fs_info-&gt;nodesize_bits);
 		unsigned long flags;
 
@@ -2999,6 +3001,63 @@ static inline void btrfs_release_extent_buffer(struct extent_buffer *eb)
 	kmem_cache_free(extent_buffer_cache, eb);
 }
 
+/*
+ * btrfs_inhibit_eb_writeback - Inhibit writeback on buffer during transaction
+ * @trans: transaction handle that will own the inhibitor
+ * @eb: extent buffer to inhibit writeback on
+ *
+ * Attempts to track this extent buffer in the transaction&#x27;s inhibited set.
+ * If memory allocation fails, the buffer is simply not tracked. It may
+ * be written back and need re-COW, which is the original behavior.
+ * This is acceptable since inhibiting writeback is an optimization.
+ */
+void btrfs_inhibit_eb_writeback(struct btrfs_trans_handle *trans,
+				struct extent_buffer *eb)
+{
+	unsigned long index = eb-&gt;start &gt;&gt; trans-&gt;fs_info-&gt;nodesize_bits;
+	void *old;
+
+	/* Check if already inhibited by this handle */
+	old = xa_load(&amp;trans-&gt;writeback_inhibited_ebs, index);
+	if (old == eb)
+		return;
+
+	refcount_inc(&amp;eb-&gt;refs);	/* Take reference */
+
+	old = xa_store(&amp;trans-&gt;writeback_inhibited_ebs, index, eb, GFP_NOFS);
+	if (xa_is_err(old)) {
+		/* Allocation failed, just skip inhibiting this buffer */
+		free_extent_buffer(eb);
+		return;
+	}
+
+	/* Handle replacement of different eb at same index */
+	if (old &amp;&amp; old != eb) {
+		struct extent_buffer *old_eb = old;
+
+		atomic_dec(&amp;old_eb-&gt;writeback_inhibitors);
+		free_extent_buffer(old_eb);
+	}
+
+	atomic_inc(&amp;eb-&gt;writeback_inhibitors);
+}
+
+/*
+ * btrfs_uninhibit_all_eb_writeback - Uninhibit writeback on all buffers
+ * @trans: transaction handle to clean up
+ */
+void btrfs_uninhibit_all_eb_writeback(struct btrfs_trans_handle *trans)
+{
+	struct extent_buffer *eb;
+	unsigned long index;
+
+	xa_for_each(&amp;trans-&gt;writeback_inhibited_ebs, index, eb) {
+		atomic_dec(&amp;eb-&gt;writeback_inhibitors);
+		free_extent_buffer(eb);
+	}
+	xa_destroy(&amp;trans-&gt;writeback_inhibited_ebs);
+}
+
 static struct extent_buffer *__alloc_extent_buffer(struct btrfs_fs_info *fs_info,
 						   u64 start)
 {
@@ -3009,6 +3068,7 @@ static struct extent_buffer *__alloc_extent_buffer(struct btrfs_fs_info *fs_info
 	eb-&gt;len = fs_info-&gt;nodesize;
 	eb-&gt;fs_info = fs_info;
 	init_rwsem(&amp;eb-&gt;lock);
+	atomic_set(&amp;eb-&gt;writeback_inhibitors, 0);
 
 	btrfs_leak_debug_add_eb(eb);
 
diff --git a/fs/btrfs/extent_io.h b/fs/btrfs/extent_io.h
index 73571d5d3d5a..4b15a5d8bc0f 100644
--- a/fs/btrfs/extent_io.h
+++ b/fs/btrfs/extent_io.h
@@ -102,6 +102,7 @@ struct extent_buffer {
 	/* &gt;= 0 if eb belongs to a log tree, -1 otherwise */
 	s8 log_index;
 	u8 folio_shift;
+	atomic_t writeback_inhibitors;	/* inhibits writeback when &gt; 0 */
 	struct rcu_head rcu_head;
 
 	struct rw_semaphore lock;
@@ -381,4 +382,8 @@ void btrfs_extent_buffer_leak_debug_check(struct btrfs_fs_info *fs_info);
 #define btrfs_extent_buffer_leak_debug_check(fs_info)	do {} while (0)
 #endif
 
+void btrfs_inhibit_eb_writeback(struct btrfs_trans_handle *trans,
+			       struct extent_buffer *eb);
+void btrfs_uninhibit_all_eb_writeback(struct btrfs_trans_handle *trans);
+
 #endif
diff --git a/fs/btrfs/transaction.c b/fs/btrfs/transaction.c
index f4cc9e1a1b93..a9a22629b49d 100644
--- a/fs/btrfs/transaction.c
+++ b/fs/btrfs/transaction.c
@@ -15,6 +15,7 @@
 #include &quot;misc.h&quot;
 #include &quot;ctree.h&quot;
 #include &quot;disk-io.h&quot;
+#include &quot;extent_io.h&quot;
 #include &quot;transaction.h&quot;
 #include &quot;locking.h&quot;
 #include &quot;tree-log.h&quot;
@@ -688,6 +689,8 @@ start_transaction(struct btrfs_root *root, unsigned int num_items,
 		goto alloc_fail;
 	}
 
+	xa_init(&amp;h-&gt;writeback_inhibited_ebs);
+
 	/*
 	 * If we are JOIN_NOLOCK we&#x27;re already committing a transaction and
 	 * waiting on this guy, so we don&#x27;t need to do the sb_start_intwrite
@@ -1083,6 +1086,13 @@ static int __btrfs_end_transaction(struct btrfs_trans_handle *trans,
 	if (trans-&gt;type &amp; __TRANS_FREEZABLE)
 		sb_end_intwrite(info-&gt;sb);
 
+	/*
+	 * Uninhibit extent buffer writeback before decrementing num_writers,
+	 * since the decrement wakes the committing thread which needs all
+	 * buffers uninhibited to write them to disk.
+	 */
+	btrfs_uninhibit_all_eb_writeback(trans);
+
 	WARN_ON(cur_trans != info-&gt;running_transaction);
 	WARN_ON(atomic_read(&amp;cur_trans-&gt;num_writers) &lt; 1);
 	atomic_dec(&amp;cur_trans-&gt;num_writers);
@@ -2110,6 +2120,7 @@ static void cleanup_transaction(struct btrfs_trans_handle *trans, int err)
 	if (!test_bit(BTRFS_FS_RELOC_RUNNING, &amp;fs_info-&gt;flags))
 		btrfs_scrub_cancel(fs_info);
 
+	btrfs_uninhibit_all_eb_writeback(trans);
 	kmem_cache_free(btrfs_trans_handle_cachep, trans);
 }
 
@@ -2556,6 +2567,14 @@ int btrfs_commit_transaction(struct btrfs_trans_handle *trans)
 	    fs_info-&gt;cleaner_kthread)
 		wake_up_process(fs_info-&gt;cleaner_kthread);
 
+	/*
+	 * Uninhibit writeback on all extent buffers inhibited during this
+	 * transaction before writing them to disk. Inhibiting prevented
+	 * writeback while the transaction was building, but now we need
+	 * them written.
+	 */
+	btrfs_uninhibit_all_eb_writeback(trans);
+
 	ret = btrfs_write_and_wait_transaction(trans);
 	if (unlikely(ret)) {
 		btrfs_err(fs_info, &quot;error while writing out transaction: %d&quot;, ret);
diff --git a/fs/btrfs/transaction.h b/fs/btrfs/transaction.h
index 18ef069197e5..f0d12c16d796 100644
--- a/fs/btrfs/transaction.h
+++ b/fs/btrfs/transaction.h
@@ -12,6 +12,7 @@
 #include &lt;linux/time64.h&gt;
 #include &lt;linux/mutex.h&gt;
 #include &lt;linux/wait.h&gt;
+#include &lt;linux/xarray.h&gt;
 #include &quot;btrfs_inode.h&quot;
 #include &quot;delayed-ref.h&quot;
 
@@ -162,6 +163,7 @@ struct btrfs_trans_handle {
 	struct btrfs_fs_info *fs_info;
 	struct list_head new_bgs;
 	struct btrfs_block_rsv delayed_rsv;
+	struct xarray writeback_inhibited_ebs;	/* ebs with writeback inhibited */
 };
 
 /*
-- 
2.47.3



---

Add a btrfs_search_slot_stats tracepoint to btrfs_search_slot() for
measuring COW amplification.

The tracepoint fires when a search with at least one COW completes,
recording the root, total cow_count, restart_count, and return value.
cow_count and restart_count per search_slot call are useful metrics
for tracking COW amplification.

Signed-off-by: Leo Martins &lt;loemra.dev@gmail.com&gt;
---
 fs/btrfs/ctree.c             | 15 +++++++++++++--
 include/trace/events/btrfs.h | 26 ++++++++++++++++++++++++++
 2 files changed, 39 insertions(+), 2 deletions(-)

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 55187ba59cc0..1971d7bb5f60 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2069,6 +2069,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 	u8 lowest_level = 0;
 	int min_write_lock_level;
 	int prev_cmp;
+	int cow_count = 0;
+	int restart_count = 0;
 
 	if (!root)
 		return -EINVAL;
@@ -2157,6 +2159,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			    p-&gt;nodes[level + 1])) {
 				write_lock_level = level + 1;
 				btrfs_release_path(p);
+				restart_count++;
 				goto again;
 			}
 
@@ -2172,6 +2175,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 				ret = ret2;
 				goto done;
 			}
+			cow_count++;
 		}
 cow_done:
 		p-&gt;nodes[level] = b;
@@ -2219,8 +2223,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		p-&gt;slots[level] = slot;
 		ret2 = setup_nodes_for_search(trans, root, p, b, level, ins_len,
 					      &amp;write_lock_level);
-		if (ret2 == -EAGAIN)
+		if (ret2 == -EAGAIN) {
+			restart_count++;
 			goto again;
+		}
 		if (ret2) {
 			ret = ret2;
 			goto done;
@@ -2236,6 +2242,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		if (slot == 0 &amp;&amp; ins_len &amp;&amp; write_lock_level &lt; level + 1) {
 			write_lock_level = level + 1;
 			btrfs_release_path(p);
+			restart_count++;
 			goto again;
 		}
 
@@ -2249,8 +2256,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		}
 
 		ret2 = read_block_for_search(root, p, &amp;b, slot, key);
-		if (ret2 == -EAGAIN &amp;&amp; !p-&gt;nowait)
+		if (ret2 == -EAGAIN &amp;&amp; !p-&gt;nowait) {
+			restart_count++;
 			goto again;
+		}
 		if (ret2) {
 			ret = ret2;
 			goto done;
@@ -2281,6 +2290,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 	}
 	ret = 1;
 done:
+	if (cow_count &gt; 0)
+		trace_btrfs_search_slot_stats(root, cow_count, restart_count, ret);
 	if (ret &lt; 0 &amp;&amp; !p-&gt;skip_release_on_error)
 		btrfs_release_path(p);
 
diff --git a/include/trace/events/btrfs.h b/include/trace/events/btrfs.h
index 125bdc166bfe..b8934938a087 100644
--- a/include/trace/events/btrfs.h
+++ b/include/trace/events/btrfs.h
@@ -1110,6 +1110,32 @@ TRACE_EVENT(btrfs_cow_block,
 		  __entry-&gt;cow_level)
 );
 
+TRACE_EVENT(btrfs_search_slot_stats,
+
+	TP_PROTO(const struct btrfs_root *root,
+		 int cow_count, int restart_count, int ret),
+
+	TP_ARGS(root, cow_count, restart_count, ret),
+
+	TP_STRUCT__entry_btrfs(
+		__field(	u64,	root_objectid		)
+		__field(	int,	cow_count		)
+		__field(	int,	restart_count		)
+		__field(	int,	ret			)
+	),
+
+	TP_fast_assign_btrfs(root-&gt;fs_info,
+		__entry-&gt;root_objectid	= btrfs_root_id(root);
+		__entry-&gt;cow_count	= cow_count;
+		__entry-&gt;restart_count	= restart_count;
+		__entry-&gt;ret		= ret;
+	),
+
+	TP_printk_btrfs(&quot;root=%llu(%s) cow_count=%d restarts=%d ret=%d&quot;,
+		  show_root_type(__entry-&gt;root_objectid),
+		  __entry-&gt;cow_count, __entry-&gt;restart_count, __entry-&gt;ret)
+);
+
 TRACE_EVENT(btrfs_space_reservation,
 
 	TP_PROTO(const struct btrfs_fs_info *fs_info, const char *type, u64 val,
-- 
2.47.3



---

On Mon, 16 Feb 2026 12:18:48 +0000 Filipe Manana &lt;fdmanana@kernel.org&gt; wrote:

&gt; On Fri, Feb 13, 2026 at 8:38\u202fPM Leo Martins &lt;loemra.dev@gmail.com&gt; wrote:
&gt; &gt;
&gt; &gt; When memory pressure causes writeback of a recently COW&#x27;d buffer,
&gt; &gt; btrfs sets BTRFS_HEADER_FLAG_WRITTEN on it. Subsequent
&gt; &gt; btrfs_search_slot() restarts then see the WRITTEN flag and re-COW
&gt; &gt; the buffer unnecessarily, causing COW amplification that can exhaust
&gt; &gt; block reservations and degrade throughput.
&gt; &gt;
&gt; &gt; Overwriting in place is crash-safe because the committed superblock
&gt; &gt; does not reference buffers allocated in the current (uncommitted)
&gt; &gt; transaction, so no on-disk tree points to this block yet.
&gt; &gt;
&gt; &gt; When should_cow_block() encounters a WRITTEN buffer whose generation
&gt; &gt; matches the current transaction, instead of requesting a COW, re-dirty
&gt; &gt; the buffer and re-register its range in the transaction&#x27;s dirty_pages.
&gt; &gt;
&gt; &gt; Both are necessary because btrfs tracks dirty metadata through two
&gt; &gt; independent mechanisms. set_extent_buffer_dirty() sets the
&gt; &gt; EXTENT_BUFFER_DIRTY flag and the buffer_tree xarray PAGECACHE_TAG_DIRTY
&gt; &gt; mark, which is what background writeback (btree_write_cache_pages) uses
&gt; &gt; to find and write dirty buffers. The transaction&#x27;s dirty_pages io tree
&gt; &gt; is a separate structure used by btrfs_write_and_wait_transaction() at
&gt; &gt; commit time to ensure all buffers allocated during the transaction are
&gt; &gt; persisted. The dirty_pages range was originally registered in
&gt; &gt; btrfs_init_new_buffer() when the block was first allocated, but
&gt; &gt; background writeback may have already written and cleared it.
&gt; 
&gt; This is not quite correct, the dirty_pages range is never cleared on
&gt; background writeback.
&gt; We only clear it during a transaction commit, in
&gt; btrfs_write_and_wait_transaction().
&gt; 
&gt; Normally we shouldn&#x27;t care about setting the range again in
&gt; dirty_pages, because after
&gt; we call  btrfs_write_and_wait_transaction(), no more COW should be
&gt; possible using this
&gt; transaction (which is in the unblocked state, so any new COW attempt
&gt; will be in another transaction).
&gt; 
&gt; The exception is if we have snapshots to create and qgroups are
&gt; enabled, since in qgroup_account_snapshot() we
&gt; call btrfs_write_and_wait_transaction() and after that we can get more
&gt; COW, due to all the stuff we need to do to
&gt; create a snapshot, before we get to the final call to
&gt; btrfs_write_and_wait_transaction() right before we write the
&gt; super blocks in btrfs_commit_transaction().

Got it, thanks for the correction. Updated in v3.

Thanks,
Leo

&gt; 
&gt; &gt;
&gt; &gt; Keep BTRFS_HEADER_FLAG_WRITTEN set so that btrfs_free_tree_block()
&gt; &gt; correctly pins the block if it is freed later.
&gt; &gt;
&gt; &gt; Exclude cases where in-place overwrite is not safe:
&gt; &gt;  - EXTENT_BUFFER_WRITEBACK: buffer is mid-I/O
&gt; &gt;  - Zoned devices: require sequential writes
&gt; &gt;  - Log trees: log blocks are immediately referenced by a committed
&gt; &gt;    superblock via btrfs_sync_log(), so overwriting could corrupt the
&gt; &gt;    committed log
&gt; &gt;  - BTRFS_ROOT_FORCE_COW: snapshot in progress
&gt; &gt;  - BTRFS_HEADER_FLAG_RELOC: block being relocated
&gt; &gt;
&gt; &gt; Signed-off-by: Leo Martins &lt;loemra.dev@gmail.com&gt;
&gt; &gt; ---
&gt; &gt;  fs/btrfs/ctree.c | 53 +++++++++++++++++++++++++++++++++++++++++++++---
&gt; &gt;  1 file changed, 50 insertions(+), 3 deletions(-)
&gt; &gt;
&gt; &gt; diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
&gt; &gt; index 7267b2502665..a345e1be24d8 100644
&gt; &gt; --- a/fs/btrfs/ctree.c
&gt; &gt; +++ b/fs/btrfs/ctree.c
&gt; &gt; @@ -599,9 +599,9 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,
&gt; &gt;         return ret;
&gt; &gt;  }
&gt; &gt;
&gt; &gt; -static inline bool should_cow_block(const struct btrfs_trans_handle *trans,
&gt; &gt; +static inline bool should_cow_block(struct btrfs_trans_handle *trans,
&gt; &gt;                                     const struct btrfs_root *root,
&gt; &gt; -                                   const struct extent_buffer *buf)
&gt; &gt; +                                   struct extent_buffer *buf)
&gt; &gt;  {
&gt; &gt;         if (btrfs_is_testing(root-&gt;fs_info))
&gt; &gt;                 return false;
&gt; &gt; @@ -621,8 +621,55 @@ static inline bool should_cow_block(const struct btrfs_trans_handle *trans,
&gt; &gt;         if (btrfs_header_generation(buf) != trans-&gt;transid)
&gt; &gt;                 return true;
&gt; &gt;
&gt; &gt; -       if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN))
&gt; &gt; +       if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {
&gt; &gt; +               /*
&gt; &gt; +                * The buffer was allocated in this transaction and has been
&gt; &gt; +                * written back to disk (WRITTEN is set). Normally we&#x27;d COW
&gt; &gt; +                * it again, but since the committed superblock doesn&#x27;t
&gt; &gt; +                * reference this buffer (it was allocated this transaction),
&gt; 
&gt; Missing an &quot;in&quot; before &quot;this transaction&quot;.
&gt; 
&gt; &gt; +                * we can safely overwrite it in place.
&gt; &gt; +                *
&gt; &gt; +                * We keep BTRFS_HEADER_FLAG_WRITTEN set. The block has been
&gt; &gt; +                * persisted at this bytenr and will be again after the
&gt; &gt; +                * in-place update. This is important so that
&gt; &gt; +                * btrfs_free_tree_block() correctly pins the block if it is
&gt; &gt; +                * freed later (e.g., during tree rebalancing or FORCE_COW).
&gt; &gt; +                *
&gt; &gt; +                * We re-dirty the buffer to ensure the in-place modifications
&gt; &gt; +                * will be written back to disk.
&gt; &gt; +                *
&gt; &gt; +                * Exclusions:
&gt; &gt; +                * - Log trees: log blocks are written and immediately
&gt; &gt; +                *   referenced by a committed superblock via
&gt; &gt; +                *   btrfs_sync_log(), bypassing the normal transaction
&gt; &gt; +                *   commit. Overwriting in place could corrupt the
&gt; &gt; +                *   committed log.
&gt; &gt; +                * - Zoned devices: require sequential writes
&gt; &gt; +                * - FORCE_COW: snapshot in progress
&gt; &gt; +                * - RELOC flag: block being relocated
&gt; &gt; +                */
&gt; &gt; +               if (!test_bit(EXTENT_BUFFER_WRITEBACK, &amp;buf-&gt;bflags) &amp;&amp;
&gt; &gt; +                   !btrfs_is_zoned(root-&gt;fs_info) &amp;&amp;
&gt; &gt; +                   btrfs_root_id(root) != BTRFS_TREE_LOG_OBJECTID &amp;&amp;
&gt; &gt; +                   !test_bit(BTRFS_ROOT_FORCE_COW, &amp;root-&gt;state) &amp;&amp;
&gt; 
&gt; We need a  smp_mb__before_atomic() before checking FORCE_COW, see the
&gt; existing code below.
&gt; 
&gt; &gt; +                   !btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC)) {
&gt; &gt; +                       /*
&gt; &gt; +                        * Re-register this block&#x27;s range in the current
&gt; &gt; +                        * transaction&#x27;s dirty_pages so that
&gt; &gt; +                        * btrfs_write_and_wait_transaction() writes it.
&gt; &gt; +                        * The range was originally registered when the block
&gt; &gt; +                        * was allocated, but that transaction&#x27;s dirty_pages
&gt; &gt; +                        * may have already been released.
&gt; 
&gt; I think it&#x27;s worth adding something like: &quot;... already been released
&gt; if we are in a transaction that creates snapshots and we have qgroups
&gt; enabled.&quot;
&gt; 
&gt; Otherwise it looks good, thanks!
&gt; 
&gt; &gt; +                        */
&gt; &gt; +                       btrfs_set_extent_bit(&amp;trans-&gt;transaction-&gt;dirty_pages,
&gt; &gt; +                                            buf-&gt;start,
&gt; &gt; +                                            buf-&gt;start + buf-&gt;len - 1,
&gt; &gt; +                                            EXTENT_DIRTY, NULL);
&gt; &gt; +                       set_extent_buffer_dirty(buf);
&gt; &gt; +                       return false;
&gt; &gt; +               }
&gt; &gt;                 return true;
&gt; &gt; +       }
&gt; &gt;
&gt; &gt;         /* Ensure we can see the FORCE_COW bit. */
&gt; &gt;         smp_mb__before_atomic();
&gt; &gt; --
&gt; &gt; 2.47.3
&gt; &gt;
&gt; &gt;


---

On Sat, 14 Feb 2026 09:25:03 +0800 Sun YangKai &lt;sunk67188@gmail.com&gt; wrote:

&gt; Thanks for your working on this and I&#x27;ve expecting this for a long time :)

Thanks for the review!

&gt; 
&gt; On 2026/2/14 04:30, Leo Martins wrote:
&gt; &gt; When memory pressure causes writeback of a recently COW&#x27;d buffer,
&gt; &gt; btrfs sets BTRFS_HEADER_FLAG_WRITTEN on it. Subsequent
&gt; &gt; btrfs_search_slot() restarts then see the WRITTEN flag and re-COW
&gt; &gt; the buffer unnecessarily, causing COW amplification that can exhaust
&gt; &gt; block reservations and degrade throughput.
&gt; &gt; 
&gt; &gt; Overwriting in place is crash-safe because the committed superblock
&gt; &gt; does not reference buffers allocated in the current (uncommitted)
&gt; &gt; transaction, so no on-disk tree points to this block yet.
&gt; &gt; 
&gt; &gt; When should_cow_block() encounters a WRITTEN buffer whose generation
&gt; &gt; matches the current transaction, instead of requesting a COW, re-dirty
&gt; &gt; the buffer and re-register its range in the transaction&#x27;s dirty_pages.
&gt; &gt; 
&gt; &gt; Both are necessary because btrfs tracks dirty metadata through two
&gt; &gt; independent mechanisms. set_extent_buffer_dirty() sets the
&gt; &gt; EXTENT_BUFFER_DIRTY flag and the buffer_tree xarray PAGECACHE_TAG_DIRTY
&gt; &gt; mark, which is what background writeback (btree_write_cache_pages) uses
&gt; &gt; to find and write dirty buffers. The transaction&#x27;s dirty_pages io tree
&gt; &gt; is a separate structure used by btrfs_write_and_wait_transaction() at
&gt; &gt; commit time to ensure all buffers allocated during the transaction are
&gt; &gt; persisted. The dirty_pages range was originally registered in
&gt; &gt; btrfs_init_new_buffer() when the block was first allocated, but
&gt; &gt; background writeback may have already written and cleared it.
&gt; &gt; 
&gt; &gt; Keep BTRFS_HEADER_FLAG_WRITTEN set so that btrfs_free_tree_block()
&gt; &gt; correctly pins the block if it is freed later.
&gt; &gt; 
&gt; &gt; Exclude cases where in-place overwrite is not safe:
&gt; &gt;   - EXTENT_BUFFER_WRITEBACK: buffer is mid-I/O
&gt; &gt;   - Zoned devices: require sequential writes
&gt; &gt;   - Log trees: log blocks are immediately referenced by a committed
&gt; &gt;     superblock via btrfs_sync_log(), so overwriting could corrupt the
&gt; &gt;     committed log
&gt; &gt;   - BTRFS_ROOT_FORCE_COW: snapshot in progress
&gt; &gt;   - BTRFS_HEADER_FLAG_RELOC: block being relocated
&gt; &gt; 
&gt; &gt; Signed-off-by: Leo Martins &lt;loemra.dev@gmail.com&gt;
&gt; &gt; ---
&gt; &gt;   fs/btrfs/ctree.c | 53 +++++++++++++++++++++++++++++++++++++++++++++---
&gt; &gt;   1 file changed, 50 insertions(+), 3 deletions(-)
&gt; &gt; 
&gt; &gt; diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
&gt; &gt; index 7267b2502665..a345e1be24d8 100644
&gt; &gt; --- a/fs/btrfs/ctree.c
&gt; &gt; +++ b/fs/btrfs/ctree.c
&gt; &gt; @@ -599,9 +599,9 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,
&gt; &gt;   	return ret;
&gt; &gt;   }
&gt; &gt;   
&gt; &gt; -static inline bool should_cow_block(const struct btrfs_trans_handle *trans,
&gt; &gt; +static inline bool should_cow_block(struct btrfs_trans_handle *trans,
&gt; &gt;   				    const struct btrfs_root *root,
&gt; &gt; -				    const struct extent_buffer *buf)
&gt; &gt; +				    struct extent_buffer *buf)
&gt; &gt;   {
&gt; &gt;   	if (btrfs_is_testing(root-&gt;fs_info))
&gt; &gt;   		return false;
&gt; &gt; @@ -621,8 +621,55 @@ static inline bool should_cow_block(const struct btrfs_trans_handle *trans,
&gt; &gt;   	if (btrfs_header_generation(buf) != trans-&gt;transid)
&gt; &gt;   		return true;
&gt; &gt;   
&gt; &gt; -	if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN))
&gt; &gt; +	if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {
&gt; &gt; +		/*
&gt; &gt; +		 * The buffer was allocated in this transaction and has been
&gt; &gt; +		 * written back to disk (WRITTEN is set). Normally we&#x27;d COW
&gt; &gt; +		 * it again, but since the committed superblock doesn&#x27;t
&gt; &gt; +		 * reference this buffer (it was allocated this transaction),
&gt; &gt; +		 * we can safely overwrite it in place.
&gt; &gt; +		 *
&gt; &gt; +		 * We keep BTRFS_HEADER_FLAG_WRITTEN set. The block has been
&gt; &gt; +		 * persisted at this bytenr and will be again after the
&gt; &gt; +		 * in-place update. This is important so that
&gt; &gt; +		 * btrfs_free_tree_block() correctly pins the block if it is
&gt; &gt; +		 * freed later (e.g., during tree rebalancing or FORCE_COW).
&gt; &gt; +		 *
&gt; &gt; +		 * We re-dirty the buffer to ensure the in-place modifications
&gt; &gt; +		 * will be written back to disk.
&gt; &gt; +		 *
&gt; &gt; +		 * Exclusions:
&gt; &gt; +		 * - Log trees: log blocks are written and immediately
&gt; &gt; +		 *   referenced by a committed superblock via
&gt; &gt; +		 *   btrfs_sync_log(), bypassing the normal transaction
&gt; &gt; +		 *   commit. Overwriting in place could corrupt the
&gt; &gt; +		 *   committed log.
&gt; &gt; +		 * - Zoned devices: require sequential writes
&gt; &gt; +		 * - FORCE_COW: snapshot in progress
&gt; &gt; +		 * - RELOC flag: block being relocated
&gt; &gt; +		 */
&gt; &gt; +		if (!test_bit(EXTENT_BUFFER_WRITEBACK, &amp;buf-&gt;bflags) &amp;&amp;
&gt; &gt; +		    !btrfs_is_zoned(root-&gt;fs_info) &amp;&amp;
&gt; &gt; +		    btrfs_root_id(root) != BTRFS_TREE_LOG_OBJECTID &amp;&amp;
&gt; &gt; +		    !test_bit(BTRFS_ROOT_FORCE_COW, &amp;root-&gt;state) &amp;&amp;
&gt; it seems we need smp_mb__before_atomic() to see the FORCE_COW bit?

Good call, fixed in v3.

&gt; &gt; +		    !btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC)) {
&gt; &gt; +			/*
&gt; &gt; +			 * Re-register this block&#x27;s range in the current
&gt; &gt; +			 * transaction&#x27;s dirty_pages so that
&gt; &gt; +			 * btrfs_write_and_wait_transaction() writes it.
&gt; &gt; +			 * The range was originally registered when the block
&gt; &gt; +			 * was allocated, but that transaction&#x27;s dirty_pages
&gt; &gt; +			 * may have already been released.
&gt; &gt; +			 */
&gt; &gt; +			btrfs_set_extent_bit(&amp;trans-&gt;transaction-&gt;dirty_pages,
&gt; &gt; +					     buf-&gt;start,
&gt; &gt; +					     buf-&gt;start + buf-&gt;len - 1,
&gt; &gt; +					     EXTENT_DIRTY, NULL);
&gt; &gt; +			set_extent_buffer_dirty(buf);
&gt; why use set_extent_buffer_dirty() instead of btrfs_mark_buffer_dirty()? 
&gt; I don&#x27;t see any other callers doing this.

btrfs_mark_buffer_dirty() calls btrfs_assert_tree_write_locked(buf),
but should_cow_block() may be called from btrfs_search_slot() when the
buffer only holds a read lock (root node acquired with BTRFS_READ_LOCK
in btrfs_search_slot_get_root()). Added a comment explaining this in 
v3.

&gt; &gt; +			return false;
&gt; &gt; +		}
&gt; &gt;   		return true;
&gt; &gt; +	}
&gt; &gt;   
&gt; &gt;   	/* Ensure we can see the FORCE_COW bit. */
&gt; &gt;   	smp_mb__before_atomic();
&gt; 
&gt; And I wonder if we could have something more readable like this:
&gt; 
&gt; 	if (btrfs_header_generation(buf) != trans-&gt;transid)
&gt; 		return true;
&gt; 
&gt; 	if (test_bit(EXTENT_BUFFER_WRITEBACK, &amp;buf-&gt;bflags))
&gt; 		return true;
&gt; 
&gt; 	if (btrfs_root_id(root) != BTRFS_TREE_RELOC_OBJECTID &amp;&amp;
&gt; 	    btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC))
&gt; 		return true;
&gt; 
&gt; 	/* Ensure we can see the FORCE_COW bit. */
&gt; 	smp_mb__before_atomic();
&gt; 	if (test_bit(BTRFS_ROOT_FORCE_COW, &amp;root-&gt;state))
&gt; 		return true;
&gt; 
&gt; 	if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {
&gt; 		if (btrfs_root_id(root) == BTRFS_TREE_LOG_OBJECTID ||
&gt; 		    btrfs_is_zoned(root-&gt;fs_info))
&gt; 				return true;
&gt; 		btrfs_set_extent_bit(&amp;trans-&gt;transaction-&gt;dirty_pages,
&gt; 				     buf-&gt;start,
&gt; 				     buf-&gt;start + buf-&gt;len - 1,
&gt; 				     EXTENT_DIRTY, NULL);
&gt; 		btrfs_mark_buffer_dirty(trans, buf);
&gt; 	}
&gt; 
&gt; 	return false;

Updated in v3!

&gt; 
&gt; Thanks,
&gt; Sun YangKai


---

On Mon, 16 Feb 2026 12:40:04 +0000 Filipe Manana &lt;fdmanana@kernel.org&gt; wrote:

&gt; On Fri, Feb 13, 2026 at 8:37\u202fPM Leo Martins &lt;loemra.dev@gmail.com&gt; wrote:
&gt; &gt;
&gt; &gt; Add a btrfs_search_slot_stats tracepoint to btrfs_search_slot() for
&gt; &gt; measuring COW amplification.
&gt; &gt;
&gt; &gt; The tracepoint fires when a search with at least one COW completes,
&gt; &gt; recording the root, total cow_count, restart_count, and return value.
&gt; &gt; cow_count and restart_count per search_slot call are useful metrics
&gt; &gt; for tracking COW amplification.
&gt; &gt;
&gt; &gt; Signed-off-by: Leo Martins &lt;loemra.dev@gmail.com&gt;
&gt; &gt; ---
&gt; &gt;  fs/btrfs/ctree.c             | 15 +++++++++++++--
&gt; &gt;  include/trace/events/btrfs.h | 26 ++++++++++++++++++++++++++
&gt; &gt;  2 files changed, 39 insertions(+), 2 deletions(-)
&gt; &gt;
&gt; &gt; diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
&gt; &gt; index 55187ba59cc0..1971d7bb5f60 100644
&gt; &gt; --- a/fs/btrfs/ctree.c
&gt; &gt; +++ b/fs/btrfs/ctree.c
&gt; &gt; @@ -2069,6 +2069,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt; &gt;         u8 lowest_level = 0;
&gt; &gt;         int min_write_lock_level;
&gt; &gt;         int prev_cmp;
&gt; &gt; +       int cow_count = 0;
&gt; &gt; +       int restart_count = 0;
&gt; &gt;
&gt; &gt;         if (!root)
&gt; &gt;                 return -EINVAL;
&gt; &gt; @@ -2157,6 +2159,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt; &gt;                             p-&gt;nodes[level + 1])) {
&gt; &gt;                                 write_lock_level = level + 1;
&gt; &gt;                                 btrfs_release_path(p);
&gt; &gt; +                               restart_count++;
&gt; &gt;                                 goto again;
&gt; &gt;                         }
&gt; &gt;
&gt; &gt; @@ -2172,6 +2175,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt; &gt;                                 ret = ret2;
&gt; &gt;                                 goto done;
&gt; &gt;                         }
&gt; &gt; +                       cow_count++;
&gt; &gt;                 }
&gt; &gt;  cow_done:
&gt; &gt;                 p-&gt;nodes[level] = b;
&gt; &gt; @@ -2219,8 +2223,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt; &gt;                 p-&gt;slots[level] = slot;
&gt; &gt;                 ret2 = setup_nodes_for_search(trans, root, p, b, level, ins_len,
&gt; &gt;                                               &amp;write_lock_level);
&gt; &gt; -               if (ret2 == -EAGAIN)
&gt; &gt; +               if (ret2 == -EAGAIN) {
&gt; &gt; +                       restart_count++;
&gt; &gt;                         goto again;
&gt; &gt; +               }
&gt; &gt;                 if (ret2) {
&gt; &gt;                         ret = ret2;
&gt; &gt;                         goto done;
&gt; &gt; @@ -2236,6 +2242,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt; &gt;                 if (slot == 0 &amp;&amp; ins_len &amp;&amp; write_lock_level &lt; level + 1) {
&gt; &gt;                         write_lock_level = level + 1;
&gt; &gt;                         btrfs_release_path(p);
&gt; &gt; +                       restart_count++;
&gt; &gt;                         goto again;
&gt; &gt;                 }
&gt; &gt;
&gt; &gt; @@ -2249,8 +2256,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt; &gt;                 }
&gt; &gt;
&gt; &gt;                 ret2 = read_block_for_search(root, p, &amp;b, slot, key);
&gt; &gt; -               if (ret2 == -EAGAIN &amp;&amp; !p-&gt;nowait)
&gt; &gt; +               if (ret2 == -EAGAIN &amp;&amp; !p-&gt;nowait) {
&gt; &gt; +                       restart_count++;
&gt; &gt;                         goto again;
&gt; &gt; +               }
&gt; &gt;                 if (ret2) {
&gt; &gt;                         ret = ret2;
&gt; &gt;                         goto done;
&gt; &gt; @@ -2281,6 +2290,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt; &gt;         }
&gt; &gt;         ret = 1;
&gt; &gt;  done:
&gt; &gt; +       if (cow_count &gt; 0)
&gt; &gt; +               trace_btrfs_search_slot_stats(root, cow_count, restart_count, ret);
&gt; 
&gt; So I find this way too specific, plus even if trace points are
&gt; disabled we have the overhead of the counters (and inside critical
&gt; sections).
&gt; 
&gt; We already have a tracepoint for COW, trace_btrfs_cow_block(), and we
&gt; could have one just for the retry thing, maybe naming it like
&gt; trace_btrfs_search_slot_restart() or something.
&gt; So we could use those two tracepoints to measure things (bpftrace
&gt; scripts could easily report a count of each trace point and such),
&gt; instead of this highly specialized tracepoint that adds some overhead
&gt; when tracepoints are disabled.

Good point, added a per-restart-site trace_btrfs_search_slot_restart()
tracepoint in v3.

Thanks,
Leo

&gt; 
&gt; Thanks.
&gt; 
&gt; 
&gt; &gt;         if (ret &lt; 0 &amp;&amp; !p-&gt;skip_release_on_error)
&gt; &gt;                 btrfs_release_path(p);
&gt; &gt;
&gt; &gt; diff --git a/include/trace/events/btrfs.h b/include/trace/events/btrfs.h
&gt; &gt; index 125bdc166bfe..b8934938a087 100644
&gt; &gt; --- a/include/trace/events/btrfs.h
&gt; &gt; +++ b/include/trace/events/btrfs.h
&gt; &gt; @@ -1110,6 +1110,32 @@ TRACE_EVENT(btrfs_cow_block,
&gt; &gt;                   __entry-&gt;cow_level)
&gt; &gt;  );
&gt; &gt;
&gt; &gt; +TRACE_EVENT(btrfs_search_slot_stats,
&gt; &gt; +
&gt; &gt; +       TP_PROTO(const struct btrfs_root *root,
&gt; &gt; +                int cow_count, int restart_count, int ret),
&gt; &gt; +
&gt; &gt; +       TP_ARGS(root, cow_count, restart_count, ret),
&gt; &gt; +
&gt; &gt; +       TP_STRUCT__entry_btrfs(
&gt; &gt; +               __field(        u64,    root_objectid           )
&gt; &gt; +               __field(        int,    cow_count               )
&gt; &gt; +               __field(        int,    restart_count           )
&gt; &gt; +               __field(        int,    ret                     )
&gt; &gt; +       ),
&gt; &gt; +
&gt; &gt; +       TP_fast_assign_btrfs(root-&gt;fs_info,
&gt; &gt; +               __entry-&gt;root_objectid  = btrfs_root_id(root);
&gt; &gt; +               __entry-&gt;cow_count      = cow_count;
&gt; &gt; +               __entry-&gt;restart_count  = restart_count;
&gt; &gt; +               __entry-&gt;ret            = ret;
&gt; &gt; +       ),
&gt; &gt; +
&gt; &gt; +       TP_printk_btrfs(&quot;root=%llu(%s) cow_count=%d restarts=%d ret=%d&quot;,
&gt; &gt; +                 show_root_type(__entry-&gt;root_objectid),
&gt; &gt; +                 __entry-&gt;cow_count, __entry-&gt;restart_count, __entry-&gt;ret)
&gt; &gt; +);
&gt; &gt; +
&gt; &gt;  TRACE_EVENT(btrfs_space_reservation,
&gt; &gt;
&gt; &gt;         TP_PROTO(const struct btrfs_fs_info *fs_info, const char *type, u64 val,
&gt; &gt; --
&gt; &gt; 2.47.3
&gt; &gt;
&gt; &gt;

</pre>
</details>
<div class="review-comment-signals">Signals: acknowledged corrections, no specific concerns</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Sun YangKai</span>
<a class="date-chip" href="../2026-02-13_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-13">2026-02-13</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The reviewer raised two technical concerns: the need for smp_mb__before_atomic() to ensure visibility of the FORCE_COW bit and a suggestion to use btrfs_mark_buffer_dirty() instead of set_extent_buffer_dirty().</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Thanks for your working on this and I&#x27;ve expecting this for a long time :)

On 2026/2/14 04:30, Leo Martins wrote:
&gt; When memory pressure causes writeback of a recently COW&#x27;d buffer,
&gt; btrfs sets BTRFS_HEADER_FLAG_WRITTEN on it. Subsequent
&gt; btrfs_search_slot() restarts then see the WRITTEN flag and re-COW
&gt; the buffer unnecessarily, causing COW amplification that can exhaust
&gt; block reservations and degrade throughput.
&gt; 
&gt; Overwriting in place is crash-safe because the committed superblock
&gt; does not reference buffers allocated in the current (uncommitted)
&gt; transaction, so no on-disk tree points to this block yet.
&gt; 
&gt; When should_cow_block() encounters a WRITTEN buffer whose generation
&gt; matches the current transaction, instead of requesting a COW, re-dirty
&gt; the buffer and re-register its range in the transaction&#x27;s dirty_pages.
&gt; 
&gt; Both are necessary because btrfs tracks dirty metadata through two
&gt; independent mechanisms. set_extent_buffer_dirty() sets the
&gt; EXTENT_BUFFER_DIRTY flag and the buffer_tree xarray PAGECACHE_TAG_DIRTY
&gt; mark, which is what background writeback (btree_write_cache_pages) uses
&gt; to find and write dirty buffers. The transaction&#x27;s dirty_pages io tree
&gt; is a separate structure used by btrfs_write_and_wait_transaction() at
&gt; commit time to ensure all buffers allocated during the transaction are
&gt; persisted. The dirty_pages range was originally registered in
&gt; btrfs_init_new_buffer() when the block was first allocated, but
&gt; background writeback may have already written and cleared it.
&gt; 
&gt; Keep BTRFS_HEADER_FLAG_WRITTEN set so that btrfs_free_tree_block()
&gt; correctly pins the block if it is freed later.
&gt; 
&gt; Exclude cases where in-place overwrite is not safe:
&gt;   - EXTENT_BUFFER_WRITEBACK: buffer is mid-I/O
&gt;   - Zoned devices: require sequential writes
&gt;   - Log trees: log blocks are immediately referenced by a committed
&gt;     superblock via btrfs_sync_log(), so overwriting could corrupt the
&gt;     committed log
&gt;   - BTRFS_ROOT_FORCE_COW: snapshot in progress
&gt;   - BTRFS_HEADER_FLAG_RELOC: block being relocated
&gt; 
&gt; Signed-off-by: Leo Martins &lt;loemra.dev@gmail.com&gt;
&gt; ---
&gt;   fs/btrfs/ctree.c | 53 +++++++++++++++++++++++++++++++++++++++++++++---
&gt;   1 file changed, 50 insertions(+), 3 deletions(-)
&gt; 
&gt; diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
&gt; index 7267b2502665..a345e1be24d8 100644
&gt; --- a/fs/btrfs/ctree.c
&gt; +++ b/fs/btrfs/ctree.c
&gt; @@ -599,9 +599,9 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,
&gt;   	return ret;
&gt;   }
&gt;   
&gt; -static inline bool should_cow_block(const struct btrfs_trans_handle *trans,
&gt; +static inline bool should_cow_block(struct btrfs_trans_handle *trans,
&gt;   				    const struct btrfs_root *root,
&gt; -				    const struct extent_buffer *buf)
&gt; +				    struct extent_buffer *buf)
&gt;   {
&gt;   	if (btrfs_is_testing(root-&gt;fs_info))
&gt;   		return false;
&gt; @@ -621,8 +621,55 @@ static inline bool should_cow_block(const struct btrfs_trans_handle *trans,
&gt;   	if (btrfs_header_generation(buf) != trans-&gt;transid)
&gt;   		return true;
&gt;   
&gt; -	if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN))
&gt; +	if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {
&gt; +		/*
&gt; +		 * The buffer was allocated in this transaction and has been
&gt; +		 * written back to disk (WRITTEN is set). Normally we&#x27;d COW
&gt; +		 * it again, but since the committed superblock doesn&#x27;t
&gt; +		 * reference this buffer (it was allocated this transaction),
&gt; +		 * we can safely overwrite it in place.
&gt; +		 *
&gt; +		 * We keep BTRFS_HEADER_FLAG_WRITTEN set. The block has been
&gt; +		 * persisted at this bytenr and will be again after the
&gt; +		 * in-place update. This is important so that
&gt; +		 * btrfs_free_tree_block() correctly pins the block if it is
&gt; +		 * freed later (e.g., during tree rebalancing or FORCE_COW).
&gt; +		 *
&gt; +		 * We re-dirty the buffer to ensure the in-place modifications
&gt; +		 * will be written back to disk.
&gt; +		 *
&gt; +		 * Exclusions:
&gt; +		 * - Log trees: log blocks are written and immediately
&gt; +		 *   referenced by a committed superblock via
&gt; +		 *   btrfs_sync_log(), bypassing the normal transaction
&gt; +		 *   commit. Overwriting in place could corrupt the
&gt; +		 *   committed log.
&gt; +		 * - Zoned devices: require sequential writes
&gt; +		 * - FORCE_COW: snapshot in progress
&gt; +		 * - RELOC flag: block being relocated
&gt; +		 */
&gt; +		if (!test_bit(EXTENT_BUFFER_WRITEBACK, &amp;buf-&gt;bflags) &amp;&amp;
&gt; +		    !btrfs_is_zoned(root-&gt;fs_info) &amp;&amp;
&gt; +		    btrfs_root_id(root) != BTRFS_TREE_LOG_OBJECTID &amp;&amp;
&gt; +		    !test_bit(BTRFS_ROOT_FORCE_COW, &amp;root-&gt;state) &amp;&amp;
it seems we need smp_mb__before_atomic() to see the FORCE_COW bit?
&gt; +		    !btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC)) {
&gt; +			/*
&gt; +			 * Re-register this block&#x27;s range in the current
&gt; +			 * transaction&#x27;s dirty_pages so that
&gt; +			 * btrfs_write_and_wait_transaction() writes it.
&gt; +			 * The range was originally registered when the block
&gt; +			 * was allocated, but that transaction&#x27;s dirty_pages
&gt; +			 * may have already been released.
&gt; +			 */
&gt; +			btrfs_set_extent_bit(&amp;trans-&gt;transaction-&gt;dirty_pages,
&gt; +					     buf-&gt;start,
&gt; +					     buf-&gt;start + buf-&gt;len - 1,
&gt; +					     EXTENT_DIRTY, NULL);
&gt; +			set_extent_buffer_dirty(buf);
why use set_extent_buffer_dirty() instead of btrfs_mark_buffer_dirty()? 
I don&#x27;t see any other callers doing this.
&gt; +			return false;
&gt; +		}
&gt;   		return true;
&gt; +	}
&gt;   
&gt;   	/* Ensure we can see the FORCE_COW bit. */
&gt;   	smp_mb__before_atomic();

And I wonder if we could have something more readable like this:

	if (btrfs_header_generation(buf) != trans-&gt;transid)
		return true;

	if (test_bit(EXTENT_BUFFER_WRITEBACK, &amp;buf-&gt;bflags))
		return true;

	if (btrfs_root_id(root) != BTRFS_TREE_RELOC_OBJECTID &amp;&amp;
	    btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC))
		return true;

	/* Ensure we can see the FORCE_COW bit. */
	smp_mb__before_atomic();
	if (test_bit(BTRFS_ROOT_FORCE_COW, &amp;root-&gt;state))
		return true;

	if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {
		if (btrfs_root_id(root) == BTRFS_TREE_LOG_OBJECTID ||
		    btrfs_is_zoned(root-&gt;fs_info))
				return true;
		btrfs_set_extent_bit(&amp;trans-&gt;transaction-&gt;dirty_pages,
				     buf-&gt;start,
				     buf-&gt;start + buf-&gt;len - 1,
				     EXTENT_DIRTY, NULL);
		btrfs_mark_buffer_dirty(trans, buf);
	}

	return false;

Thanks,
Sun YangKai


</pre>
</details>
<div class="review-comment-signals">Signals: requested changes, suggested alternative</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Filipe Manana</span>
<a class="date-chip" href="../2026-02-13_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-13">2026-02-13</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Filipe Manana reviewed the patch to inhibit extent buffer writeback, pointing out several minor issues such as missing punctuation and inconsistent comment placement. He also suggested a different approach for tracking COW amplification using existing tracepoints.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Fri, Feb 13, 2026 at 8:38 PM Leo Martins &lt;loemra.dev@gmail.com&gt; wrote:
&gt;
&gt; When memory pressure causes writeback of a recently COW&#x27;d buffer,
&gt; btrfs sets BTRFS_HEADER_FLAG_WRITTEN on it. Subsequent
&gt; btrfs_search_slot() restarts then see the WRITTEN flag and re-COW
&gt; the buffer unnecessarily, causing COW amplification that can exhaust
&gt; block reservations and degrade throughput.
&gt;
&gt; Overwriting in place is crash-safe because the committed superblock
&gt; does not reference buffers allocated in the current (uncommitted)
&gt; transaction, so no on-disk tree points to this block yet.
&gt;
&gt; When should_cow_block() encounters a WRITTEN buffer whose generation
&gt; matches the current transaction, instead of requesting a COW, re-dirty
&gt; the buffer and re-register its range in the transaction&#x27;s dirty_pages.
&gt;
&gt; Both are necessary because btrfs tracks dirty metadata through two
&gt; independent mechanisms. set_extent_buffer_dirty() sets the
&gt; EXTENT_BUFFER_DIRTY flag and the buffer_tree xarray PAGECACHE_TAG_DIRTY
&gt; mark, which is what background writeback (btree_write_cache_pages) uses
&gt; to find and write dirty buffers. The transaction&#x27;s dirty_pages io tree
&gt; is a separate structure used by btrfs_write_and_wait_transaction() at
&gt; commit time to ensure all buffers allocated during the transaction are
&gt; persisted. The dirty_pages range was originally registered in
&gt; btrfs_init_new_buffer() when the block was first allocated, but
&gt; background writeback may have already written and cleared it.

This is not quite correct, the dirty_pages range is never cleared on
background writeback.
We only clear it during a transaction commit, in
btrfs_write_and_wait_transaction().

Normally we shouldn&#x27;t care about setting the range again in
dirty_pages, because after
we call  btrfs_write_and_wait_transaction(), no more COW should be
possible using this
transaction (which is in the unblocked state, so any new COW attempt
will be in another transaction).

The exception is if we have snapshots to create and qgroups are
enabled, since in qgroup_account_snapshot() we
call btrfs_write_and_wait_transaction() and after that we can get more
COW, due to all the stuff we need to do to
create a snapshot, before we get to the final call to
btrfs_write_and_wait_transaction() right before we write the
super blocks in btrfs_commit_transaction().

&gt;
&gt; Keep BTRFS_HEADER_FLAG_WRITTEN set so that btrfs_free_tree_block()
&gt; correctly pins the block if it is freed later.
&gt;
&gt; Exclude cases where in-place overwrite is not safe:
&gt;  - EXTENT_BUFFER_WRITEBACK: buffer is mid-I/O
&gt;  - Zoned devices: require sequential writes
&gt;  - Log trees: log blocks are immediately referenced by a committed
&gt;    superblock via btrfs_sync_log(), so overwriting could corrupt the
&gt;    committed log
&gt;  - BTRFS_ROOT_FORCE_COW: snapshot in progress
&gt;  - BTRFS_HEADER_FLAG_RELOC: block being relocated
&gt;
&gt; Signed-off-by: Leo Martins &lt;loemra.dev@gmail.com&gt;
&gt; ---
&gt;  fs/btrfs/ctree.c | 53 +++++++++++++++++++++++++++++++++++++++++++++---
&gt;  1 file changed, 50 insertions(+), 3 deletions(-)
&gt;
&gt; diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
&gt; index 7267b2502665..a345e1be24d8 100644
&gt; --- a/fs/btrfs/ctree.c
&gt; +++ b/fs/btrfs/ctree.c
&gt; @@ -599,9 +599,9 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,
&gt;         return ret;
&gt;  }
&gt;
&gt; -static inline bool should_cow_block(const struct btrfs_trans_handle *trans,
&gt; +static inline bool should_cow_block(struct btrfs_trans_handle *trans,
&gt;                                     const struct btrfs_root *root,
&gt; -                                   const struct extent_buffer *buf)
&gt; +                                   struct extent_buffer *buf)
&gt;  {
&gt;         if (btrfs_is_testing(root-&gt;fs_info))
&gt;                 return false;
&gt; @@ -621,8 +621,55 @@ static inline bool should_cow_block(const struct btrfs_trans_handle *trans,
&gt;         if (btrfs_header_generation(buf) != trans-&gt;transid)
&gt;                 return true;
&gt;
&gt; -       if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN))
&gt; +       if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {
&gt; +               /*
&gt; +                * The buffer was allocated in this transaction and has been
&gt; +                * written back to disk (WRITTEN is set). Normally we&#x27;d COW
&gt; +                * it again, but since the committed superblock doesn&#x27;t
&gt; +                * reference this buffer (it was allocated this transaction),

Missing an &quot;in&quot; before &quot;this transaction&quot;.

&gt; +                * we can safely overwrite it in place.
&gt; +                *
&gt; +                * We keep BTRFS_HEADER_FLAG_WRITTEN set. The block has been
&gt; +                * persisted at this bytenr and will be again after the
&gt; +                * in-place update. This is important so that
&gt; +                * btrfs_free_tree_block() correctly pins the block if it is
&gt; +                * freed later (e.g., during tree rebalancing or FORCE_COW).
&gt; +                *
&gt; +                * We re-dirty the buffer to ensure the in-place modifications
&gt; +                * will be written back to disk.
&gt; +                *
&gt; +                * Exclusions:
&gt; +                * - Log trees: log blocks are written and immediately
&gt; +                *   referenced by a committed superblock via
&gt; +                *   btrfs_sync_log(), bypassing the normal transaction
&gt; +                *   commit. Overwriting in place could corrupt the
&gt; +                *   committed log.
&gt; +                * - Zoned devices: require sequential writes
&gt; +                * - FORCE_COW: snapshot in progress
&gt; +                * - RELOC flag: block being relocated
&gt; +                */
&gt; +               if (!test_bit(EXTENT_BUFFER_WRITEBACK, &amp;buf-&gt;bflags) &amp;&amp;
&gt; +                   !btrfs_is_zoned(root-&gt;fs_info) &amp;&amp;
&gt; +                   btrfs_root_id(root) != BTRFS_TREE_LOG_OBJECTID &amp;&amp;
&gt; +                   !test_bit(BTRFS_ROOT_FORCE_COW, &amp;root-&gt;state) &amp;&amp;

We need a  smp_mb__before_atomic() before checking FORCE_COW, see the
existing code below.

&gt; +                   !btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC)) {
&gt; +                       /*
&gt; +                        * Re-register this block&#x27;s range in the current
&gt; +                        * transaction&#x27;s dirty_pages so that
&gt; +                        * btrfs_write_and_wait_transaction() writes it.
&gt; +                        * The range was originally registered when the block
&gt; +                        * was allocated, but that transaction&#x27;s dirty_pages
&gt; +                        * may have already been released.

I think it&#x27;s worth adding something like: &quot;... already been released
if we are in a transaction that creates snapshots and we have qgroups
enabled.&quot;

Otherwise it looks good, thanks!

&gt; +                        */
&gt; +                       btrfs_set_extent_bit(&amp;trans-&gt;transaction-&gt;dirty_pages,
&gt; +                                            buf-&gt;start,
&gt; +                                            buf-&gt;start + buf-&gt;len - 1,
&gt; +                                            EXTENT_DIRTY, NULL);
&gt; +                       set_extent_buffer_dirty(buf);
&gt; +                       return false;
&gt; +               }
&gt;                 return true;
&gt; +       }
&gt;
&gt;         /* Ensure we can see the FORCE_COW bit. */
&gt;         smp_mb__before_atomic();
&gt; --
&gt; 2.47.3
&gt;
&gt;


---

On Fri, Feb 13, 2026 at 8:38 PM Leo Martins &lt;loemra.dev@gmail.com&gt; wrote:
&gt;
&gt; Inhibit writeback on COW&#x27;d extent buffers for the lifetime of the
&gt; transaction handle, preventing background writeback from setting
&gt; BTRFS_HEADER_FLAG_WRITTEN and causing unnecessary re-COW.
&gt;
&gt; COW amplification occurs when background writeback flushes an extent
&gt; buffer that a transaction handle is still actively modifying. When
&gt; lock_extent_buffer_for_io() transitions a buffer from dirty to
&gt; writeback, it sets BTRFS_HEADER_FLAG_WRITTEN, marking the block as
&gt; having been persisted to disk at its current bytenr. Once WRITTEN is
&gt; set, should_cow_block() must either COW the block again or overwrite
&gt; it in place, both of which are unnecessary overhead when the buffer
&gt; is still being modified by the same handle that allocated it. By
&gt; inhibiting background writeback on actively-used buffers, WRITTEN is
&gt; never set while a transaction handle holds a reference to the buffer,
&gt; avoiding this overhead entirely.
&gt;
&gt; Add an atomic_t writeback_inhibitors counter to struct extent_buffer,
&gt; which fits in an existing 6-byte hole without increasing struct size.
&gt; When a buffer is COW&#x27;d in btrfs_force_cow_block(), call
&gt; btrfs_inhibit_eb_writeback() to store the eb in the transaction
&gt; handle&#x27;s writeback_inhibited_ebs xarray (keyed by eb-&gt;start), take a
&gt; reference, and increment writeback_inhibitors. The function handles
&gt; dedup (same eb inhibited twice by the same handle) and replacement
&gt; (different eb at the same logical address). Allocation failure is
&gt; graceful: the buffer simply falls back to the pre-existing behavior
&gt; where it may be written back and re-COW&#x27;d.
&gt;
&gt; In lock_extent_buffer_for_io(), when writeback_inhibitors is non-zero
&gt; and the writeback mode is WB_SYNC_NONE, skip the buffer. WB_SYNC_NONE
&gt; is used by the VM flusher threads for background and periodic
&gt; writeback, which are the only paths that cause COW amplification by
&gt; opportunistically writing out dirty extent buffers mid-transaction.
&gt; Skipping these is safe because the buffers remain dirty in the page
&gt; cache and will be written out at transaction commit time.
&gt;
&gt; WB_SYNC_ALL must always proceed regardless of writeback_inhibitors.
&gt; This is required for correctness in the fsync path: btrfs_sync_log()
&gt; writes log tree blocks via filemap_fdatawrite_range() (WB_SYNC_ALL)
&gt; while the transaction handle that inhibited those same blocks is still
&gt; active. Without the WB_SYNC_ALL bypass, those inhibited log tree
&gt; blocks would be silently skipped, resulting in an incomplete log on
&gt; disk and corruption on replay. btrfs_write_and_wait_transaction()
&gt; also uses WB_SYNC_ALL via filemap_fdatawrite_range(); for that path,
&gt; inhibitors are already cleared beforehand, but the bypass ensures
&gt; correctness regardless.
&gt;
&gt; Uninhibit in __btrfs_end_transaction() before atomic_dec(num_writers)
&gt; to prevent a race where the committer proceeds while buffers are still
&gt; inhibited. Also uninhibit in btrfs_commit_transaction() before writing
&gt; and in cleanup_transaction() for the error path.
&gt;
&gt; Signed-off-by: Leo Martins &lt;loemra.dev@gmail.com&gt;
&gt; ---
&gt;  fs/btrfs/ctree.c       |  4 +++
&gt;  fs/btrfs/extent_io.c   | 62 +++++++++++++++++++++++++++++++++++++++++-
&gt;  fs/btrfs/extent_io.h   |  5 ++++
&gt;  fs/btrfs/transaction.c | 19 +++++++++++++
&gt;  fs/btrfs/transaction.h |  2 ++
&gt;  5 files changed, 91 insertions(+), 1 deletion(-)
&gt;
&gt; diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
&gt; index a345e1be24d8..55187ba59cc0 100644
&gt; --- a/fs/btrfs/ctree.c
&gt; +++ b/fs/btrfs/ctree.c
&gt; @@ -590,6 +590,10 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,
&gt;                 btrfs_tree_unlock(buf);
&gt;         free_extent_buffer_stale(buf);
&gt;         btrfs_mark_buffer_dirty(trans, cow);
&gt; +
&gt; +       /* Inhibit writeback on the COW&#x27;d buffer for this transaction handle */

Please always end sentences in a comment with punctuation.


&gt; +       btrfs_inhibit_eb_writeback(trans, cow);
&gt; +
&gt;         *cow_ret = cow;
&gt;         return 0;
&gt;
&gt; diff --git a/fs/btrfs/extent_io.c b/fs/btrfs/extent_io.c
&gt; index dfc17c292217..0c9276cff299 100644
&gt; --- a/fs/btrfs/extent_io.c
&gt; +++ b/fs/btrfs/extent_io.c
&gt; @@ -1940,7 +1940,9 @@ static noinline_for_stack bool lock_extent_buffer_for_io(struct extent_buffer *e
&gt;          * of time.
&gt;          */
&gt;         spin_lock(&amp;eb-&gt;refs_lock);
&gt; -       if (test_and_clear_bit(EXTENT_BUFFER_DIRTY, &amp;eb-&gt;bflags)) {
&gt; +       if ((wbc-&gt;sync_mode == WB_SYNC_ALL ||
&gt; +            atomic_read(&amp;eb-&gt;writeback_inhibitors) == 0) &amp;&amp;
&gt; +           test_and_clear_bit(EXTENT_BUFFER_DIRTY, &amp;eb-&gt;bflags)) {
&gt;                 XA_STATE(xas, &amp;fs_info-&gt;buffer_tree, eb-&gt;start &gt;&gt; fs_info-&gt;nodesize_bits);
&gt;                 unsigned long flags;
&gt;
&gt; @@ -2999,6 +3001,63 @@ static inline void btrfs_release_extent_buffer(struct extent_buffer *eb)
&gt;         kmem_cache_free(extent_buffer_cache, eb);
&gt;  }
&gt;
&gt; +/*
&gt; + * btrfs_inhibit_eb_writeback - Inhibit writeback on buffer during transaction
&gt; + * @trans: transaction handle that will own the inhibitor
&gt; + * @eb: extent buffer to inhibit writeback on
&gt; + *
&gt; + * Attempts to track this extent buffer in the transaction&#x27;s inhibited set.
&gt; + * If memory allocation fails, the buffer is simply not tracked. It may
&gt; + * be written back and need re-COW, which is the original behavior.
&gt; + * This is acceptable since inhibiting writeback is an optimization.
&gt; + */
&gt; +void btrfs_inhibit_eb_writeback(struct btrfs_trans_handle *trans,
&gt; +                               struct extent_buffer *eb)
&gt; +{
&gt; +       unsigned long index = eb-&gt;start &gt;&gt; trans-&gt;fs_info-&gt;nodesize_bits;
&gt; +       void *old;
&gt; +
&gt; +       /* Check if already inhibited by this handle */

Same here.

&gt; +       old = xa_load(&amp;trans-&gt;writeback_inhibited_ebs, index);
&gt; +       if (old == eb)
&gt; +               return;
&gt; +
&gt; +       refcount_inc(&amp;eb-&gt;refs);        /* Take reference */

Always place comments above the code line, not in the same line.

&gt; +
&gt; +       old = xa_store(&amp;trans-&gt;writeback_inhibited_ebs, index, eb, GFP_NOFS);
&gt; +       if (xa_is_err(old)) {
&gt; +               /* Allocation failed, just skip inhibiting this buffer */

Punctuation missing.

&gt; +               free_extent_buffer(eb);
&gt; +               return;
&gt; +       }
&gt; +
&gt; +       /* Handle replacement of different eb at same index */

Punctuation missing.

&gt; +       if (old &amp;&amp; old != eb) {
&gt; +               struct extent_buffer *old_eb = old;
&gt; +
&gt; +               atomic_dec(&amp;old_eb-&gt;writeback_inhibitors);
&gt; +               free_extent_buffer(old_eb);
&gt; +       }
&gt; +
&gt; +       atomic_inc(&amp;eb-&gt;writeback_inhibitors);
&gt; +}
&gt; +
&gt; +/*
&gt; + * btrfs_uninhibit_all_eb_writeback - Uninhibit writeback on all buffers
&gt; + * @trans: transaction handle to clean up
&gt; + */
&gt; +void btrfs_uninhibit_all_eb_writeback(struct btrfs_trans_handle *trans)
&gt; +{
&gt; +       struct extent_buffer *eb;
&gt; +       unsigned long index;
&gt; +
&gt; +       xa_for_each(&amp;trans-&gt;writeback_inhibited_ebs, index, eb) {
&gt; +               atomic_dec(&amp;eb-&gt;writeback_inhibitors);
&gt; +               free_extent_buffer(eb);
&gt; +       }
&gt; +       xa_destroy(&amp;trans-&gt;writeback_inhibited_ebs);
&gt; +}
&gt; +
&gt;  static struct extent_buffer *__alloc_extent_buffer(struct btrfs_fs_info *fs_info,
&gt;                                                    u64 start)
&gt;  {
&gt; @@ -3009,6 +3068,7 @@ static struct extent_buffer *__alloc_extent_buffer(struct btrfs_fs_info *fs_info
&gt;         eb-&gt;len = fs_info-&gt;nodesize;
&gt;         eb-&gt;fs_info = fs_info;
&gt;         init_rwsem(&amp;eb-&gt;lock);
&gt; +       atomic_set(&amp;eb-&gt;writeback_inhibitors, 0);
&gt;
&gt;         btrfs_leak_debug_add_eb(eb);
&gt;
&gt; diff --git a/fs/btrfs/extent_io.h b/fs/btrfs/extent_io.h
&gt; index 73571d5d3d5a..4b15a5d8bc0f 100644
&gt; --- a/fs/btrfs/extent_io.h
&gt; +++ b/fs/btrfs/extent_io.h
&gt; @@ -102,6 +102,7 @@ struct extent_buffer {
&gt;         /* &gt;= 0 if eb belongs to a log tree, -1 otherwise */
&gt;         s8 log_index;
&gt;         u8 folio_shift;
&gt; +       atomic_t writeback_inhibitors;  /* inhibits writeback when &gt; 0 */

Always place the comment above the structure&#x27;s member (just like for
code), and add punctuation and capitalize the first word.

We have old code that does follow this, from the old days where
&quot;anything goes&quot;, but we try to be consistent nowadays, see:

https://btrfs.readthedocs.io/en/latest/dev/Development-notes.html#comments

Otherwise it looks good, with those minor changes:

Reviewed-by: Filipe Manana &lt;fdmanana@suse.com&gt;

Thanks.



&gt;         struct rcu_head rcu_head;
&gt;
&gt;         struct rw_semaphore lock;
&gt; @@ -381,4 +382,8 @@ void btrfs_extent_buffer_leak_debug_check(struct btrfs_fs_info *fs_info);
&gt;  #define btrfs_extent_buffer_leak_debug_check(fs_info)  do {} while (0)
&gt;  #endif
&gt;
&gt; +void btrfs_inhibit_eb_writeback(struct btrfs_trans_handle *trans,
&gt; +                              struct extent_buffer *eb);
&gt; +void btrfs_uninhibit_all_eb_writeback(struct btrfs_trans_handle *trans);
&gt; +
&gt;  #endif
&gt; diff --git a/fs/btrfs/transaction.c b/fs/btrfs/transaction.c
&gt; index f4cc9e1a1b93..a9a22629b49d 100644
&gt; --- a/fs/btrfs/transaction.c
&gt; +++ b/fs/btrfs/transaction.c
&gt; @@ -15,6 +15,7 @@
&gt;  #include &quot;misc.h&quot;
&gt;  #include &quot;ctree.h&quot;
&gt;  #include &quot;disk-io.h&quot;
&gt; +#include &quot;extent_io.h&quot;
&gt;  #include &quot;transaction.h&quot;
&gt;  #include &quot;locking.h&quot;
&gt;  #include &quot;tree-log.h&quot;
&gt; @@ -688,6 +689,8 @@ start_transaction(struct btrfs_root *root, unsigned int num_items,
&gt;                 goto alloc_fail;
&gt;         }
&gt;
&gt; +       xa_init(&amp;h-&gt;writeback_inhibited_ebs);
&gt; +
&gt;         /*
&gt;          * If we are JOIN_NOLOCK we&#x27;re already committing a transaction and
&gt;          * waiting on this guy, so we don&#x27;t need to do the sb_start_intwrite
&gt; @@ -1083,6 +1086,13 @@ static int __btrfs_end_transaction(struct btrfs_trans_handle *trans,
&gt;         if (trans-&gt;type &amp; __TRANS_FREEZABLE)
&gt;                 sb_end_intwrite(info-&gt;sb);
&gt;
&gt; +       /*
&gt; +        * Uninhibit extent buffer writeback before decrementing num_writers,
&gt; +        * since the decrement wakes the committing thread which needs all
&gt; +        * buffers uninhibited to write them to disk.
&gt; +        */
&gt; +       btrfs_uninhibit_all_eb_writeback(trans);
&gt; +
&gt;         WARN_ON(cur_trans != info-&gt;running_transaction);
&gt;         WARN_ON(atomic_read(&amp;cur_trans-&gt;num_writers) &lt; 1);
&gt;         atomic_dec(&amp;cur_trans-&gt;num_writers);
&gt; @@ -2110,6 +2120,7 @@ static void cleanup_transaction(struct btrfs_trans_handle *trans, int err)
&gt;         if (!test_bit(BTRFS_FS_RELOC_RUNNING, &amp;fs_info-&gt;flags))
&gt;                 btrfs_scrub_cancel(fs_info);
&gt;
&gt; +       btrfs_uninhibit_all_eb_writeback(trans);
&gt;         kmem_cache_free(btrfs_trans_handle_cachep, trans);
&gt;  }
&gt;
&gt; @@ -2556,6 +2567,14 @@ int btrfs_commit_transaction(struct btrfs_trans_handle *trans)
&gt;             fs_info-&gt;cleaner_kthread)
&gt;                 wake_up_process(fs_info-&gt;cleaner_kthread);
&gt;
&gt; +       /*
&gt; +        * Uninhibit writeback on all extent buffers inhibited during this
&gt; +        * transaction before writing them to disk. Inhibiting prevented
&gt; +        * writeback while the transaction was building, but now we need
&gt; +        * them written.
&gt; +        */
&gt; +       btrfs_uninhibit_all_eb_writeback(trans);
&gt; +
&gt;         ret = btrfs_write_and_wait_transaction(trans);
&gt;         if (unlikely(ret)) {
&gt;                 btrfs_err(fs_info, &quot;error while writing out transaction: %d&quot;, ret);
&gt; diff --git a/fs/btrfs/transaction.h b/fs/btrfs/transaction.h
&gt; index 18ef069197e5..f0d12c16d796 100644
&gt; --- a/fs/btrfs/transaction.h
&gt; +++ b/fs/btrfs/transaction.h
&gt; @@ -12,6 +12,7 @@
&gt;  #include &lt;linux/time64.h&gt;
&gt;  #include &lt;linux/mutex.h&gt;
&gt;  #include &lt;linux/wait.h&gt;
&gt; +#include &lt;linux/xarray.h&gt;
&gt;  #include &quot;btrfs_inode.h&quot;
&gt;  #include &quot;delayed-ref.h&quot;
&gt;
&gt; @@ -162,6 +163,7 @@ struct btrfs_trans_handle {
&gt;         struct btrfs_fs_info *fs_info;
&gt;         struct list_head new_bgs;
&gt;         struct btrfs_block_rsv delayed_rsv;
&gt; +       struct xarray writeback_inhibited_ebs;  /* ebs with writeback inhibited */
&gt;  };
&gt;
&gt;  /*
&gt; --
&gt; 2.47.3
&gt;
&gt;


---

On Fri, Feb 13, 2026 at 8:37 PM Leo Martins &lt;loemra.dev@gmail.com&gt; wrote:
&gt;
&gt; Add a btrfs_search_slot_stats tracepoint to btrfs_search_slot() for
&gt; measuring COW amplification.
&gt;
&gt; The tracepoint fires when a search with at least one COW completes,
&gt; recording the root, total cow_count, restart_count, and return value.
&gt; cow_count and restart_count per search_slot call are useful metrics
&gt; for tracking COW amplification.
&gt;
&gt; Signed-off-by: Leo Martins &lt;loemra.dev@gmail.com&gt;
&gt; ---
&gt;  fs/btrfs/ctree.c             | 15 +++++++++++++--
&gt;  include/trace/events/btrfs.h | 26 ++++++++++++++++++++++++++
&gt;  2 files changed, 39 insertions(+), 2 deletions(-)
&gt;
&gt; diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
&gt; index 55187ba59cc0..1971d7bb5f60 100644
&gt; --- a/fs/btrfs/ctree.c
&gt; +++ b/fs/btrfs/ctree.c
&gt; @@ -2069,6 +2069,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt;         u8 lowest_level = 0;
&gt;         int min_write_lock_level;
&gt;         int prev_cmp;
&gt; +       int cow_count = 0;
&gt; +       int restart_count = 0;
&gt;
&gt;         if (!root)
&gt;                 return -EINVAL;
&gt; @@ -2157,6 +2159,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt;                             p-&gt;nodes[level + 1])) {
&gt;                                 write_lock_level = level + 1;
&gt;                                 btrfs_release_path(p);
&gt; +                               restart_count++;
&gt;                                 goto again;
&gt;                         }
&gt;
&gt; @@ -2172,6 +2175,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt;                                 ret = ret2;
&gt;                                 goto done;
&gt;                         }
&gt; +                       cow_count++;
&gt;                 }
&gt;  cow_done:
&gt;                 p-&gt;nodes[level] = b;
&gt; @@ -2219,8 +2223,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt;                 p-&gt;slots[level] = slot;
&gt;                 ret2 = setup_nodes_for_search(trans, root, p, b, level, ins_len,
&gt;                                               &amp;write_lock_level);
&gt; -               if (ret2 == -EAGAIN)
&gt; +               if (ret2 == -EAGAIN) {
&gt; +                       restart_count++;
&gt;                         goto again;
&gt; +               }
&gt;                 if (ret2) {
&gt;                         ret = ret2;
&gt;                         goto done;
&gt; @@ -2236,6 +2242,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt;                 if (slot == 0 &amp;&amp; ins_len &amp;&amp; write_lock_level &lt; level + 1) {
&gt;                         write_lock_level = level + 1;
&gt;                         btrfs_release_path(p);
&gt; +                       restart_count++;
&gt;                         goto again;
&gt;                 }
&gt;
&gt; @@ -2249,8 +2256,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt;                 }
&gt;
&gt;                 ret2 = read_block_for_search(root, p, &amp;b, slot, key);
&gt; -               if (ret2 == -EAGAIN &amp;&amp; !p-&gt;nowait)
&gt; +               if (ret2 == -EAGAIN &amp;&amp; !p-&gt;nowait) {
&gt; +                       restart_count++;
&gt;                         goto again;
&gt; +               }
&gt;                 if (ret2) {
&gt;                         ret = ret2;
&gt;                         goto done;
&gt; @@ -2281,6 +2290,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt;         }
&gt;         ret = 1;
&gt;  done:
&gt; +       if (cow_count &gt; 0)
&gt; +               trace_btrfs_search_slot_stats(root, cow_count, restart_count, ret);

So I find this way too specific, plus even if trace points are
disabled we have the overhead of the counters (and inside critical
sections).

We already have a tracepoint for COW, trace_btrfs_cow_block(), and we
could have one just for the retry thing, maybe naming it like
trace_btrfs_search_slot_restart() or something.
So we could use those two tracepoints to measure things (bpftrace
scripts could easily report a count of each trace point and such),
instead of this highly specialized tracepoint that adds some overhead
when tracepoints are disabled.

Thanks.


&gt;         if (ret &lt; 0 &amp;&amp; !p-&gt;skip_release_on_error)
&gt;                 btrfs_release_path(p);
&gt;
&gt; diff --git a/include/trace/events/btrfs.h b/include/trace/events/btrfs.h
&gt; index 125bdc166bfe..b8934938a087 100644
&gt; --- a/include/trace/events/btrfs.h
&gt; +++ b/include/trace/events/btrfs.h
&gt; @@ -1110,6 +1110,32 @@ TRACE_EVENT(btrfs_cow_block,
&gt;                   __entry-&gt;cow_level)
&gt;  );
&gt;
&gt; +TRACE_EVENT(btrfs_search_slot_stats,
&gt; +
&gt; +       TP_PROTO(const struct btrfs_root *root,
&gt; +                int cow_count, int restart_count, int ret),
&gt; +
&gt; +       TP_ARGS(root, cow_count, restart_count, ret),
&gt; +
&gt; +       TP_STRUCT__entry_btrfs(
&gt; +               __field(        u64,    root_objectid           )
&gt; +               __field(        int,    cow_count               )
&gt; +               __field(        int,    restart_count           )
&gt; +               __field(        int,    ret                     )
&gt; +       ),
&gt; +
&gt; +       TP_fast_assign_btrfs(root-&gt;fs_info,
&gt; +               __entry-&gt;root_objectid  = btrfs_root_id(root);
&gt; +               __entry-&gt;cow_count      = cow_count;
&gt; +               __entry-&gt;restart_count  = restart_count;
&gt; +               __entry-&gt;ret            = ret;
&gt; +       ),
&gt; +
&gt; +       TP_printk_btrfs(&quot;root=%llu(%s) cow_count=%d restarts=%d ret=%d&quot;,
&gt; +                 show_root_type(__entry-&gt;root_objectid),
&gt; +                 __entry-&gt;cow_count, __entry-&gt;restart_count, __entry-&gt;ret)
&gt; +);
&gt; +
&gt;  TRACE_EVENT(btrfs_space_reservation,
&gt;
&gt;         TP_PROTO(const struct btrfs_fs_info *fs_info, const char *type, u64 val,
&gt; --
&gt; 2.47.3
&gt;
&gt;
</pre>
</details>
<div class="review-comment-signals">Signals: minor technical concerns, suggested alternative solution</div>
</div>
</div>
<div class="thread-node depth-0" id="2026-02-17">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Leo Martins (author)</span>
<a class="date-chip" href="../2026-02-17_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-17">2026-02-17</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The reviewer raised no specific technical concerns or objections to the patch, but instead provided feedback on other patches in the series and acknowledged corrections made by the author.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">When memory pressure causes writeback of a recently COW&#x27;d buffer,
btrfs sets BTRFS_HEADER_FLAG_WRITTEN on it. Subsequent
btrfs_search_slot() restarts then see the WRITTEN flag and re-COW
the buffer unnecessarily, causing COW amplification that can exhaust
block reservations and degrade throughput.

Overwriting in place is crash-safe because the committed superblock
does not reference buffers allocated in the current (uncommitted)
transaction, so no on-disk tree points to this block yet.

When should_cow_block() encounters a WRITTEN buffer whose generation
matches the current transaction, instead of requesting a COW, re-dirty
the buffer and re-register its range in the transaction&#x27;s dirty_pages.

Both are necessary because btrfs tracks dirty metadata through two
independent mechanisms. set_extent_buffer_dirty() sets the
EXTENT_BUFFER_DIRTY flag and the buffer_tree xarray PAGECACHE_TAG_DIRTY
mark, which is what background writeback (btree_write_cache_pages) uses
to find and write dirty buffers. The transaction&#x27;s dirty_pages io tree
is a separate structure used by btrfs_write_and_wait_transaction() at
commit time to ensure all buffers allocated during the transaction are
persisted. The dirty_pages range was originally registered in
btrfs_init_new_buffer() when the block was first allocated, but
background writeback may have already written and cleared it.

Keep BTRFS_HEADER_FLAG_WRITTEN set so that btrfs_free_tree_block()
correctly pins the block if it is freed later.

Exclude cases where in-place overwrite is not safe:
 - EXTENT_BUFFER_WRITEBACK: buffer is mid-I/O
 - Zoned devices: require sequential writes
 - Log trees: log blocks are immediately referenced by a committed
   superblock via btrfs_sync_log(), so overwriting could corrupt the
   committed log
 - BTRFS_ROOT_FORCE_COW: snapshot in progress
 - BTRFS_HEADER_FLAG_RELOC: block being relocated

Signed-off-by: Leo Martins &lt;loemra.dev@gmail.com&gt;
---
 fs/btrfs/ctree.c | 53 +++++++++++++++++++++++++++++++++++++++++++++---
 1 file changed, 50 insertions(+), 3 deletions(-)

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 7267b2502665..a345e1be24d8 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -599,9 +599,9 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,
 	return ret;
 }
 
-static inline bool should_cow_block(const struct btrfs_trans_handle *trans,
+static inline bool should_cow_block(struct btrfs_trans_handle *trans,
 				    const struct btrfs_root *root,
-				    const struct extent_buffer *buf)
+				    struct extent_buffer *buf)
 {
 	if (btrfs_is_testing(root-&gt;fs_info))
 		return false;
@@ -621,8 +621,55 @@ static inline bool should_cow_block(const struct btrfs_trans_handle *trans,
 	if (btrfs_header_generation(buf) != trans-&gt;transid)
 		return true;
 
-	if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN))
+	if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {
+		/*
+		 * The buffer was allocated in this transaction and has been
+		 * written back to disk (WRITTEN is set). Normally we&#x27;d COW
+		 * it again, but since the committed superblock doesn&#x27;t
+		 * reference this buffer (it was allocated this transaction),
+		 * we can safely overwrite it in place.
+		 *
+		 * We keep BTRFS_HEADER_FLAG_WRITTEN set. The block has been
+		 * persisted at this bytenr and will be again after the
+		 * in-place update. This is important so that
+		 * btrfs_free_tree_block() correctly pins the block if it is
+		 * freed later (e.g., during tree rebalancing or FORCE_COW).
+		 *
+		 * We re-dirty the buffer to ensure the in-place modifications
+		 * will be written back to disk.
+		 *
+		 * Exclusions:
+		 * - Log trees: log blocks are written and immediately
+		 *   referenced by a committed superblock via
+		 *   btrfs_sync_log(), bypassing the normal transaction
+		 *   commit. Overwriting in place could corrupt the
+		 *   committed log.
+		 * - Zoned devices: require sequential writes
+		 * - FORCE_COW: snapshot in progress
+		 * - RELOC flag: block being relocated
+		 */
+		if (!test_bit(EXTENT_BUFFER_WRITEBACK, &amp;buf-&gt;bflags) &amp;&amp;
+		    !btrfs_is_zoned(root-&gt;fs_info) &amp;&amp;
+		    btrfs_root_id(root) != BTRFS_TREE_LOG_OBJECTID &amp;&amp;
+		    !test_bit(BTRFS_ROOT_FORCE_COW, &amp;root-&gt;state) &amp;&amp;
+		    !btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC)) {
+			/*
+			 * Re-register this block&#x27;s range in the current
+			 * transaction&#x27;s dirty_pages so that
+			 * btrfs_write_and_wait_transaction() writes it.
+			 * The range was originally registered when the block
+			 * was allocated, but that transaction&#x27;s dirty_pages
+			 * may have already been released.
+			 */
+			btrfs_set_extent_bit(&amp;trans-&gt;transaction-&gt;dirty_pages,
+					     buf-&gt;start,
+					     buf-&gt;start + buf-&gt;len - 1,
+					     EXTENT_DIRTY, NULL);
+			set_extent_buffer_dirty(buf);
+			return false;
+		}
 		return true;
+	}
 
 	/* Ensure we can see the FORCE_COW bit. */
 	smp_mb__before_atomic();
-- 
2.47.3



---

Inhibit writeback on COW&#x27;d extent buffers for the lifetime of the
transaction handle, preventing background writeback from setting
BTRFS_HEADER_FLAG_WRITTEN and causing unnecessary re-COW.

COW amplification occurs when background writeback flushes an extent
buffer that a transaction handle is still actively modifying. When
lock_extent_buffer_for_io() transitions a buffer from dirty to
writeback, it sets BTRFS_HEADER_FLAG_WRITTEN, marking the block as
having been persisted to disk at its current bytenr. Once WRITTEN is
set, should_cow_block() must either COW the block again or overwrite
it in place, both of which are unnecessary overhead when the buffer
is still being modified by the same handle that allocated it. By
inhibiting background writeback on actively-used buffers, WRITTEN is
never set while a transaction handle holds a reference to the buffer,
avoiding this overhead entirely.

Add an atomic_t writeback_inhibitors counter to struct extent_buffer,
which fits in an existing 6-byte hole without increasing struct size.
When a buffer is COW&#x27;d in btrfs_force_cow_block(), call
btrfs_inhibit_eb_writeback() to store the eb in the transaction
handle&#x27;s writeback_inhibited_ebs xarray (keyed by eb-&gt;start), take a
reference, and increment writeback_inhibitors. The function handles
dedup (same eb inhibited twice by the same handle) and replacement
(different eb at the same logical address). Allocation failure is
graceful: the buffer simply falls back to the pre-existing behavior
where it may be written back and re-COW&#x27;d.

In lock_extent_buffer_for_io(), when writeback_inhibitors is non-zero
and the writeback mode is WB_SYNC_NONE, skip the buffer. WB_SYNC_NONE
is used by the VM flusher threads for background and periodic
writeback, which are the only paths that cause COW amplification by
opportunistically writing out dirty extent buffers mid-transaction.
Skipping these is safe because the buffers remain dirty in the page
cache and will be written out at transaction commit time.

WB_SYNC_ALL must always proceed regardless of writeback_inhibitors.
This is required for correctness in the fsync path: btrfs_sync_log()
writes log tree blocks via filemap_fdatawrite_range() (WB_SYNC_ALL)
while the transaction handle that inhibited those same blocks is still
active. Without the WB_SYNC_ALL bypass, those inhibited log tree
blocks would be silently skipped, resulting in an incomplete log on
disk and corruption on replay. btrfs_write_and_wait_transaction()
also uses WB_SYNC_ALL via filemap_fdatawrite_range(); for that path,
inhibitors are already cleared beforehand, but the bypass ensures
correctness regardless.

Uninhibit in __btrfs_end_transaction() before atomic_dec(num_writers)
to prevent a race where the committer proceeds while buffers are still
inhibited. Also uninhibit in btrfs_commit_transaction() before writing
and in cleanup_transaction() for the error path.

Signed-off-by: Leo Martins &lt;loemra.dev@gmail.com&gt;
---
 fs/btrfs/ctree.c       |  4 +++
 fs/btrfs/extent_io.c   | 62 +++++++++++++++++++++++++++++++++++++++++-
 fs/btrfs/extent_io.h   |  5 ++++
 fs/btrfs/transaction.c | 19 +++++++++++++
 fs/btrfs/transaction.h |  2 ++
 5 files changed, 91 insertions(+), 1 deletion(-)

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index a345e1be24d8..55187ba59cc0 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -590,6 +590,10 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,
 		btrfs_tree_unlock(buf);
 	free_extent_buffer_stale(buf);
 	btrfs_mark_buffer_dirty(trans, cow);
+
+	/* Inhibit writeback on the COW&#x27;d buffer for this transaction handle */
+	btrfs_inhibit_eb_writeback(trans, cow);
+
 	*cow_ret = cow;
 	return 0;
 
diff --git a/fs/btrfs/extent_io.c b/fs/btrfs/extent_io.c
index dfc17c292217..0c9276cff299 100644
--- a/fs/btrfs/extent_io.c
+++ b/fs/btrfs/extent_io.c
@@ -1940,7 +1940,9 @@ static noinline_for_stack bool lock_extent_buffer_for_io(struct extent_buffer *e
 	 * of time.
 	 */
 	spin_lock(&amp;eb-&gt;refs_lock);
-	if (test_and_clear_bit(EXTENT_BUFFER_DIRTY, &amp;eb-&gt;bflags)) {
+	if ((wbc-&gt;sync_mode == WB_SYNC_ALL ||
+	     atomic_read(&amp;eb-&gt;writeback_inhibitors) == 0) &amp;&amp;
+	    test_and_clear_bit(EXTENT_BUFFER_DIRTY, &amp;eb-&gt;bflags)) {
 		XA_STATE(xas, &amp;fs_info-&gt;buffer_tree, eb-&gt;start &gt;&gt; fs_info-&gt;nodesize_bits);
 		unsigned long flags;
 
@@ -2999,6 +3001,63 @@ static inline void btrfs_release_extent_buffer(struct extent_buffer *eb)
 	kmem_cache_free(extent_buffer_cache, eb);
 }
 
+/*
+ * btrfs_inhibit_eb_writeback - Inhibit writeback on buffer during transaction
+ * @trans: transaction handle that will own the inhibitor
+ * @eb: extent buffer to inhibit writeback on
+ *
+ * Attempts to track this extent buffer in the transaction&#x27;s inhibited set.
+ * If memory allocation fails, the buffer is simply not tracked. It may
+ * be written back and need re-COW, which is the original behavior.
+ * This is acceptable since inhibiting writeback is an optimization.
+ */
+void btrfs_inhibit_eb_writeback(struct btrfs_trans_handle *trans,
+				struct extent_buffer *eb)
+{
+	unsigned long index = eb-&gt;start &gt;&gt; trans-&gt;fs_info-&gt;nodesize_bits;
+	void *old;
+
+	/* Check if already inhibited by this handle */
+	old = xa_load(&amp;trans-&gt;writeback_inhibited_ebs, index);
+	if (old == eb)
+		return;
+
+	refcount_inc(&amp;eb-&gt;refs);	/* Take reference */
+
+	old = xa_store(&amp;trans-&gt;writeback_inhibited_ebs, index, eb, GFP_NOFS);
+	if (xa_is_err(old)) {
+		/* Allocation failed, just skip inhibiting this buffer */
+		free_extent_buffer(eb);
+		return;
+	}
+
+	/* Handle replacement of different eb at same index */
+	if (old &amp;&amp; old != eb) {
+		struct extent_buffer *old_eb = old;
+
+		atomic_dec(&amp;old_eb-&gt;writeback_inhibitors);
+		free_extent_buffer(old_eb);
+	}
+
+	atomic_inc(&amp;eb-&gt;writeback_inhibitors);
+}
+
+/*
+ * btrfs_uninhibit_all_eb_writeback - Uninhibit writeback on all buffers
+ * @trans: transaction handle to clean up
+ */
+void btrfs_uninhibit_all_eb_writeback(struct btrfs_trans_handle *trans)
+{
+	struct extent_buffer *eb;
+	unsigned long index;
+
+	xa_for_each(&amp;trans-&gt;writeback_inhibited_ebs, index, eb) {
+		atomic_dec(&amp;eb-&gt;writeback_inhibitors);
+		free_extent_buffer(eb);
+	}
+	xa_destroy(&amp;trans-&gt;writeback_inhibited_ebs);
+}
+
 static struct extent_buffer *__alloc_extent_buffer(struct btrfs_fs_info *fs_info,
 						   u64 start)
 {
@@ -3009,6 +3068,7 @@ static struct extent_buffer *__alloc_extent_buffer(struct btrfs_fs_info *fs_info
 	eb-&gt;len = fs_info-&gt;nodesize;
 	eb-&gt;fs_info = fs_info;
 	init_rwsem(&amp;eb-&gt;lock);
+	atomic_set(&amp;eb-&gt;writeback_inhibitors, 0);
 
 	btrfs_leak_debug_add_eb(eb);
 
diff --git a/fs/btrfs/extent_io.h b/fs/btrfs/extent_io.h
index 73571d5d3d5a..4b15a5d8bc0f 100644
--- a/fs/btrfs/extent_io.h
+++ b/fs/btrfs/extent_io.h
@@ -102,6 +102,7 @@ struct extent_buffer {
 	/* &gt;= 0 if eb belongs to a log tree, -1 otherwise */
 	s8 log_index;
 	u8 folio_shift;
+	atomic_t writeback_inhibitors;	/* inhibits writeback when &gt; 0 */
 	struct rcu_head rcu_head;
 
 	struct rw_semaphore lock;
@@ -381,4 +382,8 @@ void btrfs_extent_buffer_leak_debug_check(struct btrfs_fs_info *fs_info);
 #define btrfs_extent_buffer_leak_debug_check(fs_info)	do {} while (0)
 #endif
 
+void btrfs_inhibit_eb_writeback(struct btrfs_trans_handle *trans,
+			       struct extent_buffer *eb);
+void btrfs_uninhibit_all_eb_writeback(struct btrfs_trans_handle *trans);
+
 #endif
diff --git a/fs/btrfs/transaction.c b/fs/btrfs/transaction.c
index f4cc9e1a1b93..a9a22629b49d 100644
--- a/fs/btrfs/transaction.c
+++ b/fs/btrfs/transaction.c
@@ -15,6 +15,7 @@
 #include &quot;misc.h&quot;
 #include &quot;ctree.h&quot;
 #include &quot;disk-io.h&quot;
+#include &quot;extent_io.h&quot;
 #include &quot;transaction.h&quot;
 #include &quot;locking.h&quot;
 #include &quot;tree-log.h&quot;
@@ -688,6 +689,8 @@ start_transaction(struct btrfs_root *root, unsigned int num_items,
 		goto alloc_fail;
 	}
 
+	xa_init(&amp;h-&gt;writeback_inhibited_ebs);
+
 	/*
 	 * If we are JOIN_NOLOCK we&#x27;re already committing a transaction and
 	 * waiting on this guy, so we don&#x27;t need to do the sb_start_intwrite
@@ -1083,6 +1086,13 @@ static int __btrfs_end_transaction(struct btrfs_trans_handle *trans,
 	if (trans-&gt;type &amp; __TRANS_FREEZABLE)
 		sb_end_intwrite(info-&gt;sb);
 
+	/*
+	 * Uninhibit extent buffer writeback before decrementing num_writers,
+	 * since the decrement wakes the committing thread which needs all
+	 * buffers uninhibited to write them to disk.
+	 */
+	btrfs_uninhibit_all_eb_writeback(trans);
+
 	WARN_ON(cur_trans != info-&gt;running_transaction);
 	WARN_ON(atomic_read(&amp;cur_trans-&gt;num_writers) &lt; 1);
 	atomic_dec(&amp;cur_trans-&gt;num_writers);
@@ -2110,6 +2120,7 @@ static void cleanup_transaction(struct btrfs_trans_handle *trans, int err)
 	if (!test_bit(BTRFS_FS_RELOC_RUNNING, &amp;fs_info-&gt;flags))
 		btrfs_scrub_cancel(fs_info);
 
+	btrfs_uninhibit_all_eb_writeback(trans);
 	kmem_cache_free(btrfs_trans_handle_cachep, trans);
 }
 
@@ -2556,6 +2567,14 @@ int btrfs_commit_transaction(struct btrfs_trans_handle *trans)
 	    fs_info-&gt;cleaner_kthread)
 		wake_up_process(fs_info-&gt;cleaner_kthread);
 
+	/*
+	 * Uninhibit writeback on all extent buffers inhibited during this
+	 * transaction before writing them to disk. Inhibiting prevented
+	 * writeback while the transaction was building, but now we need
+	 * them written.
+	 */
+	btrfs_uninhibit_all_eb_writeback(trans);
+
 	ret = btrfs_write_and_wait_transaction(trans);
 	if (unlikely(ret)) {
 		btrfs_err(fs_info, &quot;error while writing out transaction: %d&quot;, ret);
diff --git a/fs/btrfs/transaction.h b/fs/btrfs/transaction.h
index 18ef069197e5..f0d12c16d796 100644
--- a/fs/btrfs/transaction.h
+++ b/fs/btrfs/transaction.h
@@ -12,6 +12,7 @@
 #include &lt;linux/time64.h&gt;
 #include &lt;linux/mutex.h&gt;
 #include &lt;linux/wait.h&gt;
+#include &lt;linux/xarray.h&gt;
 #include &quot;btrfs_inode.h&quot;
 #include &quot;delayed-ref.h&quot;
 
@@ -162,6 +163,7 @@ struct btrfs_trans_handle {
 	struct btrfs_fs_info *fs_info;
 	struct list_head new_bgs;
 	struct btrfs_block_rsv delayed_rsv;
+	struct xarray writeback_inhibited_ebs;	/* ebs with writeback inhibited */
 };
 
 /*
-- 
2.47.3



---

Add a btrfs_search_slot_stats tracepoint to btrfs_search_slot() for
measuring COW amplification.

The tracepoint fires when a search with at least one COW completes,
recording the root, total cow_count, restart_count, and return value.
cow_count and restart_count per search_slot call are useful metrics
for tracking COW amplification.

Signed-off-by: Leo Martins &lt;loemra.dev@gmail.com&gt;
---
 fs/btrfs/ctree.c             | 15 +++++++++++++--
 include/trace/events/btrfs.h | 26 ++++++++++++++++++++++++++
 2 files changed, 39 insertions(+), 2 deletions(-)

diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index 55187ba59cc0..1971d7bb5f60 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2069,6 +2069,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 	u8 lowest_level = 0;
 	int min_write_lock_level;
 	int prev_cmp;
+	int cow_count = 0;
+	int restart_count = 0;
 
 	if (!root)
 		return -EINVAL;
@@ -2157,6 +2159,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			    p-&gt;nodes[level + 1])) {
 				write_lock_level = level + 1;
 				btrfs_release_path(p);
+				restart_count++;
 				goto again;
 			}
 
@@ -2172,6 +2175,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 				ret = ret2;
 				goto done;
 			}
+			cow_count++;
 		}
 cow_done:
 		p-&gt;nodes[level] = b;
@@ -2219,8 +2223,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		p-&gt;slots[level] = slot;
 		ret2 = setup_nodes_for_search(trans, root, p, b, level, ins_len,
 					      &amp;write_lock_level);
-		if (ret2 == -EAGAIN)
+		if (ret2 == -EAGAIN) {
+			restart_count++;
 			goto again;
+		}
 		if (ret2) {
 			ret = ret2;
 			goto done;
@@ -2236,6 +2242,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		if (slot == 0 &amp;&amp; ins_len &amp;&amp; write_lock_level &lt; level + 1) {
 			write_lock_level = level + 1;
 			btrfs_release_path(p);
+			restart_count++;
 			goto again;
 		}
 
@@ -2249,8 +2256,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 		}
 
 		ret2 = read_block_for_search(root, p, &amp;b, slot, key);
-		if (ret2 == -EAGAIN &amp;&amp; !p-&gt;nowait)
+		if (ret2 == -EAGAIN &amp;&amp; !p-&gt;nowait) {
+			restart_count++;
 			goto again;
+		}
 		if (ret2) {
 			ret = ret2;
 			goto done;
@@ -2281,6 +2290,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 	}
 	ret = 1;
 done:
+	if (cow_count &gt; 0)
+		trace_btrfs_search_slot_stats(root, cow_count, restart_count, ret);
 	if (ret &lt; 0 &amp;&amp; !p-&gt;skip_release_on_error)
 		btrfs_release_path(p);
 
diff --git a/include/trace/events/btrfs.h b/include/trace/events/btrfs.h
index 125bdc166bfe..b8934938a087 100644
--- a/include/trace/events/btrfs.h
+++ b/include/trace/events/btrfs.h
@@ -1110,6 +1110,32 @@ TRACE_EVENT(btrfs_cow_block,
 		  __entry-&gt;cow_level)
 );
 
+TRACE_EVENT(btrfs_search_slot_stats,
+
+	TP_PROTO(const struct btrfs_root *root,
+		 int cow_count, int restart_count, int ret),
+
+	TP_ARGS(root, cow_count, restart_count, ret),
+
+	TP_STRUCT__entry_btrfs(
+		__field(	u64,	root_objectid		)
+		__field(	int,	cow_count		)
+		__field(	int,	restart_count		)
+		__field(	int,	ret			)
+	),
+
+	TP_fast_assign_btrfs(root-&gt;fs_info,
+		__entry-&gt;root_objectid	= btrfs_root_id(root);
+		__entry-&gt;cow_count	= cow_count;
+		__entry-&gt;restart_count	= restart_count;
+		__entry-&gt;ret		= ret;
+	),
+
+	TP_printk_btrfs(&quot;root=%llu(%s) cow_count=%d restarts=%d ret=%d&quot;,
+		  show_root_type(__entry-&gt;root_objectid),
+		  __entry-&gt;cow_count, __entry-&gt;restart_count, __entry-&gt;ret)
+);
+
 TRACE_EVENT(btrfs_space_reservation,
 
 	TP_PROTO(const struct btrfs_fs_info *fs_info, const char *type, u64 val,
-- 
2.47.3



---

On Mon, 16 Feb 2026 12:18:48 +0000 Filipe Manana &lt;fdmanana@kernel.org&gt; wrote:

&gt; On Fri, Feb 13, 2026 at 8:38\u202fPM Leo Martins &lt;loemra.dev@gmail.com&gt; wrote:
&gt; &gt;
&gt; &gt; When memory pressure causes writeback of a recently COW&#x27;d buffer,
&gt; &gt; btrfs sets BTRFS_HEADER_FLAG_WRITTEN on it. Subsequent
&gt; &gt; btrfs_search_slot() restarts then see the WRITTEN flag and re-COW
&gt; &gt; the buffer unnecessarily, causing COW amplification that can exhaust
&gt; &gt; block reservations and degrade throughput.
&gt; &gt;
&gt; &gt; Overwriting in place is crash-safe because the committed superblock
&gt; &gt; does not reference buffers allocated in the current (uncommitted)
&gt; &gt; transaction, so no on-disk tree points to this block yet.
&gt; &gt;
&gt; &gt; When should_cow_block() encounters a WRITTEN buffer whose generation
&gt; &gt; matches the current transaction, instead of requesting a COW, re-dirty
&gt; &gt; the buffer and re-register its range in the transaction&#x27;s dirty_pages.
&gt; &gt;
&gt; &gt; Both are necessary because btrfs tracks dirty metadata through two
&gt; &gt; independent mechanisms. set_extent_buffer_dirty() sets the
&gt; &gt; EXTENT_BUFFER_DIRTY flag and the buffer_tree xarray PAGECACHE_TAG_DIRTY
&gt; &gt; mark, which is what background writeback (btree_write_cache_pages) uses
&gt; &gt; to find and write dirty buffers. The transaction&#x27;s dirty_pages io tree
&gt; &gt; is a separate structure used by btrfs_write_and_wait_transaction() at
&gt; &gt; commit time to ensure all buffers allocated during the transaction are
&gt; &gt; persisted. The dirty_pages range was originally registered in
&gt; &gt; btrfs_init_new_buffer() when the block was first allocated, but
&gt; &gt; background writeback may have already written and cleared it.
&gt; 
&gt; This is not quite correct, the dirty_pages range is never cleared on
&gt; background writeback.
&gt; We only clear it during a transaction commit, in
&gt; btrfs_write_and_wait_transaction().
&gt; 
&gt; Normally we shouldn&#x27;t care about setting the range again in
&gt; dirty_pages, because after
&gt; we call  btrfs_write_and_wait_transaction(), no more COW should be
&gt; possible using this
&gt; transaction (which is in the unblocked state, so any new COW attempt
&gt; will be in another transaction).
&gt; 
&gt; The exception is if we have snapshots to create and qgroups are
&gt; enabled, since in qgroup_account_snapshot() we
&gt; call btrfs_write_and_wait_transaction() and after that we can get more
&gt; COW, due to all the stuff we need to do to
&gt; create a snapshot, before we get to the final call to
&gt; btrfs_write_and_wait_transaction() right before we write the
&gt; super blocks in btrfs_commit_transaction().

Got it, thanks for the correction. Updated in v3.

Thanks,
Leo

&gt; 
&gt; &gt;
&gt; &gt; Keep BTRFS_HEADER_FLAG_WRITTEN set so that btrfs_free_tree_block()
&gt; &gt; correctly pins the block if it is freed later.
&gt; &gt;
&gt; &gt; Exclude cases where in-place overwrite is not safe:
&gt; &gt;  - EXTENT_BUFFER_WRITEBACK: buffer is mid-I/O
&gt; &gt;  - Zoned devices: require sequential writes
&gt; &gt;  - Log trees: log blocks are immediately referenced by a committed
&gt; &gt;    superblock via btrfs_sync_log(), so overwriting could corrupt the
&gt; &gt;    committed log
&gt; &gt;  - BTRFS_ROOT_FORCE_COW: snapshot in progress
&gt; &gt;  - BTRFS_HEADER_FLAG_RELOC: block being relocated
&gt; &gt;
&gt; &gt; Signed-off-by: Leo Martins &lt;loemra.dev@gmail.com&gt;
&gt; &gt; ---
&gt; &gt;  fs/btrfs/ctree.c | 53 +++++++++++++++++++++++++++++++++++++++++++++---
&gt; &gt;  1 file changed, 50 insertions(+), 3 deletions(-)
&gt; &gt;
&gt; &gt; diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
&gt; &gt; index 7267b2502665..a345e1be24d8 100644
&gt; &gt; --- a/fs/btrfs/ctree.c
&gt; &gt; +++ b/fs/btrfs/ctree.c
&gt; &gt; @@ -599,9 +599,9 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,
&gt; &gt;         return ret;
&gt; &gt;  }
&gt; &gt;
&gt; &gt; -static inline bool should_cow_block(const struct btrfs_trans_handle *trans,
&gt; &gt; +static inline bool should_cow_block(struct btrfs_trans_handle *trans,
&gt; &gt;                                     const struct btrfs_root *root,
&gt; &gt; -                                   const struct extent_buffer *buf)
&gt; &gt; +                                   struct extent_buffer *buf)
&gt; &gt;  {
&gt; &gt;         if (btrfs_is_testing(root-&gt;fs_info))
&gt; &gt;                 return false;
&gt; &gt; @@ -621,8 +621,55 @@ static inline bool should_cow_block(const struct btrfs_trans_handle *trans,
&gt; &gt;         if (btrfs_header_generation(buf) != trans-&gt;transid)
&gt; &gt;                 return true;
&gt; &gt;
&gt; &gt; -       if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN))
&gt; &gt; +       if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {
&gt; &gt; +               /*
&gt; &gt; +                * The buffer was allocated in this transaction and has been
&gt; &gt; +                * written back to disk (WRITTEN is set). Normally we&#x27;d COW
&gt; &gt; +                * it again, but since the committed superblock doesn&#x27;t
&gt; &gt; +                * reference this buffer (it was allocated this transaction),
&gt; 
&gt; Missing an &quot;in&quot; before &quot;this transaction&quot;.
&gt; 
&gt; &gt; +                * we can safely overwrite it in place.
&gt; &gt; +                *
&gt; &gt; +                * We keep BTRFS_HEADER_FLAG_WRITTEN set. The block has been
&gt; &gt; +                * persisted at this bytenr and will be again after the
&gt; &gt; +                * in-place update. This is important so that
&gt; &gt; +                * btrfs_free_tree_block() correctly pins the block if it is
&gt; &gt; +                * freed later (e.g., during tree rebalancing or FORCE_COW).
&gt; &gt; +                *
&gt; &gt; +                * We re-dirty the buffer to ensure the in-place modifications
&gt; &gt; +                * will be written back to disk.
&gt; &gt; +                *
&gt; &gt; +                * Exclusions:
&gt; &gt; +                * - Log trees: log blocks are written and immediately
&gt; &gt; +                *   referenced by a committed superblock via
&gt; &gt; +                *   btrfs_sync_log(), bypassing the normal transaction
&gt; &gt; +                *   commit. Overwriting in place could corrupt the
&gt; &gt; +                *   committed log.
&gt; &gt; +                * - Zoned devices: require sequential writes
&gt; &gt; +                * - FORCE_COW: snapshot in progress
&gt; &gt; +                * - RELOC flag: block being relocated
&gt; &gt; +                */
&gt; &gt; +               if (!test_bit(EXTENT_BUFFER_WRITEBACK, &amp;buf-&gt;bflags) &amp;&amp;
&gt; &gt; +                   !btrfs_is_zoned(root-&gt;fs_info) &amp;&amp;
&gt; &gt; +                   btrfs_root_id(root) != BTRFS_TREE_LOG_OBJECTID &amp;&amp;
&gt; &gt; +                   !test_bit(BTRFS_ROOT_FORCE_COW, &amp;root-&gt;state) &amp;&amp;
&gt; 
&gt; We need a  smp_mb__before_atomic() before checking FORCE_COW, see the
&gt; existing code below.
&gt; 
&gt; &gt; +                   !btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC)) {
&gt; &gt; +                       /*
&gt; &gt; +                        * Re-register this block&#x27;s range in the current
&gt; &gt; +                        * transaction&#x27;s dirty_pages so that
&gt; &gt; +                        * btrfs_write_and_wait_transaction() writes it.
&gt; &gt; +                        * The range was originally registered when the block
&gt; &gt; +                        * was allocated, but that transaction&#x27;s dirty_pages
&gt; &gt; +                        * may have already been released.
&gt; 
&gt; I think it&#x27;s worth adding something like: &quot;... already been released
&gt; if we are in a transaction that creates snapshots and we have qgroups
&gt; enabled.&quot;
&gt; 
&gt; Otherwise it looks good, thanks!
&gt; 
&gt; &gt; +                        */
&gt; &gt; +                       btrfs_set_extent_bit(&amp;trans-&gt;transaction-&gt;dirty_pages,
&gt; &gt; +                                            buf-&gt;start,
&gt; &gt; +                                            buf-&gt;start + buf-&gt;len - 1,
&gt; &gt; +                                            EXTENT_DIRTY, NULL);
&gt; &gt; +                       set_extent_buffer_dirty(buf);
&gt; &gt; +                       return false;
&gt; &gt; +               }
&gt; &gt;                 return true;
&gt; &gt; +       }
&gt; &gt;
&gt; &gt;         /* Ensure we can see the FORCE_COW bit. */
&gt; &gt;         smp_mb__before_atomic();
&gt; &gt; --
&gt; &gt; 2.47.3
&gt; &gt;
&gt; &gt;


---

On Sat, 14 Feb 2026 09:25:03 +0800 Sun YangKai &lt;sunk67188@gmail.com&gt; wrote:

&gt; Thanks for your working on this and I&#x27;ve expecting this for a long time :)

Thanks for the review!

&gt; 
&gt; On 2026/2/14 04:30, Leo Martins wrote:
&gt; &gt; When memory pressure causes writeback of a recently COW&#x27;d buffer,
&gt; &gt; btrfs sets BTRFS_HEADER_FLAG_WRITTEN on it. Subsequent
&gt; &gt; btrfs_search_slot() restarts then see the WRITTEN flag and re-COW
&gt; &gt; the buffer unnecessarily, causing COW amplification that can exhaust
&gt; &gt; block reservations and degrade throughput.
&gt; &gt; 
&gt; &gt; Overwriting in place is crash-safe because the committed superblock
&gt; &gt; does not reference buffers allocated in the current (uncommitted)
&gt; &gt; transaction, so no on-disk tree points to this block yet.
&gt; &gt; 
&gt; &gt; When should_cow_block() encounters a WRITTEN buffer whose generation
&gt; &gt; matches the current transaction, instead of requesting a COW, re-dirty
&gt; &gt; the buffer and re-register its range in the transaction&#x27;s dirty_pages.
&gt; &gt; 
&gt; &gt; Both are necessary because btrfs tracks dirty metadata through two
&gt; &gt; independent mechanisms. set_extent_buffer_dirty() sets the
&gt; &gt; EXTENT_BUFFER_DIRTY flag and the buffer_tree xarray PAGECACHE_TAG_DIRTY
&gt; &gt; mark, which is what background writeback (btree_write_cache_pages) uses
&gt; &gt; to find and write dirty buffers. The transaction&#x27;s dirty_pages io tree
&gt; &gt; is a separate structure used by btrfs_write_and_wait_transaction() at
&gt; &gt; commit time to ensure all buffers allocated during the transaction are
&gt; &gt; persisted. The dirty_pages range was originally registered in
&gt; &gt; btrfs_init_new_buffer() when the block was first allocated, but
&gt; &gt; background writeback may have already written and cleared it.
&gt; &gt; 
&gt; &gt; Keep BTRFS_HEADER_FLAG_WRITTEN set so that btrfs_free_tree_block()
&gt; &gt; correctly pins the block if it is freed later.
&gt; &gt; 
&gt; &gt; Exclude cases where in-place overwrite is not safe:
&gt; &gt;   - EXTENT_BUFFER_WRITEBACK: buffer is mid-I/O
&gt; &gt;   - Zoned devices: require sequential writes
&gt; &gt;   - Log trees: log blocks are immediately referenced by a committed
&gt; &gt;     superblock via btrfs_sync_log(), so overwriting could corrupt the
&gt; &gt;     committed log
&gt; &gt;   - BTRFS_ROOT_FORCE_COW: snapshot in progress
&gt; &gt;   - BTRFS_HEADER_FLAG_RELOC: block being relocated
&gt; &gt; 
&gt; &gt; Signed-off-by: Leo Martins &lt;loemra.dev@gmail.com&gt;
&gt; &gt; ---
&gt; &gt;   fs/btrfs/ctree.c | 53 +++++++++++++++++++++++++++++++++++++++++++++---
&gt; &gt;   1 file changed, 50 insertions(+), 3 deletions(-)
&gt; &gt; 
&gt; &gt; diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
&gt; &gt; index 7267b2502665..a345e1be24d8 100644
&gt; &gt; --- a/fs/btrfs/ctree.c
&gt; &gt; +++ b/fs/btrfs/ctree.c
&gt; &gt; @@ -599,9 +599,9 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,
&gt; &gt;   	return ret;
&gt; &gt;   }
&gt; &gt;   
&gt; &gt; -static inline bool should_cow_block(const struct btrfs_trans_handle *trans,
&gt; &gt; +static inline bool should_cow_block(struct btrfs_trans_handle *trans,
&gt; &gt;   				    const struct btrfs_root *root,
&gt; &gt; -				    const struct extent_buffer *buf)
&gt; &gt; +				    struct extent_buffer *buf)
&gt; &gt;   {
&gt; &gt;   	if (btrfs_is_testing(root-&gt;fs_info))
&gt; &gt;   		return false;
&gt; &gt; @@ -621,8 +621,55 @@ static inline bool should_cow_block(const struct btrfs_trans_handle *trans,
&gt; &gt;   	if (btrfs_header_generation(buf) != trans-&gt;transid)
&gt; &gt;   		return true;
&gt; &gt;   
&gt; &gt; -	if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN))
&gt; &gt; +	if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {
&gt; &gt; +		/*
&gt; &gt; +		 * The buffer was allocated in this transaction and has been
&gt; &gt; +		 * written back to disk (WRITTEN is set). Normally we&#x27;d COW
&gt; &gt; +		 * it again, but since the committed superblock doesn&#x27;t
&gt; &gt; +		 * reference this buffer (it was allocated this transaction),
&gt; &gt; +		 * we can safely overwrite it in place.
&gt; &gt; +		 *
&gt; &gt; +		 * We keep BTRFS_HEADER_FLAG_WRITTEN set. The block has been
&gt; &gt; +		 * persisted at this bytenr and will be again after the
&gt; &gt; +		 * in-place update. This is important so that
&gt; &gt; +		 * btrfs_free_tree_block() correctly pins the block if it is
&gt; &gt; +		 * freed later (e.g., during tree rebalancing or FORCE_COW).
&gt; &gt; +		 *
&gt; &gt; +		 * We re-dirty the buffer to ensure the in-place modifications
&gt; &gt; +		 * will be written back to disk.
&gt; &gt; +		 *
&gt; &gt; +		 * Exclusions:
&gt; &gt; +		 * - Log trees: log blocks are written and immediately
&gt; &gt; +		 *   referenced by a committed superblock via
&gt; &gt; +		 *   btrfs_sync_log(), bypassing the normal transaction
&gt; &gt; +		 *   commit. Overwriting in place could corrupt the
&gt; &gt; +		 *   committed log.
&gt; &gt; +		 * - Zoned devices: require sequential writes
&gt; &gt; +		 * - FORCE_COW: snapshot in progress
&gt; &gt; +		 * - RELOC flag: block being relocated
&gt; &gt; +		 */
&gt; &gt; +		if (!test_bit(EXTENT_BUFFER_WRITEBACK, &amp;buf-&gt;bflags) &amp;&amp;
&gt; &gt; +		    !btrfs_is_zoned(root-&gt;fs_info) &amp;&amp;
&gt; &gt; +		    btrfs_root_id(root) != BTRFS_TREE_LOG_OBJECTID &amp;&amp;
&gt; &gt; +		    !test_bit(BTRFS_ROOT_FORCE_COW, &amp;root-&gt;state) &amp;&amp;
&gt; it seems we need smp_mb__before_atomic() to see the FORCE_COW bit?

Good call, fixed in v3.

&gt; &gt; +		    !btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC)) {
&gt; &gt; +			/*
&gt; &gt; +			 * Re-register this block&#x27;s range in the current
&gt; &gt; +			 * transaction&#x27;s dirty_pages so that
&gt; &gt; +			 * btrfs_write_and_wait_transaction() writes it.
&gt; &gt; +			 * The range was originally registered when the block
&gt; &gt; +			 * was allocated, but that transaction&#x27;s dirty_pages
&gt; &gt; +			 * may have already been released.
&gt; &gt; +			 */
&gt; &gt; +			btrfs_set_extent_bit(&amp;trans-&gt;transaction-&gt;dirty_pages,
&gt; &gt; +					     buf-&gt;start,
&gt; &gt; +					     buf-&gt;start + buf-&gt;len - 1,
&gt; &gt; +					     EXTENT_DIRTY, NULL);
&gt; &gt; +			set_extent_buffer_dirty(buf);
&gt; why use set_extent_buffer_dirty() instead of btrfs_mark_buffer_dirty()? 
&gt; I don&#x27;t see any other callers doing this.

btrfs_mark_buffer_dirty() calls btrfs_assert_tree_write_locked(buf),
but should_cow_block() may be called from btrfs_search_slot() when the
buffer only holds a read lock (root node acquired with BTRFS_READ_LOCK
in btrfs_search_slot_get_root()). Added a comment explaining this in 
v3.

&gt; &gt; +			return false;
&gt; &gt; +		}
&gt; &gt;   		return true;
&gt; &gt; +	}
&gt; &gt;   
&gt; &gt;   	/* Ensure we can see the FORCE_COW bit. */
&gt; &gt;   	smp_mb__before_atomic();
&gt; 
&gt; And I wonder if we could have something more readable like this:
&gt; 
&gt; 	if (btrfs_header_generation(buf) != trans-&gt;transid)
&gt; 		return true;
&gt; 
&gt; 	if (test_bit(EXTENT_BUFFER_WRITEBACK, &amp;buf-&gt;bflags))
&gt; 		return true;
&gt; 
&gt; 	if (btrfs_root_id(root) != BTRFS_TREE_RELOC_OBJECTID &amp;&amp;
&gt; 	    btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC))
&gt; 		return true;
&gt; 
&gt; 	/* Ensure we can see the FORCE_COW bit. */
&gt; 	smp_mb__before_atomic();
&gt; 	if (test_bit(BTRFS_ROOT_FORCE_COW, &amp;root-&gt;state))
&gt; 		return true;
&gt; 
&gt; 	if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {
&gt; 		if (btrfs_root_id(root) == BTRFS_TREE_LOG_OBJECTID ||
&gt; 		    btrfs_is_zoned(root-&gt;fs_info))
&gt; 				return true;
&gt; 		btrfs_set_extent_bit(&amp;trans-&gt;transaction-&gt;dirty_pages,
&gt; 				     buf-&gt;start,
&gt; 				     buf-&gt;start + buf-&gt;len - 1,
&gt; 				     EXTENT_DIRTY, NULL);
&gt; 		btrfs_mark_buffer_dirty(trans, buf);
&gt; 	}
&gt; 
&gt; 	return false;

Updated in v3!

&gt; 
&gt; Thanks,
&gt; Sun YangKai


---

On Mon, 16 Feb 2026 12:40:04 +0000 Filipe Manana &lt;fdmanana@kernel.org&gt; wrote:

&gt; On Fri, Feb 13, 2026 at 8:37\u202fPM Leo Martins &lt;loemra.dev@gmail.com&gt; wrote:
&gt; &gt;
&gt; &gt; Add a btrfs_search_slot_stats tracepoint to btrfs_search_slot() for
&gt; &gt; measuring COW amplification.
&gt; &gt;
&gt; &gt; The tracepoint fires when a search with at least one COW completes,
&gt; &gt; recording the root, total cow_count, restart_count, and return value.
&gt; &gt; cow_count and restart_count per search_slot call are useful metrics
&gt; &gt; for tracking COW amplification.
&gt; &gt;
&gt; &gt; Signed-off-by: Leo Martins &lt;loemra.dev@gmail.com&gt;
&gt; &gt; ---
&gt; &gt;  fs/btrfs/ctree.c             | 15 +++++++++++++--
&gt; &gt;  include/trace/events/btrfs.h | 26 ++++++++++++++++++++++++++
&gt; &gt;  2 files changed, 39 insertions(+), 2 deletions(-)
&gt; &gt;
&gt; &gt; diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
&gt; &gt; index 55187ba59cc0..1971d7bb5f60 100644
&gt; &gt; --- a/fs/btrfs/ctree.c
&gt; &gt; +++ b/fs/btrfs/ctree.c
&gt; &gt; @@ -2069,6 +2069,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt; &gt;         u8 lowest_level = 0;
&gt; &gt;         int min_write_lock_level;
&gt; &gt;         int prev_cmp;
&gt; &gt; +       int cow_count = 0;
&gt; &gt; +       int restart_count = 0;
&gt; &gt;
&gt; &gt;         if (!root)
&gt; &gt;                 return -EINVAL;
&gt; &gt; @@ -2157,6 +2159,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt; &gt;                             p-&gt;nodes[level + 1])) {
&gt; &gt;                                 write_lock_level = level + 1;
&gt; &gt;                                 btrfs_release_path(p);
&gt; &gt; +                               restart_count++;
&gt; &gt;                                 goto again;
&gt; &gt;                         }
&gt; &gt;
&gt; &gt; @@ -2172,6 +2175,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt; &gt;                                 ret = ret2;
&gt; &gt;                                 goto done;
&gt; &gt;                         }
&gt; &gt; +                       cow_count++;
&gt; &gt;                 }
&gt; &gt;  cow_done:
&gt; &gt;                 p-&gt;nodes[level] = b;
&gt; &gt; @@ -2219,8 +2223,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt; &gt;                 p-&gt;slots[level] = slot;
&gt; &gt;                 ret2 = setup_nodes_for_search(trans, root, p, b, level, ins_len,
&gt; &gt;                                               &amp;write_lock_level);
&gt; &gt; -               if (ret2 == -EAGAIN)
&gt; &gt; +               if (ret2 == -EAGAIN) {
&gt; &gt; +                       restart_count++;
&gt; &gt;                         goto again;
&gt; &gt; +               }
&gt; &gt;                 if (ret2) {
&gt; &gt;                         ret = ret2;
&gt; &gt;                         goto done;
&gt; &gt; @@ -2236,6 +2242,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt; &gt;                 if (slot == 0 &amp;&amp; ins_len &amp;&amp; write_lock_level &lt; level + 1) {
&gt; &gt;                         write_lock_level = level + 1;
&gt; &gt;                         btrfs_release_path(p);
&gt; &gt; +                       restart_count++;
&gt; &gt;                         goto again;
&gt; &gt;                 }
&gt; &gt;
&gt; &gt; @@ -2249,8 +2256,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt; &gt;                 }
&gt; &gt;
&gt; &gt;                 ret2 = read_block_for_search(root, p, &amp;b, slot, key);
&gt; &gt; -               if (ret2 == -EAGAIN &amp;&amp; !p-&gt;nowait)
&gt; &gt; +               if (ret2 == -EAGAIN &amp;&amp; !p-&gt;nowait) {
&gt; &gt; +                       restart_count++;
&gt; &gt;                         goto again;
&gt; &gt; +               }
&gt; &gt;                 if (ret2) {
&gt; &gt;                         ret = ret2;
&gt; &gt;                         goto done;
&gt; &gt; @@ -2281,6 +2290,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt; &gt;         }
&gt; &gt;         ret = 1;
&gt; &gt;  done:
&gt; &gt; +       if (cow_count &gt; 0)
&gt; &gt; +               trace_btrfs_search_slot_stats(root, cow_count, restart_count, ret);
&gt; 
&gt; So I find this way too specific, plus even if trace points are
&gt; disabled we have the overhead of the counters (and inside critical
&gt; sections).
&gt; 
&gt; We already have a tracepoint for COW, trace_btrfs_cow_block(), and we
&gt; could have one just for the retry thing, maybe naming it like
&gt; trace_btrfs_search_slot_restart() or something.
&gt; So we could use those two tracepoints to measure things (bpftrace
&gt; scripts could easily report a count of each trace point and such),
&gt; instead of this highly specialized tracepoint that adds some overhead
&gt; when tracepoints are disabled.

Good point, added a per-restart-site trace_btrfs_search_slot_restart()
tracepoint in v3.

Thanks,
Leo

&gt; 
&gt; Thanks.
&gt; 
&gt; 
&gt; &gt;         if (ret &lt; 0 &amp;&amp; !p-&gt;skip_release_on_error)
&gt; &gt;                 btrfs_release_path(p);
&gt; &gt;
&gt; &gt; diff --git a/include/trace/events/btrfs.h b/include/trace/events/btrfs.h
&gt; &gt; index 125bdc166bfe..b8934938a087 100644
&gt; &gt; --- a/include/trace/events/btrfs.h
&gt; &gt; +++ b/include/trace/events/btrfs.h
&gt; &gt; @@ -1110,6 +1110,32 @@ TRACE_EVENT(btrfs_cow_block,
&gt; &gt;                   __entry-&gt;cow_level)
&gt; &gt;  );
&gt; &gt;
&gt; &gt; +TRACE_EVENT(btrfs_search_slot_stats,
&gt; &gt; +
&gt; &gt; +       TP_PROTO(const struct btrfs_root *root,
&gt; &gt; +                int cow_count, int restart_count, int ret),
&gt; &gt; +
&gt; &gt; +       TP_ARGS(root, cow_count, restart_count, ret),
&gt; &gt; +
&gt; &gt; +       TP_STRUCT__entry_btrfs(
&gt; &gt; +               __field(        u64,    root_objectid           )
&gt; &gt; +               __field(        int,    cow_count               )
&gt; &gt; +               __field(        int,    restart_count           )
&gt; &gt; +               __field(        int,    ret                     )
&gt; &gt; +       ),
&gt; &gt; +
&gt; &gt; +       TP_fast_assign_btrfs(root-&gt;fs_info,
&gt; &gt; +               __entry-&gt;root_objectid  = btrfs_root_id(root);
&gt; &gt; +               __entry-&gt;cow_count      = cow_count;
&gt; &gt; +               __entry-&gt;restart_count  = restart_count;
&gt; &gt; +               __entry-&gt;ret            = ret;
&gt; &gt; +       ),
&gt; &gt; +
&gt; &gt; +       TP_printk_btrfs(&quot;root=%llu(%s) cow_count=%d restarts=%d ret=%d&quot;,
&gt; &gt; +                 show_root_type(__entry-&gt;root_objectid),
&gt; &gt; +                 __entry-&gt;cow_count, __entry-&gt;restart_count, __entry-&gt;ret)
&gt; &gt; +);
&gt; &gt; +
&gt; &gt;  TRACE_EVENT(btrfs_space_reservation,
&gt; &gt;
&gt; &gt;         TP_PROTO(const struct btrfs_fs_info *fs_info, const char *type, u64 val,
&gt; &gt; --
&gt; &gt; 2.47.3
&gt; &gt;
&gt; &gt;

</pre>
</details>
<div class="review-comment-signals">Signals: acknowledged corrections, provided feedback on other patches</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Sun YangKai</span>
<a class="date-chip" href="../2026-02-17_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-17">2026-02-17</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Sun YangKai raised two specific technical concerns: the need for smp_mb__before_atomic() to see the FORCE_COW bit and the use of set_extent_buffer_dirty() instead of btrfs_mark_buffer_dirty(). He also suggested a more readable code reorganization.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Thanks for your working on this and I&#x27;ve expecting this for a long time :)

On 2026/2/14 04:30, Leo Martins wrote:
&gt; When memory pressure causes writeback of a recently COW&#x27;d buffer,
&gt; btrfs sets BTRFS_HEADER_FLAG_WRITTEN on it. Subsequent
&gt; btrfs_search_slot() restarts then see the WRITTEN flag and re-COW
&gt; the buffer unnecessarily, causing COW amplification that can exhaust
&gt; block reservations and degrade throughput.
&gt; 
&gt; Overwriting in place is crash-safe because the committed superblock
&gt; does not reference buffers allocated in the current (uncommitted)
&gt; transaction, so no on-disk tree points to this block yet.
&gt; 
&gt; When should_cow_block() encounters a WRITTEN buffer whose generation
&gt; matches the current transaction, instead of requesting a COW, re-dirty
&gt; the buffer and re-register its range in the transaction&#x27;s dirty_pages.
&gt; 
&gt; Both are necessary because btrfs tracks dirty metadata through two
&gt; independent mechanisms. set_extent_buffer_dirty() sets the
&gt; EXTENT_BUFFER_DIRTY flag and the buffer_tree xarray PAGECACHE_TAG_DIRTY
&gt; mark, which is what background writeback (btree_write_cache_pages) uses
&gt; to find and write dirty buffers. The transaction&#x27;s dirty_pages io tree
&gt; is a separate structure used by btrfs_write_and_wait_transaction() at
&gt; commit time to ensure all buffers allocated during the transaction are
&gt; persisted. The dirty_pages range was originally registered in
&gt; btrfs_init_new_buffer() when the block was first allocated, but
&gt; background writeback may have already written and cleared it.
&gt; 
&gt; Keep BTRFS_HEADER_FLAG_WRITTEN set so that btrfs_free_tree_block()
&gt; correctly pins the block if it is freed later.
&gt; 
&gt; Exclude cases where in-place overwrite is not safe:
&gt;   - EXTENT_BUFFER_WRITEBACK: buffer is mid-I/O
&gt;   - Zoned devices: require sequential writes
&gt;   - Log trees: log blocks are immediately referenced by a committed
&gt;     superblock via btrfs_sync_log(), so overwriting could corrupt the
&gt;     committed log
&gt;   - BTRFS_ROOT_FORCE_COW: snapshot in progress
&gt;   - BTRFS_HEADER_FLAG_RELOC: block being relocated
&gt; 
&gt; Signed-off-by: Leo Martins &lt;loemra.dev@gmail.com&gt;
&gt; ---
&gt;   fs/btrfs/ctree.c | 53 +++++++++++++++++++++++++++++++++++++++++++++---
&gt;   1 file changed, 50 insertions(+), 3 deletions(-)
&gt; 
&gt; diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
&gt; index 7267b2502665..a345e1be24d8 100644
&gt; --- a/fs/btrfs/ctree.c
&gt; +++ b/fs/btrfs/ctree.c
&gt; @@ -599,9 +599,9 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,
&gt;   	return ret;
&gt;   }
&gt;   
&gt; -static inline bool should_cow_block(const struct btrfs_trans_handle *trans,
&gt; +static inline bool should_cow_block(struct btrfs_trans_handle *trans,
&gt;   				    const struct btrfs_root *root,
&gt; -				    const struct extent_buffer *buf)
&gt; +				    struct extent_buffer *buf)
&gt;   {
&gt;   	if (btrfs_is_testing(root-&gt;fs_info))
&gt;   		return false;
&gt; @@ -621,8 +621,55 @@ static inline bool should_cow_block(const struct btrfs_trans_handle *trans,
&gt;   	if (btrfs_header_generation(buf) != trans-&gt;transid)
&gt;   		return true;
&gt;   
&gt; -	if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN))
&gt; +	if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {
&gt; +		/*
&gt; +		 * The buffer was allocated in this transaction and has been
&gt; +		 * written back to disk (WRITTEN is set). Normally we&#x27;d COW
&gt; +		 * it again, but since the committed superblock doesn&#x27;t
&gt; +		 * reference this buffer (it was allocated this transaction),
&gt; +		 * we can safely overwrite it in place.
&gt; +		 *
&gt; +		 * We keep BTRFS_HEADER_FLAG_WRITTEN set. The block has been
&gt; +		 * persisted at this bytenr and will be again after the
&gt; +		 * in-place update. This is important so that
&gt; +		 * btrfs_free_tree_block() correctly pins the block if it is
&gt; +		 * freed later (e.g., during tree rebalancing or FORCE_COW).
&gt; +		 *
&gt; +		 * We re-dirty the buffer to ensure the in-place modifications
&gt; +		 * will be written back to disk.
&gt; +		 *
&gt; +		 * Exclusions:
&gt; +		 * - Log trees: log blocks are written and immediately
&gt; +		 *   referenced by a committed superblock via
&gt; +		 *   btrfs_sync_log(), bypassing the normal transaction
&gt; +		 *   commit. Overwriting in place could corrupt the
&gt; +		 *   committed log.
&gt; +		 * - Zoned devices: require sequential writes
&gt; +		 * - FORCE_COW: snapshot in progress
&gt; +		 * - RELOC flag: block being relocated
&gt; +		 */
&gt; +		if (!test_bit(EXTENT_BUFFER_WRITEBACK, &amp;buf-&gt;bflags) &amp;&amp;
&gt; +		    !btrfs_is_zoned(root-&gt;fs_info) &amp;&amp;
&gt; +		    btrfs_root_id(root) != BTRFS_TREE_LOG_OBJECTID &amp;&amp;
&gt; +		    !test_bit(BTRFS_ROOT_FORCE_COW, &amp;root-&gt;state) &amp;&amp;
it seems we need smp_mb__before_atomic() to see the FORCE_COW bit?
&gt; +		    !btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC)) {
&gt; +			/*
&gt; +			 * Re-register this block&#x27;s range in the current
&gt; +			 * transaction&#x27;s dirty_pages so that
&gt; +			 * btrfs_write_and_wait_transaction() writes it.
&gt; +			 * The range was originally registered when the block
&gt; +			 * was allocated, but that transaction&#x27;s dirty_pages
&gt; +			 * may have already been released.
&gt; +			 */
&gt; +			btrfs_set_extent_bit(&amp;trans-&gt;transaction-&gt;dirty_pages,
&gt; +					     buf-&gt;start,
&gt; +					     buf-&gt;start + buf-&gt;len - 1,
&gt; +					     EXTENT_DIRTY, NULL);
&gt; +			set_extent_buffer_dirty(buf);
why use set_extent_buffer_dirty() instead of btrfs_mark_buffer_dirty()? 
I don&#x27;t see any other callers doing this.
&gt; +			return false;
&gt; +		}
&gt;   		return true;
&gt; +	}
&gt;   
&gt;   	/* Ensure we can see the FORCE_COW bit. */
&gt;   	smp_mb__before_atomic();

And I wonder if we could have something more readable like this:

	if (btrfs_header_generation(buf) != trans-&gt;transid)
		return true;

	if (test_bit(EXTENT_BUFFER_WRITEBACK, &amp;buf-&gt;bflags))
		return true;

	if (btrfs_root_id(root) != BTRFS_TREE_RELOC_OBJECTID &amp;&amp;
	    btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC))
		return true;

	/* Ensure we can see the FORCE_COW bit. */
	smp_mb__before_atomic();
	if (test_bit(BTRFS_ROOT_FORCE_COW, &amp;root-&gt;state))
		return true;

	if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {
		if (btrfs_root_id(root) == BTRFS_TREE_LOG_OBJECTID ||
		    btrfs_is_zoned(root-&gt;fs_info))
				return true;
		btrfs_set_extent_bit(&amp;trans-&gt;transaction-&gt;dirty_pages,
				     buf-&gt;start,
				     buf-&gt;start + buf-&gt;len - 1,
				     EXTENT_DIRTY, NULL);
		btrfs_mark_buffer_dirty(trans, buf);
	}

	return false;

Thanks,
Sun YangKai


</pre>
</details>
<div class="review-comment-signals">Signals: requested changes, code review suggestions</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Filipe Manana</span>
<a class="date-chip" href="../2026-02-17_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-17">2026-02-17</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Filipe Manana reviewed the patch to inhibit extent buffer writeback, pointing out several minor issues and suggesting improvements for consistency in comments and code formatting.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Fri, Feb 13, 2026 at 8:38 PM Leo Martins &lt;loemra.dev@gmail.com&gt; wrote:
&gt;
&gt; When memory pressure causes writeback of a recently COW&#x27;d buffer,
&gt; btrfs sets BTRFS_HEADER_FLAG_WRITTEN on it. Subsequent
&gt; btrfs_search_slot() restarts then see the WRITTEN flag and re-COW
&gt; the buffer unnecessarily, causing COW amplification that can exhaust
&gt; block reservations and degrade throughput.
&gt;
&gt; Overwriting in place is crash-safe because the committed superblock
&gt; does not reference buffers allocated in the current (uncommitted)
&gt; transaction, so no on-disk tree points to this block yet.
&gt;
&gt; When should_cow_block() encounters a WRITTEN buffer whose generation
&gt; matches the current transaction, instead of requesting a COW, re-dirty
&gt; the buffer and re-register its range in the transaction&#x27;s dirty_pages.
&gt;
&gt; Both are necessary because btrfs tracks dirty metadata through two
&gt; independent mechanisms. set_extent_buffer_dirty() sets the
&gt; EXTENT_BUFFER_DIRTY flag and the buffer_tree xarray PAGECACHE_TAG_DIRTY
&gt; mark, which is what background writeback (btree_write_cache_pages) uses
&gt; to find and write dirty buffers. The transaction&#x27;s dirty_pages io tree
&gt; is a separate structure used by btrfs_write_and_wait_transaction() at
&gt; commit time to ensure all buffers allocated during the transaction are
&gt; persisted. The dirty_pages range was originally registered in
&gt; btrfs_init_new_buffer() when the block was first allocated, but
&gt; background writeback may have already written and cleared it.

This is not quite correct, the dirty_pages range is never cleared on
background writeback.
We only clear it during a transaction commit, in
btrfs_write_and_wait_transaction().

Normally we shouldn&#x27;t care about setting the range again in
dirty_pages, because after
we call  btrfs_write_and_wait_transaction(), no more COW should be
possible using this
transaction (which is in the unblocked state, so any new COW attempt
will be in another transaction).

The exception is if we have snapshots to create and qgroups are
enabled, since in qgroup_account_snapshot() we
call btrfs_write_and_wait_transaction() and after that we can get more
COW, due to all the stuff we need to do to
create a snapshot, before we get to the final call to
btrfs_write_and_wait_transaction() right before we write the
super blocks in btrfs_commit_transaction().

&gt;
&gt; Keep BTRFS_HEADER_FLAG_WRITTEN set so that btrfs_free_tree_block()
&gt; correctly pins the block if it is freed later.
&gt;
&gt; Exclude cases where in-place overwrite is not safe:
&gt;  - EXTENT_BUFFER_WRITEBACK: buffer is mid-I/O
&gt;  - Zoned devices: require sequential writes
&gt;  - Log trees: log blocks are immediately referenced by a committed
&gt;    superblock via btrfs_sync_log(), so overwriting could corrupt the
&gt;    committed log
&gt;  - BTRFS_ROOT_FORCE_COW: snapshot in progress
&gt;  - BTRFS_HEADER_FLAG_RELOC: block being relocated
&gt;
&gt; Signed-off-by: Leo Martins &lt;loemra.dev@gmail.com&gt;
&gt; ---
&gt;  fs/btrfs/ctree.c | 53 +++++++++++++++++++++++++++++++++++++++++++++---
&gt;  1 file changed, 50 insertions(+), 3 deletions(-)
&gt;
&gt; diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
&gt; index 7267b2502665..a345e1be24d8 100644
&gt; --- a/fs/btrfs/ctree.c
&gt; +++ b/fs/btrfs/ctree.c
&gt; @@ -599,9 +599,9 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,
&gt;         return ret;
&gt;  }
&gt;
&gt; -static inline bool should_cow_block(const struct btrfs_trans_handle *trans,
&gt; +static inline bool should_cow_block(struct btrfs_trans_handle *trans,
&gt;                                     const struct btrfs_root *root,
&gt; -                                   const struct extent_buffer *buf)
&gt; +                                   struct extent_buffer *buf)
&gt;  {
&gt;         if (btrfs_is_testing(root-&gt;fs_info))
&gt;                 return false;
&gt; @@ -621,8 +621,55 @@ static inline bool should_cow_block(const struct btrfs_trans_handle *trans,
&gt;         if (btrfs_header_generation(buf) != trans-&gt;transid)
&gt;                 return true;
&gt;
&gt; -       if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN))
&gt; +       if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {
&gt; +               /*
&gt; +                * The buffer was allocated in this transaction and has been
&gt; +                * written back to disk (WRITTEN is set). Normally we&#x27;d COW
&gt; +                * it again, but since the committed superblock doesn&#x27;t
&gt; +                * reference this buffer (it was allocated this transaction),

Missing an &quot;in&quot; before &quot;this transaction&quot;.

&gt; +                * we can safely overwrite it in place.
&gt; +                *
&gt; +                * We keep BTRFS_HEADER_FLAG_WRITTEN set. The block has been
&gt; +                * persisted at this bytenr and will be again after the
&gt; +                * in-place update. This is important so that
&gt; +                * btrfs_free_tree_block() correctly pins the block if it is
&gt; +                * freed later (e.g., during tree rebalancing or FORCE_COW).
&gt; +                *
&gt; +                * We re-dirty the buffer to ensure the in-place modifications
&gt; +                * will be written back to disk.
&gt; +                *
&gt; +                * Exclusions:
&gt; +                * - Log trees: log blocks are written and immediately
&gt; +                *   referenced by a committed superblock via
&gt; +                *   btrfs_sync_log(), bypassing the normal transaction
&gt; +                *   commit. Overwriting in place could corrupt the
&gt; +                *   committed log.
&gt; +                * - Zoned devices: require sequential writes
&gt; +                * - FORCE_COW: snapshot in progress
&gt; +                * - RELOC flag: block being relocated
&gt; +                */
&gt; +               if (!test_bit(EXTENT_BUFFER_WRITEBACK, &amp;buf-&gt;bflags) &amp;&amp;
&gt; +                   !btrfs_is_zoned(root-&gt;fs_info) &amp;&amp;
&gt; +                   btrfs_root_id(root) != BTRFS_TREE_LOG_OBJECTID &amp;&amp;
&gt; +                   !test_bit(BTRFS_ROOT_FORCE_COW, &amp;root-&gt;state) &amp;&amp;

We need a  smp_mb__before_atomic() before checking FORCE_COW, see the
existing code below.

&gt; +                   !btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC)) {
&gt; +                       /*
&gt; +                        * Re-register this block&#x27;s range in the current
&gt; +                        * transaction&#x27;s dirty_pages so that
&gt; +                        * btrfs_write_and_wait_transaction() writes it.
&gt; +                        * The range was originally registered when the block
&gt; +                        * was allocated, but that transaction&#x27;s dirty_pages
&gt; +                        * may have already been released.

I think it&#x27;s worth adding something like: &quot;... already been released
if we are in a transaction that creates snapshots and we have qgroups
enabled.&quot;

Otherwise it looks good, thanks!

&gt; +                        */
&gt; +                       btrfs_set_extent_bit(&amp;trans-&gt;transaction-&gt;dirty_pages,
&gt; +                                            buf-&gt;start,
&gt; +                                            buf-&gt;start + buf-&gt;len - 1,
&gt; +                                            EXTENT_DIRTY, NULL);
&gt; +                       set_extent_buffer_dirty(buf);
&gt; +                       return false;
&gt; +               }
&gt;                 return true;
&gt; +       }
&gt;
&gt;         /* Ensure we can see the FORCE_COW bit. */
&gt;         smp_mb__before_atomic();
&gt; --
&gt; 2.47.3
&gt;
&gt;


---

On Fri, Feb 13, 2026 at 8:38 PM Leo Martins &lt;loemra.dev@gmail.com&gt; wrote:
&gt;
&gt; Inhibit writeback on COW&#x27;d extent buffers for the lifetime of the
&gt; transaction handle, preventing background writeback from setting
&gt; BTRFS_HEADER_FLAG_WRITTEN and causing unnecessary re-COW.
&gt;
&gt; COW amplification occurs when background writeback flushes an extent
&gt; buffer that a transaction handle is still actively modifying. When
&gt; lock_extent_buffer_for_io() transitions a buffer from dirty to
&gt; writeback, it sets BTRFS_HEADER_FLAG_WRITTEN, marking the block as
&gt; having been persisted to disk at its current bytenr. Once WRITTEN is
&gt; set, should_cow_block() must either COW the block again or overwrite
&gt; it in place, both of which are unnecessary overhead when the buffer
&gt; is still being modified by the same handle that allocated it. By
&gt; inhibiting background writeback on actively-used buffers, WRITTEN is
&gt; never set while a transaction handle holds a reference to the buffer,
&gt; avoiding this overhead entirely.
&gt;
&gt; Add an atomic_t writeback_inhibitors counter to struct extent_buffer,
&gt; which fits in an existing 6-byte hole without increasing struct size.
&gt; When a buffer is COW&#x27;d in btrfs_force_cow_block(), call
&gt; btrfs_inhibit_eb_writeback() to store the eb in the transaction
&gt; handle&#x27;s writeback_inhibited_ebs xarray (keyed by eb-&gt;start), take a
&gt; reference, and increment writeback_inhibitors. The function handles
&gt; dedup (same eb inhibited twice by the same handle) and replacement
&gt; (different eb at the same logical address). Allocation failure is
&gt; graceful: the buffer simply falls back to the pre-existing behavior
&gt; where it may be written back and re-COW&#x27;d.
&gt;
&gt; In lock_extent_buffer_for_io(), when writeback_inhibitors is non-zero
&gt; and the writeback mode is WB_SYNC_NONE, skip the buffer. WB_SYNC_NONE
&gt; is used by the VM flusher threads for background and periodic
&gt; writeback, which are the only paths that cause COW amplification by
&gt; opportunistically writing out dirty extent buffers mid-transaction.
&gt; Skipping these is safe because the buffers remain dirty in the page
&gt; cache and will be written out at transaction commit time.
&gt;
&gt; WB_SYNC_ALL must always proceed regardless of writeback_inhibitors.
&gt; This is required for correctness in the fsync path: btrfs_sync_log()
&gt; writes log tree blocks via filemap_fdatawrite_range() (WB_SYNC_ALL)
&gt; while the transaction handle that inhibited those same blocks is still
&gt; active. Without the WB_SYNC_ALL bypass, those inhibited log tree
&gt; blocks would be silently skipped, resulting in an incomplete log on
&gt; disk and corruption on replay. btrfs_write_and_wait_transaction()
&gt; also uses WB_SYNC_ALL via filemap_fdatawrite_range(); for that path,
&gt; inhibitors are already cleared beforehand, but the bypass ensures
&gt; correctness regardless.
&gt;
&gt; Uninhibit in __btrfs_end_transaction() before atomic_dec(num_writers)
&gt; to prevent a race where the committer proceeds while buffers are still
&gt; inhibited. Also uninhibit in btrfs_commit_transaction() before writing
&gt; and in cleanup_transaction() for the error path.
&gt;
&gt; Signed-off-by: Leo Martins &lt;loemra.dev@gmail.com&gt;
&gt; ---
&gt;  fs/btrfs/ctree.c       |  4 +++
&gt;  fs/btrfs/extent_io.c   | 62 +++++++++++++++++++++++++++++++++++++++++-
&gt;  fs/btrfs/extent_io.h   |  5 ++++
&gt;  fs/btrfs/transaction.c | 19 +++++++++++++
&gt;  fs/btrfs/transaction.h |  2 ++
&gt;  5 files changed, 91 insertions(+), 1 deletion(-)
&gt;
&gt; diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
&gt; index a345e1be24d8..55187ba59cc0 100644
&gt; --- a/fs/btrfs/ctree.c
&gt; +++ b/fs/btrfs/ctree.c
&gt; @@ -590,6 +590,10 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,
&gt;                 btrfs_tree_unlock(buf);
&gt;         free_extent_buffer_stale(buf);
&gt;         btrfs_mark_buffer_dirty(trans, cow);
&gt; +
&gt; +       /* Inhibit writeback on the COW&#x27;d buffer for this transaction handle */

Please always end sentences in a comment with punctuation.


&gt; +       btrfs_inhibit_eb_writeback(trans, cow);
&gt; +
&gt;         *cow_ret = cow;
&gt;         return 0;
&gt;
&gt; diff --git a/fs/btrfs/extent_io.c b/fs/btrfs/extent_io.c
&gt; index dfc17c292217..0c9276cff299 100644
&gt; --- a/fs/btrfs/extent_io.c
&gt; +++ b/fs/btrfs/extent_io.c
&gt; @@ -1940,7 +1940,9 @@ static noinline_for_stack bool lock_extent_buffer_for_io(struct extent_buffer *e
&gt;          * of time.
&gt;          */
&gt;         spin_lock(&amp;eb-&gt;refs_lock);
&gt; -       if (test_and_clear_bit(EXTENT_BUFFER_DIRTY, &amp;eb-&gt;bflags)) {
&gt; +       if ((wbc-&gt;sync_mode == WB_SYNC_ALL ||
&gt; +            atomic_read(&amp;eb-&gt;writeback_inhibitors) == 0) &amp;&amp;
&gt; +           test_and_clear_bit(EXTENT_BUFFER_DIRTY, &amp;eb-&gt;bflags)) {
&gt;                 XA_STATE(xas, &amp;fs_info-&gt;buffer_tree, eb-&gt;start &gt;&gt; fs_info-&gt;nodesize_bits);
&gt;                 unsigned long flags;
&gt;
&gt; @@ -2999,6 +3001,63 @@ static inline void btrfs_release_extent_buffer(struct extent_buffer *eb)
&gt;         kmem_cache_free(extent_buffer_cache, eb);
&gt;  }
&gt;
&gt; +/*
&gt; + * btrfs_inhibit_eb_writeback - Inhibit writeback on buffer during transaction
&gt; + * @trans: transaction handle that will own the inhibitor
&gt; + * @eb: extent buffer to inhibit writeback on
&gt; + *
&gt; + * Attempts to track this extent buffer in the transaction&#x27;s inhibited set.
&gt; + * If memory allocation fails, the buffer is simply not tracked. It may
&gt; + * be written back and need re-COW, which is the original behavior.
&gt; + * This is acceptable since inhibiting writeback is an optimization.
&gt; + */
&gt; +void btrfs_inhibit_eb_writeback(struct btrfs_trans_handle *trans,
&gt; +                               struct extent_buffer *eb)
&gt; +{
&gt; +       unsigned long index = eb-&gt;start &gt;&gt; trans-&gt;fs_info-&gt;nodesize_bits;
&gt; +       void *old;
&gt; +
&gt; +       /* Check if already inhibited by this handle */

Same here.

&gt; +       old = xa_load(&amp;trans-&gt;writeback_inhibited_ebs, index);
&gt; +       if (old == eb)
&gt; +               return;
&gt; +
&gt; +       refcount_inc(&amp;eb-&gt;refs);        /* Take reference */

Always place comments above the code line, not in the same line.

&gt; +
&gt; +       old = xa_store(&amp;trans-&gt;writeback_inhibited_ebs, index, eb, GFP_NOFS);
&gt; +       if (xa_is_err(old)) {
&gt; +               /* Allocation failed, just skip inhibiting this buffer */

Punctuation missing.

&gt; +               free_extent_buffer(eb);
&gt; +               return;
&gt; +       }
&gt; +
&gt; +       /* Handle replacement of different eb at same index */

Punctuation missing.

&gt; +       if (old &amp;&amp; old != eb) {
&gt; +               struct extent_buffer *old_eb = old;
&gt; +
&gt; +               atomic_dec(&amp;old_eb-&gt;writeback_inhibitors);
&gt; +               free_extent_buffer(old_eb);
&gt; +       }
&gt; +
&gt; +       atomic_inc(&amp;eb-&gt;writeback_inhibitors);
&gt; +}
&gt; +
&gt; +/*
&gt; + * btrfs_uninhibit_all_eb_writeback - Uninhibit writeback on all buffers
&gt; + * @trans: transaction handle to clean up
&gt; + */
&gt; +void btrfs_uninhibit_all_eb_writeback(struct btrfs_trans_handle *trans)
&gt; +{
&gt; +       struct extent_buffer *eb;
&gt; +       unsigned long index;
&gt; +
&gt; +       xa_for_each(&amp;trans-&gt;writeback_inhibited_ebs, index, eb) {
&gt; +               atomic_dec(&amp;eb-&gt;writeback_inhibitors);
&gt; +               free_extent_buffer(eb);
&gt; +       }
&gt; +       xa_destroy(&amp;trans-&gt;writeback_inhibited_ebs);
&gt; +}
&gt; +
&gt;  static struct extent_buffer *__alloc_extent_buffer(struct btrfs_fs_info *fs_info,
&gt;                                                    u64 start)
&gt;  {
&gt; @@ -3009,6 +3068,7 @@ static struct extent_buffer *__alloc_extent_buffer(struct btrfs_fs_info *fs_info
&gt;         eb-&gt;len = fs_info-&gt;nodesize;
&gt;         eb-&gt;fs_info = fs_info;
&gt;         init_rwsem(&amp;eb-&gt;lock);
&gt; +       atomic_set(&amp;eb-&gt;writeback_inhibitors, 0);
&gt;
&gt;         btrfs_leak_debug_add_eb(eb);
&gt;
&gt; diff --git a/fs/btrfs/extent_io.h b/fs/btrfs/extent_io.h
&gt; index 73571d5d3d5a..4b15a5d8bc0f 100644
&gt; --- a/fs/btrfs/extent_io.h
&gt; +++ b/fs/btrfs/extent_io.h
&gt; @@ -102,6 +102,7 @@ struct extent_buffer {
&gt;         /* &gt;= 0 if eb belongs to a log tree, -1 otherwise */
&gt;         s8 log_index;
&gt;         u8 folio_shift;
&gt; +       atomic_t writeback_inhibitors;  /* inhibits writeback when &gt; 0 */

Always place the comment above the structure&#x27;s member (just like for
code), and add punctuation and capitalize the first word.

We have old code that does follow this, from the old days where
&quot;anything goes&quot;, but we try to be consistent nowadays, see:

https://btrfs.readthedocs.io/en/latest/dev/Development-notes.html#comments

Otherwise it looks good, with those minor changes:

Reviewed-by: Filipe Manana &lt;fdmanana@suse.com&gt;

Thanks.



&gt;         struct rcu_head rcu_head;
&gt;
&gt;         struct rw_semaphore lock;
&gt; @@ -381,4 +382,8 @@ void btrfs_extent_buffer_leak_debug_check(struct btrfs_fs_info *fs_info);
&gt;  #define btrfs_extent_buffer_leak_debug_check(fs_info)  do {} while (0)
&gt;  #endif
&gt;
&gt; +void btrfs_inhibit_eb_writeback(struct btrfs_trans_handle *trans,
&gt; +                              struct extent_buffer *eb);
&gt; +void btrfs_uninhibit_all_eb_writeback(struct btrfs_trans_handle *trans);
&gt; +
&gt;  #endif
&gt; diff --git a/fs/btrfs/transaction.c b/fs/btrfs/transaction.c
&gt; index f4cc9e1a1b93..a9a22629b49d 100644
&gt; --- a/fs/btrfs/transaction.c
&gt; +++ b/fs/btrfs/transaction.c
&gt; @@ -15,6 +15,7 @@
&gt;  #include &quot;misc.h&quot;
&gt;  #include &quot;ctree.h&quot;
&gt;  #include &quot;disk-io.h&quot;
&gt; +#include &quot;extent_io.h&quot;
&gt;  #include &quot;transaction.h&quot;
&gt;  #include &quot;locking.h&quot;
&gt;  #include &quot;tree-log.h&quot;
&gt; @@ -688,6 +689,8 @@ start_transaction(struct btrfs_root *root, unsigned int num_items,
&gt;                 goto alloc_fail;
&gt;         }
&gt;
&gt; +       xa_init(&amp;h-&gt;writeback_inhibited_ebs);
&gt; +
&gt;         /*
&gt;          * If we are JOIN_NOLOCK we&#x27;re already committing a transaction and
&gt;          * waiting on this guy, so we don&#x27;t need to do the sb_start_intwrite
&gt; @@ -1083,6 +1086,13 @@ static int __btrfs_end_transaction(struct btrfs_trans_handle *trans,
&gt;         if (trans-&gt;type &amp; __TRANS_FREEZABLE)
&gt;                 sb_end_intwrite(info-&gt;sb);
&gt;
&gt; +       /*
&gt; +        * Uninhibit extent buffer writeback before decrementing num_writers,
&gt; +        * since the decrement wakes the committing thread which needs all
&gt; +        * buffers uninhibited to write them to disk.
&gt; +        */
&gt; +       btrfs_uninhibit_all_eb_writeback(trans);
&gt; +
&gt;         WARN_ON(cur_trans != info-&gt;running_transaction);
&gt;         WARN_ON(atomic_read(&amp;cur_trans-&gt;num_writers) &lt; 1);
&gt;         atomic_dec(&amp;cur_trans-&gt;num_writers);
&gt; @@ -2110,6 +2120,7 @@ static void cleanup_transaction(struct btrfs_trans_handle *trans, int err)
&gt;         if (!test_bit(BTRFS_FS_RELOC_RUNNING, &amp;fs_info-&gt;flags))
&gt;                 btrfs_scrub_cancel(fs_info);
&gt;
&gt; +       btrfs_uninhibit_all_eb_writeback(trans);
&gt;         kmem_cache_free(btrfs_trans_handle_cachep, trans);
&gt;  }
&gt;
&gt; @@ -2556,6 +2567,14 @@ int btrfs_commit_transaction(struct btrfs_trans_handle *trans)
&gt;             fs_info-&gt;cleaner_kthread)
&gt;                 wake_up_process(fs_info-&gt;cleaner_kthread);
&gt;
&gt; +       /*
&gt; +        * Uninhibit writeback on all extent buffers inhibited during this
&gt; +        * transaction before writing them to disk. Inhibiting prevented
&gt; +        * writeback while the transaction was building, but now we need
&gt; +        * them written.
&gt; +        */
&gt; +       btrfs_uninhibit_all_eb_writeback(trans);
&gt; +
&gt;         ret = btrfs_write_and_wait_transaction(trans);
&gt;         if (unlikely(ret)) {
&gt;                 btrfs_err(fs_info, &quot;error while writing out transaction: %d&quot;, ret);
&gt; diff --git a/fs/btrfs/transaction.h b/fs/btrfs/transaction.h
&gt; index 18ef069197e5..f0d12c16d796 100644
&gt; --- a/fs/btrfs/transaction.h
&gt; +++ b/fs/btrfs/transaction.h
&gt; @@ -12,6 +12,7 @@
&gt;  #include &lt;linux/time64.h&gt;
&gt;  #include &lt;linux/mutex.h&gt;
&gt;  #include &lt;linux/wait.h&gt;
&gt; +#include &lt;linux/xarray.h&gt;
&gt;  #include &quot;btrfs_inode.h&quot;
&gt;  #include &quot;delayed-ref.h&quot;
&gt;
&gt; @@ -162,6 +163,7 @@ struct btrfs_trans_handle {
&gt;         struct btrfs_fs_info *fs_info;
&gt;         struct list_head new_bgs;
&gt;         struct btrfs_block_rsv delayed_rsv;
&gt; +       struct xarray writeback_inhibited_ebs;  /* ebs with writeback inhibited */
&gt;  };
&gt;
&gt;  /*
&gt; --
&gt; 2.47.3
&gt;
&gt;


---

On Fri, Feb 13, 2026 at 8:37 PM Leo Martins &lt;loemra.dev@gmail.com&gt; wrote:
&gt;
&gt; Add a btrfs_search_slot_stats tracepoint to btrfs_search_slot() for
&gt; measuring COW amplification.
&gt;
&gt; The tracepoint fires when a search with at least one COW completes,
&gt; recording the root, total cow_count, restart_count, and return value.
&gt; cow_count and restart_count per search_slot call are useful metrics
&gt; for tracking COW amplification.
&gt;
&gt; Signed-off-by: Leo Martins &lt;loemra.dev@gmail.com&gt;
&gt; ---
&gt;  fs/btrfs/ctree.c             | 15 +++++++++++++--
&gt;  include/trace/events/btrfs.h | 26 ++++++++++++++++++++++++++
&gt;  2 files changed, 39 insertions(+), 2 deletions(-)
&gt;
&gt; diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
&gt; index 55187ba59cc0..1971d7bb5f60 100644
&gt; --- a/fs/btrfs/ctree.c
&gt; +++ b/fs/btrfs/ctree.c
&gt; @@ -2069,6 +2069,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt;         u8 lowest_level = 0;
&gt;         int min_write_lock_level;
&gt;         int prev_cmp;
&gt; +       int cow_count = 0;
&gt; +       int restart_count = 0;
&gt;
&gt;         if (!root)
&gt;                 return -EINVAL;
&gt; @@ -2157,6 +2159,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt;                             p-&gt;nodes[level + 1])) {
&gt;                                 write_lock_level = level + 1;
&gt;                                 btrfs_release_path(p);
&gt; +                               restart_count++;
&gt;                                 goto again;
&gt;                         }
&gt;
&gt; @@ -2172,6 +2175,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt;                                 ret = ret2;
&gt;                                 goto done;
&gt;                         }
&gt; +                       cow_count++;
&gt;                 }
&gt;  cow_done:
&gt;                 p-&gt;nodes[level] = b;
&gt; @@ -2219,8 +2223,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt;                 p-&gt;slots[level] = slot;
&gt;                 ret2 = setup_nodes_for_search(trans, root, p, b, level, ins_len,
&gt;                                               &amp;write_lock_level);
&gt; -               if (ret2 == -EAGAIN)
&gt; +               if (ret2 == -EAGAIN) {
&gt; +                       restart_count++;
&gt;                         goto again;
&gt; +               }
&gt;                 if (ret2) {
&gt;                         ret = ret2;
&gt;                         goto done;
&gt; @@ -2236,6 +2242,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt;                 if (slot == 0 &amp;&amp; ins_len &amp;&amp; write_lock_level &lt; level + 1) {
&gt;                         write_lock_level = level + 1;
&gt;                         btrfs_release_path(p);
&gt; +                       restart_count++;
&gt;                         goto again;
&gt;                 }
&gt;
&gt; @@ -2249,8 +2256,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt;                 }
&gt;
&gt;                 ret2 = read_block_for_search(root, p, &amp;b, slot, key);
&gt; -               if (ret2 == -EAGAIN &amp;&amp; !p-&gt;nowait)
&gt; +               if (ret2 == -EAGAIN &amp;&amp; !p-&gt;nowait) {
&gt; +                       restart_count++;
&gt;                         goto again;
&gt; +               }
&gt;                 if (ret2) {
&gt;                         ret = ret2;
&gt;                         goto done;
&gt; @@ -2281,6 +2290,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,
&gt;         }
&gt;         ret = 1;
&gt;  done:
&gt; +       if (cow_count &gt; 0)
&gt; +               trace_btrfs_search_slot_stats(root, cow_count, restart_count, ret);

So I find this way too specific, plus even if trace points are
disabled we have the overhead of the counters (and inside critical
sections).

We already have a tracepoint for COW, trace_btrfs_cow_block(), and we
could have one just for the retry thing, maybe naming it like
trace_btrfs_search_slot_restart() or something.
So we could use those two tracepoints to measure things (bpftrace
scripts could easily report a count of each trace point and such),
instead of this highly specialized tracepoint that adds some overhead
when tracepoints are disabled.

Thanks.


&gt;         if (ret &lt; 0 &amp;&amp; !p-&gt;skip_release_on_error)
&gt;                 btrfs_release_path(p);
&gt;
&gt; diff --git a/include/trace/events/btrfs.h b/include/trace/events/btrfs.h
&gt; index 125bdc166bfe..b8934938a087 100644
&gt; --- a/include/trace/events/btrfs.h
&gt; +++ b/include/trace/events/btrfs.h
&gt; @@ -1110,6 +1110,32 @@ TRACE_EVENT(btrfs_cow_block,
&gt;                   __entry-&gt;cow_level)
&gt;  );
&gt;
&gt; +TRACE_EVENT(btrfs_search_slot_stats,
&gt; +
&gt; +       TP_PROTO(const struct btrfs_root *root,
&gt; +                int cow_count, int restart_count, int ret),
&gt; +
&gt; +       TP_ARGS(root, cow_count, restart_count, ret),
&gt; +
&gt; +       TP_STRUCT__entry_btrfs(
&gt; +               __field(        u64,    root_objectid           )
&gt; +               __field(        int,    cow_count               )
&gt; +               __field(        int,    restart_count           )
&gt; +               __field(        int,    ret                     )
&gt; +       ),
&gt; +
&gt; +       TP_fast_assign_btrfs(root-&gt;fs_info,
&gt; +               __entry-&gt;root_objectid  = btrfs_root_id(root);
&gt; +               __entry-&gt;cow_count      = cow_count;
&gt; +               __entry-&gt;restart_count  = restart_count;
&gt; +               __entry-&gt;ret            = ret;
&gt; +       ),
&gt; +
&gt; +       TP_printk_btrfs(&quot;root=%llu(%s) cow_count=%d restarts=%d ret=%d&quot;,
&gt; +                 show_root_type(__entry-&gt;root_objectid),
&gt; +                 __entry-&gt;cow_count, __entry-&gt;restart_count, __entry-&gt;ret)
&gt; +);
&gt; +
&gt;  TRACE_EVENT(btrfs_space_reservation,
&gt;
&gt;         TP_PROTO(const struct btrfs_fs_info *fs_info, const char *type, u64 val,
&gt; --
&gt; 2.47.3
&gt;
&gt;
</pre>
</details>
<div class="review-comment-signals">Signals: minor issues, suggestions for improvement</div>
</div>
</div>
</div>

    <footer>LKML Daily Activity Tracker</footer>
    <script>
    // When arriving via a date anchor (e.g. #2026-02-15 from a daily report),
    // scroll the anchor into view after a brief delay so layout is complete.
    (function () {
        var hash = window.location.hash;
        if (!hash) return;
        var target = document.getElementById(hash.slice(1));
        if (!target) return;
        setTimeout(function () {
            target.scrollIntoView({behavior: 'smooth', block: 'start'});
        }, 80);
    })();
    </script>
</body>
</html>