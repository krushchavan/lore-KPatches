{
  "thread_id": "aZxn34ebvSKFCWth@thinkstation",
  "subject": "[PATCHv6 08/17] mm: Make page_zonenum() use head page",
  "url": "https://lore.kernel.org/all/aZxn34ebvSKFCWth@thinkstation/",
  "dates": {
    "2026-02-02": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author is addressing a concern about the MAX_FOLIO_ORDER definition being moved from mm.h to mmzone.h, explaining that it's necessary for adding the vmemmap_tails array to struct pglist_data.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Move MAX_FOLIO_ORDER definition from mm.h to mmzone.h.\n\nThis is preparation for adding the vmemmap_tails array to struct\npglist_data, which requires MAX_FOLIO_ORDER to be available in mmzone.h.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nAcked-by: David Hildenbrand (Red Hat) <david@kernel.org>\nAcked-by: Zi Yan <ziy@nvidia.com>\nAcked-by: Muchun Song <muchun.song@linux.dev>\n---\n include/linux/mm.h     | 31 -------------------------------\n include/linux/mmzone.h | 31 +++++++++++++++++++++++++++++++\n 2 files changed, 31 insertions(+), 31 deletions(-)\n\ndiff --git a/include/linux/mm.h b/include/linux/mm.h\nindex f8a8fd47399c..8d5fa655fea4 100644\n--- a/include/linux/mm.h\n+++ b/include/linux/mm.h\n@@ -27,7 +27,6 @@\n #include <linux/page-flags.h>\n #include <linux/page_ref.h>\n #include <linux/overflow.h>\n-#include <linux/sizes.h>\n #include <linux/sched.h>\n #include <linux/pgtable.h>\n #include <linux/kasan.h>\n@@ -2477,36 +2476,6 @@ static inline unsigned long folio_nr_pages(const struct folio *folio)\n \treturn folio_large_nr_pages(folio);\n }\n \n-#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)\n-/*\n- * We don't expect any folios that exceed buddy sizes (and consequently\n- * memory sections).\n- */\n-#define MAX_FOLIO_ORDER\t\tMAX_PAGE_ORDER\n-#elif defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\n-/*\n- * Only pages within a single memory section are guaranteed to be\n- * contiguous. By limiting folios to a single memory section, all folio\n- * pages are guaranteed to be contiguous.\n- */\n-#define MAX_FOLIO_ORDER\t\tPFN_SECTION_SHIFT\n-#elif defined(CONFIG_HUGETLB_PAGE)\n-/*\n- * There is no real limit on the folio size. We limit them to the maximum we\n- * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect\n- * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.\n- */\n-#define MAX_FOLIO_ORDER\t\tget_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)\n-#else\n-/*\n- * Without hugetlb, gigantic folios that are bigger than a single PUD are\n- * currently impossible.\n- */\n-#define MAX_FOLIO_ORDER\t\tPUD_ORDER\n-#endif\n-\n-#define MAX_FOLIO_NR_PAGES\t(1UL << MAX_FOLIO_ORDER)\n-\n /*\n  * compound_nr() returns the number of pages in this potentially compound\n  * page.  compound_nr() can be called on a tail page, and is defined to\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 3e51190a55e4..be8ce40b5638 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -23,6 +23,7 @@\n #include <linux/page-flags.h>\n #include <linux/local_lock.h>\n #include <linux/zswap.h>\n+#include <linux/sizes.h>\n #include <asm/page.h>\n \n /* Free memory management - zoned buddy allocator.  */\n@@ -61,6 +62,36 @@\n  */\n #define PAGE_ALLOC_COSTLY_ORDER 3\n \n+#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)\n+/*\n+ * We don't expect any folios that exceed buddy sizes (and consequently\n+ * memory sections).\n+ */\n+#define MAX_FOLIO_ORDER\t\tMAX_PAGE_ORDER\n+#elif defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\n+/*\n+ * Only pages within a single memory section are guaranteed to be\n+ * contiguous. By limiting folios to a single memory section, all folio\n+ * pages are guaranteed to be contiguous.\n+ */\n+#define MAX_FOLIO_ORDER\t\tPFN_SECTION_SHIFT\n+#elif defined(CONFIG_HUGETLB_PAGE)\n+/*\n+ * There is no real limit on the folio size. We limit them to the maximum we\n+ * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect\n+ * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.\n+ */\n+#define MAX_FOLIO_ORDER\t\tget_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)\n+#else\n+/*\n+ * Without hugetlb, gigantic folios that are bigger than a single PUD are\n+ * currently impossible.\n+ */\n+#define MAX_FOLIO_ORDER\t\tPUD_ORDER\n+#endif\n+\n+#define MAX_FOLIO_NR_PAGES\t(1UL << MAX_FOLIO_ORDER)\n+\n enum migratetype {\n \tMIGRATE_UNMOVABLE,\n \tMIGRATE_MOVABLE,\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-02"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author is addressing a concern about the correctness of page_zonenum() in the context of huge pages, specifically that it must use the head page. The author's response is to change how the head position is encoded in the tail page by passing down the tail and head pages directly, as well as the order of the compound page, which will allow for a single page of tail struct pages to be shared across all huge pages of the same order on a NUMA node. The author does not mention if this change addresses the correctness issue or if further revision is needed.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "no clear resolution signal",
            "change in approach"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Instead of passing down the head page and tail page index, pass the tail\nand head pages directly, as well as the order of the compound page.\n\nThis is a preparation for changing how the head position is encoded in\nthe tail page.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\n---\n include/linux/page-flags.h |  4 +++-\n mm/hugetlb.c               |  8 +++++---\n mm/internal.h              | 12 ++++++------\n mm/mm_init.c               |  2 +-\n mm/page_alloc.c            |  2 +-\n 5 files changed, 16 insertions(+), 12 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex f7a0e4af0c73..8a3694369e15 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -865,7 +865,9 @@ static inline bool folio_test_large(const struct folio *folio)\n \treturn folio_test_head(folio);\n }\n \n-static __always_inline void set_compound_head(struct page *page, struct page *head)\n+static __always_inline void set_compound_head(struct page *page,\n+\t\t\t\t\t      const struct page *head,\n+\t\t\t\t\t      unsigned int order)\n {\n \tWRITE_ONCE(page->compound_head, (unsigned long)head + 1);\n }\ndiff --git a/mm/hugetlb.c b/mm/hugetlb.c\nindex 6e855a32de3d..54ba7cd05a86 100644\n--- a/mm/hugetlb.c\n+++ b/mm/hugetlb.c\n@@ -3168,6 +3168,7 @@ int __alloc_bootmem_huge_page(struct hstate *h, int nid)\n \n /* Initialize [start_page:end_page_number] tail struct pages of a hugepage */\n static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n+\t\t\t\t\tstruct hstate *h,\n \t\t\t\t\tunsigned long start_page_number,\n \t\t\t\t\tunsigned long end_page_number)\n {\n@@ -3176,6 +3177,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n \tstruct page *page = folio_page(folio, start_page_number);\n \tunsigned long head_pfn = folio_pfn(folio);\n \tunsigned long pfn, end_pfn = head_pfn + end_page_number;\n+\tunsigned int order = huge_page_order(h);\n \n \t/*\n \t * As we marked all tail pages with memblock_reserved_mark_noinit(),\n@@ -3183,7 +3185,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n \t */\n \tfor (pfn = head_pfn + start_page_number; pfn < end_pfn; page++, pfn++) {\n \t\t__init_single_page(page, pfn, zone, nid);\n-\t\tprep_compound_tail((struct page *)folio, pfn - head_pfn);\n+\t\tprep_compound_tail(page, &folio->page, order);\n \t\tset_page_count(page, 0);\n \t}\n }\n@@ -3203,7 +3205,7 @@ static void __init hugetlb_folio_init_vmemmap(struct folio *folio,\n \t__folio_set_head(folio);\n \tret = folio_ref_freeze(folio, 1);\n \tVM_BUG_ON(!ret);\n-\thugetlb_folio_init_tail_vmemmap(folio, 1, nr_pages);\n+\thugetlb_folio_init_tail_vmemmap(folio, h, 1, nr_pages);\n \tprep_compound_head(&folio->page, huge_page_order(h));\n }\n \n@@ -3260,7 +3262,7 @@ static void __init prep_and_add_bootmem_folios(struct hstate *h,\n \t\t\t * time as this is early in boot and there should\n \t\t\t * be no contention.\n \t\t\t */\n-\t\t\thugetlb_folio_init_tail_vmemmap(folio,\n+\t\t\thugetlb_folio_init_tail_vmemmap(folio, h,\n \t\t\t\t\tHUGETLB_VMEMMAP_RESERVE_PAGES,\n \t\t\t\t\tpages_per_huge_page(h));\n \t\t}\ndiff --git a/mm/internal.h b/mm/internal.h\nindex d67e8bb75734..037ddcda25ff 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -879,13 +879,13 @@ static inline void prep_compound_head(struct page *page, unsigned int order)\n \t\tINIT_LIST_HEAD(&folio->_deferred_list);\n }\n \n-static inline void prep_compound_tail(struct page *head, int tail_idx)\n+static inline void prep_compound_tail(struct page *tail,\n+\t\t\t\t      const struct page *head,\n+\t\t\t\t      unsigned int order)\n {\n-\tstruct page *p = head + tail_idx;\n-\n-\tp->mapping = TAIL_MAPPING;\n-\tset_compound_head(p, head);\n-\tset_page_private(p, 0);\n+\ttail->mapping = TAIL_MAPPING;\n+\tset_compound_head(tail, head, order);\n+\tset_page_private(tail, 0);\n }\n \n void post_alloc_hook(struct page *page, unsigned int order, gfp_t gfp_flags);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..ba50f4c4337b 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -1099,7 +1099,7 @@ static void __ref memmap_init_compound(struct page *head,\n \t\tstruct page *page = pfn_to_page(pfn);\n \n \t\t__init_zone_device_page(page, pfn, zone_idx, nid, pgmap);\n-\t\tprep_compound_tail(head, pfn - head_pfn);\n+\t\tprep_compound_tail(page, head, order);\n \t\tset_page_count(page, 0);\n \t}\n \tprep_compound_head(head, order);\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..00c7ea958767 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -744,7 +744,7 @@ void prep_compound_page(struct page *page, unsigned int order)\n \n \t__SetPageHead(page);\n \tfor (i = 1; i < nr_pages; i++)\n-\t\tprep_compound_tail(page, i);\n+\t\tprep_compound_tail(page + i, page, order);\n \n \tprep_compound_head(page, order);\n }\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-02"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author is addressing a concern about the correctness of page_zonenum() in the context of the vmemmap optimization, specifically that it must use the head page. The author explains that the compound_info field encodes whether a page is a tail and where to locate the head page, but does not provide information on how this affects page_zonenum().",
          "sentiment": "neutral",
          "sentiment_signals": [
            "no clear resolution signal",
            "author provides explanation without acknowledging need for fix"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "The 'compound_head' field in the 'struct page' encodes whether the page\nis a tail and where to locate the head page. Bit 0 is set if the page is\na tail, and the remaining bits in the field point to the head page.\n\nAs preparation for changing how the field encodes information about the\nhead page, rename the field to 'compound_info'.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\n---\n .../admin-guide/kdump/vmcoreinfo.rst          |  2 +-\n Documentation/mm/vmemmap_dedup.rst            |  6 +++---\n include/linux/mm_types.h                      | 20 +++++++++----------\n include/linux/page-flags.h                    | 18 ++++++++---------\n include/linux/types.h                         |  2 +-\n kernel/vmcore_info.c                          |  2 +-\n mm/page_alloc.c                               |  2 +-\n mm/slab.h                                     |  2 +-\n mm/util.c                                     |  2 +-\n 9 files changed, 28 insertions(+), 28 deletions(-)\n\ndiff --git a/Documentation/admin-guide/kdump/vmcoreinfo.rst b/Documentation/admin-guide/kdump/vmcoreinfo.rst\nindex 404a15f6782c..7663c610fe90 100644\n--- a/Documentation/admin-guide/kdump/vmcoreinfo.rst\n+++ b/Documentation/admin-guide/kdump/vmcoreinfo.rst\n@@ -141,7 +141,7 @@ nodemask_t\n The size of a nodemask_t type. Used to compute the number of online\n nodes.\n \n-(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_head)\n+(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_info)\n ----------------------------------------------------------------------------------\n \n User-space tools compute their values based on the offset of these\ndiff --git a/Documentation/mm/vmemmap_dedup.rst b/Documentation/mm/vmemmap_dedup.rst\nindex b4a55b6569fa..1863d88d2dcb 100644\n--- a/Documentation/mm/vmemmap_dedup.rst\n+++ b/Documentation/mm/vmemmap_dedup.rst\n@@ -24,7 +24,7 @@ For each base page, there is a corresponding ``struct page``.\n Within the HugeTLB subsystem, only the first 4 ``struct page`` are used to\n contain unique information about a HugeTLB page. ``__NR_USED_SUBPAGE`` provides\n this upper limit. The only 'useful' information in the remaining ``struct page``\n-is the compound_head field, and this field is the same for all tail pages.\n+is the compound_info field, and this field is the same for all tail pages.\n \n By removing redundant ``struct page`` for HugeTLB pages, memory can be returned\n to the buddy allocator for other uses.\n@@ -124,10 +124,10 @@ Here is how things look before optimization::\n  |           |\n  +-----------+\n \n-The value of page->compound_head is the same for all tail pages. The first\n+The value of page->compound_info is the same for all tail pages. The first\n page of ``struct page`` (page 0) associated with the HugeTLB page contains the 4\n ``struct page`` necessary to describe the HugeTLB. The only use of the remaining\n-pages of ``struct page`` (page 1 to page 7) is to point to page->compound_head.\n+pages of ``struct page`` (page 1 to page 7) is to point to page->compound_info.\n Therefore, we can remap pages 1 to 7 to page 0. Only 1 page of ``struct page``\n will be used for each HugeTLB page. This will allow us to free the remaining\n 7 pages to the buddy allocator.\ndiff --git a/include/linux/mm_types.h b/include/linux/mm_types.h\nindex 3cc8ae722886..7bc82a2b889f 100644\n--- a/include/linux/mm_types.h\n+++ b/include/linux/mm_types.h\n@@ -126,14 +126,14 @@ struct page {\n \t\t\tatomic_long_t pp_ref_count;\n \t\t};\n \t\tstruct {\t/* Tail pages of compound page */\n-\t\t\tunsigned long compound_head;\t/* Bit zero is set */\n+\t\t\tunsigned long compound_info;\t/* Bit zero is set */\n \t\t};\n \t\tstruct {\t/* ZONE_DEVICE pages */\n \t\t\t/*\n-\t\t\t * The first word is used for compound_head or folio\n+\t\t\t * The first word is used for compound_info or folio\n \t\t\t * pgmap\n \t\t\t */\n-\t\t\tvoid *_unused_pgmap_compound_head;\n+\t\t\tvoid *_unused_pgmap_compound_info;\n \t\t\tvoid *zone_device_data;\n \t\t\t/*\n \t\t\t * ZONE_DEVICE private pages are counted as being\n@@ -409,7 +409,7 @@ struct folio {\n \t/* private: avoid cluttering the output */\n \t\t\t\t/* For the Unevictable \"LRU list\" slot */\n \t\t\t\tstruct {\n-\t\t\t\t\t/* Avoid compound_head */\n+\t\t\t\t\t/* Avoid compound_info */\n \t\t\t\t\tvoid *__filler;\n \t/* public: */\n \t\t\t\t\tunsigned int mlock_count;\n@@ -510,7 +510,7 @@ struct folio {\n FOLIO_MATCH(flags, flags);\n FOLIO_MATCH(lru, lru);\n FOLIO_MATCH(mapping, mapping);\n-FOLIO_MATCH(compound_head, lru);\n+FOLIO_MATCH(compound_info, lru);\n FOLIO_MATCH(__folio_index, index);\n FOLIO_MATCH(private, private);\n FOLIO_MATCH(_mapcount, _mapcount);\n@@ -529,7 +529,7 @@ FOLIO_MATCH(_last_cpupid, _last_cpupid);\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + sizeof(struct page))\n FOLIO_MATCH(flags, _flags_1);\n-FOLIO_MATCH(compound_head, _head_1);\n+FOLIO_MATCH(compound_info, _head_1);\n FOLIO_MATCH(_mapcount, _mapcount_1);\n FOLIO_MATCH(_refcount, _refcount_1);\n #undef FOLIO_MATCH\n@@ -537,13 +537,13 @@ FOLIO_MATCH(_refcount, _refcount_1);\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + 2 * sizeof(struct page))\n FOLIO_MATCH(flags, _flags_2);\n-FOLIO_MATCH(compound_head, _head_2);\n+FOLIO_MATCH(compound_info, _head_2);\n #undef FOLIO_MATCH\n #define FOLIO_MATCH(pg, fl)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + 3 * sizeof(struct page))\n FOLIO_MATCH(flags, _flags_3);\n-FOLIO_MATCH(compound_head, _head_3);\n+FOLIO_MATCH(compound_info, _head_3);\n #undef FOLIO_MATCH\n \n /**\n@@ -609,8 +609,8 @@ struct ptdesc {\n #define TABLE_MATCH(pg, pt)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct page, pg) == offsetof(struct ptdesc, pt))\n TABLE_MATCH(flags, pt_flags);\n-TABLE_MATCH(compound_head, pt_list);\n-TABLE_MATCH(compound_head, _pt_pad_1);\n+TABLE_MATCH(compound_info, pt_list);\n+TABLE_MATCH(compound_info, _pt_pad_1);\n TABLE_MATCH(mapping, __page_mapping);\n TABLE_MATCH(__folio_index, pt_index);\n TABLE_MATCH(rcu_head, pt_rcu_head);\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex 8a3694369e15..aa46d49e82f7 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -213,7 +213,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t/*\n \t * Only addresses aligned with PAGE_SIZE of struct page may be fake head\n \t * struct page. The alignment check aims to avoid access the fields (\n-\t * e.g. compound_head) of the @page[1]. It can avoid touch a (possibly)\n+\t * e.g. compound_info) of the @page[1]. It can avoid touch a (possibly)\n \t * cold cacheline in some cases.\n \t */\n \tif (IS_ALIGNED((unsigned long)page, PAGE_SIZE) &&\n@@ -223,7 +223,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t\t * because the @page is a compound page composed with at least\n \t\t * two contiguous pages.\n \t\t */\n-\t\tunsigned long head = READ_ONCE(page[1].compound_head);\n+\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n \n \t\tif (likely(head & 1))\n \t\t\treturn (const struct page *)(head - 1);\n@@ -281,7 +281,7 @@ static __always_inline int page_is_fake_head(const struct page *page)\n \n static __always_inline unsigned long _compound_head(const struct page *page)\n {\n-\tunsigned long head = READ_ONCE(page->compound_head);\n+\tunsigned long head = READ_ONCE(page->compound_info);\n \n \tif (unlikely(head & 1))\n \t\treturn head - 1;\n@@ -320,13 +320,13 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n \n static __always_inline int PageTail(const struct page *page)\n {\n-\treturn READ_ONCE(page->compound_head) & 1 || page_is_fake_head(page);\n+\treturn READ_ONCE(page->compound_info) & 1 || page_is_fake_head(page);\n }\n \n static __always_inline int PageCompound(const struct page *page)\n {\n \treturn test_bit(PG_head, &page->flags.f) ||\n-\t       READ_ONCE(page->compound_head) & 1;\n+\t       READ_ONCE(page->compound_info) & 1;\n }\n \n #define\tPAGE_POISON_PATTERN\t-1l\n@@ -348,7 +348,7 @@ static const unsigned long *const_folio_flags(const struct folio *folio,\n {\n \tconst struct page *page = &folio->page;\n \n-\tVM_BUG_ON_PGFLAGS(page->compound_head & 1, page);\n+\tVM_BUG_ON_PGFLAGS(page->compound_info & 1, page);\n \tVM_BUG_ON_PGFLAGS(n > 0 && !test_bit(PG_head, &page->flags.f), page);\n \treturn &page[n].flags.f;\n }\n@@ -357,7 +357,7 @@ static unsigned long *folio_flags(struct folio *folio, unsigned n)\n {\n \tstruct page *page = &folio->page;\n \n-\tVM_BUG_ON_PGFLAGS(page->compound_head & 1, page);\n+\tVM_BUG_ON_PGFLAGS(page->compound_info & 1, page);\n \tVM_BUG_ON_PGFLAGS(n > 0 && !test_bit(PG_head, &page->flags.f), page);\n \treturn &page[n].flags.f;\n }\n@@ -869,12 +869,12 @@ static __always_inline void set_compound_head(struct page *page,\n \t\t\t\t\t      const struct page *head,\n \t\t\t\t\t      unsigned int order)\n {\n-\tWRITE_ONCE(page->compound_head, (unsigned long)head + 1);\n+\tWRITE_ONCE(page->compound_info, (unsigned long)head + 1);\n }\n \n static __always_inline void clear_compound_head(struct page *page)\n {\n-\tWRITE_ONCE(page->compound_head, 0);\n+\tWRITE_ONCE(page->compound_info, 0);\n }\n \n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\ndiff --git a/include/linux/types.h b/include/linux/types.h\nindex f69be881369f..604697abf151 100644\n--- a/include/linux/types.h\n+++ b/include/linux/types.h\n@@ -234,7 +234,7 @@ struct ustat {\n  *\n  * This guarantee is important for few reasons:\n  *  - future call_rcu_lazy() will make use of lower bits in the pointer;\n- *  - the structure shares storage space in struct page with @compound_head,\n+ *  - the structure shares storage space in struct page with @compound_info,\n  *    which encode PageTail() in bit 0. The guarantee is needed to avoid\n  *    false-positive PageTail().\n  */\ndiff --git a/kernel/vmcore_info.c b/kernel/vmcore_info.c\nindex 46198580373a..0a46df3e3db9 100644\n--- a/kernel/vmcore_info.c\n+++ b/kernel/vmcore_info.c\n@@ -198,7 +198,7 @@ static int __init crash_save_vmcoreinfo_init(void)\n \tVMCOREINFO_OFFSET(page, lru);\n \tVMCOREINFO_OFFSET(page, _mapcount);\n \tVMCOREINFO_OFFSET(page, private);\n-\tVMCOREINFO_OFFSET(page, compound_head);\n+\tVMCOREINFO_OFFSET(page, compound_info);\n \tVMCOREINFO_OFFSET(pglist_data, node_zones);\n \tVMCOREINFO_OFFSET(pglist_data, nr_zones);\n #ifdef CONFIG_FLATMEM\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 00c7ea958767..cb7375eb1713 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -731,7 +731,7 @@ static inline bool pcp_allowed_order(unsigned int order)\n  * The first PAGE_SIZE page is called the \"head page\" and have PG_head set.\n  *\n  * The remaining PAGE_SIZE pages are called \"tail pages\". PageTail() is encoded\n- * in bit 0 of page->compound_head. The rest of bits is pointer to head page.\n+ * in bit 0 of page->compound_info. The rest of bits is pointer to head page.\n  *\n  * The first tail page's ->compound_order holds the order of allocation.\n  * This usage means that zero-order pages may not be compound.\ndiff --git a/mm/slab.h b/mm/slab.h\nindex e767aa7e91b0..8a2a9c6c697b 100644\n--- a/mm/slab.h\n+++ b/mm/slab.h\n@@ -100,7 +100,7 @@ struct slab {\n #define SLAB_MATCH(pg, sl)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct page, pg) == offsetof(struct slab, sl))\n SLAB_MATCH(flags, flags);\n-SLAB_MATCH(compound_head, slab_cache);\t/* Ensure bit 0 is clear */\n+SLAB_MATCH(compound_info, slab_cache);\t/* Ensure bit 0 is clear */\n SLAB_MATCH(_refcount, __page_refcount);\n #ifdef CONFIG_MEMCG\n SLAB_MATCH(memcg_data, obj_exts);\ndiff --git a/mm/util.c b/mm/util.c\nindex b05ab6f97e11..3ebcb9e6035c 100644\n--- a/mm/util.c\n+++ b/mm/util.c\n@@ -1247,7 +1247,7 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n again:\n \tmemset(&ps->folio_snapshot, 0, sizeof(struct folio));\n \tmemcpy(&ps->page_snapshot, page, sizeof(*page));\n-\thead = ps->page_snapshot.compound_head;\n+\thead = ps->page_snapshot.compound_info;\n \tif ((head & 1) == 0) {\n \t\tps->idx = 0;\n \t\tfoliop = (struct folio *)&ps->page_snapshot;\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-02"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author is addressing a concern about the placement of set_compound_head() and clear_compound_head() functions in page-flags.h, agreeing that they should be adjacent to compound_head(). The author has reorganized the code to move these functions next to compound_head(), making it easier to verify their logic consistency.",
          "sentiment": "positive",
          "sentiment_signals": [
            "agreement",
            "action_taken"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Move set_compound_head() and clear_compound_head() to be adjacent to the\ncompound_head() function in page-flags.h.\n\nThese functions encode and decode the same compound_info field, so\nkeeping them together makes it easier to verify their logic is\nconsistent, especially when the encoding changes.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\n---\n include/linux/page-flags.h | 24 ++++++++++++------------\n 1 file changed, 12 insertions(+), 12 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex aa46d49e82f7..d14a17ffb55b 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -290,6 +290,18 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n \n #define compound_head(page)\t((typeof(page))_compound_head(page))\n \n+static __always_inline void set_compound_head(struct page *page,\n+\t\t\t\t\t      const struct page *head,\n+\t\t\t\t\t      unsigned int order)\n+{\n+\tWRITE_ONCE(page->compound_info, (unsigned long)head + 1);\n+}\n+\n+static __always_inline void clear_compound_head(struct page *page)\n+{\n+\tWRITE_ONCE(page->compound_info, 0);\n+}\n+\n /**\n  * page_folio - Converts from page to folio.\n  * @p: The page.\n@@ -865,18 +877,6 @@ static inline bool folio_test_large(const struct folio *folio)\n \treturn folio_test_head(folio);\n }\n \n-static __always_inline void set_compound_head(struct page *page,\n-\t\t\t\t\t      const struct page *head,\n-\t\t\t\t\t      unsigned int order)\n-{\n-\tWRITE_ONCE(page->compound_info, (unsigned long)head + 1);\n-}\n-\n-static __always_inline void clear_compound_head(struct page *page)\n-{\n-\tWRITE_ONCE(page->compound_info, 0);\n-}\n-\n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\n static inline void ClearPageCompound(struct page *page)\n {\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-02"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author is addressing a concern about alignment of struct pages in the HugeTLB vmemmap optimization (HVO) and has agreed to align vmemmap to MAX_FOLIO_NR_PAGES, which will be done by changing the VMEMMAP_ADDR_ALIGN definition in arch/riscv/mm/init.c.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "patch revision needed"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\nstruct pages of the head page to be naturally aligned with regard to the\nfolio size.\n\nAlign vmemmap to MAX_FOLIO_NR_PAGES.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\n---\n arch/riscv/mm/init.c | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a/arch/riscv/mm/init.c b/arch/riscv/mm/init.c\nindex 21d534824624..c555b9a4fdce 100644\n--- a/arch/riscv/mm/init.c\n+++ b/arch/riscv/mm/init.c\n@@ -63,7 +63,8 @@ phys_addr_t phys_ram_base __ro_after_init;\n EXPORT_SYMBOL(phys_ram_base);\n \n #ifdef CONFIG_SPARSEMEM_VMEMMAP\n-#define VMEMMAP_ADDR_ALIGN\t(1ULL << SECTION_SIZE_BITS)\n+#define VMEMMAP_ADDR_ALIGN\tmax(1ULL << SECTION_SIZE_BITS, \\\n+\t\t\t\t    MAX_FOLIO_NR_PAGES * sizeof(struct page))\n \n unsigned long vmemmap_start_pfn __ro_after_init;\n EXPORT_SYMBOL(vmemmap_start_pfn);\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-02"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author is addressing a concern about the alignment of struct pages in the HugeTLB vmemmap optimization (HVO). They are making changes to align vmemmap to MAX_FOLIO_NR_PAGES, which will ensure that struct pages of the head page are naturally aligned with regard to the folio size.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged a fix is needed"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\nstruct pages of the head page to be naturally aligned with regard to the\nfolio size.\n\nAlign vmemmap to MAX_FOLIO_NR_PAGES.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\n---\n arch/loongarch/include/asm/pgtable.h | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h\nindex c33b3bcb733e..f9416acb9156 100644\n--- a/arch/loongarch/include/asm/pgtable.h\n+++ b/arch/loongarch/include/asm/pgtable.h\n@@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];\n \t min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)\n #endif\n \n-#define vmemmap\t\t((struct page *)((VMALLOC_END + PMD_SIZE) & PMD_MASK))\n+#define VMEMMAP_ALIGN\tmax(PMD_SIZE, MAX_FOLIO_NR_PAGES * sizeof(struct page))\n+#define vmemmap\t\t((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))\n #define VMEMMAP_END\t((unsigned long)vmemmap + VMEMMAP_SIZE - 1)\n \n #define KFENCE_AREA_START\t(VMEMMAP_END + 1)\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-02"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author addressed a concern about the correctness of page_zonenum() in the context of HugeTLB vmemmap optimization, explaining that it must use the head page address when compound_info has a mask. They clarified the logic behind compound_info_has_mask(), which is limited to HugeTLB vmemmap optimization where it makes a difference, and acknowledged that struct pages need to be naturally aligned for all orders up to MAX_FOLIO_ORDER.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "For tail pages, the kernel uses the 'compound_info' field to get to the\nhead page. The bit 0 of the field indicates whether the page is a\ntail page, and if set, the remaining bits represent a pointer to the\nhead page.\n\nFor cases when size of struct page is power-of-2, change the encoding of\ncompound_info to store a mask that can be applied to the virtual address\nof the tail page in order to access the head page. It is possible\nbecause struct page of the head page is naturally aligned with regards\nto order of the page.\n\nThe significant impact of this modification is that all tail pages of\nthe same order will now have identical 'compound_info', regardless of\nthe compound page they are associated with. This paves the way for\neliminating fake heads.\n\nThe HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only\napplied when the sizeof(struct page) is power-of-2. Having identical\ntail pages allows the same page to be mapped into the vmemmap of all\npages, maintaining memory savings without fake heads.\n\nIf sizeof(struct page) is not power-of-2, there is no functional\nchanges.\n\nLimit mask usage to HugeTLB vmemmap optimization (HVO) where it makes\na difference. The approach with mask would work in the wider set of\nconditions, but it requires validating that struct pages are naturally\naligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\n---\n include/linux/page-flags.h | 81 ++++++++++++++++++++++++++++++++++----\n mm/slab.h                  | 16 ++++++--\n mm/util.c                  | 16 ++++++--\n 3 files changed, 97 insertions(+), 16 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex d14a17ffb55b..8f2c7fbc739b 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -198,6 +198,29 @@ enum pageflags {\n \n #ifndef __GENERATING_BOUNDS_H\n \n+/*\n+ * For tail pages, if the size of struct page is power-of-2 ->compound_info\n+ * encodes the mask that converts the address of the tail page address to\n+ * the head page address.\n+ *\n+ * Otherwise, ->compound_info has direct pointer to head pages.\n+ */\n+static __always_inline bool compound_info_has_mask(void)\n+{\n+\t/*\n+\t * Limit mask usage to HugeTLB vmemmap optimization (HVO) where it\n+\t * makes a difference.\n+\t *\n+\t * The approach with mask would work in the wider set of conditions,\n+\t * but it requires validating that struct pages are naturally aligned\n+\t * for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n+\t */\n+\tif (!IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP))\n+\t\treturn false;\n+\n+\treturn is_power_of_2(sizeof(struct page));\n+}\n+\n #ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP\n DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\n \n@@ -210,6 +233,10 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \tif (!static_branch_unlikely(&hugetlb_optimize_vmemmap_key))\n \t\treturn page;\n \n+\t/* Fake heads only exists if compound_info_has_mask() is true */\n+\tif (!compound_info_has_mask())\n+\t\treturn page;\n+\n \t/*\n \t * Only addresses aligned with PAGE_SIZE of struct page may be fake head\n \t * struct page. The alignment check aims to avoid access the fields (\n@@ -223,10 +250,14 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t\t * because the @page is a compound page composed with at least\n \t\t * two contiguous pages.\n \t\t */\n-\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n+\t\tunsigned long info = READ_ONCE(page[1].compound_info);\n \n-\t\tif (likely(head & 1))\n-\t\t\treturn (const struct page *)(head - 1);\n+\t\t/* See set_compound_head() */\n+\t\tif (likely(info & 1)) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\n+\t\t\treturn (const struct page *)(p & info);\n+\t\t}\n \t}\n \treturn page;\n }\n@@ -281,11 +312,26 @@ static __always_inline int page_is_fake_head(const struct page *page)\n \n static __always_inline unsigned long _compound_head(const struct page *page)\n {\n-\tunsigned long head = READ_ONCE(page->compound_info);\n+\tunsigned long info = READ_ONCE(page->compound_info);\n \n-\tif (unlikely(head & 1))\n-\t\treturn head - 1;\n-\treturn (unsigned long)page_fixed_fake_head(page);\n+\t/* Bit 0 encodes PageTail() */\n+\tif (!(info & 1))\n+\t\treturn (unsigned long)page_fixed_fake_head(page);\n+\n+\t/*\n+\t * If compound_info_has_mask() is false, the rest of compound_info is\n+\t * the pointer to the head page.\n+\t */\n+\tif (!compound_info_has_mask())\n+\t\treturn info - 1;\n+\n+\t/*\n+\t * If compoun_info_has_mask() is true the rest of the info encodes\n+\t * the mask that converts the address of the tail page to the head page.\n+\t *\n+\t * No need to clear bit 0 in the mask as 'page' always has it clear.\n+\t */\n+\treturn (unsigned long)page & info;\n }\n \n #define compound_head(page)\t((typeof(page))_compound_head(page))\n@@ -294,7 +340,26 @@ static __always_inline void set_compound_head(struct page *page,\n \t\t\t\t\t      const struct page *head,\n \t\t\t\t\t      unsigned int order)\n {\n-\tWRITE_ONCE(page->compound_info, (unsigned long)head + 1);\n+\tunsigned int shift;\n+\tunsigned long mask;\n+\n+\tif (!compound_info_has_mask()) {\n+\t\tWRITE_ONCE(page->compound_info, (unsigned long)head | 1);\n+\t\treturn;\n+\t}\n+\n+\t/*\n+\t * If the size of struct page is power-of-2, bits [shift:0] of the\n+\t * virtual address of compound head are zero.\n+\t *\n+\t * Calculate mask that can be applied to the virtual address of\n+\t * the tail page to get address of the head page.\n+\t */\n+\tshift = order + order_base_2(sizeof(struct page));\n+\tmask = GENMASK(BITS_PER_LONG - 1, shift);\n+\n+\t/* Bit 0 encodes PageTail() */\n+\tWRITE_ONCE(page->compound_info, mask | 1);\n }\n \n static __always_inline void clear_compound_head(struct page *page)\ndiff --git a/mm/slab.h b/mm/slab.h\nindex 8a2a9c6c697b..f68c3ac8126f 100644\n--- a/mm/slab.h\n+++ b/mm/slab.h\n@@ -137,11 +137,19 @@ static_assert(IS_ALIGNED(offsetof(struct slab, freelist), sizeof(struct freelist\n  */\n static inline struct slab *page_slab(const struct page *page)\n {\n-\tunsigned long head;\n+\tunsigned long info;\n+\n+\tinfo = READ_ONCE(page->compound_info);\n+\tif (info & 1) {\n+\t\t/* See compound_head() */\n+\t\tif (compound_info_has_mask()) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\t\t\tpage = (struct page *)(p & info);\n+\t\t} else {\n+\t\t\tpage = (struct page *)(info - 1);\n+\t\t}\n+\t}\n \n-\thead = READ_ONCE(page->compound_head);\n-\tif (head & 1)\n-\t\tpage = (struct page *)(head - 1);\n \tif (data_race(page->page_type >> 24) != PGTY_slab)\n \t\tpage = NULL;\n \ndiff --git a/mm/util.c b/mm/util.c\nindex 3ebcb9e6035c..20dccf2881d7 100644\n--- a/mm/util.c\n+++ b/mm/util.c\n@@ -1237,7 +1237,7 @@ static void set_ps_flags(struct page_snapshot *ps, const struct folio *folio,\n  */\n void snapshot_page(struct page_snapshot *ps, const struct page *page)\n {\n-\tunsigned long head, nr_pages = 1;\n+\tunsigned long info, nr_pages = 1;\n \tstruct folio *foliop;\n \tint loops = 5;\n \n@@ -1247,8 +1247,8 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n again:\n \tmemset(&ps->folio_snapshot, 0, sizeof(struct folio));\n \tmemcpy(&ps->page_snapshot, page, sizeof(*page));\n-\thead = ps->page_snapshot.compound_info;\n-\tif ((head & 1) == 0) {\n+\tinfo = ps->page_snapshot.compound_info;\n+\tif (!(info & 1)) {\n \t\tps->idx = 0;\n \t\tfoliop = (struct folio *)&ps->page_snapshot;\n \t\tif (!folio_test_large(foliop)) {\n@@ -1259,7 +1259,15 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n \t\t}\n \t\tfoliop = (struct folio *)page;\n \t} else {\n-\t\tfoliop = (struct folio *)(head - 1);\n+\t\t/* See compound_head() */\n+\t\tif (compound_info_has_mask()) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\n+\t\t\tfoliop = (struct folio *)(p & info);\n+\t\t} else {\n+\t\t\tfoliop = (struct folio *)(info - 1);\n+\t\t}\n+\n \t\tps->idx = folio_page_idx(foliop, page);\n \t}\n \n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-02"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author acknowledged that the zone information stored in shared tail page flags would be incorrect due to huge pages on the same node belonging to different zones, and agreed to always fetch zone information from the head page.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged a technical issue",
            "agreed to make changes"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "With the upcoming changes to HVO, a single page of tail struct pages\nwill be shared across all huge pages of the same order on a node. Since\nhuge pages on the same node may belong to different zones, the zone\ninformation stored in shared tail page flags would be incorrect.\n\nAlways fetch zone information from the head page, which has unique and\ncorrect zone flags for each compound page.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nAcked-by: Zi Yan <ziy@nvidia.com>\n---\n include/linux/mmzone.h | 1 +\n 1 file changed, 1 insertion(+)\n\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex be8ce40b5638..192143b5cdc0 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n \n static inline enum zone_type page_zonenum(const struct page *page)\n {\n+\tpage = compound_head(page);\n \treturn memdesc_zonenum(page->flags);\n }\n \n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-02"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author is addressing a concern about the alignment of vmemmap pages when compound_info encodes a mask. They have added a VM_BUG_ON() to check for natural alignment, indicating that they agree with the feedback and plan to fix it.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged a concern",
            "added a fix"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "If page->compound_info encodes a mask, it is expected that vmemmap to be\nnaturally aligned to the maximum folio size.\n\nAdd a VM_BUG_ON() to check the alignment.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nAcked-by: Zi Yan <ziy@nvidia.com>\n---\n mm/sparse.c | 7 +++++++\n 1 file changed, 7 insertions(+)\n\ndiff --git a/mm/sparse.c b/mm/sparse.c\nindex b5b2b6f7041b..6c9b62607f3f 100644\n--- a/mm/sparse.c\n+++ b/mm/sparse.c\n@@ -600,6 +600,13 @@ void __init sparse_init(void)\n \tBUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));\n \tmemblocks_present();\n \n+\tif (compound_info_has_mask()) {\n+\t\tunsigned long alignment;\n+\n+\t\talignment = MAX_FOLIO_NR_PAGES * sizeof(struct page);\n+\t\tVM_BUG_ON(!IS_ALIGNED((unsigned long) pfn_to_page(0), alignment));\n+\t}\n+\n \tpnum_begin = first_present_section_nr();\n \tnid_begin = sparse_early_nid(__nr_to_section(pnum_begin));\n \n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-02"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), agreed to restructure in v2.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged a fix is needed",
            "agreed to restructure"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "To prepare for removing fake head pages, the vmemmap_walk code is being\nreworked.\n\nThe reuse_page and reuse_addr variables are being eliminated. There will\nno longer be an expectation regarding the reuse address in relation to\nthe operated range. Instead, the caller will provide head and tail\nvmemmap pages.\n\nCurrently, vmemmap_head and vmemmap_tail are set to the same page, but\nthis will change in the future.\n\nThe only functional change is that __hugetlb_vmemmap_optimize_folio()\nwill abandon optimization if memory allocation fails.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\n---\n mm/hugetlb_vmemmap.c | 226 +++++++++++++++++--------------------------\n 1 file changed, 90 insertions(+), 136 deletions(-)\n\ndiff --git a/mm/hugetlb_vmemmap.c b/mm/hugetlb_vmemmap.c\nindex a9280259e12a..a39a301e08b9 100644\n--- a/mm/hugetlb_vmemmap.c\n+++ b/mm/hugetlb_vmemmap.c\n@@ -25,8 +25,8 @@\n  *\n  * @remap_pte:\t\tcalled for each lowest-level entry (PTE).\n  * @nr_walked:\t\tthe number of walked pte.\n- * @reuse_page:\t\tthe page which is reused for the tail vmemmap pages.\n- * @reuse_addr:\t\tthe virtual address of the @reuse_page page.\n+ * @vmemmap_head:\tthe page to be installed as first in the vmemmap range\n+ * @vmemmap_tail:\tthe page to be installed as non-first in the vmemmap range\n  * @vmemmap_pages:\tthe list head of the vmemmap pages that can be freed\n  *\t\t\tor is mapped from.\n  * @flags:\t\tused to modify behavior in vmemmap page table walking\n@@ -35,11 +35,13 @@\n struct vmemmap_remap_walk {\n \tvoid\t\t\t(*remap_pte)(pte_t *pte, unsigned long addr,\n \t\t\t\t\t     struct vmemmap_remap_walk *walk);\n+\n \tunsigned long\t\tnr_walked;\n-\tstruct page\t\t*reuse_page;\n-\tunsigned long\t\treuse_addr;\n+\tstruct page\t\t*vmemmap_head;\n+\tstruct page\t\t*vmemmap_tail;\n \tstruct list_head\t*vmemmap_pages;\n \n+\n /* Skip the TLB flush when we split the PMD */\n #define VMEMMAP_SPLIT_NO_TLB_FLUSH\tBIT(0)\n /* Skip the TLB flush when we remap the PTE */\n@@ -141,14 +143,7 @@ static int vmemmap_pte_entry(pte_t *pte, unsigned long addr,\n {\n \tstruct vmemmap_remap_walk *vmemmap_walk = walk->private;\n \n-\t/*\n-\t * The reuse_page is found 'first' in page table walking before\n-\t * starting remapping.\n-\t */\n-\tif (!vmemmap_walk->reuse_page)\n-\t\tvmemmap_walk->reuse_page = pte_page(ptep_get(pte));\n-\telse\n-\t\tvmemmap_walk->remap_pte(pte, addr, vmemmap_walk);\n+\tvmemmap_walk->remap_pte(pte, addr, vmemmap_walk);\n \tvmemmap_walk->nr_walked++;\n \n \treturn 0;\n@@ -208,18 +203,12 @@ static void free_vmemmap_page_list(struct list_head *list)\n static void vmemmap_remap_pte(pte_t *pte, unsigned long addr,\n \t\t\t      struct vmemmap_remap_walk *walk)\n {\n-\t/*\n-\t * Remap the tail pages as read-only to catch illegal write operation\n-\t * to the tail pages.\n-\t */\n-\tpgprot_t pgprot = PAGE_KERNEL_RO;\n \tstruct page *page = pte_page(ptep_get(pte));\n \tpte_t entry;\n \n \t/* Remapping the head page requires r/w */\n-\tif (unlikely(addr == walk->reuse_addr)) {\n-\t\tpgprot = PAGE_KERNEL;\n-\t\tlist_del(&walk->reuse_page->lru);\n+\tif (unlikely(walk->nr_walked == 0 && walk->vmemmap_head)) {\n+\t\tlist_del(&walk->vmemmap_head->lru);\n \n \t\t/*\n \t\t * Makes sure that preceding stores to the page contents from\n@@ -227,53 +216,50 @@ static void vmemmap_remap_pte(pte_t *pte, unsigned long addr,\n \t\t * write.\n \t\t */\n \t\tsmp_wmb();\n+\n+\t\tentry = mk_pte(walk->vmemmap_head, PAGE_KERNEL);\n+\t} else {\n+\t\t/*\n+\t\t * Remap the tail pages as read-only to catch illegal write\n+\t\t * operation to the tail pages.\n+\t\t */\n+\t\tentry = mk_pte(walk->vmemmap_tail, PAGE_KERNEL_RO);\n \t}\n \n-\tentry = mk_pte(walk->reuse_page, pgprot);\n \tlist_add(&page->lru, walk->vmemmap_pages);\n \tset_pte_at(&init_mm, addr, pte, entry);\n }\n \n-/*\n- * How many struct page structs need to be reset. When we reuse the head\n- * struct page, the special metadata (e.g. page->flags or page->mapping)\n- * cannot copy to the tail struct page structs. The invalid value will be\n- * checked in the free_tail_page_prepare(). In order to avoid the message\n- * of \"corrupted mapping in tail page\". We need to reset at least 4 (one\n- * head struct page struct and three tail struct page structs) struct page\n- * structs.\n- */\n-#define NR_RESET_STRUCT_PAGE\t\t4\n-\n-static inline void reset_struct_pages(struct page *start)\n-{\n-\tstruct page *from = start + NR_RESET_STRUCT_PAGE;\n-\n-\tBUILD_BUG_ON(NR_RESET_STRUCT_PAGE * 2 > PAGE_SIZE / sizeof(struct page));\n-\tmemcpy(start, from, sizeof(*from) * NR_RESET_STRUCT_PAGE);\n-}\n-\n static void vmemmap_restore_pte(pte_t *pte, unsigned long addr,\n \t\t\t\tstruct vmemmap_remap_walk *walk)\n {\n-\tpgprot_t pgprot = PAGE_KERNEL;\n \tstruct page *page;\n-\tvoid *to;\n-\n-\tBUG_ON(pte_page(ptep_get(pte)) != walk->reuse_page);\n+\tstruct page *from, *to;\n \n \tpage = list_first_entry(walk->vmemmap_pages, struct page, lru);\n \tlist_del(&page->lru);\n+\n+\t/*\n+\t * Initialize tail pages in the newly allocated vmemmap page.\n+\t *\n+\t * There is folio-scope metadata that is encoded in the first few\n+\t * tail pages.\n+\t *\n+\t * Use the value last tail page in the page with the head page\n+\t * to initialize the rest of tail pages.\n+\t */\n+\tfrom = compound_head((struct page *)addr) +\n+\t\tPAGE_SIZE / sizeof(struct page) - 1;\n \tto = page_to_virt(page);\n-\tcopy_page(to, (void *)walk->reuse_addr);\n-\treset_struct_pages(to);\n+\tfor (int i = 0; i < PAGE_SIZE / sizeof(struct page); i++, to++)\n+\t\t*to = *from;\n \n \t/*\n \t * Makes sure that preceding stores to the page contents become visible\n \t * before the set_pte_at() write.\n \t */\n \tsmp_wmb();\n-\tset_pte_at(&init_mm, addr, pte, mk_pte(page, pgprot));\n+\tset_pte_at(&init_mm, addr, pte, mk_pte(page, PAGE_KERNEL));\n }\n \n /**\n@@ -283,33 +269,28 @@ static void vmemmap_restore_pte(pte_t *pte, unsigned long addr,\n  *             to remap.\n  * @end:       end address of the vmemmap virtual address range that we want to\n  *             remap.\n- * @reuse:     reuse address.\n- *\n  * Return: %0 on success, negative error code otherwise.\n  */\n-static int vmemmap_remap_split(unsigned long start, unsigned long end,\n-\t\t\t       unsigned long reuse)\n+static int vmemmap_remap_split(unsigned long start, unsigned long end)\n {\n \tstruct vmemmap_remap_walk walk = {\n \t\t.remap_pte\t= NULL,\n \t\t.flags\t\t= VMEMMAP_SPLIT_NO_TLB_FLUSH,\n \t};\n \n-\t/* See the comment in the vmemmap_remap_free(). */\n-\tBUG_ON(start - reuse != PAGE_SIZE);\n-\n-\treturn vmemmap_remap_range(reuse, end, &walk);\n+\treturn vmemmap_remap_range(start, end, &walk);\n }\n \n /**\n  * vmemmap_remap_free - remap the vmemmap virtual address range [@start, @end)\n- *\t\t\tto the page which @reuse is mapped to, then free vmemmap\n- *\t\t\twhich the range are mapped to.\n+ *\t\t\tto use @vmemmap_head/tail, then free vmemmap which\n+ *\t\t\tthe range are mapped to.\n  * @start:\tstart address of the vmemmap virtual address range that we want\n  *\t\tto remap.\n  * @end:\tend address of the vmemmap virtual address range that we want to\n  *\t\tremap.\n- * @reuse:\treuse address.\n+ * @vmemmap_head: the page to be installed as first in the vmemmap range\n+ * @vmemmap_tail: the page to be installed as non-first in the vmemmap range\n  * @vmemmap_pages: list to deposit vmemmap pages to be freed.  It is callers\n  *\t\tresponsibility to free pages.\n  * @flags:\tmodifications to vmemmap_remap_walk flags\n@@ -317,69 +298,38 @@ static int vmemmap_remap_split(unsigned long start, unsigned long end,\n  * Return: %0 on success, negative error code otherwise.\n  */\n static int vmemmap_remap_free(unsigned long start, unsigned long end,\n-\t\t\t      unsigned long reuse,\n+\t\t\t      struct page *vmemmap_head,\n+\t\t\t      struct page *vmemmap_tail,\n \t\t\t      struct list_head *vmemmap_pages,\n \t\t\t      unsigned long flags)\n {\n \tint ret;\n \tstruct vmemmap_remap_walk walk = {\n \t\t.remap_pte\t= vmemmap_remap_pte,\n-\t\t.reuse_addr\t= reuse,\n+\t\t.vmemmap_head\t= vmemmap_head,\n+\t\t.vmemmap_tail\t= vmemmap_tail,\n \t\t.vmemmap_pages\t= vmemmap_pages,\n \t\t.flags\t\t= flags,\n \t};\n-\tint nid = page_to_nid((struct page *)reuse);\n-\tgfp_t gfp_mask = GFP_KERNEL | __GFP_NORETRY | __GFP_NOWARN;\n+\n+\tret = vmemmap_remap_range(start, end, &walk);\n+\tif (!ret || !walk.nr_walked)\n+\t\treturn ret;\n+\n+\tend = start + walk.nr_walked * PAGE_SIZE;\n \n \t/*\n-\t * Allocate a new head vmemmap page to avoid breaking a contiguous\n-\t * block of struct page memory when freeing it back to page allocator\n-\t * in free_vmemmap_page_list(). This will allow the likely contiguous\n-\t * struct page backing memory to be kept contiguous and allowing for\n-\t * more allocations of hugepages. Fallback to the currently\n-\t * mapped head page in case should it fail to allocate.\n+\t * vmemmap_pages contains pages from the previous vmemmap_remap_range()\n+\t * call which failed.  These are pages which were removed from\n+\t * the vmemmap. They will be restored in the following call.\n \t */\n-\twalk.reuse_page = alloc_pages_node(nid, gfp_mask, 0);\n-\tif (walk.reuse_page) {\n-\t\tcopy_page(page_to_virt(walk.reuse_page),\n-\t\t\t  (void *)walk.reuse_addr);\n-\t\tlist_add(&walk.reuse_page->lru, vmemmap_pages);\n-\t\tmemmap_pages_add(1);\n-\t}\n+\twalk = (struct vmemmap_remap_walk) {\n+\t\t.remap_pte\t= vmemmap_restore_pte,\n+\t\t.vmemmap_pages\t= vmemmap_pages,\n+\t\t.flags\t\t= 0,\n+\t};\n \n-\t/*\n-\t * In order to make remapping routine most efficient for the huge pages,\n-\t * the routine of vmemmap page table walking has the following rules\n-\t * (see more details from the vmemmap_pte_range()):\n-\t *\n-\t * - The range [@start, @end) and the range [@reuse, @reuse + PAGE_SIZE)\n-\t *   should be continuous.\n-\t * - The @reuse address is part of the range [@reuse, @end) that we are\n-\t *   walking which is passed to vmemmap_remap_range().\n-\t * - The @reuse address is the first in the complete range.\n-\t *\n-\t * So we need to make sure that @start and @reuse meet the above rules.\n-\t */\n-\tBUG_ON(start - reuse != PAGE_SIZE);\n-\n-\tret = vmemmap_remap_range(reuse, end, &walk);\n-\tif (ret && walk.nr_walked) {\n-\t\tend = reuse + walk.nr_walked * PAGE_SIZE;\n-\t\t/*\n-\t\t * vmemmap_pages contains pages from the previous\n-\t\t * vmemmap_remap_range call which failed.  These\n-\t\t * are pages which were removed from the vmemmap.\n-\t\t * They will be restored in the following call.\n-\t\t */\n-\t\twalk = (struct vmemmap_remap_walk) {\n-\t\t\t.remap_pte\t= vmemmap_restore_pte,\n-\t\t\t.reuse_addr\t= reuse,\n-\t\t\t.vmemmap_pages\t= vmemmap_pages,\n-\t\t\t.flags\t\t= 0,\n-\t\t};\n-\n-\t\tvmemmap_remap_range(reuse, end, &walk);\n-\t}\n+\tvmemmap_remap_range(start, end, &walk);\n \n \treturn ret;\n }\n@@ -416,29 +366,24 @@ static int alloc_vmemmap_page_list(unsigned long start, unsigned long end,\n  *\t\tto remap.\n  * @end:\tend address of the vmemmap virtual address range that we want to\n  *\t\tremap.\n- * @reuse:\treuse address.\n  * @flags:\tmodifications to vmemmap_remap_walk flags\n  *\n  * Return: %0 on success, negative error code otherwise.\n  */\n static int vmemmap_remap_alloc(unsigned long start, unsigned long end,\n-\t\t\t       unsigned long reuse, unsigned long flags)\n+\t\t\t       unsigned long flags)\n {\n \tLIST_HEAD(vmemmap_pages);\n \tstruct vmemmap_remap_walk walk = {\n \t\t.remap_pte\t= vmemmap_restore_pte,\n-\t\t.reuse_addr\t= reuse,\n \t\t.vmemmap_pages\t= &vmemmap_pages,\n \t\t.flags\t\t= flags,\n \t};\n \n-\t/* See the comment in the vmemmap_remap_free(). */\n-\tBUG_ON(start - reuse != PAGE_SIZE);\n-\n \tif (alloc_vmemmap_page_list(start, end, &vmemmap_pages))\n \t\treturn -ENOMEM;\n \n-\treturn vmemmap_remap_range(reuse, end, &walk);\n+\treturn vmemmap_remap_range(start, end, &walk);\n }\n \n DEFINE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\n@@ -455,8 +400,7 @@ static int __hugetlb_vmemmap_restore_folio(const struct hstate *h,\n \t\t\t\t\t   struct folio *folio, unsigned long flags)\n {\n \tint ret;\n-\tunsigned long vmemmap_start = (unsigned long)&folio->page, vmemmap_end;\n-\tunsigned long vmemmap_reuse;\n+\tunsigned long vmemmap_start, vmemmap_end;\n \n \tVM_WARN_ON_ONCE_FOLIO(!folio_test_hugetlb(folio), folio);\n \tVM_WARN_ON_ONCE_FOLIO(folio_ref_count(folio), folio);\n@@ -467,18 +411,18 @@ static int __hugetlb_vmemmap_restore_folio(const struct hstate *h,\n \tif (flags & VMEMMAP_SYNCHRONIZE_RCU)\n \t\tsynchronize_rcu();\n \n+\tvmemmap_start\t= (unsigned long)&folio->page;\n \tvmemmap_end\t= vmemmap_start + hugetlb_vmemmap_size(h);\n-\tvmemmap_reuse\t= vmemmap_start;\n+\n \tvmemmap_start\t+= HUGETLB_VMEMMAP_RESERVE_SIZE;\n \n \t/*\n \t * The pages which the vmemmap virtual address range [@vmemmap_start,\n-\t * @vmemmap_end) are mapped to are freed to the buddy allocator, and\n-\t * the range is mapped to the page which @vmemmap_reuse is mapped to.\n+\t * @vmemmap_end) are mapped to are freed to the buddy allocator.\n \t * When a HugeTLB page is freed to the buddy allocator, previously\n \t * discarded vmemmap pages must be allocated and remapping.\n \t */\n-\tret = vmemmap_remap_alloc(vmemmap_start, vmemmap_end, vmemmap_reuse, flags);\n+\tret = vmemmap_remap_alloc(vmemmap_start, vmemmap_end, flags);\n \tif (!ret) {\n \t\tfolio_clear_hugetlb_vmemmap_optimized(folio);\n \t\tstatic_branch_dec(&hugetlb_optimize_vmemmap_key);\n@@ -566,9 +510,9 @@ static int __hugetlb_vmemmap_optimize_folio(const struct hstate *h,\n \t\t\t\t\t    struct list_head *vmemmap_pages,\n \t\t\t\t\t    unsigned long flags)\n {\n-\tint ret = 0;\n-\tunsigned long vmemmap_start = (unsigned long)&folio->page, vmemmap_end;\n-\tunsigned long vmemmap_reuse;\n+\tunsigned long vmemmap_start, vmemmap_end;\n+\tstruct page *vmemmap_head, *vmemmap_tail;\n+\tint nid, ret = 0;\n \n \tVM_WARN_ON_ONCE_FOLIO(!folio_test_hugetlb(folio), folio);\n \tVM_WARN_ON_ONCE_FOLIO(folio_ref_count(folio), folio);\n@@ -593,18 +537,30 @@ static int __hugetlb_vmemmap_optimize_folio(const struct hstate *h,\n \t */\n \tfolio_set_hugetlb_vmemmap_optimized(folio);\n \n+\tnid = folio_nid(folio);\n+\tvmemmap_head = alloc_pages_node(nid, GFP_KERNEL, 0);\n+\tif (!vmemmap_head) {\n+\t\tret = -ENOMEM;\n+\t\tgoto out;\n+\t}\n+\n+\tcopy_page(page_to_virt(vmemmap_head), folio);\n+\tlist_add(&vmemmap_head->lru, vmemmap_pages);\n+\tmemmap_pages_add(1);\n+\n+\tvmemmap_tail\t= vmemmap_head;\n+\tvmemmap_start\t= (unsigned long)&folio->page;\n \tvmemmap_end\t= vmemmap_start + hugetlb_vmemmap_size(h);\n-\tvmemmap_reuse\t= vmemmap_start;\n-\tvmemmap_start\t+= HUGETLB_VMEMMAP_RESERVE_SIZE;\n \n \t/*\n-\t * Remap the vmemmap virtual address range [@vmemmap_start, @vmemmap_end)\n-\t * to the page which @vmemmap_reuse is mapped to.  Add pages previously\n-\t * mapping the range to vmemmap_pages list so that they can be freed by\n-\t * the caller.\n+\t * Remap the vmemmap virtual address range [@vmemmap_start, @vmemmap_end).\n+\t * Add pages previously mapping the range to vmemmap_pages list so that\n+\t * they can be freed by the caller.\n \t */\n-\tret = vmemmap_remap_free(vmemmap_start, vmemmap_end, vmemmap_reuse,\n+\tret = vmemmap_remap_free(vmemmap_start, vmemmap_end,\n+\t\t\t\t vmemmap_head, vmemmap_tail,\n \t\t\t\t vmemmap_pages, flags);\n+out:\n \tif (ret) {\n \t\tstatic_branch_dec(&hugetlb_optimize_vmemmap_key);\n \t\tfolio_clear_hugetlb_vmemmap_optimized(folio);\n@@ -633,21 +589,19 @@ void hugetlb_vmemmap_optimize_folio(const struct hstate *h, struct folio *folio)\n \n static int hugetlb_vmemmap_split_folio(const struct hstate *h, struct folio *folio)\n {\n-\tunsigned long vmemmap_start = (unsigned long)&folio->page, vmemmap_end;\n-\tunsigned long vmemmap_reuse;\n+\tunsigned long vmemmap_start, vmemmap_end;\n \n \tif (!vmemmap_should_optimize_folio(h, folio))\n \t\treturn 0;\n \n+\tvmemmap_start\t= (unsigned long)&folio->page;\n \tvmemmap_end\t= vmemmap_start + hugetlb_vmemmap_size(h);\n-\tvmemmap_reuse\t= vmemmap_start;\n-\tvmemmap_start\t+= HUGETLB_VMEMMAP_RESERVE_SIZE;\n \n \t/*\n \t * Split PMDs on the vmemmap virtual address range [@vmemmap_start,\n \t * @vmemmap_end]\n \t */\n-\treturn vmemmap_remap_split(vmemmap_start, vmemmap_end, vmemmap_reuse);\n+\treturn vmemmap_remap_split(vmemmap_start, vmemmap_end);\n }\n \n static void __hugetlb_vmemmap_optimize_folios(struct hstate *h,\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-02"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author addressed a concern about the correctness of page_zonenum() in the context of huge pages and vmemmap optimization. They explained that the function must use the head page, not the tail page, to determine the zone number. The author confirmed that this is a correctness fix and provided code changes to implement it.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged a correctness issue",
            "confirmed a fix"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "HugeTLB Vmemmap Optimization (HVO) reduces memory usage by freeing most\nvmemmap pages for huge pages and remapping the freed range to a single\npage containing the struct page metadata.\n\nWith the new mask-based compound_info encoding (for power-of-2 struct\npage sizes), all tail pages of the same order are now identical\nregardless of which compound page they belong to. This means the tail\npages can be truly shared without fake heads.\n\nAllocate a single page of initialized tail struct pages per NUMA node\nper order in the vmemmap_tails[] array in pglist_data. All huge pages of\nthat order on the node share this tail page, mapped read-only into their\nvmemmap. The head page remains unique per huge page.\n\nRedefine MAX_FOLIO_ORDER using ilog2(). The define has to produce a\ncompile-constant as it is used to specify vmemmap_tail array size.\nFor some reason, compiler is not able to solve get_order() at\ncompile-time, but ilog2() works.\n\nAvoid PUD_ORDER to define MAX_FOLIO_ORDER as it adds dependency to\n<linux/pgtable.h> which generates hard-to-break include loop.\n\nThis eliminates fake heads while maintaining the same memory savings,\nand simplifies compound_head() by removing fake head detection.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\n---\n include/linux/mmzone.h | 19 +++++++++++++++++--\n mm/hugetlb_vmemmap.c   | 34 +++++++++++++++++++++++++++++++--\n mm/sparse-vmemmap.c    | 43 ++++++++++++++++++++++++++++++++++--------\n 3 files changed, 84 insertions(+), 12 deletions(-)\n\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 192143b5cdc0..c01f8235743b 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -81,13 +81,17 @@\n  * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect\n  * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.\n  */\n-#define MAX_FOLIO_ORDER\t\tget_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)\n+#ifdef CONFIG_64BIT\n+#define MAX_FOLIO_ORDER\t\t(ilog2(SZ_16G) - PAGE_SHIFT)\n+#else\n+#define MAX_FOLIO_ORDER\t\t(ilog2(SZ_1G) - PAGE_SHIFT)\n+#endif\n #else\n /*\n  * Without hugetlb, gigantic folios that are bigger than a single PUD are\n  * currently impossible.\n  */\n-#define MAX_FOLIO_ORDER\t\tPUD_ORDER\n+#define MAX_FOLIO_ORDER\t\t(PUD_SHIFT - PAGE_SHIFT)\n #endif\n \n #define MAX_FOLIO_NR_PAGES\t(1UL << MAX_FOLIO_ORDER)\n@@ -1402,6 +1406,14 @@ struct memory_failure_stats {\n };\n #endif\n \n+/*\n+ * vmemmap optimization (like HVO) is only possible for page orders that fill\n+ * two or more pages with struct pages.\n+ */\n+#define VMEMMAP_TAIL_MIN_ORDER (ilog2(2 * PAGE_SIZE / sizeof(struct page)))\n+#define __NR_VMEMMAP_TAILS (MAX_FOLIO_ORDER - VMEMMAP_TAIL_MIN_ORDER + 1)\n+#define NR_VMEMMAP_TAILS (__NR_VMEMMAP_TAILS > 0 ? __NR_VMEMMAP_TAILS : 0)\n+\n /*\n  * On NUMA machines, each NUMA node would have a pg_data_t to describe\n  * it's memory layout. On UMA machines there is a single pglist_data which\n@@ -1550,6 +1562,9 @@ typedef struct pglist_data {\n #ifdef CONFIG_MEMORY_FAILURE\n \tstruct memory_failure_stats mf_stats;\n #endif\n+#ifdef CONFIG_SPARSEMEM_VMEMMAP\n+\tstruct page *vmemmap_tails[NR_VMEMMAP_TAILS];\n+#endif\n } pg_data_t;\n \n #define node_present_pages(nid)\t(NODE_DATA(nid)->node_present_pages)\ndiff --git a/mm/hugetlb_vmemmap.c b/mm/hugetlb_vmemmap.c\nindex a39a301e08b9..688764c52c72 100644\n--- a/mm/hugetlb_vmemmap.c\n+++ b/mm/hugetlb_vmemmap.c\n@@ -19,6 +19,7 @@\n \n #include <asm/tlbflush.h>\n #include \"hugetlb_vmemmap.h\"\n+#include \"internal.h\"\n \n /**\n  * struct vmemmap_remap_walk - walk vmemmap page table\n@@ -505,6 +506,32 @@ static bool vmemmap_should_optimize_folio(const struct hstate *h, struct folio *\n \treturn true;\n }\n \n+static struct page *vmemmap_get_tail(unsigned int order, int node)\n+{\n+\tstruct page *tail, *p;\n+\tunsigned int idx;\n+\n+\tidx = order - VMEMMAP_TAIL_MIN_ORDER;\n+\ttail = READ_ONCE(NODE_DATA(node)->vmemmap_tails[idx]);\n+\tif (tail)\n+\t\treturn tail;\n+\n+\ttail = alloc_pages_node(node, GFP_KERNEL | __GFP_ZERO, 0);\n+\tif (!tail)\n+\t\treturn NULL;\n+\n+\tp = page_to_virt(tail);\n+\tfor (int i = 0; i < PAGE_SIZE / sizeof(struct page); i++)\n+\t\tprep_compound_tail(p + i, NULL, order);\n+\n+\tif (cmpxchg(&NODE_DATA(node)->vmemmap_tails[idx], NULL, tail)) {\n+\t\t__free_page(tail);\n+\t\ttail = READ_ONCE(NODE_DATA(node)->vmemmap_tails[idx]);\n+\t}\n+\n+\treturn tail;\n+}\n+\n static int __hugetlb_vmemmap_optimize_folio(const struct hstate *h,\n \t\t\t\t\t    struct folio *folio,\n \t\t\t\t\t    struct list_head *vmemmap_pages,\n@@ -520,6 +547,11 @@ static int __hugetlb_vmemmap_optimize_folio(const struct hstate *h,\n \tif (!vmemmap_should_optimize_folio(h, folio))\n \t\treturn ret;\n \n+\tnid = folio_nid(folio);\n+\tvmemmap_tail = vmemmap_get_tail(h->order, nid);\n+\tif (!vmemmap_tail)\n+\t\treturn -ENOMEM;\n+\n \tstatic_branch_inc(&hugetlb_optimize_vmemmap_key);\n \n \tif (flags & VMEMMAP_SYNCHRONIZE_RCU)\n@@ -537,7 +569,6 @@ static int __hugetlb_vmemmap_optimize_folio(const struct hstate *h,\n \t */\n \tfolio_set_hugetlb_vmemmap_optimized(folio);\n \n-\tnid = folio_nid(folio);\n \tvmemmap_head = alloc_pages_node(nid, GFP_KERNEL, 0);\n \tif (!vmemmap_head) {\n \t\tret = -ENOMEM;\n@@ -548,7 +579,6 @@ static int __hugetlb_vmemmap_optimize_folio(const struct hstate *h,\n \tlist_add(&vmemmap_head->lru, vmemmap_pages);\n \tmemmap_pages_add(1);\n \n-\tvmemmap_tail\t= vmemmap_head;\n \tvmemmap_start\t= (unsigned long)&folio->page;\n \tvmemmap_end\t= vmemmap_start + hugetlb_vmemmap_size(h);\n \ndiff --git a/mm/sparse-vmemmap.c b/mm/sparse-vmemmap.c\nindex 37522d6cb398..13bcf5562f1b 100644\n--- a/mm/sparse-vmemmap.c\n+++ b/mm/sparse-vmemmap.c\n@@ -378,16 +378,44 @@ void vmemmap_wrprotect_hvo(unsigned long addr, unsigned long end,\n \t}\n }\n \n-/*\n- * Populate vmemmap pages HVO-style. The first page contains the head\n- * page and needed tail pages, the other ones are mirrors of the first\n- * page.\n- */\n+static __meminit unsigned long vmemmap_get_tail(unsigned int order, int node)\n+{\n+\tstruct page *p, *tail;\n+\tunsigned int idx;\n+\n+\tBUG_ON(order < VMEMMAP_TAIL_MIN_ORDER);\n+\tBUG_ON(order > MAX_FOLIO_ORDER);\n+\n+\tidx = order - VMEMMAP_TAIL_MIN_ORDER;\n+\ttail = NODE_DATA(node)->vmemmap_tails[idx];\n+\tif (tail)\n+\t\treturn page_to_pfn(tail);\n+\n+\tp = vmemmap_alloc_block_zero(PAGE_SIZE, node);\n+\tif (!p)\n+\t\treturn 0;\n+\n+\tfor (int i = 0; i < PAGE_SIZE / sizeof(struct page); i++)\n+\t\tprep_compound_tail(p + i, NULL, order);\n+\n+\ttail = virt_to_page(p);\n+\tNODE_DATA(node)->vmemmap_tails[idx] = tail;\n+\n+\treturn page_to_pfn(tail);\n+}\n+\n int __meminit vmemmap_populate_hvo(unsigned long addr, unsigned long end,\n \t\t\t\t       int node, unsigned long headsize)\n {\n+\tunsigned long maddr, len, tail_pfn;\n+\tunsigned int order;\n \tpte_t *pte;\n-\tunsigned long maddr;\n+\n+\tlen = end - addr;\n+\torder = ilog2(len * sizeof(struct page) / PAGE_SIZE);\n+\ttail_pfn = vmemmap_get_tail(order, node);\n+\tif (!tail_pfn)\n+\t\treturn -ENOMEM;\n \n \tfor (maddr = addr; maddr < addr + headsize; maddr += PAGE_SIZE) {\n \t\tpte = vmemmap_populate_address(maddr, node, NULL, -1, 0);\n@@ -398,8 +426,7 @@ int __meminit vmemmap_populate_hvo(unsigned long addr, unsigned long end,\n \t/*\n \t * Reuse the last page struct page mapped above for the rest.\n \t */\n-\treturn vmemmap_populate_range(maddr, end, node, NULL,\n-\t\t\t\t\tpte_pfn(ptep_get(pte)), 0);\n+\treturn vmemmap_populate_range(maddr, end, node, NULL, tail_pfn, 0);\n }\n \n void __weak __meminit vmemmap_set_pmd(pmd_t *pmd, void *p, int node,\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-02"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author acknowledged that the patch for page_zonenum() must use the head page and agreed to restructure in v2.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged a fix is needed",
            "agreed to restructure"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "This series removes \"fake head pages\" from the HugeTLB vmemmap\noptimization (HVO) by changing how tail pages encode their relationship\nto the head page.\n\nIt simplifies compound_head() and page_ref_add_unless(). Both are in the\nhot path.\n\nBackground\n==========\n\nHVO reduces memory overhead by freeing vmemmap pages for HugeTLB pages\nand remapping the freed virtual addresses to a single physical page.\nPreviously, all tail page vmemmap entries were remapped to the first\nvmemmap page (containing the head struct page), creating \"fake heads\" -\ntail pages that appear to have PG_head set when accessed through the\ndeduplicated vmemmap.\n\nThis required special handling in compound_head() to detect and work\naround fake heads, adding complexity and overhead to a very hot path.\n\nNew Approach\n============\n\nFor architectures/configs where sizeof(struct page) is a power of 2 (the\ncommon case), this series changes how position of the head page is encoded\nin the tail pages.\n\nInstead of storing a pointer to the head page, the ->compound_info\n(renamed from ->compound_head) now stores a mask.\n\nThe mask can be applied to any tail page's virtual address to compute\nthe head page address. Critically, all tail pages of the same order now\nhave identical compound_info values, regardless of which compound page\nthey belong to.\n\nThe key insight is that all tail pages of the same order now have\nidentical compound_info values, regardless of which compound page they\nbelong to. This allows a single page of tail struct pages to be shared\nacross all huge pages of the same order on a NUMA node.\n\nBenefits\n========\n\n1. Simplified compound_head(): No fake head detection needed, can be\n   implemented in a branchless manner.\n\n2. Simplified page_ref_add_unless(): RCU protection removed since there's\n   no race with fake head remapping.\n\n3. Cleaner architecture: The shared tail pages are truly read-only and\n   contain valid tail page metadata.\n\nIf sizeof(struct page) is not power-of-2, there are no functional changes.\nHVO is not supported in this configuration.\n\nI had hoped to see performance improvement, but my testing thus far has\nshown either no change or only a slight improvement within the noise.\n\nSeries Organization\n===================\n\nPatch 1: Preparation - move MAX_FOLIO_ORDER to mmzone.h\nPatches 2-4: Refactoring - interface changes, field rename, code movement\nPatches 5-6: Arch fixes - align vmemmap for riscv and LoongArch\nPatch 7: Core change - new mask-based compound_head() encoding\nPatch 8: Correctness fix - page_zonenum() must use head page\nPatch 9: Add memmap alignment check for compound_info_has_mask()\nPatch 10: Refactor vmemmap_walk for new design\nPatch 11: Eliminate fake heads with shared tail pages\nPatches 12-15: Cleanup - remove fake head infrastructure\nPatch 16: Documentation update\nPatch 17: Get rid of opencoded compound_head() in page_slab()\n\nChanges in v6:\n==============\n  - Simplify memmap alignment check in mm/sparse.c: use VM_BUG_ON()\n    (Muchun)\n\n  - Store struct page pointers in vmemmap_tails[] instead of PFNs.\n    (Muchun)\n\n  - Fix build error on powerpc due to negative NR_VMEMMAP_TAILS.\n\nChanges in v5:\n==============\n  - Rebased to mm-everything-2026-01-27-04-35\n\n  - Add arch-specific patches to align vmemmap to maximal folio size\n    for riscv and LoongArch architectures.\n\n  - Strengthen the memmap alignment check in mm/sparse.c: use BUG()\n    for CONFIG_DEBUG_VM, WARN() otherwise. (Muchun)\n\n  - Use cmpxchg() instead of hugetlb_lock to update vmemmap_tails\n    array. (Muchun)\n\n  - Update page_slab().\n\nChanges in v4:\n==============\n  - Fix build issues due to linux/mmzone.h <-> linux/pgtable.h\n    dependency loop by avoiding including linux/pgtable.h into\n    linux/mmzone.h\n\n  - Rework vmemmap_remap_alloc() interface. (Muchun)\n\n  - Use &folio->page instead of folio address for optimization\n    target. (Muchun)\n\nChanges in v3:\n==============\n  - Fixed error recovery path in vmemmap_remap_free() to pass correct start\n    address for TLB flush. (Muchun)\n\n  - Wrapped the mask-based compound_info encoding within CONFIG_SPARSEMEM_VMEMMAP\n    check via compound_info_has_mask(). For other memory models, alignment\n    guarantees are harder to verify. (Muchun)\n\n  - Updated vmemmap_dedup.rst documentation wording: changed \"vmemmap_tail\n    shared for the struct hstate\" to \"A single, per-node page frame shared\n    among all hugepages of the same size\". (Muchun)\n\n  - Fixed build error with MAX_FOLIO_ORDER expanding to undefined PUD_ORDER\n    in certain configurations. (kernel test robot)\n\nChanges in v2:\n==============\n\n- Handle boot-allocated huge pages correctly. (Frank)\n\n- Changed from per-hstate vmemmap_tail to per-node vmemmap_tails[] array\n  in pglist_data. (Muchun)\n\n- Added spin_lock(&hugetlb_lock) protection in vmemmap_get_tail() to fix\n  a race condition where two threads could both allocate tail pages.\n  The losing thread now properly frees its allocated page. (Usama)\n\n- Add warning if memmap is not aligned to MAX_FOLIO_SIZE, which is\n  required for the mask approach. (Muchun)\n\n- Make page_zonenum() use head page - correctness fix since shared\n  tail pages cannot have valid zone information. (Muchun)\n\n- Added 'const' qualifier to head parameter in set_compound_head() and\n  prep_compound_tail(). (Usama)\n\n- Updated commit messages.\n\nKiryl Shutsemau (17):\n  mm: Move MAX_FOLIO_ORDER definition to mmzone.h\n  mm: Change the interface of prep_compound_tail()\n  mm: Rename the 'compound_head' field in the 'struct page' to\n    'compound_info'\n  mm: Move set/clear_compound_head() next to compound_head()\n  riscv/mm: Align vmemmap to maximal folio size\n  LoongArch/mm: Align vmemmap to maximal folio size\n  mm: Rework compound_head() for power-of-2 sizeof(struct page)\n  mm: Make page_zonenum() use head page\n  mm/sparse: Check memmap alignment for compound_info_has_mask()\n  mm/hugetlb: Refactor code around vmemmap_walk\n  mm/hugetlb: Remove fake head pages\n  mm: Drop fake head checks\n  hugetlb: Remove VMEMMAP_SYNCHRONIZE_RCU\n  mm/hugetlb: Remove hugetlb_optimize_vmemmap_key static key\n  mm: Remove the branch from compound_head()\n  hugetlb: Update vmemmap_dedup.rst\n  mm/slab: Use compound_head() in page_slab()\n\n .../admin-guide/kdump/vmcoreinfo.rst          |   2 +-\n Documentation/mm/vmemmap_dedup.rst            |  62 ++--\n arch/loongarch/include/asm/pgtable.h          |   3 +-\n arch/riscv/mm/init.c                          |   3 +-\n include/linux/mm.h                            |  31 --\n include/linux/mm_types.h                      |  20 +-\n include/linux/mmzone.h                        |  47 +++\n include/linux/page-flags.h                    | 167 +++++-----\n include/linux/page_ref.h                      |   8 +-\n include/linux/types.h                         |   2 +-\n kernel/vmcore_info.c                          |   2 +-\n mm/hugetlb.c                                  |   8 +-\n mm/hugetlb_vmemmap.c                          | 288 ++++++++----------\n mm/internal.h                                 |  12 +-\n mm/mm_init.c                                  |   2 +-\n mm/page_alloc.c                               |   4 +-\n mm/slab.h                                     |   8 +-\n mm/sparse-vmemmap.c                           |  43 ++-\n mm/sparse.c                                   |   7 +\n mm/util.c                                     |  16 +-\n 20 files changed, 363 insertions(+), 372 deletions(-)\n\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-02"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author is addressing the reviewer's concern that page_zonenum() must use the head page. The author agrees and promises to restructure in v2, implying a fix will be needed.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged a fix is needed",
            "promised to restructure"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "With fake head pages eliminated in the previous commit, remove the\nsupporting infrastructure:\n\n  - page_fixed_fake_head(): no longer needed to detect fake heads;\n  - page_is_fake_head(): no longer needed;\n  - page_count_writable(): no longer needed for RCU protection;\n  - RCU read_lock in page_ref_add_unless(): no longer needed;\n\nThis substantially simplifies compound_head() and page_ref_add_unless(),\nremoving both branches and RCU overhead from these hot paths.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\n---\n include/linux/page-flags.h | 93 ++------------------------------------\n include/linux/page_ref.h   |  8 +---\n 2 files changed, 4 insertions(+), 97 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex 8f2c7fbc739b..5a8f6fab2255 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -221,102 +221,15 @@ static __always_inline bool compound_info_has_mask(void)\n \treturn is_power_of_2(sizeof(struct page));\n }\n \n-#ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP\n DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\n \n-/*\n- * Return the real head page struct iff the @page is a fake head page, otherwise\n- * return the @page itself. See Documentation/mm/vmemmap_dedup.rst.\n- */\n-static __always_inline const struct page *page_fixed_fake_head(const struct page *page)\n-{\n-\tif (!static_branch_unlikely(&hugetlb_optimize_vmemmap_key))\n-\t\treturn page;\n-\n-\t/* Fake heads only exists if compound_info_has_mask() is true */\n-\tif (!compound_info_has_mask())\n-\t\treturn page;\n-\n-\t/*\n-\t * Only addresses aligned with PAGE_SIZE of struct page may be fake head\n-\t * struct page. The alignment check aims to avoid access the fields (\n-\t * e.g. compound_info) of the @page[1]. It can avoid touch a (possibly)\n-\t * cold cacheline in some cases.\n-\t */\n-\tif (IS_ALIGNED((unsigned long)page, PAGE_SIZE) &&\n-\t    test_bit(PG_head, &page->flags.f)) {\n-\t\t/*\n-\t\t * We can safely access the field of the @page[1] with PG_head\n-\t\t * because the @page is a compound page composed with at least\n-\t\t * two contiguous pages.\n-\t\t */\n-\t\tunsigned long info = READ_ONCE(page[1].compound_info);\n-\n-\t\t/* See set_compound_head() */\n-\t\tif (likely(info & 1)) {\n-\t\t\tunsigned long p = (unsigned long)page;\n-\n-\t\t\treturn (const struct page *)(p & info);\n-\t\t}\n-\t}\n-\treturn page;\n-}\n-\n-static __always_inline bool page_count_writable(const struct page *page, int u)\n-{\n-\tif (!static_branch_unlikely(&hugetlb_optimize_vmemmap_key))\n-\t\treturn true;\n-\n-\t/*\n-\t * The refcount check is ordered before the fake-head check to prevent\n-\t * the following race:\n-\t *   CPU 1 (HVO)                     CPU 2 (speculative PFN walker)\n-\t *\n-\t *   page_ref_freeze()\n-\t *   synchronize_rcu()\n-\t *                                   rcu_read_lock()\n-\t *                                   page_is_fake_head() is false\n-\t *   vmemmap_remap_pte()\n-\t *   XXX: struct page[] becomes r/o\n-\t *\n-\t *   page_ref_unfreeze()\n-\t *                                   page_ref_count() is not zero\n-\t *\n-\t *                                   atomic_add_unless(&page->_refcount)\n-\t *                                   XXX: try to modify r/o struct page[]\n-\t *\n-\t * The refcount check also prevents modification attempts to other (r/o)\n-\t * tail pages that are not fake heads.\n-\t */\n-\tif (atomic_read_acquire(&page->_refcount) == u)\n-\t\treturn false;\n-\n-\treturn page_fixed_fake_head(page) == page;\n-}\n-#else\n-static inline const struct page *page_fixed_fake_head(const struct page *page)\n-{\n-\treturn page;\n-}\n-\n-static inline bool page_count_writable(const struct page *page, int u)\n-{\n-\treturn true;\n-}\n-#endif\n-\n-static __always_inline int page_is_fake_head(const struct page *page)\n-{\n-\treturn page_fixed_fake_head(page) != page;\n-}\n-\n static __always_inline unsigned long _compound_head(const struct page *page)\n {\n \tunsigned long info = READ_ONCE(page->compound_info);\n \n \t/* Bit 0 encodes PageTail() */\n \tif (!(info & 1))\n-\t\treturn (unsigned long)page_fixed_fake_head(page);\n+\t\treturn (unsigned long)page;\n \n \t/*\n \t * If compound_info_has_mask() is false, the rest of compound_info is\n@@ -397,7 +310,7 @@ static __always_inline void clear_compound_head(struct page *page)\n \n static __always_inline int PageTail(const struct page *page)\n {\n-\treturn READ_ONCE(page->compound_info) & 1 || page_is_fake_head(page);\n+\treturn READ_ONCE(page->compound_info) & 1;\n }\n \n static __always_inline int PageCompound(const struct page *page)\n@@ -924,7 +837,7 @@ static __always_inline bool folio_test_head(const struct folio *folio)\n static __always_inline int PageHead(const struct page *page)\n {\n \tPF_POISONED_CHECK(page);\n-\treturn test_bit(PG_head, &page->flags.f) && !page_is_fake_head(page);\n+\treturn test_bit(PG_head, &page->flags.f);\n }\n \n __SETPAGEFLAG(Head, head, PF_ANY)\ndiff --git a/include/linux/page_ref.h b/include/linux/page_ref.h\nindex 544150d1d5fd..490d0ad6e56d 100644\n--- a/include/linux/page_ref.h\n+++ b/include/linux/page_ref.h\n@@ -230,13 +230,7 @@ static inline int folio_ref_dec_return(struct folio *folio)\n \n static inline bool page_ref_add_unless(struct page *page, int nr, int u)\n {\n-\tbool ret = false;\n-\n-\trcu_read_lock();\n-\t/* avoid writing to the vmemmap area being remapped */\n-\tif (page_count_writable(page, u))\n-\t\tret = atomic_add_unless(&page->_refcount, nr, u);\n-\trcu_read_unlock();\n+\tbool ret = atomic_add_unless(&page->_refcount, nr, u);\n \n \tif (page_ref_tracepoint_active(page_ref_mod_unless))\n \t\t__page_ref_mod_unless(page, nr, ret);\n-- \n2.51.2",
          "reply_to": "",
          "message_date": "2026-02-02"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author is addressing a concern about the VMEMMAP_SYNCHRONIZE_RCU flag and synchronize_rcu() calls in hugetlb_vmemmap.c, which were necessary to prevent a race between HVO remapping and page_ref_add_unless(). The author agrees that with fake heads eliminated, these RCU protections are no longer needed and plans to remove them.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged fix is needed"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "The VMEMMAP_SYNCHRONIZE_RCU flag triggered synchronize_rcu() calls to\nprevent a race between HVO remapping and page_ref_add_unless(). The\nrace could occur when a speculative PFN walker tried to modify the\nrefcount on a struct page that was in the process of being remapped\nto a fake head.\n\nWith fake heads eliminated, page_ref_add_unless() no longer needs RCU\nprotection.\n\nRemove the flag and synchronize_rcu() calls.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\n---\n mm/hugetlb_vmemmap.c | 20 ++++----------------\n 1 file changed, 4 insertions(+), 16 deletions(-)\n\ndiff --git a/mm/hugetlb_vmemmap.c b/mm/hugetlb_vmemmap.c\nindex 688764c52c72..6088fc77865c 100644\n--- a/mm/hugetlb_vmemmap.c\n+++ b/mm/hugetlb_vmemmap.c\n@@ -47,8 +47,6 @@ struct vmemmap_remap_walk {\n #define VMEMMAP_SPLIT_NO_TLB_FLUSH\tBIT(0)\n /* Skip the TLB flush when we remap the PTE */\n #define VMEMMAP_REMAP_NO_TLB_FLUSH\tBIT(1)\n-/* synchronize_rcu() to avoid writes from page_ref_add_unless() */\n-#define VMEMMAP_SYNCHRONIZE_RCU\t\tBIT(2)\n \tunsigned long\t\tflags;\n };\n \n@@ -409,9 +407,6 @@ static int __hugetlb_vmemmap_restore_folio(const struct hstate *h,\n \tif (!folio_test_hugetlb_vmemmap_optimized(folio))\n \t\treturn 0;\n \n-\tif (flags & VMEMMAP_SYNCHRONIZE_RCU)\n-\t\tsynchronize_rcu();\n-\n \tvmemmap_start\t= (unsigned long)&folio->page;\n \tvmemmap_end\t= vmemmap_start + hugetlb_vmemmap_size(h);\n \n@@ -444,7 +439,7 @@ static int __hugetlb_vmemmap_restore_folio(const struct hstate *h,\n  */\n int hugetlb_vmemmap_restore_folio(const struct hstate *h, struct folio *folio)\n {\n-\treturn __hugetlb_vmemmap_restore_folio(h, folio, VMEMMAP_SYNCHRONIZE_RCU);\n+\treturn __hugetlb_vmemmap_restore_folio(h, folio, 0);\n }\n \n /**\n@@ -467,14 +462,11 @@ long hugetlb_vmemmap_restore_folios(const struct hstate *h,\n \tstruct folio *folio, *t_folio;\n \tlong restored = 0;\n \tlong ret = 0;\n-\tunsigned long flags = VMEMMAP_REMAP_NO_TLB_FLUSH | VMEMMAP_SYNCHRONIZE_RCU;\n+\tunsigned long flags = VMEMMAP_REMAP_NO_TLB_FLUSH;\n \n \tlist_for_each_entry_safe(folio, t_folio, folio_list, lru) {\n \t\tif (folio_test_hugetlb_vmemmap_optimized(folio)) {\n \t\t\tret = __hugetlb_vmemmap_restore_folio(h, folio, flags);\n-\t\t\t/* only need to synchronize_rcu() once for each batch */\n-\t\t\tflags &= ~VMEMMAP_SYNCHRONIZE_RCU;\n-\n \t\t\tif (ret)\n \t\t\t\tbreak;\n \t\t\trestored++;\n@@ -554,8 +546,6 @@ static int __hugetlb_vmemmap_optimize_folio(const struct hstate *h,\n \n \tstatic_branch_inc(&hugetlb_optimize_vmemmap_key);\n \n-\tif (flags & VMEMMAP_SYNCHRONIZE_RCU)\n-\t\tsynchronize_rcu();\n \t/*\n \t * Very Subtle\n \t * If VMEMMAP_REMAP_NO_TLB_FLUSH is set, TLB flushing is not performed\n@@ -613,7 +603,7 @@ void hugetlb_vmemmap_optimize_folio(const struct hstate *h, struct folio *folio)\n {\n \tLIST_HEAD(vmemmap_pages);\n \n-\t__hugetlb_vmemmap_optimize_folio(h, folio, &vmemmap_pages, VMEMMAP_SYNCHRONIZE_RCU);\n+\t__hugetlb_vmemmap_optimize_folio(h, folio, &vmemmap_pages, 0);\n \tfree_vmemmap_page_list(&vmemmap_pages);\n }\n \n@@ -641,7 +631,7 @@ static void __hugetlb_vmemmap_optimize_folios(struct hstate *h,\n \tstruct folio *folio;\n \tint nr_to_optimize;\n \tLIST_HEAD(vmemmap_pages);\n-\tunsigned long flags = VMEMMAP_REMAP_NO_TLB_FLUSH | VMEMMAP_SYNCHRONIZE_RCU;\n+\tunsigned long flags = VMEMMAP_REMAP_NO_TLB_FLUSH;\n \n \tnr_to_optimize = 0;\n \tlist_for_each_entry(folio, folio_list, lru) {\n@@ -694,8 +684,6 @@ static void __hugetlb_vmemmap_optimize_folios(struct hstate *h,\n \t\tint ret;\n \n \t\tret = __hugetlb_vmemmap_optimize_folio(h, folio, &vmemmap_pages, flags);\n-\t\t/* only need to synchronize_rcu() once for each batch */\n-\t\tflags &= ~VMEMMAP_SYNCHRONIZE_RCU;\n \n \t\t/*\n \t\t * Pages to be freed may have been accumulated.  If we\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-02"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author is addressing a concern about the static key hugetlb_optimize_vmemmap_key, which was used to guard fake head detection in compound_head() and related functions. The author agrees that the static key serves no purpose now that fake heads have been eliminated and the detection code has been removed. They plan to remove its definition and all increment/decrement calls.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledges fix is needed"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "The hugetlb_optimize_vmemmap_key static key was used to guard fake head\ndetection in compound_head() and related functions. It allowed skipping\nthe fake head checks entirely when HVO was not in use.\n\nWith fake heads eliminated and the detection code removed, the static\nkey serves no purpose. Remove its definition and all increment/decrement\ncalls.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\n---\n include/linux/page-flags.h |  2 --\n mm/hugetlb_vmemmap.c       | 14 ++------------\n 2 files changed, 2 insertions(+), 14 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex 5a8f6fab2255..1aaa604f4b9b 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -221,8 +221,6 @@ static __always_inline bool compound_info_has_mask(void)\n \treturn is_power_of_2(sizeof(struct page));\n }\n \n-DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\n-\n static __always_inline unsigned long _compound_head(const struct page *page)\n {\n \tunsigned long info = READ_ONCE(page->compound_info);\ndiff --git a/mm/hugetlb_vmemmap.c b/mm/hugetlb_vmemmap.c\nindex 6088fc77865c..bdb68779d7b2 100644\n--- a/mm/hugetlb_vmemmap.c\n+++ b/mm/hugetlb_vmemmap.c\n@@ -385,9 +385,6 @@ static int vmemmap_remap_alloc(unsigned long start, unsigned long end,\n \treturn vmemmap_remap_range(start, end, &walk);\n }\n \n-DEFINE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\n-EXPORT_SYMBOL(hugetlb_optimize_vmemmap_key);\n-\n static bool vmemmap_optimize_enabled = IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP_DEFAULT_ON);\n static int __init hugetlb_vmemmap_optimize_param(char *buf)\n {\n@@ -419,10 +416,8 @@ static int __hugetlb_vmemmap_restore_folio(const struct hstate *h,\n \t * discarded vmemmap pages must be allocated and remapping.\n \t */\n \tret = vmemmap_remap_alloc(vmemmap_start, vmemmap_end, flags);\n-\tif (!ret) {\n+\tif (!ret)\n \t\tfolio_clear_hugetlb_vmemmap_optimized(folio);\n-\t\tstatic_branch_dec(&hugetlb_optimize_vmemmap_key);\n-\t}\n \n \treturn ret;\n }\n@@ -544,8 +539,6 @@ static int __hugetlb_vmemmap_optimize_folio(const struct hstate *h,\n \tif (!vmemmap_tail)\n \t\treturn -ENOMEM;\n \n-\tstatic_branch_inc(&hugetlb_optimize_vmemmap_key);\n-\n \t/*\n \t * Very Subtle\n \t * If VMEMMAP_REMAP_NO_TLB_FLUSH is set, TLB flushing is not performed\n@@ -581,10 +574,8 @@ static int __hugetlb_vmemmap_optimize_folio(const struct hstate *h,\n \t\t\t\t vmemmap_head, vmemmap_tail,\n \t\t\t\t vmemmap_pages, flags);\n out:\n-\tif (ret) {\n-\t\tstatic_branch_dec(&hugetlb_optimize_vmemmap_key);\n+\tif (ret)\n \t\tfolio_clear_hugetlb_vmemmap_optimized(folio);\n-\t}\n \n \treturn ret;\n }\n@@ -650,7 +641,6 @@ static void __hugetlb_vmemmap_optimize_folios(struct hstate *h,\n \t\t\tregister_page_bootmem_memmap(pfn_to_section_nr(spfn),\n \t\t\t\t\t&folio->page,\n \t\t\t\t\tHUGETLB_VMEMMAP_RESERVE_SIZE);\n-\t\t\tstatic_branch_inc(&hugetlb_optimize_vmemmap_key);\n \t\t\tcontinue;\n \t\t}\n \n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-02"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author is addressing a concern about the performance impact of the patch, specifically that the compound_head() function is a hot path and may be vulnerable to CPU branch misprediction. The author agrees that this is an issue and proposes rewriting the helper function in a branchless manner to mitigate it.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledges fix needed",
            "proposes rework"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "The compound_head() function is a hot path. For example, the zap path\ncalls it for every leaf page table entry.\n\nRewrite the helper function in a branchless manner to eliminate the risk\nof CPU branch misprediction.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\n---\n include/linux/page-flags.h | 27 +++++++++++++++++----------\n 1 file changed, 17 insertions(+), 10 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex 1aaa604f4b9b..16384cb6f962 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -224,25 +224,32 @@ static __always_inline bool compound_info_has_mask(void)\n static __always_inline unsigned long _compound_head(const struct page *page)\n {\n \tunsigned long info = READ_ONCE(page->compound_info);\n+\tunsigned long mask;\n+\n+\tif (!compound_info_has_mask()) {\n+\t\t/* Bit 0 encodes PageTail() */\n+\t\tif (info & 1)\n+\t\t\treturn info - 1;\n \n-\t/* Bit 0 encodes PageTail() */\n-\tif (!(info & 1))\n \t\treturn (unsigned long)page;\n-\n-\t/*\n-\t * If compound_info_has_mask() is false, the rest of compound_info is\n-\t * the pointer to the head page.\n-\t */\n-\tif (!compound_info_has_mask())\n-\t\treturn info - 1;\n+\t}\n \n \t/*\n \t * If compoun_info_has_mask() is true the rest of the info encodes\n \t * the mask that converts the address of the tail page to the head page.\n \t *\n \t * No need to clear bit 0 in the mask as 'page' always has it clear.\n+\t *\n+\t * Let's do it in a branchless manner.\n \t */\n-\treturn (unsigned long)page & info;\n+\n+\t/* Non-tail: -1UL, Tail: 0 */\n+\tmask = (info & 1) - 1;\n+\n+\t/* Non-tail: -1UL, Tail: info */\n+\tmask |= info;\n+\n+\treturn (unsigned long)page & mask;\n }\n \n #define compound_head(page)\t((typeof(page))_compound_head(page))\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-02"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author is addressing a concern about the documentation reflecting the changes in how the kernel maps tail pages after eliminating fake heads. The author agrees to update the documentation, specifically removing the description of fake heads and updating the explanation of vmemmap optimization for HugeTLB.",
          "sentiment": "positive",
          "sentiment_signals": [
            "agreement",
            "commitment"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Update the documentation regarding vmemmap optimization for hugetlb to\nreflect the changes in how the kernel maps the tail pages.\n\nFake heads no longer exist. Remove their description.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\n---\n Documentation/mm/vmemmap_dedup.rst | 60 +++++++++++++-----------------\n 1 file changed, 26 insertions(+), 34 deletions(-)\n\ndiff --git a/Documentation/mm/vmemmap_dedup.rst b/Documentation/mm/vmemmap_dedup.rst\nindex 1863d88d2dcb..fca9d0ce282a 100644\n--- a/Documentation/mm/vmemmap_dedup.rst\n+++ b/Documentation/mm/vmemmap_dedup.rst\n@@ -124,33 +124,35 @@ Here is how things look before optimization::\n  |           |\n  +-----------+\n \n-The value of page->compound_info is the same for all tail pages. The first\n-page of ``struct page`` (page 0) associated with the HugeTLB page contains the 4\n-``struct page`` necessary to describe the HugeTLB. The only use of the remaining\n-pages of ``struct page`` (page 1 to page 7) is to point to page->compound_info.\n-Therefore, we can remap pages 1 to 7 to page 0. Only 1 page of ``struct page``\n-will be used for each HugeTLB page. This will allow us to free the remaining\n-7 pages to the buddy allocator.\n+The first page of ``struct page`` (page 0) associated with the HugeTLB page\n+contains the 4 ``struct page`` necessary to describe the HugeTLB. The remaining\n+pages of ``struct page`` (page 1 to page 7) are tail pages.\n+\n+The optimization is only applied when the size of the struct page is a power-of-2\n+In this case, all tail pages of the same order are identical. See\n+compound_head(). This allows us to remap the tail pages of the vmemmap to a\n+shared, read-only page. The head page is also remapped to a new page. This\n+allows the original vmemmap pages to be freed.\n \n Here is how things look after remapping::\n \n-    HugeTLB                  struct pages(8 pages)         page frame(8 pages)\n- +-----------+ ---virt_to_page---> +-----------+   mapping to   +-----------+\n- |           |                     |     0     | -------------> |     0     |\n- |           |                     +-----------+                +-----------+\n- |           |                     |     1     | ---------------^ ^ ^ ^ ^ ^ ^\n- |           |                     +-----------+                  | | | | | |\n- |           |                     |     2     | -----------------+ | | | | |\n- |           |                     +-----------+                    | | | | |\n- |           |                     |     3     | -------------------+ | | | |\n- |           |                     +-----------+                      | | | |\n- |           |                     |     4     | ---------------------+ | | |\n- |    PMD    |                     +-----------+                        | | |\n- |   level   |                     |     5     | -----------------------+ | |\n- |  mapping  |                     +-----------+                          | |\n- |           |                     |     6     | -------------------------+ |\n- |           |                     +-----------+                            |\n- |           |                     |     7     | ---------------------------+\n+    HugeTLB                  struct pages(8 pages)                 page frame\n+ +-----------+ ---virt_to_page---> +-----------+   mapping to   +----------------+\n+ |           |                     |     0     | -------------> |       0        |\n+ |           |                     +-----------+                +----------------+\n+ |           |                     |     1     | ------\n+ |           |                     +-----------+       |\n+ |           |                     |     2     | ------        +----------------------------+\n+ |           |                     +-----------+       |        | A single, per-node page    |\n+ |           |                     |     3     | ------------> | frame shared among all     |\n+ |           |                     +-----------+       |        | hugepages of the same size |\n+ |           |                     |     4     | ------        +----------------------------+\n+ |           |                     +-----------+       |\n+ |           |                     |     5     | ------\n+ |    PMD    |                     +-----------+       |\n+ |   level   |                     |     6     | ------\n+ |  mapping  |                     +-----------+       |\n+ |           |                     |     7     | ------\n  |           |                     +-----------+\n  |           |\n  |           |\n@@ -172,16 +174,6 @@ The contiguous bit is used to increase the mapping size at the pmd and pte\n (last) level. So this type of HugeTLB page can be optimized only when its\n size of the ``struct page`` structs is greater than **1** page.\n \n-Notice: The head vmemmap page is not freed to the buddy allocator and all\n-tail vmemmap pages are mapped to the head vmemmap page frame. So we can see\n-more than one ``struct page`` struct with ``PG_head`` (e.g. 8 per 2 MB HugeTLB\n-page) associated with each HugeTLB page. The ``compound_head()`` can handle\n-this correctly. There is only **one** head ``struct page``, the tail\n-``struct page`` with ``PG_head`` are fake head ``struct page``.  We need an\n-approach to distinguish between those two different types of ``struct page`` so\n-that ``compound_head()`` can return the real head ``struct page`` when the\n-parameter is the tail ``struct page`` but with ``PG_head``.\n-\n Device DAX\n ==========\n \n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-02"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author is addressing a concern about duplicated code in page_slab() and has decided to replace it with a direct call to compound_head(). They are unsure if this change is intentional for memdesc transition, suggesting that the patch should be dropped if so.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged fix needed"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "page_slab() contained an open-coded implementation of compound_head().\n\nReplace the duplicated code with a direct call to compound_head().\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\n\n---\n\nI am not sure if this open-coded version is intentional and required for\nmemdesc transition. Drop the patch if it is.\n---\n mm/slab.h | 14 +-------------\n 1 file changed, 1 insertion(+), 13 deletions(-)\n\ndiff --git a/mm/slab.h b/mm/slab.h\nindex f68c3ac8126f..970a13ac5b8e 100644\n--- a/mm/slab.h\n+++ b/mm/slab.h\n@@ -137,19 +137,7 @@ static_assert(IS_ALIGNED(offsetof(struct slab, freelist), sizeof(struct freelist\n  */\n static inline struct slab *page_slab(const struct page *page)\n {\n-\tunsigned long info;\n-\n-\tinfo = READ_ONCE(page->compound_info);\n-\tif (info & 1) {\n-\t\t/* See compound_head() */\n-\t\tif (compound_info_has_mask()) {\n-\t\t\tunsigned long p = (unsigned long)page;\n-\t\t\tpage = (struct page *)(p & info);\n-\t\t} else {\n-\t\t\tpage = (struct page *)(info - 1);\n-\t\t}\n-\t}\n-\n+\tpage = compound_head(page);\n \tif (data_race(page->page_type >> 24) != PGTY_slab)\n \t\tpage = NULL;\n \n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-02"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-03": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "Muchun Song",
          "summary": "Reviewer Muchun Song noted that the patch to make page_zonenum() use head page does not handle the case where the head page is not present, and requested a fix to ensure that the function returns the correct zone number even when the head page is absent.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Reviewed-by: Muchun Song <muchun.song@linux.dev>\n\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-03"
        },
        {
          "author": "Muchun Song",
          "summary": "Reviewer Muchun Song pointed out that the patch does not handle the case where the head page is not present, which would cause a null pointer dereference in page_zonenum().",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "null pointer dereference",
            "handle edge cases"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Reviewed-by: Muchun Song <muchun.song@linux.dev>\n\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-03"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-04": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "Muchun Song",
          "summary": "Reviewer Muchun Song noted that page_zonenum() must use the head page, and this patch corrects that requirement.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "patch correction"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Acked-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Acked-by: Muchun Song <muchun.song@linux.dev>\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-04"
        },
        {
          "author": "Muchun Song",
          "summary": "Reviewer Muchun Song pointed out that page_zonenum() must use the head page, citing a correctness issue.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "correctness fix",
            "must use"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Acked-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Acked-by: Muchun Song <muchun.song@linux.dev>\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-04"
        },
        {
          "author": "David (arm)",
          "summary": "Reviewer David (arm) suggested consistency between page_zonenum() and set_compound_head() by either renaming the former to 'struct page *page' or adding 'tail' in the latter, aiming for uniform naming conventions.\n\nGave Acked-by",
          "sentiment": "neutral",
          "sentiment_signals": [
            "suggested improvement",
            "questioning consistency"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by",
            "Acked-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Just wondering whether we should call this \"struct page *page\" for \nconsistency with set_compound_head().\n\nOr alternatively, call it also \"tail\" in set_compound_head().",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-04"
        },
        {
          "author": "David (arm)",
          "summary": "Gave Acked-by",
          "sentiment": "positive",
          "sentiment_signals": [],
          "has_inline_review": false,
          "tags_given": [
            "Acked-by"
          ],
          "analysis_source": "heuristic",
          "raw_body": "",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-04"
        },
        {
          "author": "David (arm)",
          "summary": "Gave Acked-by",
          "sentiment": "positive",
          "sentiment_signals": [],
          "has_inline_review": false,
          "tags_given": [
            "Acked-by"
          ],
          "analysis_source": "heuristic",
          "raw_body": "",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-04"
        },
        {
          "author": "David (arm)",
          "summary": "Reviewer David (arm) pointed out that MAX_FOLIO_NR_PAGES * sizeof(struct page) does not represent the maximum folio size, but rather the size of the memmap a large folio could span at maximum. They questioned why this calculation is used and suggested avoiding it altogether or limiting its use to cases where HVO is active.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "questioned assumption"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "I think neither that statement nor the one in the patch description is \ncorrect?\n\n\"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is neither the maximum folio \nsize nor MAX_FOLIO_NR_PAGES.\n\nIt's the size of the memmap that a large folio could span at maximum.\n\n\nAssuming we have a 16 GiB folio, the calculation would give us\n\n\t4194304 * sizeof(struct page)\n\nWhich could be something like (assuming 80 bytes)\n\n\t335544320\n\n-> not even a power of 2, weird? (for HVO you wouldn't care as HVO would \nbe disabled, but that aliment is super weird?)\n\n\nAssuming 64 bytes, it would be a power of two (as 64 is a power of two).\n\n\t268435456 (1<< 28)\n\n\nWhich makes me wonder whether there is a way to avoid sizeof(struct \npage) here completely.\n\nOr limit the alignment to the case where HVO is actually active and \nsizeof(struct page) makes any sense?",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-04"
        },
        {
          "author": "David (arm)",
          "summary": "Reviewer David (arm) pointed out that the description of the situation is incorrect and suggested pulling a magic number into the core, defining it conditionally based on HVO configuration and architecture.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "suggested improvement"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Same comment, the \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is just black magic here\nand the description of the situation is wrong.\n\nMaybe you want to pull the magic \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" into the core and call it\n\n#define MAX_FOLIO_VMEMMAP_ALIGN\t(MAX_FOLIO_NR_PAGES * sizeof(struct page))\n\nBut then special case it base on (a) HVO being configured in an (b) HVO being possible\n\n#ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP && is_power_of_2(sizeof(struct page)\n/* A very helpful comment explaining the situation. */\n#define MAX_FOLIO_VMEMMAP_ALIGN\t(MAX_FOLIO_NR_PAGES * sizeof(struct page))\n#else\n#define MAX_FOLIO_VMEMMAP_ALIGN\t0\n#endif\n\nSomething like that.\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-04"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-05": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "Author is addressing a concern about inconsistent coding style in the patch, specifically regarding function declaration alignment, and has expressed a desire to have a unified coding style across the kernel.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "inconsistent_coding_style",
            "unified_kernel_style"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Do we have this coding style preference written down somewhere?\n\n-tip tree wants the opposite. Documentation/process/maintainer-tip.rst:\n\n\tWhen splitting function declarations or function calls, then please align\n\tthe first argument in the second line with the first argument in the first\n\tline::\n\nI want the editor to do The Right Thing without my brain involvement.\nHaving different coding styles in different corners of the kernel makes\nit hard.",
          "reply_to": "David (arm)",
          "message_date": "2026-02-05"
        },
        {
          "author": "David (Arm)",
          "summary": "Reviewer David (Arm) pointed out that aligning the vmemmap start to MAX_FOLIO_NR_PAGES is sufficient, as it ensures struct-page alignment and allows for efficient computation of the head page address using a mask.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "requested change",
            "provided alternative solution"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Thinking about this ...\n\nthe vmemmap start is always struct-page-aligned. Otherwise we'd be in \ntrouble already.\n\nIsn't it then sufficient to just align the start to MAX_FOLIO_NR_PAGES?\n\nLet's assume sizeof(struct page) == 64 and MAX_FOLIO_NR_PAGES = 512 for \nsimplicity.\n\nvmemmap start would be multiples of 512 (0x0010000000).\n\n512, 1024, 1536, 2048 ...\n\nAssume we have an 256-pages folio at 1536+256 = 0x111000000\n\nAssume we have the last page of that folio (0x011111111111), we would \njust get to the start of that folio by AND-ing with ~(256-1).\n\nWhich case am I ignoring?\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-05"
        },
        {
          "author": "David (Arm)",
          "summary": "Reviewer David (Arm) noted that the patch changes page_zonenum() to use the head page, but this may lead to a performance issue due to frequent compound page lookups for every PageHighMem() and page_zone() call. He suggested limiting this special casing to kernels that require HVO.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "performance concern",
            "special casing"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "We end up calling page_zonenum() without holding a reference.\n\nGiven that _compound_head() does a READ_ONCE(), this should work even if \nwe see concurrent page freeing etc.\n\nHowever, this change implies that we now perform a compound page lookup \nfor every PageHighMem() [meh], page_zone() [quite some users in the \nbuddy, including for pageblock access and page freeing].\n\nThat's a nasty compromise for making HVO better? :)\n\nWe should likely limit that special casing to kernels that really rquire \nit (HVO).\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-05"
        },
        {
          "author": "David (Arm)",
          "summary": "Reviewer David (Arm) suggested replacing VM_BUG_ON with VM_WARN_ON_ONCE in the patch, and questioned whether checking for MAX_FOLIO_NR_PAGES alignment is sufficient.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "question",
            "suggestion"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "No VM_BUG_ON. VM_WARN_ON_ONCE() should be good enough, no?\n\nAs discussed in the other thread, is checking for MAX_FOLIO_NR_PAGES \nalignment sufficient?\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-05"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "Author responded to David's feedback on patch 8, stating that the calculation of the last page in a folio is incorrect and cannot be simplified by applying a mask.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarifying question",
            "technical explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "IIUC, you are ignoring the actual size of struct page. It is not 1 byte :P\n\nThe last page of this 256-page folio is at 1536+256 + (64 * 255) which\nis 0b100011011000000. There's no mask that you can AND that gets you to\n0b11100000000.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-05"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "Author is addressing a concern about the use of roundup_pow_of_two() in patch 8, suggesting an alternative approach to handle architectures where sizeof(struct page) is not a power-of-2.\n\nThe author is addressing a concern about the HVO optimization being unknown at compile-time, which affects whether it will be used. The author explains that the kernel can be compiled with HVO but not activated due to a debug config option on non-power-of-2 architectures.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarifying question",
            "alternative solution",
            "clarification",
            "explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "I don't think we can. See the other thread.\n\nWhat about using roundup_pow_of_two(sizeof(struct page)) here.\n\n---\n\nThe annoying part of HVO is that it is unknown at compile-time if it\nwill be used. You can compile kernel with HVO that will no be activated\ndue to non-power-of-2 sizeof(struct page) because of a debug config option.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "David (arm)",
          "message_date": "2026-02-05"
        },
        {
          "author": "David (Arm)",
          "summary": "Reviewer noted that the patch description should be updated to explain why MAX_FOLIO_VMEMMAP_ALIGN is a power of 2, and provided additional context on aligning vmemmap to this value.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "additional_context"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "I thought it wouldn't matter but, yeah, that's it.\n\n\"Align the vmemmap to the maximum folio metadata size\" it is.\n\nThen you can explain the situation also alongside \nMAX_FOLIO_VMEMMAP_ALIGN, and that we expect this to be a power of 2.\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-05"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "Author acknowledged that making the page_zonenum() use head page requires some kind of assembly-level magic to conditionally apply the change based on struct page size, but did not commit to implementing it.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged a fix is needed",
            "did not commit to implementing"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "This would require some kind of asm-offsets.c/bounds.c magic to pull the\nstruct page size condition to the preprocessor level.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "David (arm)",
          "message_date": "2026-02-05"
        },
        {
          "author": "David (Arm)",
          "summary": "Reviewer suggested moving the page_zonenum() check into a macro, allowing the compiler to handle it",
          "sentiment": "neutral",
          "sentiment_signals": [
            "neutral suggestion"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Right.\n\nI guess you could move that into the macro and let the compiler handle it.\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-05"
        },
        {
          "author": "David (Arm)",
          "summary": "Reviewer David (Arm) noted that the patch is not yet complete, as MAX_FOLIO_VMEMMAP_ALIGN macro is still missing and requested to use it once available.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "patch incomplete",
            "requested change"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "And after further discussions, we could use MAX_FOLIO_VMEMMAP_ALIGN \nmacro once we have that.\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-05"
        },
        {
          "author": "David (Arm)",
          "summary": "Reviewer David (Arm) noted that clearing bit0 before the & operation in page_zonenum() is unnecessary because the head page's pointer should not have set it initially.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "acknowledgment",
            "minor comment"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Acked-by"
          ],
          "analysis_source": "llm",
          "raw_body": "IIUC, we don't care about clearing bit0 before the & as the page pointer \nshouldn't have set it in the first page.\n\nPretty neat\n\nAcked-by: David Hildenbrand (Arm) <david@kernel.org>\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-05"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-06": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "David (Arm)",
          "summary": "Reviewer David suggested a minor optimization for the patch by moving the calculation of 'idx' to be above where it is currently used, which would simplify the code.\n\nReviewer suggested adding a likely() check to the page_zonenum() function, noting that it will typically be executed quickly on systems that have been running for a while.\n\nReviewer David (Arm) noted that the __init_single_page() function does not properly initialize tail pages, specifically failing to set the node and mapcount fields correctly for huge pages.\n\nReviewer David suggested that the caller should provide the head page to use in page_zonenum(), and proposed moving the vmemmap_tails array to be HVO-only.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "minor suggestion",
            "suggested improvement",
            "requested initialization of page flags",
            "concern about potential core-mm behavior",
            "requested changes"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Could do\n\nconst unsigned int idx = order - VMEMMAP_TAIL_MIN_ORDER;\n\nabove.\n\n---\n\nWondering if a likely() would be a good idea here. I guess we'll usually \ngo through that fast path on a system that has been running for a bit.\n\n---\n\nThis leaves all pageflags, refcount etc. set to 0, which is mostly \nexpected for tail pages.\n\nBut, I would have expected something a bit more from \n__init_single_page() that initialized the page properly.\n\nIn particular:\n* set_page_node(page, node), or how is page_to_nid() handled?\n* atomic_set(&page->_mapcount, -1), to not indicate something odd to\n   core-mm where we would suddenly have a page mapping for a hugetlb\n   folio.\n\n---\n\nCould initialize them as const above.\n\nBut I am wondering whether it shouldn't be the caller that provides this \nto use? After all, it's all hugetlb code that allocates and prepares that.\n\nThen we could maybe change\n\n#ifdefCONFIG_SPARSEMEM_VMEMMAP\n\tstructpage*vmemmap_tails[NR_VMEMMAP_TAILS];\n#endif\n\nto be HVO-only.\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-06"
        },
        {
          "author": "David (Arm)",
          "summary": "Reviewer David (Arm) asked for clarification on why RCU protection was required in the past and is no longer needed, specifically mentioning a potential race condition when reusing the real head page for a tail page.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "NEEDS_WORK"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Acked-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Can you say a bit words more why RCU was required and is now no longer \nneeded?\n\nIIRC, it's because we now no longer reuse the real head page (page 0) \nfor a tail, and there could have been a race where we could have \nattempted to write to that page0 while already mapped (r/o) to page1.\n\nAlso good to mention that the corresponding RCU sync will be removed \nseparately in a following commit.\n\nNothing jumped at me and it's a great simplification for core-mm.\n\nAcked-by: David Hildenbrand (Arm) <david@kernel.org>\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-06"
        },
        {
          "author": "David (Arm)",
          "summary": "Gave Reviewed-by",
          "sentiment": "positive",
          "sentiment_signals": [],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "heuristic",
          "raw_body": "",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-06"
        },
        {
          "author": "David (Arm)",
          "summary": "Gave Acked-by",
          "sentiment": "positive",
          "sentiment_signals": [],
          "has_inline_review": false,
          "tags_given": [
            "Acked-by"
          ],
          "analysis_source": "heuristic",
          "raw_body": "",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-06"
        },
        {
          "author": "David (Arm)",
          "summary": "Gave Acked-by",
          "sentiment": "positive",
          "sentiment_signals": [],
          "has_inline_review": false,
          "tags_given": [
            "Acked-by"
          ],
          "analysis_source": "heuristic",
          "raw_body": "",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-06"
        },
        {
          "author": "David (Arm)",
          "summary": "Reviewer David noted that the patch allocates a new head page, which was not previously clear from the code, and requested this detail be highlighted.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "request for clarification"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "You could highlight that we allocate a new head page like \"page frame \n(new)\".\n\nWasn't aware of that detail before reading your change above.\n\nReviewed-by: David Hildenbrand (Arm) <david@kernel.org>\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-06"
        },
        {
          "author": "David (Arm)",
          "summary": "Reviewer David (Arm) noted that page_zonenum() must use the head page, and suggested that using compound_head() is fine for memdescs as it needs to look up the memdesc anyway",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Acked-by"
          ],
          "analysis_source": "llm",
          "raw_body": "commit 2bcd3800f2da1be13b972858f63c66d035b1ec6d\nAuthor: Matthew Wilcox (Oracle) <willy@infradead.org>\nDate:   Thu Nov 13 00:09:15 2025 +0000\n\n     slab: Reimplement page_slab()\n     \n     In order to separate slabs from folios, we need to convert from any page\n     in a slab to the slab directly without going through a page to folio\n     conversion first.\n\n     Up to this point, page_slab() has followed the example of other memdesc\n     converters (page_folio(), page_ptdesc() etc) and just cast the pointer\n     to the requested type, regardless of whether the pointer is actually a\n     pointer to the correct type or not.\n     \n     That changes with this commit; we check that the page actually belongs\n     to a slab and return NULL if it does not.  Other memdesc converters will\n     adopt this convention in future.\n\n\nI think using compound_head() is fine. For memdescs the function has to be changed to\nlookup the memdesc either way, and not go through the head page.\n\nAcked-by: David Hildenbrand (Arm) <david@kernel.org>\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-06"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-07": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "Usama Arif",
          "summary": "Reviewer noted that the patch does not work for ARM architecture, specifically when using a 2MB contiguous-PTE hugetlb on arm64, as it results in an undefined ilog2 value due to integer overflow; suggested alternative calculation using order = ilog2(len / sizeof(struct page))\n\nReviewer noted that the patch does not work for ARM architectures, specifically pointing out a calculation error in the compound_info encoding for large HugeTLB pages on arm64, and suggested an alternative approach using ilog2(len / sizeof(struct page))\n\nReviewer noted that the page_zonenum() function must use the head page, as it incorrectly uses the first tail page when there are multiple compound pages.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "patch does not work for ARM",
            "requested changes",
            "patch fails to work on specific architecture",
            "alternative solution proposed",
            "requested correction"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "This doesnt work for ARM. For len = 32 (2MB contiguous-PTE hugetlb on arm64):\nilog2(32 * 64 / 65536) = ilog2(2048 / 65536) = ilog2(0) which is undefined.\n\nIs order = ilog2(len / sizeof(struct page)) better?\n\n---\n\nThis doesnt work for ARM. For len = 32 (2MB contiguous-PTE hugetlb on arm64):\nilog2(32 * 64 / 65536) = ilog2(2048 / 65536) = ilog2(0) which is undefined.\n\nIs order = ilog2(len / sizeof(struct page)) better?\n\n---\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-07"
        },
        {
          "author": "Usama Arif",
          "summary": "Gave Acked-by\n\nReviewer noted that the patch description incorrectly used 'compoun_info_has_mask' instead of 'compound_info_has_mask', suggesting a typo rather than a technical issue.",
          "sentiment": "neutral",
          "sentiment_signals": [],
          "has_inline_review": false,
          "tags_given": [
            "Acked-by",
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "s/compoun_info_has_mask/compound_info_has_mask/\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-07"
        },
        {
          "author": "Usama Arif",
          "summary": "Reviewer noted that page_zonenum() may return incorrect results if the head page is not properly set, and suggested using the head page to avoid this issue.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "patch correction",
            "potential bug"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Acked-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Acked-by: Usama Arif <usamaarif642@gmail.com>\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-07"
        },
        {
          "author": "David (Arm)",
          "summary": "Reviewer David (Arm) pointed out that the HugeTLB vmemmap optimization (HVO) should not be possible for architectures where sizeof(struct page) is a power of 2, as it requires metadata to span at least two pages. He suggested that the code should never reach this point.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "HVO should not be possible for that size, and we should never reach that \npoint, no?\n\nRemember that for HVO, the metadata must span at least two pages.\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Usama Arif",
          "message_date": "2026-02-07"
        },
        {
          "author": "Usama Arif",
          "summary": "Reviewer noted that the patch is also checked in hugetlb_vmemmap_optimizable_size, making the change redundant and unnecessary.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "redundancy",
            "unnecessary"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Ah yeah thats right, ignore me. Its also checked in hugetlb_vmemmap_optimizable_size,\nso its all good.\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-07"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-10": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "Vlastimil Babka",
          "summary": "Reviewer noted that page_zonenum() uses the head page of a compound page, but this is incorrect and should use the head page's zone instead.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested correction"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Reviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-10"
        },
        {
          "author": "Vlastimil Babka",
          "summary": "Reviewer noted that page_zonenum() incorrectly uses the first tail page instead of the head page, and requested a fix to use the head page.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested correction"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Reviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-10"
        },
        {
          "author": "Vlastimil Babka",
          "summary": "Reviewer noted that page_zonenum() uses the first tail page instead of the head page, which can lead to incorrect zonenum values for compound pages, and requested a fix to use the head page.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Reviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-10"
        },
        {
          "author": "Vlastimil Babka",
          "summary": "Reviewer noted that page_zonenum() must use the head page, but the patch does not handle the case where the head page is a compound page",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "patch needs to be updated to handle compound pages"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Reviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-10"
        },
        {
          "author": "Vlastimil Babka",
          "summary": "Gave Reviewed-by\n\nReviewer noted that the patch does not handle the case where the head page is not present, which can lead to incorrect results in page_zonenum().",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "patch needs additional testing",
            "potential bug"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-10"
        },
        {
          "author": "Vlastimil Babka",
          "summary": "Reviewer suggested that shared tail pages be stored per node+zone instead of just per node, expressing initial dissatisfaction with the patch.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "NEUTRAL",
            "NO_CONCERN"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Not thrilled by this indeed. Would it be a problem to have the shared tail\npages per node+zone instead of just per node?\n\n\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-10"
        },
        {
          "author": "Vlastimil Babka",
          "summary": "Reviewer Vlastimil Babka noted that the patch to make page_zonenum() use head page introduces a potential issue on architectures where sizeof(struct page) is not a power of 2, as it may lead to incorrect zonenum calculation due to the changed encoding scheme.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "potential issue",
            "incorrect zonenum calculation"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Reviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-10"
        },
        {
          "author": "Vlastimil Babka",
          "summary": "Reviewer noted that page_zonenum() must use the head page, but pointed out a potential issue on RISC-V architecture where the head page may not be contiguous with the tail pages.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "potential issue",
            "RISC-V architecture"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Reviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-10"
        },
        {
          "author": "Vlastimil Babka",
          "summary": "Gave Reviewed-by\n\nReviewer noted that the patch does not handle the case where the head page is not present, which can lead to incorrect results in page_zonenum().",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "patch does not handle edge cases"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-10"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-15": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "Matthew Wilcox",
          "summary": "Reviewer questioned whether returning the zone number of the head page is correct, suggesting that the caller may be interested in the zone number of the current page rather than the head page.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "You're right that different pages in the same folio can have different\nzone number.  But does it matter ... or to put it another way, why is\nreturning the zone number of the head page the correct way to resolve\nthis?\n\nArguably, the caller is asking for the zone number of _this page_, and\ndoes not care about the zone number of the head page.  It would be good\nto have a short discussion of this in the commit message (but probably\nnot worth putting this in a comment).",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-15"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-16": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "David (Arm)",
          "summary": "Reviewer David (Arm) questioned how a folio can cross zones, specifically pointing out that runtime-allocated hugetlb folios from the CMA/buddy fall into a single zone and suggesting that it's also not allowed for ones allocated early during boot. He wondered whether we should disallow it if possible.\n\nReviewer noted that page_zonenum() must use the head page, not the zone of the tail page, to avoid incorrect results in shrink_zone_span(), and requested confirmation that there would be no functional change.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "functional change",
            "zone of the tail page"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "How can a folio cross zones?\n\nRuntime allocated hugetlb folios from the CMA/buddy (alloc_contig_range) \ndefinitely fall into a single zone.\n\nSo is it about ones allocated early during boot, where, by chance, we \nmanage to cross ZONE_NORMAL + ZONE_MOVABLE etc?\n\nI thought that it's also not allowed there, and I wonder whether we \nshould disallow it if it's possible.\n\n---\n\nAgreed, in particular, if there would be a functional change. So far I \nassumed there would be no such change.\n\nThings like shrink_zone_span() really need to know the zone of that \npage, not the one of the head; unless both fall into the same zone.\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Matthew Wilcox",
          "message_date": "2026-02-16"
        },
        {
          "author": "Vlastimil Babka",
          "summary": "Reviewer Vlastimil Babka expressed concern that allowing crossing zones in a single folio could lead to catastrophic failures, and also reiterated his dislike for the proposed solution.\n\nReviewer noted that the patch to make page_zonenum() use head page does not handle the case where the head page is not present, potentially leading to incorrect results.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "disagreement with solution",
            "potential bug",
            "edge case"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "I would be surprised if things didn't break horribly if we allowed crossing\nzones in a single folio. I'd rather not allow it.\n\n(And I still don't like how this patch solves the issue)\n\n---\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-16"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "Author agreed to move the MAX_FOLIO_ORDER variable to per-zone, in response to reviewer feedback that it would be unused for most nodes.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "agreed_to_restructure"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "I thought it would be overkill. It likely is going to be unused for most\nnodes. But sure, move it to per-zone.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "Vlastimil Babka",
          "message_date": "2026-02-16"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-23": {
      "report_file": "2026-02-23.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "I gave it a try, but stumbled on a problem. We need to know the zone in hugetlb_vmemmap_init_early(), but zones are not yet defined. hugetlb_vmemmap_init_early() is called from within sparse_init(), but span of zones is defined in free_area_init() after sparse_init(). Any ideas, how get past this? :/",
          "sentiment": "neutral",
          "sentiment_signals": [],
          "has_inline_review": true,
          "tags_given": [],
          "analysis_source": "heuristic",
          "raw_body": "On Mon, Feb 16, 2026 at 11:30:22AM +0000, Kiryl Shutsemau wrote:\n> On Tue, Feb 10, 2026 at 04:57:55PM +0100, Vlastimil Babka wrote:\n> > On 2/9/26 12:52, Kiryl Shutsemau wrote:\n> > > On Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:\n> > >> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > >> > With the upcoming changes to HVO, a single page of tail struct pages\n> > >> > will be shared across all huge pages of the same order on a node. Since\n> > >> > huge pages on the same node may belong to different zones, the zone\n> > >> > information stored in shared tail page flags would be incorrect.\n> > >> > \n> > >> > Always fetch zone information from the head page, which has unique and\n> > >> > correct zone flags for each compound page.\n> > >> > \n> > >> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> > >> > Acked-by: Zi Yan <ziy@nvidia.com>\n> > >> > ---\n> > >> >   include/linux/mmzone.h | 1 +\n> > >> >   1 file changed, 1 insertion(+)\n> > >> > \n> > >> > diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n> > >> > index be8ce40b5638..192143b5cdc0 100644\n> > >> > --- a/include/linux/mmzone.h\n> > >> > +++ b/include/linux/mmzone.h\n> > >> > @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n> > >> >   static inline enum zone_type page_zonenum(const struct page *page)\n> > >> >   {\n> > >> > +\tpage = compound_head(page);\n> > >> >   \treturn memdesc_zonenum(page->flags);\n> > >> \n> > >> We end up calling page_zonenum() without holding a reference.\n> > >> \n> > >> Given that _compound_head() does a READ_ONCE(), this should work even if we\n> > >> see concurrent page freeing etc.\n> > >> \n> > >> However, this change implies that we now perform a compound page lookup for\n> > >> every PageHighMem() [meh], page_zone() [quite some users in the buddy,\n> > >> including for pageblock access and page freeing].\n> > >> \n> > >> That's a nasty compromise for making HVO better? :)\n> > >> \n> > >> We should likely limit that special casing to kernels that really rquire it\n> > >> (HVO).\n> > > \n> > > I will add compound_info_has_mask() check.\n> > \n> > Not thrilled by this indeed. Would it be a problem to have the shared tail\n> > pages per node+zone instead of just per node?\n> \n> I thought it would be overkill. It likely is going to be unused for most\n> nodes. But sure, move it to per-zone.\n\nI gave it a try, but stumbled on a problem.\n\nWe need to know the zone in hugetlb_vmemmap_init_early(), but zones are\nnot yet defined.\n\nhugetlb_vmemmap_init_early() is called from within sparse_init(), but\nspan of zones is defined in free_area_init() after sparse_init().\n\nAny ideas, how get past this? :/\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n",
          "reply_to": "",
          "message_date": "2026-02-23",
          "message_id": ""
        }
      ],
      "analysis_source": "heuristic"
    }
  }
}