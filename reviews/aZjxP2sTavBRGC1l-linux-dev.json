{
  "thread_id": "aZjxP2sTavBRGC1l@linux.dev",
  "subject": "Re: [RFC PATCH v2 0/5] mm/swap, memcg: Introduce swap tiers for cgroup based swap control",
  "url": "https://lore.kernel.org/all/aZjxP2sTavBRGC1l@linux.dev/",
  "dates": {
    "2026-01-26": {
      "report_file": "2026-02-21_ollama_llama3.1-8b.html",
      "developer": "Shakeel Butt",
      "reviews": [
        {
          "author": "Youngjun Park (author)",
          "summary": "The author addressed a concern about the interaction between cgroup hierarchy and swap tier configuration, explaining that effective tiers are calculated separately using a dedicated mask to respect the cgroup hierarchy. The author noted that configured tiers may differ from effective ones as they must be a subset of the parent's. No fix is planned.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "This patch integrates the swap tier infrastructure with cgroup,\nenabling the selection of specific swap devices per cgroup by\nconfiguring allowed swap tiers.\n\nThe new `memory.swap.tiers` interface controls allowed swap tiers via a mask.\nBy default, the mask is set to include all tiers, allowing specific tiers to\nbe excluded or restored. Note that effective tiers are calculated separately\nusing a dedicated mask to respect the cgroup hierarchy. Consequently,\nconfigured tiers may differ from effective ones, as they must be a subset\nof the parent's.\n\nNote that cgroups do not pin swap tiers. This is similar to the\n`cpuset` controller, which does not prevent CPU hotplug. This\napproach ensures flexibility by allowing tier configuration changes\nregardless of cgroup usage.\n\nSigned-off-by: Youngjun Park <youngjun.park@lge.com>\n---\n Documentation/admin-guide/cgroup-v2.rst | 27 +++++++++\n include/linux/memcontrol.h              |  3 +-\n mm/memcontrol.c                         | 80 +++++++++++++++++++++++++\n mm/swap_tier.c                          | 66 ++++++++++++++++++++\n mm/swap_tier.h                          | 21 +++++++\n mm/swapfile.c                           |  5 ++\n 6 files changed, 201 insertions(+), 1 deletion(-)\n\ndiff --git a/Documentation/admin-guide/cgroup-v2.rst b/Documentation/admin-guide/cgroup-v2.rst\nindex 7f5b59d95fce..776a908ce1b9 100644\n--- a/Documentation/admin-guide/cgroup-v2.rst\n+++ b/Documentation/admin-guide/cgroup-v2.rst\n@@ -1848,6 +1848,33 @@ The following nested keys are defined.\n \tSwap usage hard limit.  If a cgroup's swap usage reaches this\n \tlimit, anonymous memory of the cgroup will not be swapped out.\n \n+  memory.swap.tiers\n+        A read-write nested-keyed file which exists on non-root\n+        cgroups. The default is to enable all tiers.\n+\n+        This interface allows selecting which swap tiers a cgroup can\n+        use for swapping out memory.\n+\n+        The effective tiers are inherited from the parent. Only tiers\n+        effective in the parent can be effective in the child. However,\n+        the child can explicitly disable tiers allowed by the parent.\n+\n+        When read, the file shows two lines:\n+          - The first line shows the operation string that was\n+            written to this file.\n+          - The second line shows the effective operation after\n+            merging with parent settings.\n+\n+        When writing, the format is:\n+          (+/-)(TIER_NAME) (+/-)(TIER_NAME) ...\n+\n+        Valid tier names are those configured in\n+        /sys/kernel/mm/swap/tiers.\n+\n+        Each tier can be prefixed with:\n+          +    Enable this tier\n+          -    Disable this tier\n+\n   memory.swap.events\n \tA read-only flat-keyed file which exists on non-root cgroups.\n \tThe following entries are defined.  Unless specified\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex b6c82c8f73e1..542bee1b5f60 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -283,7 +283,8 @@ struct mem_cgroup {\n \t/* per-memcg mm_struct list */\n \tstruct lru_gen_mm_list mm_list;\n #endif\n-\n+\tint tier_mask;\n+\tint tier_effective_mask;\n #ifdef CONFIG_MEMCG_V1\n \t/* Legacy consumer-oriented counters */\n \tstruct page_counter kmem;\t\t/* v1 only */\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 007413a53b45..c0a0a957a630 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -68,6 +68,7 @@\n #include <net/ip.h>\n #include \"slab.h\"\n #include \"memcontrol-v1.h\"\n+#include \"swap_tier.h\"\n \n #include <linux/uaccess.h>\n \n@@ -3691,6 +3692,7 @@ static void mem_cgroup_free(struct mem_cgroup *memcg)\n {\n \tlru_gen_exit_memcg(memcg);\n \tmemcg_wb_domain_exit(memcg);\n+\tswap_tiers_memcg_sync_mask(memcg);\n \t__mem_cgroup_free(memcg);\n }\n \n@@ -3792,6 +3794,9 @@ mem_cgroup_css_alloc(struct cgroup_subsys_state *parent_css)\n \tWRITE_ONCE(memcg->zswap_writeback, true);\n #endif\n \tpage_counter_set_high(&memcg->swap, PAGE_COUNTER_MAX);\n+\tmemcg->tier_mask = TIER_ALL_MASK;\n+\tswap_tiers_memcg_inherit_mask(memcg, parent);\n+\n \tif (parent) {\n \t\tWRITE_ONCE(memcg->swappiness, mem_cgroup_swappiness(parent));\n \n@@ -5352,6 +5357,75 @@ static int swap_events_show(struct seq_file *m, void *v)\n \treturn 0;\n }\n \n+static int swap_tier_show(struct seq_file *m, void *v)\n+{\n+\tstruct mem_cgroup *memcg = mem_cgroup_from_seq(m);\n+\n+\tswap_tiers_mask_show(m, memcg->tier_mask);\n+\tswap_tiers_mask_show(m, memcg->tier_effective_mask);\n+\n+\treturn 0;\n+}\n+\n+static ssize_t swap_tier_write(struct kernfs_open_file *of,\n+\t\t\t\tchar *buf, size_t nbytes, loff_t off)\n+{\n+\tstruct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));\n+\tchar *pos, *token;\n+\tint ret = 0;\n+\n+\tpos = strstrip(buf);\n+\n+\tspin_lock(&swap_tier_lock);\n+\tif (!*pos) {\n+\t\tmemcg->tier_mask = TIER_ALL_MASK;\n+\t\tgoto sync;\n+\t}\n+\n+\twhile ((token = strsep(&pos, \" \\t\\n\")) != NULL) {\n+\t\tint mask;\n+\n+\t\tif (!*token)\n+\t\t\tcontinue;\n+\n+\t\tif (token[0] != '-' && token[0] != '+') {\n+\t\t\tret = -EINVAL;\n+\t\t\tgoto err;\n+\t\t}\n+\n+\t\tmask = swap_tiers_mask_lookup(token+1);\n+\t\tif (!mask) {\n+\t\t\tret = -EINVAL;\n+\t\t\tgoto err;\n+\t\t}\n+\n+\t\t/*\n+\t\t * if child already set, cannot add that tiers for hierarch mismatching.\n+\t\t * parent compatible, child must respect parent selected swap device.\n+\t\t */\n+\t\tswitch (token[0]) {\n+\t\tcase '-':\n+\t\t\tmemcg->tier_mask &= ~mask;\n+\t\t\tbreak;\n+\t\tcase '+':\n+\t\t\tmemcg->tier_mask |= mask;\n+\t\t\tbreak;\n+\t\tdefault:\n+\t\t\tret = -EINVAL;\n+\t\t\tbreak;\n+\t\t}\n+\n+\t\tif (ret)\n+\t\t\tgoto err;\n+\t}\n+\n+sync:\n+\t__swap_tiers_memcg_sync_mask(memcg);\n+err:\n+\tspin_unlock(&swap_tier_lock);\n+\treturn ret ? ret : nbytes;\n+}\n+\n static struct cftype swap_files[] = {\n \t{\n \t\t.name = \"swap.current\",\n@@ -5384,6 +5458,12 @@ static struct cftype swap_files[] = {\n \t\t.file_offset = offsetof(struct mem_cgroup, swap_events_file),\n \t\t.seq_show = swap_events_show,\n \t},\n+\t{\n+\t\t.name = \"swap.tiers\",\n+\t\t.flags = CFTYPE_NOT_ON_ROOT,\n+\t\t.seq_show = swap_tier_show,\n+\t\t.write = swap_tier_write,\n+\t},\n \t{ }\t/* terminate */\n };\n \ndiff --git a/mm/swap_tier.c b/mm/swap_tier.c\nindex d90f6eccb908..e860c87292e2 100644\n--- a/mm/swap_tier.c\n+++ b/mm/swap_tier.c\n@@ -384,3 +384,69 @@ bool swap_tiers_update(void)\n \n \treturn true;\n }\n+\n+void swap_tiers_mask_show(struct seq_file *m, int mask)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tspin_lock(&swap_tier_lock);\n+\tfor_each_active_tier(tier) {\n+\t\tif (mask & TIER_MASK(tier))\n+\t\t\tseq_printf(m, \"%s \", tier->name);\n+\t}\n+\tspin_unlock(&swap_tier_lock);\n+\tseq_puts(m, \"\\n\");\n+}\n+\n+int swap_tiers_mask_lookup(const char *name)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\tfor_each_active_tier(tier) {\n+\t\tif (!strcmp(name, tier->name))\n+\t\t\treturn TIER_MASK(tier);\n+\t}\n+\n+\treturn 0;\n+}\n+\n+static void __swap_tier_memcg_inherit_mask(struct mem_cgroup *memcg,\n+\tstruct mem_cgroup *parent)\n+{\n+\tint effective_mask\n+\t\t= parent ? parent->tier_effective_mask : TIER_ALL_MASK;\n+\n+\tmemcg->tier_effective_mask\n+\t\t= effective_mask & memcg->tier_mask;\n+}\n+\n+void swap_tiers_memcg_inherit_mask(struct mem_cgroup *memcg,\n+\tstruct mem_cgroup *parent)\n+{\n+\tspin_lock(&swap_tier_lock);\n+\t__swap_tier_memcg_inherit_mask(memcg, parent);\n+\tspin_unlock(&swap_tier_lock);\n+}\n+\n+void __swap_tiers_memcg_sync_mask(struct mem_cgroup *memcg)\n+{\n+\tstruct mem_cgroup *child;\n+\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\tif (memcg == root_mem_cgroup)\n+\t\treturn;\n+\n+\tfor_each_mem_cgroup_tree(child, memcg)\n+\t\t__swap_tier_memcg_inherit_mask(child, parent_mem_cgroup(child));\n+}\n+\n+void swap_tiers_memcg_sync_mask(struct mem_cgroup *memcg)\n+{\n+\tspin_lock(&swap_tier_lock);\n+\tmemcg->tier_mask = TIER_ALL_MASK;\n+\t__swap_tiers_memcg_sync_mask(memcg);\n+\tspin_unlock(&swap_tier_lock);\n+}\ndiff --git a/mm/swap_tier.h b/mm/swap_tier.h\nindex de81d540e3b5..8652a7f993ab 100644\n--- a/mm/swap_tier.h\n+++ b/mm/swap_tier.h\n@@ -46,4 +46,25 @@ bool swap_tiers_update(void);\n /* Tier assignment */\n void swap_tiers_assign_dev(struct swap_info_struct *swp);\n \n+/* Memcg related functions */\n+void swap_tiers_mask_show(struct seq_file *m, int mask);\n+void swap_tiers_memcg_inherit_mask(struct mem_cgroup *memcg,\n+\tstruct mem_cgroup *parent);\n+void swap_tiers_memcg_sync_mask(struct mem_cgroup *memcg);\n+void __swap_tiers_memcg_sync_mask(struct mem_cgroup *memcg);\n+\n+/* Mask and tier lookup */\n+int swap_tiers_mask_lookup(const char *name);\n+\n+/**\n+ * swap_tiers_mask_test - Check if the tier mask is valid\n+ * @tier_mask: The tier mask to check\n+ * @mask: The mask to compare against\n+ *\n+ * Return: true if condition matches, false otherwise\n+ */\n+static inline bool swap_tiers_mask_test(int tier_mask, int mask)\n+{\n+\treturn tier_mask & mask;\n+}\n #endif /* _SWAP_TIER_H */\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 4f8ce021c5bd..dd97e850ea2c 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -1348,10 +1348,15 @@ static bool swap_alloc_fast(struct folio *folio)\n static void swap_alloc_slow(struct folio *folio)\n {\n \tstruct swap_info_struct *si, *next;\n+\tint mask = folio_memcg(folio) ?\n+\t\tfolio_memcg(folio)->tier_effective_mask : TIER_ALL_MASK;\n \n \tspin_lock(&swap_avail_lock);\n start_over:\n \tplist_for_each_entry_safe(si, next, &swap_avail_head, avail_list) {\n+\t\tif (!swap_tiers_mask_test(si->tier_mask, mask))\n+\t\t\tcontinue;\n+\n \t\t/* Rotate the device and switch to a new cluster */\n \t\tplist_requeue(&si->avail_list, &swap_avail_head);\n \t\tspin_unlock(&swap_avail_lock);\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-01-26"
        },
        {
          "author": "Youngjun Park (author)",
          "summary": "The author is addressing a concern about swap device tier membership tracking, specifically how to ensure that devices are correctly assigned to tiers based on their priority. The author explains that a `tier_mask` is added to identify the tier membership of swap devices and that this mapping is necessary for future allocation logic. They also mention that dynamic modification of tiers, such as splitting or merging ranges, is allowed provided that the tier assignment of already configured swap devices remains unchanged.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "This patch connects swap devices to the swap tier infrastructure,\nensuring that devices are correctly assigned to tiers based on their\npriority.\n\nA `tier_mask` is added to identify the tier membership of swap devices.\nAlthough tier-based allocation logic is not yet implemented, this\nmapping is necessary to track which tier a device belongs to. Upon\nactivation, the device is assigned to a tier by matching its priority\nagainst the configured tier ranges.\n\nThe infrastructure allows dynamic modification of tiers, such as\nsplitting or merging ranges. These operations are permitted provided\nthat the tier assignment of already configured swap devices remains\nunchanged.\n\nThis patch also adds the documentation for the swap tier feature,\ncovering the core concepts, sysfs interface usage, and configuration\ndetails.\n\nSigned-off-by: Youngjun Park <youngjun.park@lge.com>\n---\n Documentation/mm/swap-tier.rst | 109 +++++++++++++++++++++++++++++++++\n include/linux/swap.h           |   1 +\n mm/swap_state.c                |   2 +-\n mm/swap_tier.c                 | 106 ++++++++++++++++++++++++++++----\n mm/swap_tier.h                 |  13 +++-\n mm/swapfile.c                  |   2 +\n 6 files changed, 219 insertions(+), 14 deletions(-)\n create mode 100644 Documentation/mm/swap-tier.rst\n\ndiff --git a/Documentation/mm/swap-tier.rst b/Documentation/mm/swap-tier.rst\nnew file mode 100644\nindex 000000000000..3386161b9b18\n--- /dev/null\n+++ b/Documentation/mm/swap-tier.rst\n@@ -0,0 +1,109 @@\n+.. SPDX-License-Identifier: GPL-2.0\n+\n+:Author: Chris Li <chrisl@kernel.org> Youngjun Park <youngjun.park@lge.com>\n+\n+==========\n+Swap Tier\n+==========\n+\n+Swap tier is a collection of user-named groups classified by priority ranges.\n+It acts as a facilitation layer, allowing users to manage swap devices based\n+on their speeds.\n+\n+Users are encouraged to assign swap device priorities according to device\n+speed to fully utilize this feature. While the current implementation is\n+integrated with cgroups, the concept is designed to be extensible for other\n+subsystems in the future.\n+\n+Use case\n+-------\n+\n+Users can perform selective swapping by choosing a swap tier assigned according\n+to speed within a cgroup.\n+\n+For more information on cgroup v2, please refer to\n+``Documentation/admin-guide/cgroup-v2.rst``.\n+\n+Priority Range\n+--------------\n+\n+The specified tiers must cover the entire priority range from -1\n+(DEF_SWAP_PRIO) to SHRT_MAX.\n+\n+Consistency\n+-----------\n+\n+Tier consistency is guaranteed with a focus on maximizing flexibility. When a\n+swap device is activated within a tier range, a reference is held from the\n+start of the tier to the priority of that swap device. This ensures that the\n+tier of region containing the active swap device does not disappear.\n+\n+If a request to add a new tier with a priority higher than the current swap\n+device is received, the existing tier can be split.\n+\n+However, specifying a tier in a cgroup does not hold a reference to the tier.\n+Consequently, the corresponding tier can disappear at any time.\n+\n+Configuration Interface\n+-----------------------\n+\n+The swap tiers can be configured via the following interface:\n+\n+/sys/kernel/mm/swap/tiers\n+\n+Operations can be performed using the following syntax:\n+\n+* Add:    ``+\"<tiername>\":\"<start_priority>\"``\n+* Remove: ``-\"<tiername>\"``\n+* Modify: ``\"<tiername>\":\"<start_priority>\"``\n+\n+Multiple operations can be provided in a single write, separated by spaces (\" \")\n+or commas (\",\").\n+\n+When configuring tiers, the specified value represents the **start priority**\n+of that tier. The end priority is automatically determined by the start\n+priority of the next higher tier. Consequently, adding or modifying a tier\n+automatically adjusts (splits or merges) the ranges of adjacent tiers to\n+ensure continuity.\n+\n+Examples\n+--------\n+\n+**1. Initialization**\n+\n+A tier starting at -1 is mandatory to cover the entire priority range up to\n+SHRT_MAX. In this example, 'HDD' starts at 50, and 'NET' covers the remaining\n+lower range starting from -1.\n+\n+::\n+\n+    # echo \"+HDD:50, +NET:-1\" > /sys/kernel/mm/swap/tiers\n+    # cat /sys/kernel/mm/swap/tiers\n+    Name             Idx   PrioStart   PrioEnd\n+    HDD              0     50          32767\n+    NET              1     -1          49\n+\n+**2. Modification and Splitting**\n+\n+Here, 'HDD' is moved to start at 80, and a new tier 'SSD' is added at 100.\n+Notice how the ranges are automatically recalculated:\n+* 'SSD' takes the top range. Split HDD Tier's range. (100 to SHRT_MAX).\n+* 'HDD' is adjusted to the range between 'NET' and 'SSD' (80 to 99).\n+* 'NET' automatically extends to fill the gap below 'HDD' (-1 to 79).\n+\n+::\n+\n+    # echo \"HDD:80, +SSD:100\" > /sys/kernel/mm/swap/tiers\n+    # cat /sys/kernel/mm/swap/tiers\n+    Name             Idx   PrioStart   PrioEnd\n+    SSD              2     100         32767\n+    HDD              0     80          99\n+    NET              1     -1          79\n+\n+**3. Removal**\n+\n+Tiers can be removed using the '-' prefix.\n+\n+::\n+\n+    # echo \"-SSD,-HDD,-NET\" > /sys/kernel/mm/swap/tiers\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 62fc7499b408..1e68c220a0e7 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -262,6 +262,7 @@ struct swap_info_struct {\n \tstruct percpu_ref users;\t/* indicate and keep swap device valid. */\n \tunsigned long\tflags;\t\t/* SWP_USED etc: see above */\n \tsigned short\tprio;\t\t/* swap priority of this type */\n+\tint tier_mask;\t\t\t/* swap tier mask */\n \tstruct plist_node list;\t\t/* entry in swap_active_head */\n \tsigned char\ttype;\t\t/* strange name for an index */\n \tunsigned int\tmax;\t\t/* extent of the swap_map */\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex f1a7d9cdc648..d46ca61d2e42 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -997,7 +997,7 @@ static ssize_t tiers_store(struct kobject *kobj,\n \t\t\tgoto restore;\n \t}\n \n-\tif (!swap_tiers_validate()) {\n+\tif (!swap_tiers_update()) {\n \t\tret = -EINVAL;\n \t\tgoto restore;\n \t}\ndiff --git a/mm/swap_tier.c b/mm/swap_tier.c\nindex 87882272eec8..d90f6eccb908 100644\n--- a/mm/swap_tier.c\n+++ b/mm/swap_tier.c\n@@ -14,7 +14,7 @@\n  * @name: name of the swap_tier.\n  * @prio: starting value of priority.\n  * @list: linked list of tiers.\n-*/\n+ */\n static struct swap_tier {\n \tchar name[MAX_TIERNAME];\n \tshort prio;\n@@ -34,6 +34,8 @@ static LIST_HEAD(swap_tier_inactive_list);\n \t(!list_is_first(&(tier)->list, &swap_tier_active_list) ? \\\n \tlist_prev_entry((tier), list)->prio - 1 : SHRT_MAX)\n \n+#define MASK_TO_TIER(mask) (&swap_tiers[__ffs((mask))])\n+\n #define for_each_tier(tier, idx) \\\n \tfor (idx = 0, tier = &swap_tiers[0]; idx < MAX_SWAPTIER; \\\n \t\tidx++, tier = &swap_tiers[idx])\n@@ -55,6 +57,26 @@ static bool swap_tier_is_active(void)\n \treturn !list_empty(&swap_tier_active_list) ? true : false;\n }\n \n+static bool swap_tier_prio_in_range(struct swap_tier *tier, short prio)\n+{\n+\tif (tier->prio <= prio && TIER_END_PRIO(tier) >= prio)\n+\t\treturn true;\n+\n+\treturn false;\n+}\n+\n+static bool swap_tier_prio_is_used(struct swap_tier *self, short prio)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tfor_each_active_tier(tier) {\n+\t\tif (tier != self && tier->prio == prio)\n+\t\t\treturn true;\n+\t}\n+\n+\treturn false;\n+}\n+\n static struct swap_tier *swap_tier_lookup(const char *name)\n {\n \tstruct swap_tier *tier;\n@@ -67,12 +89,14 @@ static struct swap_tier *swap_tier_lookup(const char *name)\n \treturn NULL;\n }\n \n+\n void swap_tiers_init(void)\n {\n \tstruct swap_tier *tier;\n \tint idx;\n \n \tBUILD_BUG_ON(BITS_PER_TYPE(int) < MAX_SWAPTIER);\n+\tBUILD_BUG_ON(MAX_SWAPTIER > TIER_DEFAULT_IDX);\n \n \tfor_each_tier(tier, idx) {\n \t\tINIT_LIST_HEAD(&tier->list);\n@@ -145,17 +169,35 @@ static struct swap_tier *swap_tier_prepare(const char *name, short prio)\n \treturn tier;\n }\n \n-static int swap_tier_check_range(short prio)\n+static int swap_tier_can_split_range(struct swap_tier *orig_tier,\n+\tshort new_prio)\n {\n+\tstruct swap_info_struct *p;\n \tstruct swap_tier *tier;\n \n \tlockdep_assert_held(&swap_lock);\n \tlockdep_assert_held(&swap_tier_lock);\n \n-\tfor_each_active_tier(tier) {\n-\t\t/* No overwrite */\n-\t\tif (tier->prio == prio)\n-\t\t\treturn -EINVAL;\n+\tplist_for_each_entry(p, &swap_active_head, list) {\n+\t\tif (p->tier_mask == TIER_DEFAULT_MASK)\n+\t\t\tcontinue;\n+\n+\t\ttier = MASK_TO_TIER(p->tier_mask);\n+\t\tif (tier->prio > new_prio)\n+\t\t\tcontinue;\n+\t\t/*\n+                 * Prohibit implicit tier reassignment.\n+                 * Case 1: Prevent orig_tier devices from dropping out\n+                 *         of the new range.\n+                 */\n+\t\tif (orig_tier == tier && (p->prio < new_prio))\n+\t\t\treturn -EBUSY;\n+                /*\n+                 * Case 2: Prevent other tier devices from entering\n+                 *         the new range.\n+                 */\n+\t\telse if (orig_tier != tier && (p->prio >= new_prio))\n+\t\t\treturn -EBUSY;\n \t}\n \n \treturn 0;\n@@ -173,7 +215,10 @@ int swap_tiers_add(const char *name, int prio)\n \tif (swap_tier_lookup(name))\n \t\treturn -EPERM;\n \n-\tret = swap_tier_check_range(prio);\n+\tif (swap_tier_prio_is_used(NULL, prio))\n+\t\treturn -EBUSY;\n+\n+\tret = swap_tier_can_split_range(NULL, prio);\n \tif (ret)\n \t\treturn ret;\n \n@@ -183,7 +228,6 @@ int swap_tiers_add(const char *name, int prio)\n \t\treturn ret;\n \t}\n \n-\n \tswap_tier_insert_by_prio(tier);\n \treturn ret;\n }\n@@ -200,11 +244,18 @@ int swap_tiers_remove(const char *name)\n \tif (!tier)\n \t\treturn -EINVAL;\n \n+\t/* Simulate adding a tier to check for conflicts */\n+\tret = swap_tier_can_split_range(NULL, tier->prio);\n+\tif (ret)\n+\t\treturn ret;\n+\n \tlist_move(&tier->list, &swap_tier_inactive_list);\n \n \t/* Removing DEF_SWAP_PRIO merges into the higher tier. */\n-\tif (swap_tier_is_active() && tier->prio == DEF_SWAP_PRIO)\n-\t\tlist_prev_entry(tier, list)->prio = DEF_SWAP_PRIO;\n+\tif (swap_tier_is_active() && tier->prio == DEF_SWAP_PRIO) {\n+\t\tlist_last_entry(&swap_tier_active_list, struct swap_tier, list)\n+\t\t\t->prio = DEF_SWAP_PRIO;\n+\t}\n \n \treturn ret;\n }\n@@ -225,7 +276,10 @@ int swap_tiers_modify(const char *name, int prio)\n \tif (tier->prio == prio)\n \t\treturn 0;\n \n-\tret = swap_tier_check_range(prio);\n+\tif (swap_tier_prio_is_used(tier, prio))\n+\t\treturn -EBUSY;\n+\n+\tret = swap_tier_can_split_range(tier, prio);\n \tif (ret)\n \t\treturn ret;\n \n@@ -283,10 +337,27 @@ void swap_tiers_restore(struct swap_tier_save_ctx ctx[])\n \t}\n }\n \n-bool swap_tiers_validate(void)\n+void swap_tiers_assign_dev(struct swap_info_struct *swp)\n {\n \tstruct swap_tier *tier;\n \n+\tlockdep_assert_held(&swap_lock);\n+\n+\tfor_each_active_tier(tier) {\n+\t\tif (swap_tier_prio_in_range(tier, swp->prio)) {\n+\t\t\tswp->tier_mask = TIER_MASK(tier);\n+\t\t\treturn;\n+\t\t}\n+\t}\n+\n+\tswp->tier_mask = TIER_DEFAULT_MASK;\n+}\n+\n+bool swap_tiers_update(void)\n+{\n+\tstruct swap_tier *tier;\n+\tstruct swap_info_struct *swp;\n+\n \t/*\n \t * Initial setting might not cover DEF_SWAP_PRIO.\n \t * Swap tier must cover the full range (DEF_SWAP_PRIO to SHRT_MAX).\n@@ -300,5 +371,16 @@ bool swap_tiers_validate(void)\n \t\t\treturn false;\n \t}\n \n+\t/*\n+\t * If applied initially, the swap tier_mask may change\n+\t * from the default value.\n+\t */\n+\tplist_for_each_entry(swp, &swap_active_head, list) {\n+\t\t/* Tier is already configured */\n+\t\tif (swp->tier_mask != TIER_DEFAULT_MASK)\n+\t\t\tbreak;\n+\t\tswap_tiers_assign_dev(swp);\n+\t}\n+\n \treturn true;\n }\ndiff --git a/mm/swap_tier.h b/mm/swap_tier.h\nindex 4b1b0602d691..de81d540e3b5 100644\n--- a/mm/swap_tier.h\n+++ b/mm/swap_tier.h\n@@ -14,6 +14,9 @@\n #define MAX_SWAPTIER\t\t8\n #endif\n \n+/* Forward declarations */\n+struct swap_info_struct;\n+\n extern spinlock_t swap_tier_lock;\n \n struct swap_tier_save_ctx {\n@@ -24,6 +27,10 @@ struct swap_tier_save_ctx {\n #define DEFINE_SWAP_TIER_SAVE_CTX(_name) \\\n \tstruct swap_tier_save_ctx _name[MAX_SWAPTIER] = {0}\n \n+#define TIER_ALL_MASK\t\t(~0)\n+#define TIER_DEFAULT_IDX\t(31)\n+#define TIER_DEFAULT_MASK\t(1 << TIER_DEFAULT_IDX)\n+\n /* Initialization and application */\n void swap_tiers_init(void);\n ssize_t swap_tiers_sysfs_show(char *buf);\n@@ -34,5 +41,9 @@ int swap_tiers_modify(const char *name, int prio);\n \n void swap_tiers_save(struct swap_tier_save_ctx ctx[]);\n void swap_tiers_restore(struct swap_tier_save_ctx ctx[]);\n-bool swap_tiers_validate(void);\n+bool swap_tiers_update(void);\n+\n+/* Tier assignment */\n+void swap_tiers_assign_dev(struct swap_info_struct *swp);\n+\n #endif /* _SWAP_TIER_H */\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex c27952b41d4f..4f8ce021c5bd 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -2672,6 +2672,8 @@ static void _enable_swap_info(struct swap_info_struct *si)\n \n \t/* Add back to available list */\n \tadd_to_avail_list(si, true);\n+\n+\tswap_tiers_assign_dev(si);\n }\n \n static void enable_swap_info(struct swap_info_struct *si, int prio,\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-01-26"
        },
        {
          "author": "Youngjun Park (author)",
          "summary": "The author addressed a concern about the potential for silent disappearance of tiers when using the '-' operator to exclude specific tiers. They explained that tier management enforces continuous priority ranges anchored by start priorities, and operations trigger range splitting or merging, but overwriting start priorities is forbidden. The author also clarified how excluded tiers can disappear silently when removing DEF_SWAP_PRIO.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "This patch introduces the \"Swap tier\" concept, which serves as an\nabstraction layer for managing swap devices based on their performance\ncharacteristics (e.g., NVMe, HDD, Network swap).\n\nSwap tiers are user-named groups representing priority ranges.\nThese tiers collectively cover the entire priority\nspace from -1 (`DEF_SWAP_PRIO`) to `SHRT_MAX`.\n\nTo configure tiers, a new sysfs interface is exposed at\n`/sys/kernel/mm/swap/tiers`. The input parser evaluates commands from\nleft to right and supports batch input, allowing users to add, remove or\nmodify multiple tiers in a single write operation.\n\nTier management enforces continuous priority ranges anchored by start\npriorities. Operations trigger range splitting or merging, but overwriting\nstart priorities is forbidden. Merging expands lower tiers upwards to\npreserve configured start priorities, except when removing `DEF_SWAP_PRIO`,\nwhich merges downwards.\n\nSuggested-by: Chris Li <chrisl@kernel.org>\nSigned-off-by: Youngjun Park <youngjun.park@lge.com>\n---\n MAINTAINERS     |   2 +\n mm/Makefile     |   2 +-\n mm/swap.h       |   4 +\n mm/swap_state.c |  70 +++++++++++\n mm/swap_tier.c  | 304 ++++++++++++++++++++++++++++++++++++++++++++++++\n mm/swap_tier.h  |  38 ++++++\n mm/swapfile.c   |   7 +-\n 7 files changed, 423 insertions(+), 4 deletions(-)\n create mode 100644 mm/swap_tier.c\n create mode 100644 mm/swap_tier.h\n\ndiff --git a/MAINTAINERS b/MAINTAINERS\nindex 18d1ebf053db..501bf46adfb4 100644\n--- a/MAINTAINERS\n+++ b/MAINTAINERS\n@@ -16743,6 +16743,8 @@ F:\tmm/swap.c\n F:\tmm/swap.h\n F:\tmm/swap_table.h\n F:\tmm/swap_state.c\n+F:\tmm/swap_tier.c\n+F:\tmm/swap_tier.h\n F:\tmm/swapfile.c\n \n MEMORY MANAGEMENT - THP (TRANSPARENT HUGE PAGE)\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 53ca5d4b1929..3b3de2de7285 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -75,7 +75,7 @@ ifdef CONFIG_MMU\n \tobj-$(CONFIG_ADVISE_SYSCALLS)\t+= madvise.o\n endif\n \n-obj-$(CONFIG_SWAP)\t+= page_io.o swap_state.o swapfile.o\n+obj-$(CONFIG_SWAP)\t+= page_io.o swap_state.o swapfile.o swap_tier.o\n obj-$(CONFIG_ZSWAP)\t+= zswap.o\n obj-$(CONFIG_HAS_DMA)\t+= dmapool.o\n obj-$(CONFIG_HUGETLBFS)\t+= hugetlb.o hugetlb_sysfs.o hugetlb_sysctl.o\ndiff --git a/mm/swap.h b/mm/swap.h\nindex bfafa637c458..55f230cbe4e7 100644\n--- a/mm/swap.h\n+++ b/mm/swap.h\n@@ -16,6 +16,10 @@ extern int page_cluster;\n #define swap_entry_order(order)\t0\n #endif\n \n+#define DEF_SWAP_PRIO  -1\n+\n+extern spinlock_t swap_lock;\n+extern struct plist_head swap_active_head;\n extern struct swap_info_struct *swap_info[];\n \n /*\ndiff --git a/mm/swap_state.c b/mm/swap_state.c\nindex 6d0eef7470be..f1a7d9cdc648 100644\n--- a/mm/swap_state.c\n+++ b/mm/swap_state.c\n@@ -25,6 +25,7 @@\n #include \"internal.h\"\n #include \"swap_table.h\"\n #include \"swap.h\"\n+#include \"swap_tier.h\"\n \n /*\n  * swapper_space is a fiction, retained to simplify the path through\n@@ -947,8 +948,77 @@ static ssize_t vma_ra_enabled_store(struct kobject *kobj,\n }\n static struct kobj_attribute vma_ra_enabled_attr = __ATTR_RW(vma_ra_enabled);\n \n+static ssize_t tiers_show(struct kobject *kobj,\n+\t\t\t\t     struct kobj_attribute *attr, char *buf)\n+{\n+\treturn swap_tiers_sysfs_show(buf);\n+}\n+\n+static ssize_t tiers_store(struct kobject *kobj,\n+\t\t\tstruct kobj_attribute *attr,\n+\t\t\tconst char *buf, size_t count)\n+{\n+\tchar *p, *token, *name, *tmp;\n+\tint ret = 0;\n+\tshort prio;\n+\tDEFINE_SWAP_TIER_SAVE_CTX(ctx);\n+\n+\ttmp = kstrdup(buf, GFP_KERNEL);\n+\tif (!tmp)\n+\t\treturn -ENOMEM;\n+\n+\tspin_lock(&swap_lock);\n+\tspin_lock(&swap_tier_lock);\n+\n+\tp = tmp;\n+\tswap_tiers_save(ctx);\n+\n+\twhile (!ret && (token = strsep(&p, \", \\t\\n\")) != NULL) {\n+\t\tif (!*token)\n+\t\t\tcontinue;\n+\n+\t\tif (token[0] == '-') {\n+\t\t\tret = swap_tiers_remove(token + 1);\n+\t\t} else {\n+\n+\t\t\tname = strsep(&token, \":\");\n+\t\t\tif (!token || kstrtos16(token, 10, &prio)) {\n+\t\t\t\tret = -EINVAL;\n+\t\t\t\tgoto out;\n+\t\t\t}\n+\n+\t\t\tif (name[0] == '+')\n+\t\t\t\tret = swap_tiers_add(name + 1, prio);\n+\t\t\telse\n+\t\t\t\tret = swap_tiers_modify(name, prio);\n+\t\t}\n+\n+\t\tif (ret)\n+\t\t\tgoto restore;\n+\t}\n+\n+\tif (!swap_tiers_validate()) {\n+\t\tret = -EINVAL;\n+\t\tgoto restore;\n+\t}\n+\n+out:\n+\tspin_unlock(&swap_tier_lock);\n+\tspin_unlock(&swap_lock);\n+\n+\tkfree(tmp);\n+\treturn ret ? ret : count;\n+\n+restore:\n+\tswap_tiers_restore(ctx);\n+\tgoto out;\n+}\n+\n+static struct kobj_attribute tier_attr = __ATTR_RW(tiers);\n+\n static struct attribute *swap_attrs[] = {\n \t&vma_ra_enabled_attr.attr,\n+\t&tier_attr.attr,\n \tNULL,\n };\n \ndiff --git a/mm/swap_tier.c b/mm/swap_tier.c\nnew file mode 100644\nindex 000000000000..87882272eec8\n--- /dev/null\n+++ b/mm/swap_tier.c\n@@ -0,0 +1,304 @@\n+// SPDX-License-Identifier: GPL-2.0\n+#include <linux/swap.h>\n+#include <linux/memcontrol.h>\n+#include \"memcontrol-v1.h\"\n+#include <linux/sysfs.h>\n+#include <linux/plist.h>\n+\n+#include \"swap.h\"\n+#include \"swap_tier.h\"\n+\n+/*\n+ * struct swap_tier - structure representing a swap tier.\n+ *\n+ * @name: name of the swap_tier.\n+ * @prio: starting value of priority.\n+ * @list: linked list of tiers.\n+*/\n+static struct swap_tier {\n+\tchar name[MAX_TIERNAME];\n+\tshort prio;\n+\tstruct list_head list;\n+} swap_tiers[MAX_SWAPTIER];\n+\n+DEFINE_SPINLOCK(swap_tier_lock);\n+/* active swap priority list, sorted in descending order */\n+static LIST_HEAD(swap_tier_active_list);\n+/* unused swap_tier object */\n+static LIST_HEAD(swap_tier_inactive_list);\n+\n+#define TIER_IDX(tier)\t((tier) - swap_tiers)\n+#define TIER_MASK(tier)\t(1 << TIER_IDX(tier))\n+#define TIER_INVALID_PRIO (DEF_SWAP_PRIO - 1)\n+#define TIER_END_PRIO(tier) \\\n+\t(!list_is_first(&(tier)->list, &swap_tier_active_list) ? \\\n+\tlist_prev_entry((tier), list)->prio - 1 : SHRT_MAX)\n+\n+#define for_each_tier(tier, idx) \\\n+\tfor (idx = 0, tier = &swap_tiers[0]; idx < MAX_SWAPTIER; \\\n+\t\tidx++, tier = &swap_tiers[idx])\n+\n+#define for_each_active_tier(tier) \\\n+\tlist_for_each_entry(tier, &swap_tier_active_list, list)\n+\n+#define for_each_inactive_tier(tier) \\\n+\tlist_for_each_entry(tier, &swap_tier_inactive_list, list)\n+\n+/*\n+ * Naming Convention:\n+ *   swap_tiers_*() - Public/exported functions\n+ *   swap_tier_*()  - Private/internal functions\n+ */\n+\n+static bool swap_tier_is_active(void)\n+{\n+\treturn !list_empty(&swap_tier_active_list) ? true : false;\n+}\n+\n+static struct swap_tier *swap_tier_lookup(const char *name)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tfor_each_active_tier(tier) {\n+\t\tif (!strcmp(tier->name, name))\n+\t\t\treturn tier;\n+\t}\n+\n+\treturn NULL;\n+}\n+\n+void swap_tiers_init(void)\n+{\n+\tstruct swap_tier *tier;\n+\tint idx;\n+\n+\tBUILD_BUG_ON(BITS_PER_TYPE(int) < MAX_SWAPTIER);\n+\n+\tfor_each_tier(tier, idx) {\n+\t\tINIT_LIST_HEAD(&tier->list);\n+\t\tlist_add_tail(&tier->list, &swap_tier_inactive_list);\n+\t}\n+}\n+\n+ssize_t swap_tiers_sysfs_show(char *buf)\n+{\n+\tstruct swap_tier *tier;\n+\tssize_t len = 0;\n+\n+\tlen += sysfs_emit_at(buf, len, \"%-16s %-5s %-11s %-11s\\n\",\n+\t\t\t \"Name\", \"Idx\", \"PrioStart\", \"PrioEnd\");\n+\n+\tspin_lock(&swap_tier_lock);\n+\tfor_each_active_tier(tier) {\n+\t\tlen += sysfs_emit_at(buf, len, \"%-16s %-5ld %-11d %-11d\\n\",\n+\t\t\t\t     tier->name,\n+\t\t\t\t     TIER_IDX(tier),\n+\t\t\t\t     tier->prio,\n+\t\t\t\t     TIER_END_PRIO(tier));\n+\t\tif (len >= PAGE_SIZE)\n+\t\t\tbreak;\n+\t}\n+\tspin_unlock(&swap_tier_lock);\n+\n+\treturn len;\n+}\n+\n+static void swap_tier_insert_by_prio(struct swap_tier *new)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tfor_each_active_tier(tier) {\n+\t\tif (tier->prio > new->prio)\n+\t\t\tcontinue;\n+\n+\t\tlist_add_tail(&new->list, &tier->list);\n+\t\treturn;\n+\t}\n+\t/* First addition, or becomes the first tier */\n+\tlist_add_tail(&new->list, &swap_tier_active_list);\n+}\n+\n+static void __swap_tier_prepare(struct swap_tier *tier, const char *name,\n+\tshort prio)\n+{\n+\tlist_del_init(&tier->list);\n+\tstrscpy(tier->name, name, MAX_TIERNAME);\n+\ttier->prio = prio;\n+}\n+\n+static struct swap_tier *swap_tier_prepare(const char *name, short prio)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\tif (prio < DEF_SWAP_PRIO)\n+\t\treturn NULL;\n+\n+\tif (list_empty(&swap_tier_inactive_list))\n+\t\treturn ERR_PTR(-EPERM);\n+\n+\ttier = list_first_entry(&swap_tier_inactive_list,\n+\t\tstruct swap_tier, list);\n+\n+\t__swap_tier_prepare(tier, name, prio);\n+\treturn tier;\n+}\n+\n+static int swap_tier_check_range(short prio)\n+{\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\tfor_each_active_tier(tier) {\n+\t\t/* No overwrite */\n+\t\tif (tier->prio == prio)\n+\t\t\treturn -EINVAL;\n+\t}\n+\n+\treturn 0;\n+}\n+\n+int swap_tiers_add(const char *name, int prio)\n+{\n+\tint ret;\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\t/* Duplicate check */\n+\tif (swap_tier_lookup(name))\n+\t\treturn -EPERM;\n+\n+\tret = swap_tier_check_range(prio);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\ttier = swap_tier_prepare(name, prio);\n+\tif (IS_ERR(tier)) {\n+\t\tret = PTR_ERR(tier);\n+\t\treturn ret;\n+\t}\n+\n+\n+\tswap_tier_insert_by_prio(tier);\n+\treturn ret;\n+}\n+\n+int swap_tiers_remove(const char *name)\n+{\n+\tint ret = 0;\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\ttier = swap_tier_lookup(name);\n+\tif (!tier)\n+\t\treturn -EINVAL;\n+\n+\tlist_move(&tier->list, &swap_tier_inactive_list);\n+\n+\t/* Removing DEF_SWAP_PRIO merges into the higher tier. */\n+\tif (swap_tier_is_active() && tier->prio == DEF_SWAP_PRIO)\n+\t\tlist_prev_entry(tier, list)->prio = DEF_SWAP_PRIO;\n+\n+\treturn ret;\n+}\n+\n+int swap_tiers_modify(const char *name, int prio)\n+{\n+\tint ret;\n+\tstruct swap_tier *tier;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\ttier = swap_tier_lookup(name);\n+\tif (!tier)\n+\t\treturn -EINVAL;\n+\n+\t/* No need to modify */\n+\tif (tier->prio == prio)\n+\t\treturn 0;\n+\n+\tret = swap_tier_check_range(prio);\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tlist_del_init(&tier->list);\n+\ttier->prio = prio;\n+\tswap_tier_insert_by_prio(tier);\n+\n+\treturn ret;\n+}\n+\n+/*\n+ * XXX: Reverting individual operations becomes complex as the number of\n+ * operations grows. Instead, we save the original state beforehand and\n+ * fully restore it if any operation fails.\n+ */\n+void swap_tiers_save(struct swap_tier_save_ctx ctx[])\n+{\n+\tstruct swap_tier *tier;\n+\tint idx;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\tfor_each_active_tier(tier) {\n+\t\tidx = TIER_IDX(tier);\n+\t\tstrcpy(ctx[idx].name, tier->name);\n+\t\tctx[idx].prio = tier->prio;\n+\t}\n+\n+\tfor_each_inactive_tier(tier) {\n+\t\tidx = TIER_IDX(tier);\n+\t\t/* Indicator of inactive */\n+\t\tctx[idx].prio = TIER_INVALID_PRIO;\n+\t}\n+}\n+\n+void swap_tiers_restore(struct swap_tier_save_ctx ctx[])\n+{\n+\tstruct swap_tier *tier;\n+\tint idx;\n+\n+\tlockdep_assert_held(&swap_lock);\n+\tlockdep_assert_held(&swap_tier_lock);\n+\n+\t/* Invalidate active list */\n+\tlist_splice_tail_init(&swap_tier_active_list,\n+\t\t\t&swap_tier_inactive_list);\n+\n+\tfor_each_tier(tier, idx) {\n+\t\tif (ctx[idx].prio != TIER_INVALID_PRIO) {\n+\t\t\t/* Preserve idx(mask) */\n+\t\t\t__swap_tier_prepare(tier, ctx[idx].name, ctx[idx].prio);\n+\t\t\tswap_tier_insert_by_prio(tier);\n+\t\t}\n+\t}\n+}\n+\n+bool swap_tiers_validate(void)\n+{\n+\tstruct swap_tier *tier;\n+\n+\t/*\n+\t * Initial setting might not cover DEF_SWAP_PRIO.\n+\t * Swap tier must cover the full range (DEF_SWAP_PRIO to SHRT_MAX).\n+\t * Also, modify operation can change only one remaining priority.\n+\t */\n+\tif (swap_tier_is_active()) {\n+\t\ttier = list_last_entry(&swap_tier_active_list,\n+\t\t\tstruct swap_tier, list);\n+\n+\t\tif (tier->prio != DEF_SWAP_PRIO)\n+\t\t\treturn false;\n+\t}\n+\n+\treturn true;\n+}\ndiff --git a/mm/swap_tier.h b/mm/swap_tier.h\nnew file mode 100644\nindex 000000000000..4b1b0602d691\n--- /dev/null\n+++ b/mm/swap_tier.h\n@@ -0,0 +1,38 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#ifndef _SWAP_TIER_H\n+#define _SWAP_TIER_H\n+\n+#include <linux/types.h>\n+#include <linux/spinlock.h>\n+\n+#define MAX_TIERNAME\t\t16\n+\n+/* Ensure MAX_SWAPTIER does not exceed MAX_SWAPFILES */\n+#if 8 > MAX_SWAPFILES\n+#define MAX_SWAPTIER\t\tMAX_SWAPFILES\n+#else\n+#define MAX_SWAPTIER\t\t8\n+#endif\n+\n+extern spinlock_t swap_tier_lock;\n+\n+struct swap_tier_save_ctx {\n+\tchar name[MAX_TIERNAME];\n+\tshort prio;\n+};\n+\n+#define DEFINE_SWAP_TIER_SAVE_CTX(_name) \\\n+\tstruct swap_tier_save_ctx _name[MAX_SWAPTIER] = {0}\n+\n+/* Initialization and application */\n+void swap_tiers_init(void);\n+ssize_t swap_tiers_sysfs_show(char *buf);\n+\n+int swap_tiers_add(const char *name, int prio);\n+int swap_tiers_remove(const char *name);\n+int swap_tiers_modify(const char *name, int prio);\n+\n+void swap_tiers_save(struct swap_tier_save_ctx ctx[]);\n+void swap_tiers_restore(struct swap_tier_save_ctx ctx[]);\n+bool swap_tiers_validate(void);\n+#endif /* _SWAP_TIER_H */\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 7b055f15d705..c27952b41d4f 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -50,6 +50,7 @@\n #include \"internal.h\"\n #include \"swap_table.h\"\n #include \"swap.h\"\n+#include \"swap_tier.h\"\n \n static bool swap_count_continued(struct swap_info_struct *, pgoff_t,\n \t\t\t\t unsigned char);\n@@ -65,7 +66,7 @@ static void move_cluster(struct swap_info_struct *si,\n \t\t\t struct swap_cluster_info *ci, struct list_head *list,\n \t\t\t enum swap_cluster_flags new_flags);\n \n-static DEFINE_SPINLOCK(swap_lock);\n+DEFINE_SPINLOCK(swap_lock);\n static unsigned int nr_swapfiles;\n atomic_long_t nr_swap_pages;\n /*\n@@ -76,7 +77,6 @@ atomic_long_t nr_swap_pages;\n EXPORT_SYMBOL_GPL(nr_swap_pages);\n /* protected with swap_lock. reading in vm_swap_full() doesn't need lock */\n long total_swap_pages;\n-#define DEF_SWAP_PRIO  -1\n unsigned long swapfile_maximum_size;\n #ifdef CONFIG_MIGRATION\n bool swap_migration_ad_supported;\n@@ -89,7 +89,7 @@ static const char Bad_offset[] = \"Bad swap offset entry \";\n  * all active swap_info_structs\n  * protected with swap_lock, and ordered by priority.\n  */\n-static PLIST_HEAD(swap_active_head);\n+PLIST_HEAD(swap_active_head);\n \n /*\n  * all available (active, not full) swap_info_structs\n@@ -3977,6 +3977,7 @@ static int __init swapfile_init(void)\n \t\tswap_migration_ad_supported = true;\n #endif\t/* CONFIG_MIGRATION */\n \n+\tswap_tiers_init();\n \treturn 0;\n }\n subsys_initcall(swapfile_init);\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-01-26"
        },
        {
          "author": "Youngjun Park (author)",
          "summary": "The author addressed a concern about caching oscillation and priority inversion in swap devices due to global percpu clusters. They reverted commit 1b7e90020eb7 to use each swap device's percpu cluster instead, citing issues with caching oscillation and priority inversion.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "reverted commit",
            "acknowledged concerns"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "This reverts commit 1b7e90020eb7 (\"mm, swap: use percpu cluster as\nallocation fast path\").\n\nBecause in the newly introduced swap tiers, the global percpu cluster\nwill cause two issues:\n1) it will cause caching oscillation in the same order of different si\n   if two different memcg can only be allowed to access different si and\n   both of them are swapping out.\n2) It can cause priority inversion on swap devices. Imagine a case where\n   there are two memcg, say memcg1 and memcg2. Memcg1 can access si A, B\n   and A is higher priority device. While memcg2 can only access si B.\n   Then memcg 2 could write the global percpu cluster with si B, then\n   memcg1 take si B in fast path even though si A is not exhausted.\n\nHence in order to support swap tier, revert commit to use\neach swap device's percpu cluster.\n\nSuggested-by: Kairui Song <kasong@tencent.com>\nCo-developed-by: Baoquan He <bhe@redhat.com>\nSigned-off-by: Baoquan He <bhe@redhat.com>\nSigned-off-by: Youngjun Park <youngjun.park@lge.com>\n---\n include/linux/swap.h |  17 ++++--\n mm/swapfile.c        | 142 ++++++++++++++-----------------------------\n 2 files changed, 57 insertions(+), 102 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 1e68c220a0e7..6921e22b14d3 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -247,11 +247,18 @@ enum {\n #define SWAP_NR_ORDERS\t\t1\n #endif\n \n-/*\n- * We keep using same cluster for rotational device so IO will be sequential.\n- * The purpose is to optimize SWAP throughput on these device.\n- */\n+ /*\n+  * We assign a cluster to each CPU, so each CPU can allocate swap entry from\n+  * its own cluster and swapout sequentially. The purpose is to optimize swapout\n+  * throughput.\n+  */\n+struct percpu_cluster {\n+\tlocal_lock_t lock; /* Protect the percpu_cluster above */\n+\tunsigned int next[SWAP_NR_ORDERS]; /* Likely next allocation offset */\n+};\n+\n struct swap_sequential_cluster {\n+\tspinlock_t lock; /* Serialize usage of global cluster */\n \tunsigned int next[SWAP_NR_ORDERS]; /* Likely next allocation offset */\n };\n \n@@ -277,8 +284,8 @@ struct swap_info_struct {\n \t\t\t\t\t/* list of cluster that are fragmented or contented */\n \tunsigned int pages;\t\t/* total of usable pages of swap */\n \tatomic_long_t inuse_pages;\t/* number of those currently in use */\n+\tstruct percpu_cluster\t__percpu *percpu_cluster; /* per cpu's swap location */\n \tstruct swap_sequential_cluster *global_cluster; /* Use one global cluster for rotating device */\n-\tspinlock_t global_cluster_lock;\t/* Serialize usage of global cluster */\n \tstruct rb_root swap_extent_root;/* root of the swap extent rbtree */\n \tstruct block_device *bdev;\t/* swap device or bdev of swap file */\n \tstruct file *swap_file;\t\t/* seldom referenced */\ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex dd97e850ea2c..5e3b87799440 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -118,18 +118,6 @@ static atomic_t proc_poll_event = ATOMIC_INIT(0);\n \n atomic_t nr_rotate_swap = ATOMIC_INIT(0);\n \n-struct percpu_swap_cluster {\n-\tstruct swap_info_struct *si[SWAP_NR_ORDERS];\n-\tunsigned long offset[SWAP_NR_ORDERS];\n-\tlocal_lock_t lock;\n-};\n-\n-static DEFINE_PER_CPU(struct percpu_swap_cluster, percpu_swap_cluster) = {\n-\t.si = { NULL },\n-\t.offset = { SWAP_ENTRY_INVALID },\n-\t.lock = INIT_LOCAL_LOCK(),\n-};\n-\n /* May return NULL on invalid type, caller must check for NULL return */\n static struct swap_info_struct *swap_type_to_info(int type)\n {\n@@ -477,7 +465,7 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \t * Swap allocator uses percpu clusters and holds the local lock.\n \t */\n \tlockdep_assert_held(&ci->lock);\n-\tlockdep_assert_held(&this_cpu_ptr(&percpu_swap_cluster)->lock);\n+\tlockdep_assert_held(this_cpu_ptr(&si->percpu_cluster->lock));\n \n \t/* The cluster must be free and was just isolated from the free list. */\n \tVM_WARN_ON_ONCE(ci->flags || !cluster_is_empty(ci));\n@@ -495,8 +483,8 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \t */\n \tspin_unlock(&ci->lock);\n \tif (!(si->flags & SWP_SOLIDSTATE))\n-\t\tspin_unlock(&si->global_cluster_lock);\n-\tlocal_unlock(&percpu_swap_cluster.lock);\n+\t\tspin_unlock(&si->global_cluster->lock);\n+\tlocal_unlock(&si->percpu_cluster->lock);\n \n \ttable = swap_table_alloc(__GFP_HIGH | __GFP_NOMEMALLOC | GFP_KERNEL);\n \n@@ -508,9 +496,9 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \t * could happen with ignoring the percpu cluster is fragmentation,\n \t * which is acceptable since this fallback and race is rare.\n \t */\n-\tlocal_lock(&percpu_swap_cluster.lock);\n+\tlocal_lock(&si->percpu_cluster->lock);\n \tif (!(si->flags & SWP_SOLIDSTATE))\n-\t\tspin_lock(&si->global_cluster_lock);\n+\t\tspin_lock(&si->global_cluster->lock);\n \tspin_lock(&ci->lock);\n \n \t/* Nothing except this helper should touch a dangling empty cluster. */\n@@ -622,7 +610,7 @@ static bool swap_do_scheduled_discard(struct swap_info_struct *si)\n \t\tci = list_first_entry(&si->discard_clusters, struct swap_cluster_info, list);\n \t\t/*\n \t\t * Delete the cluster from list to prepare for discard, but keep\n-\t\t * the CLUSTER_FLAG_DISCARD flag, percpu_swap_cluster could be\n+\t\t * the CLUSTER_FLAG_DISCARD flag, there could be percpu_cluster\n \t\t * pointing to it, or ran into by relocate_cluster.\n \t\t */\n \t\tlist_del(&ci->list);\n@@ -953,12 +941,11 @@ static unsigned int alloc_swap_scan_cluster(struct swap_info_struct *si,\n out:\n \trelocate_cluster(si, ci);\n \tswap_cluster_unlock(ci);\n-\tif (si->flags & SWP_SOLIDSTATE) {\n-\t\tthis_cpu_write(percpu_swap_cluster.offset[order], next);\n-\t\tthis_cpu_write(percpu_swap_cluster.si[order], si);\n-\t} else {\n+\tif (si->flags & SWP_SOLIDSTATE)\n+\t\tthis_cpu_write(si->percpu_cluster->next[order], next);\n+\telse\n \t\tsi->global_cluster->next[order] = next;\n-\t}\n+\n \treturn found;\n }\n \n@@ -1052,13 +1039,17 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \tif (order && !(si->flags & SWP_BLKDEV))\n \t\treturn 0;\n \n-\tif (!(si->flags & SWP_SOLIDSTATE)) {\n+\tif (si->flags & SWP_SOLIDSTATE) {\n+\t\t/* Fast path using per CPU cluster */\n+\t\tlocal_lock(&si->percpu_cluster->lock);\n+\t\toffset = __this_cpu_read(si->percpu_cluster->next[order]);\n+\t} else {\n \t\t/* Serialize HDD SWAP allocation for each device. */\n-\t\tspin_lock(&si->global_cluster_lock);\n+\t\tspin_lock(&si->global_cluster->lock);\n \t\toffset = si->global_cluster->next[order];\n-\t\tif (offset == SWAP_ENTRY_INVALID)\n-\t\t\tgoto new_cluster;\n+\t}\n \n+\tif (offset != SWAP_ENTRY_INVALID) {\n \t\tci = swap_cluster_lock(si, offset);\n \t\t/* Cluster could have been used by another order */\n \t\tif (cluster_is_usable(ci, order)) {\n@@ -1072,7 +1063,6 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \t\t\tgoto done;\n \t}\n \n-new_cluster:\n \t/*\n \t * If the device need discard, prefer new cluster over nonfull\n \t * to spread out the writes.\n@@ -1129,8 +1119,10 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \t\t\tgoto done;\n \t}\n done:\n-\tif (!(si->flags & SWP_SOLIDSTATE))\n-\t\tspin_unlock(&si->global_cluster_lock);\n+\tif (si->flags & SWP_SOLIDSTATE)\n+\t\tlocal_unlock(&si->percpu_cluster->lock);\n+\telse\n+\t\tspin_unlock(&si->global_cluster->lock);\n \n \treturn found;\n }\n@@ -1311,41 +1303,8 @@ static bool get_swap_device_info(struct swap_info_struct *si)\n \treturn true;\n }\n \n-/*\n- * Fast path try to get swap entries with specified order from current\n- * CPU's swap entry pool (a cluster).\n- */\n-static bool swap_alloc_fast(struct folio *folio)\n-{\n-\tunsigned int order = folio_order(folio);\n-\tstruct swap_cluster_info *ci;\n-\tstruct swap_info_struct *si;\n-\tunsigned int offset;\n-\n-\t/*\n-\t * Once allocated, swap_info_struct will never be completely freed,\n-\t * so checking it's liveness by get_swap_device_info is enough.\n-\t */\n-\tsi = this_cpu_read(percpu_swap_cluster.si[order]);\n-\toffset = this_cpu_read(percpu_swap_cluster.offset[order]);\n-\tif (!si || !offset || !get_swap_device_info(si))\n-\t\treturn false;\n-\n-\tci = swap_cluster_lock(si, offset);\n-\tif (cluster_is_usable(ci, order)) {\n-\t\tif (cluster_is_empty(ci))\n-\t\t\toffset = cluster_offset(si, ci);\n-\t\talloc_swap_scan_cluster(si, ci, folio, offset);\n-\t} else {\n-\t\tswap_cluster_unlock(ci);\n-\t}\n-\n-\tput_swap_device(si);\n-\treturn folio_test_swapcache(folio);\n-}\n-\n /* Rotate the device and switch to a new cluster */\n-static void swap_alloc_slow(struct folio *folio)\n+static void swap_alloc_entry(struct folio *folio)\n {\n \tstruct swap_info_struct *si, *next;\n \tint mask = folio_memcg(folio) ?\n@@ -1363,6 +1322,7 @@ static void swap_alloc_slow(struct folio *folio)\n \t\tif (get_swap_device_info(si)) {\n \t\t\tcluster_alloc_swap_entry(si, folio);\n \t\t\tput_swap_device(si);\n+\n \t\t\tif (folio_test_swapcache(folio))\n \t\t\t\treturn;\n \t\t\tif (folio_test_large(folio))\n@@ -1522,11 +1482,7 @@ int folio_alloc_swap(struct folio *folio)\n \t}\n \n again:\n-\tlocal_lock(&percpu_swap_cluster.lock);\n-\tif (!swap_alloc_fast(folio))\n-\t\tswap_alloc_slow(folio);\n-\tlocal_unlock(&percpu_swap_cluster.lock);\n-\n+\tswap_alloc_entry(folio);\n \tif (!order && unlikely(!folio_test_swapcache(folio))) {\n \t\tif (swap_sync_discard())\n \t\t\tgoto again;\n@@ -1945,9 +1901,7 @@ swp_entry_t swap_alloc_hibernation_slot(int type)\n \t\t\t * Grab the local lock to be compliant\n \t\t\t * with swap table allocation.\n \t\t\t */\n-\t\t\tlocal_lock(&percpu_swap_cluster.lock);\n \t\t\toffset = cluster_alloc_swap_entry(si, NULL);\n-\t\t\tlocal_unlock(&percpu_swap_cluster.lock);\n \t\t\tif (offset)\n \t\t\t\tentry = swp_entry(si->type, offset);\n \t\t}\n@@ -2751,28 +2705,6 @@ static void free_cluster_info(struct swap_cluster_info *cluster_info,\n \tkvfree(cluster_info);\n }\n \n-/*\n- * Called after swap device's reference count is dead, so\n- * neither scan nor allocation will use it.\n- */\n-static void flush_percpu_swap_cluster(struct swap_info_struct *si)\n-{\n-\tint cpu, i;\n-\tstruct swap_info_struct **pcp_si;\n-\n-\tfor_each_possible_cpu(cpu) {\n-\t\tpcp_si = per_cpu_ptr(percpu_swap_cluster.si, cpu);\n-\t\t/*\n-\t\t * Invalidate the percpu swap cluster cache, si->users\n-\t\t * is dead, so no new user will point to it, just flush\n-\t\t * any existing user.\n-\t\t */\n-\t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n-\t\t\tcmpxchg(&pcp_si[i], si, NULL);\n-\t}\n-}\n-\n-\n SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n {\n \tstruct swap_info_struct *p = NULL;\n@@ -2856,7 +2788,6 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \n \tflush_work(&p->discard_work);\n \tflush_work(&p->reclaim_work);\n-\tflush_percpu_swap_cluster(p);\n \n \tdestroy_swap_extents(p);\n \tif (p->flags & SWP_CONTINUED)\n@@ -2885,6 +2816,8 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \tarch_swap_invalidate_area(p->type);\n \tzswap_swapoff(p->type);\n \tmutex_unlock(&swapon_mutex);\n+\tfree_percpu(p->percpu_cluster);\n+\tp->percpu_cluster = NULL;\n \tkfree(p->global_cluster);\n \tp->global_cluster = NULL;\n \tvfree(swap_map);\n@@ -3268,7 +3201,7 @@ static struct swap_cluster_info *setup_clusters(struct swap_info_struct *si,\n {\n \tunsigned long nr_clusters = DIV_ROUND_UP(maxpages, SWAPFILE_CLUSTER);\n \tstruct swap_cluster_info *cluster_info;\n-\tint err = -ENOMEM;\n+\tint cpu, err = -ENOMEM;\n \tunsigned long i;\n \n \tcluster_info = kvcalloc(nr_clusters, sizeof(*cluster_info), GFP_KERNEL);\n@@ -3278,14 +3211,27 @@ static struct swap_cluster_info *setup_clusters(struct swap_info_struct *si,\n \tfor (i = 0; i < nr_clusters; i++)\n \t\tspin_lock_init(&cluster_info[i].lock);\n \n-\tif (!(si->flags & SWP_SOLIDSTATE)) {\n+\tif (si->flags & SWP_SOLIDSTATE) {\n+\t\tsi->percpu_cluster = alloc_percpu(struct percpu_cluster);\n+\t\tif (!si->percpu_cluster)\n+\t\t\tgoto err;\n+\n+\t\tfor_each_possible_cpu(cpu) {\n+\t\t\tstruct percpu_cluster *cluster;\n+\n+\t\t\tcluster = per_cpu_ptr(si->percpu_cluster, cpu);\n+\t\t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n+\t\t\t\tcluster->next[i] = SWAP_ENTRY_INVALID;\n+\t\t\tlocal_lock_init(&cluster->lock);\n+\t\t}\n+\t} else {\n \t\tsi->global_cluster = kmalloc(sizeof(*si->global_cluster),\n \t\t\t\t     GFP_KERNEL);\n \t\tif (!si->global_cluster)\n \t\t\tgoto err;\n \t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n \t\t\tsi->global_cluster->next[i] = SWAP_ENTRY_INVALID;\n-\t\tspin_lock_init(&si->global_cluster_lock);\n+\t\tspin_lock_init(&si->global_cluster->lock);\n \t}\n \n \t/*\n@@ -3566,6 +3512,8 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)\n bad_swap_unlock_inode:\n \tinode_unlock(inode);\n bad_swap:\n+\tfree_percpu(si->percpu_cluster);\n+\tsi->percpu_cluster = NULL;\n \tkfree(si->global_cluster);\n \tsi->global_cluster = NULL;\n \tinode = NULL;\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-01-26"
        },
        {
          "author": "Youngjun Park (author)",
          "summary": "The author addressed a concern about swap device rotation on every allocation, which leads to fragmentation and performance regression. They introduced a per-cpu cache for the swap device, updating the allocation logic to prioritize the cached device within its cluster, effectively restoring the traditional fastpath and slowpath flow.",
          "sentiment": "positive",
          "sentiment_signals": [
            "acknowledged a problem",
            "provided a solution"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "When using per-device percpu clusters (instead of a global one),\na naive allocation logic triggers swap device rotation on every\nallocation. This behavior leads to severe fragmentation and performance\nregression.\n\nTo address this, this patch introduces a per-cpu cache for the swap\ndevice. The allocation logic is updated to prioritize the per-cpu\ncluster within the cached swap device, effectively restoring the\ntraditional fastpath and slowpath flow. This approach minimizes side\neffects on the existing fastpath.\n\nWith this change, swap device rotation occurs only when the current\ncached device is unable to satisfy the allocation, rather than on\nevery attempt.\n\nSigned-off-by: Youngjun Park <youngjun.park@lge.com>\n---\n include/linux/swap.h |  1 -\n mm/swapfile.c        | 78 +++++++++++++++++++++++++++++++++++++-------\n 2 files changed, 66 insertions(+), 13 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 6921e22b14d3..ac634a21683a 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -253,7 +253,6 @@ enum {\n   * throughput.\n   */\n struct percpu_cluster {\n-\tlocal_lock_t lock; /* Protect the percpu_cluster above */\n \tunsigned int next[SWAP_NR_ORDERS]; /* Likely next allocation offset */\n };\n \ndiff --git a/mm/swapfile.c b/mm/swapfile.c\nindex 5e3b87799440..0dcd451afee5 100644\n--- a/mm/swapfile.c\n+++ b/mm/swapfile.c\n@@ -106,6 +106,16 @@ PLIST_HEAD(swap_active_head);\n static PLIST_HEAD(swap_avail_head);\n static DEFINE_SPINLOCK(swap_avail_lock);\n \n+struct percpu_swap_device {\n+\tstruct swap_info_struct *si[SWAP_NR_ORDERS];\n+\tlocal_lock_t lock;\n+};\n+\n+static DEFINE_PER_CPU(struct percpu_swap_device, percpu_swap_device) = {\n+\t.si = { NULL },\n+\t.lock = INIT_LOCAL_LOCK(),\n+};\n+\n struct swap_info_struct *swap_info[MAX_SWAPFILES];\n \n static struct kmem_cache *swap_table_cachep;\n@@ -465,7 +475,7 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \t * Swap allocator uses percpu clusters and holds the local lock.\n \t */\n \tlockdep_assert_held(&ci->lock);\n-\tlockdep_assert_held(this_cpu_ptr(&si->percpu_cluster->lock));\n+\tlockdep_assert_held(this_cpu_ptr(&percpu_swap_device.lock));\n \n \t/* The cluster must be free and was just isolated from the free list. */\n \tVM_WARN_ON_ONCE(ci->flags || !cluster_is_empty(ci));\n@@ -484,7 +494,7 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \tspin_unlock(&ci->lock);\n \tif (!(si->flags & SWP_SOLIDSTATE))\n \t\tspin_unlock(&si->global_cluster->lock);\n-\tlocal_unlock(&si->percpu_cluster->lock);\n+\tlocal_unlock(&percpu_swap_device.lock);\n \n \ttable = swap_table_alloc(__GFP_HIGH | __GFP_NOMEMALLOC | GFP_KERNEL);\n \n@@ -496,7 +506,7 @@ swap_cluster_alloc_table(struct swap_info_struct *si,\n \t * could happen with ignoring the percpu cluster is fragmentation,\n \t * which is acceptable since this fallback and race is rare.\n \t */\n-\tlocal_lock(&si->percpu_cluster->lock);\n+\tlocal_lock(&percpu_swap_device.lock);\n \tif (!(si->flags & SWP_SOLIDSTATE))\n \t\tspin_lock(&si->global_cluster->lock);\n \tspin_lock(&ci->lock);\n@@ -941,9 +951,10 @@ static unsigned int alloc_swap_scan_cluster(struct swap_info_struct *si,\n out:\n \trelocate_cluster(si, ci);\n \tswap_cluster_unlock(ci);\n-\tif (si->flags & SWP_SOLIDSTATE)\n+\tif (si->flags & SWP_SOLIDSTATE) {\n \t\tthis_cpu_write(si->percpu_cluster->next[order], next);\n-\telse\n+\t\tthis_cpu_write(percpu_swap_device.si[order], si);\n+\t} else\n \t\tsi->global_cluster->next[order] = next;\n \n \treturn found;\n@@ -1041,7 +1052,6 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \n \tif (si->flags & SWP_SOLIDSTATE) {\n \t\t/* Fast path using per CPU cluster */\n-\t\tlocal_lock(&si->percpu_cluster->lock);\n \t\toffset = __this_cpu_read(si->percpu_cluster->next[order]);\n \t} else {\n \t\t/* Serialize HDD SWAP allocation for each device. */\n@@ -1119,9 +1129,7 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,\n \t\t\tgoto done;\n \t}\n done:\n-\tif (si->flags & SWP_SOLIDSTATE)\n-\t\tlocal_unlock(&si->percpu_cluster->lock);\n-\telse\n+\tif (!(si->flags & SWP_SOLIDSTATE))\n \t\tspin_unlock(&si->global_cluster->lock);\n \n \treturn found;\n@@ -1303,8 +1311,27 @@ static bool get_swap_device_info(struct swap_info_struct *si)\n \treturn true;\n }\n \n+static bool swap_alloc_fast(struct folio *folio)\n+{\n+\tunsigned int order = folio_order(folio);\n+\tstruct swap_info_struct *si;\n+\n+\t/*\n+\t * Once allocated, swap_info_struct will never be completely freed,\n+\t * so checking it's liveness by get_swap_device_info is enough.\n+\t */\n+\tsi = this_cpu_read(percpu_swap_device.si[order]);\n+\tif (!si || !get_swap_device_info(si))\n+\t\treturn false;\n+\n+\tcluster_alloc_swap_entry(si, folio);\n+\tput_swap_device(si);\n+\n+\treturn folio_test_swapcache(folio);\n+}\n+\n /* Rotate the device and switch to a new cluster */\n-static void swap_alloc_entry(struct folio *folio)\n+static void swap_alloc_slow(struct folio *folio)\n {\n \tstruct swap_info_struct *si, *next;\n \tint mask = folio_memcg(folio) ?\n@@ -1482,7 +1509,11 @@ int folio_alloc_swap(struct folio *folio)\n \t}\n \n again:\n-\tswap_alloc_entry(folio);\n+\tlocal_lock(&percpu_swap_device.lock);\n+\tif (!swap_alloc_fast(folio))\n+\t\tswap_alloc_slow(folio);\n+\tlocal_unlock(&percpu_swap_device.lock);\n+\n \tif (!order && unlikely(!folio_test_swapcache(folio))) {\n \t\tif (swap_sync_discard())\n \t\t\tgoto again;\n@@ -1901,7 +1932,9 @@ swp_entry_t swap_alloc_hibernation_slot(int type)\n \t\t\t * Grab the local lock to be compliant\n \t\t\t * with swap table allocation.\n \t\t\t */\n+\t\t\tlocal_lock(&percpu_swap_device.lock);\n \t\t\toffset = cluster_alloc_swap_entry(si, NULL);\n+\t\t\tlocal_unlock(&percpu_swap_device.lock);\n \t\t\tif (offset)\n \t\t\t\tentry = swp_entry(si->type, offset);\n \t\t}\n@@ -2705,6 +2738,27 @@ static void free_cluster_info(struct swap_cluster_info *cluster_info,\n \tkvfree(cluster_info);\n }\n \n+/*\n+ * Called after swap device's reference count is dead, so\n+ * neither scan nor allocation will use it.\n+ */\n+static void flush_percpu_swap_device(struct swap_info_struct *si)\n+{\n+\tint cpu, i;\n+\tstruct swap_info_struct **pcp_si;\n+\n+\tfor_each_possible_cpu(cpu) {\n+\t\tpcp_si = per_cpu_ptr(percpu_swap_device.si, cpu);\n+\t\t/*\n+\t\t * Invalidate the percpu swap device cache, si->users\n+\t\t * is dead, so no new user will point to it, just flush\n+\t\t * any existing user.\n+\t\t */\n+\t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n+\t\t\tcmpxchg(&pcp_si[i], si, NULL);\n+\t}\n+}\n+\n SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n {\n \tstruct swap_info_struct *p = NULL;\n@@ -2788,6 +2842,7 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)\n \n \tflush_work(&p->discard_work);\n \tflush_work(&p->reclaim_work);\n+\tflush_percpu_swap_device(p);\n \n \tdestroy_swap_extents(p);\n \tif (p->flags & SWP_CONTINUED)\n@@ -3222,7 +3277,6 @@ static struct swap_cluster_info *setup_clusters(struct swap_info_struct *si,\n \t\t\tcluster = per_cpu_ptr(si->percpu_cluster, cpu);\n \t\t\tfor (i = 0; i < SWAP_NR_ORDERS; i++)\n \t\t\t\tcluster->next[i] = SWAP_ENTRY_INVALID;\n-\t\t\tlocal_lock_init(&cluster->lock);\n \t\t}\n \t} else {\n \t\tsi->global_cluster = kmalloc(sizeof(*si->global_cluster),\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-01-26"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-11": {
      "report_file": "2026-02-21_ollama_llama3.1-8b.html",
      "developer": "Shakeel Butt",
      "reviews": [
        {
          "author": "Chris Li",
          "summary": "Reviewer Chris Li noted that the swap_tier structure was simplified by replacing 'end prio' and priority lists with a standard list_head, as previously suggested.",
          "sentiment": "positive",
          "sentiment_signals": [
            "Chris Li's suggestion was incorporated"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Hi Youngjun,\n\nOn Sun, Jan 25, 2026 at 10:53PM Youngjun Park <youngjun.park@lge.com> wrote:",
          "reply_to": "Youngjun Park",
          "message_date": "2026-02-11"
        },
        {
          "author": "Chris Li",
          "summary": "Reviewer Chris Li suggested breaking down the patch series into smaller, more manageable steps, starting with defining the tiers bits without deleting any existing code, and then building upon that in subsequent steps.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "break down large patch series"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Thanks for the patches series.\n\nSorry for the late reply. I have been wanting to reply to it but get\nsuper busy at work.\n\nSome high level feedback for the series. Now that you demonstrated the\nwhole series, let's focus on making small mergiable baby steps. Just\nlike the swap table has different phases. Make each step minimal, each\nstep shows some value. Do the MVP, we can always add more features as\na follow up step.\n\nI suggest the first step is getting the tiers bits defined. Add only,\nno delete.  Get that reviewed and merged, then the next step is to use\nthose tiers.\n\nChris",
          "reply_to": "Youngjun Park",
          "message_date": "2026-02-11"
        },
        {
          "author": "Chris Li",
          "summary": "Reviewer Chris Li suggested replacing per-cpu allocation for each swap device with a global per-cpu cluster per tier, as the maximum number of tiers is smaller than the maximum number of swap devices.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "One idea is that, instead of using percpu per swap device.\nYou can make the global percpu cluster per tier. Because the max tier\nnumber is smaller than the max number of swap devices. That is likely\na win.\n\nChris",
          "reply_to": "Youngjun Park",
          "message_date": "2026-02-11"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-12": {
      "report_file": "2026-02-21_ollama_llama3.1-8b.html",
      "developer": "Shakeel Butt",
      "reviews": [
        {
          "author": "Chris Li",
          "summary": "Reviewer Chris Li noted that the swap_tier structure was simplified by replacing 'end prio' and priority lists with a standard list_head, as requested in his previous feedback.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Hi Yongjun,\n\nOn Sun, Jan 25, 2026 at 10:53PM Youngjun Park <youngjun.park@lge.com> wrote:",
          "reply_to": "Youngjun Park",
          "message_date": "2026-02-12"
        },
        {
          "author": "Chris Li",
          "summary": "Reviewer Chris Li suggested introducing a CONFIG option to limit the maximum number of swap tiers, recommending a default value of 4.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "We can have a CONFIG option for the MAX_SWAPTIER. I think the default\nshould be a small number like 4.",
          "reply_to": "Youngjun Park",
          "message_date": "2026-02-12"
        },
        {
          "author": "Chris Li",
          "summary": "Reviewer Chris Li noted that modifying a tier can cause swap files to move to different tiers, which could be problematic and requested further consideration of this scenario.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested further consideration",
            "potential problem"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "When we add, modify, remove a tier. The simple case is there is no\nswap file under any tiers.\nBut if the modification causes some swap files to jump to different\ntiers. That might be problematic.",
          "reply_to": "Youngjun Park",
          "message_date": "2026-02-12"
        },
        {
          "author": "Chris Li",
          "summary": "Reviewer Chris Li expressed concern about the complexity of the patch, specifically mentioning the need for save and restore operations, and requested a simpler design.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "complexity",
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "I really hope we don't have to do the save and restore thing. Is there\nanother design we can simplify this?",
          "reply_to": "Youngjun Park",
          "message_date": "2026-02-12"
        },
        {
          "author": "Chris Li",
          "summary": "Reviewer Chris Li suggested that each tier should have its own swap_active_head, so that different tiers don't compete for the same resource when releasing swap entries.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "One idea is to make each tier have swap_active_head. So different swap\nentry releases on different tiers don't need to be competing on the\nsame swap_active_head.\n\nThat will require the swapfile don't jump to another tiers.\n\nChris",
          "reply_to": "Youngjun Park",
          "message_date": "2026-02-12"
        },
        {
          "author": "Chris Li",
          "summary": "Reviewer Chris Li initially suggested simplifying the swap_tier structure by replacing 'end prio' and priority lists with a standard list_head, but later takes back this suggestion after reevaluating the series.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "reconsidered previous suggestion"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Just take a quick look at the series. I take that suggestion back.\nThis series is actually not too long. Adding the tiers name alone does\nnot add any real value. I actually need to look at the whole series\nrather than just the tier name alone.\n\nChris",
          "reply_to": "",
          "message_date": "2026-02-12"
        },
        {
          "author": "Nhat Pham",
          "summary": "Reviewer Nhat Pham questioned the consistency of the patch description, pointing out that the '+' operator was removed but its reference remained in the explanation.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "confusion",
            "questioning"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "This comment seems a bit clunky to me. The \"+\" is removed, as noted\nabove, but then why are we saying \"even if a child re-enables a tier\nwith \"+\"\" here? Am I missing something?",
          "reply_to": "Youngjun Park",
          "message_date": "2026-02-12"
        },
        {
          "author": "Nhat Pham",
          "summary": "Reviewer Nhat Pham questioned the logic for restricting child cgroup's allowed swap tiers, suggesting a simpler approach where it is a subset of its ancestors and children.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "But otherwise, I assume you mean to restrict child's allowed swap\ntiers to be a subset of children and its ancestors? That seems more\nstraightforward to me than the last system :)",
          "reply_to": "Youngjun Park",
          "message_date": "2026-02-12"
        },
        {
          "author": "Shakeel Butt",
          "summary": "Reviewer Shakeel Butt noted that the patch does not handle the case where a swap tier is removed while it still has active swap devices, which can lead to memory corruption and crashes.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "memory corruption",
            "crashes"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Hi Youngjun,\n\nOn Mon, Jan 26, 2026 at 03:52:37PM +0900, Youngjun Park wrote:",
          "reply_to": "Youngjun Park",
          "message_date": "2026-02-12"
        },
        {
          "author": "Shakeel Butt",
          "summary": "Reviewer Shakeel Butt expressed concerns that introducing a memcg interface for swap tiers is not the right approach, suggesting instead to explore using BPF (Berkeley Packet Filter) for this functionality.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "suggested alternative"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "One of the LPC feedback you missed is to not add memcg interface for\nthis functionality and explore BPF way instead.\n\nWe are normally very conservative to add new interfaces to cgroup.\nHowever I am not even convinced that memcg interface is the right way to\nexpose this functionality. Swap is currently global and the idea to\nlimit or assign specific swap devices to specific cgroups makes sense\nbut that is the decision for the job orchestator or node controller.\nAllowing workloads to pick and choose swap devices do not make sense to\nme.\n\nShakeel",
          "reply_to": "Youngjun Park",
          "message_date": "2026-02-12"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-13": {
      "report_file": "2026-02-21_ollama_llama3.1-8b.html",
      "developer": "Shakeel Butt",
      "reviews": [
        {
          "author": "YoungJun Park (author)",
          "summary": "Author addressed Chris Li's concern about breaking the series into smaller, mergeable steps by proposing a modified roadmap to demonstrate immediate value in Step 1.",
          "sentiment": "positive",
          "sentiment_signals": [
            "agreed",
            "proposed a plan"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Hi Chris,\n\nThank you for the direction.\n\nI agree that breaking the series into smaller, mergeable steps is the\nright approach. However, since introducing the definitions alone might\nlack immediate usage, I propose a slightly\nmodified roadmap to ensure Step 1 demonstrates some value.\n\nHere is the plan I have in mind.\n\n1. Swap Tier Definition & Addition\n   - Introduce the concept, grouping logic, and the 'add' interface.\n   - Value: Enables basic exception handling within the swap device\n     itself using tiers.\n\n2. Advanced Control (Delete/Modify)\n   - Implement logic to remove or update tiers.\n   - Value: Enhances the usability and management of the tiers\n     established in Step 1.\n\n3. External Integration (memcg, bpf etc ... )\n   - Apply swap tiers for broader swap control.\n   - Value: Connects swap tiers to other subsystems like memcg.\n\nDoes this roadmap look reasonable to you? I will proceed with preparing\nthe real patch series based on this structure.\n\nBest regards,\nYoungjun",
          "reply_to": "Chris Li",
          "message_date": "2026-02-13"
        },
        {
          "author": "YoungJun Park (author)",
          "summary": "Author acknowledged the need to limit swap file allocation by adding a CONFIG option and ensuring it doesn't exceed MAX_SWAPFILE, indicating a fix is planned.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged",
            "will add"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Sounds good. I will add a CONFIG option for it and ensure it doesn't exceed\nMAX_SWAPFILE.",
          "reply_to": "Chris Li",
          "message_date": "2026-02-13"
        },
        {
          "author": "YoungJun Park (author)",
          "summary": "Author addressed Chris Li's concern about mixed operations in the swap tier interface by proposing to restrict it to single operations, citing potential issues with error-prone reversal of individual operations and performance impact.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledges a fix is needed",
            "proposes an alternative solution"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "I have given this a lot of thought.\n\nSince the current interface allows mixing add (+), remove (-), and modify\noperations, we must either restore from a saved state or reverse the\nsuccessful individual operations upon failure.\n\nI implemented both approaches and concluded that reversing individual\noperations is error-prone. Also, it could be slow if there are many\noperations.\n\nAnother approach could be using a \"global clone tier\" strategy.\n(Because operation globally synchronized)\n\nTherefore, I would like to propose restricting the interface to handle a\nsingle operation at a time. What do you think?",
          "reply_to": "Chris Li",
          "message_date": "2026-02-13"
        },
        {
          "author": "YoungJun Park (author)",
          "summary": "Author agreed that limiting contention to objects within the same tier is beneficial, and suggested future optimization of swap_avail_list.",
          "sentiment": "positive",
          "sentiment_signals": [
            "agreed",
            "suggested"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "I agree. With the tier structure, we can limit contention to objects within\nthe same tier.\n\nI also think swap_avail_list could be optimized in a similar way in the\nfuture.\n\nYoungjun",
          "reply_to": "Chris Li",
          "message_date": "2026-02-13"
        },
        {
          "author": "YoungJun Park (author)",
          "summary": "Author acknowledges that stripping out the remove/modify parts from the patch is a viable direction, indicating an openness to revising the patch.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledges need for revision"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Oops, I replied to your previous email before seeing this one.\n\nStripping out the remove/modify parts is also feasible. Do you agree with\nthat direction?\n\nYoungjun",
          "reply_to": "Chris Li",
          "message_date": "2026-02-13"
        },
        {
          "author": "YoungJun Park (author)",
          "summary": "Author addressed Nhat Pham's feedback about the default state of swap tiers and how '+' is used, explaining that they are changing the model to a subtraction-based one where all tiers are selected by default and users use '-' to exclude specific ones.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "To clarify, previously, the default state used all tiers. Using \"+\"              \nswitched to \"an exclusive mode\"  where only that specific tier was used.         \n                                                                                 \nI am changing this to a subtraction-based model. By default, all tiers           \nare selected, and users use \"-\" to exclude specific ones.                        \n(Then not \"removed\" but \"changed\" is more proper?)                               \n                                                                                 \nIn this context, I intended \"+\" to be used to restore a tier that was            \npreviously excluded by \"-\".",
          "reply_to": "Nhat Pham",
          "message_date": "2026-02-13"
        },
        {
          "author": "YoungJun Park (author)",
          "summary": "Author acknowledged Nhat Pham's feedback and agreed to restructure the swapoff path in v2 to drop the per-vswap spinlock before calling try_to_unmap().",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged",
            "agreed"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Yes, that's right :)\n\nThanks \nYoungjun Park.",
          "reply_to": "Nhat Pham",
          "message_date": "2026-02-13"
        },
        {
          "author": "YoungJun Park (author)",
          "summary": "Author addressed concerns about using the BPF approach for swap control, citing potential logical contradictions and hierarchy enforcement issues. They acknowledged the flexibility of BPF but expressed concerns that it would eliminate its primary advantage if strictly constrained to adhere to cgroup semantics. Instead, they prefer a native interface within 'cgroup land' to ensure consistent enforcement of hierarchy and accounting rules.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged a fix is needed",
            "expressed concerns about BPF approach"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Apologies for overlooking the feedback regarding the BPF approach. Thank you\nfor the suggestion.\n\nI agree that using BPF would provide greater flexibility, allowing control not\njust at the memcg level, but also per-process or for complex workloads.\n(As like orchestrator and node controller)\n\nHowever, I am concerned that this level of freedom might introduce logical\ncontradictions, particularly regarding cgroup hierarchy semantics.\n\nFor example, BPF might allow a topology that violates hierarchical constraints\n(a concern that was also touched upon during LPC)\n\n  - Group A (Parent): Assigned to SSD1\n  - Group B (Child of A): Assigned to SSD2\n\nIf Group A has a `memory.swap.max` limit, and Group B swaps out to SSD2, it\ncreates a consistency issue. Group B consumes Group A's swap quota, but it is\nutilizing a device (SSD2) that is distinct from the Parent's assignment. This\ncould lead to situations where the Parent's limit is exhausted by usage on a\ndevice it effectively doesn't \"own\" or shouldn't be using.\n\nOne might suggest restricting BPF to strictly adhere to these hierarchical\nconstraints. However, doing so would effectively eliminate the primary\nadvantage of using BPF\\u2014its flexibility. If we are to enforce standard cgroup\nsemantics anyway, a native interface seems more appropriate than a constrained\nBPF hook.\n\nBeyond this specific example, I suspect that delegating this logic to BPF\nmight introduce other unforeseen edge cases regarding hierarchy enforcement.\nIn my view, the BPF approach seems more like a \"next step.\"\n\nSince you acknowledged that the idea of assigning swap devices to cgroups\n\"makes sense,\" I believe implementing this within the standard, strictly\nconstrained \"cgroup land\" is preferable. \n\nA strict cgroup interface ensures\nthat hierarchy and accounting rules are consistently enforced, avoiding the\npotential conflicts that the unrestricted freedom of BPF might create.\n\nUltimately, I hope this swap tier mechanism can serve as a foundation to be\nleveraged by other subsystems, such as BPF and DAMON. I view this proposal as\nthe necessary first step toward that future.\n\nYoungjun Park",
          "reply_to": "Shakeel Butt",
          "message_date": "2026-02-13"
        },
        {
          "author": "YoungJun Park (author)",
          "summary": "Author acknowledged that existing swapfiles' tier is immutable and removed the tier reference, instead validating the tier range at operation time to guarantee this invariant.",
          "sentiment": "positive",
          "sentiment_signals": [
            "acknowledged a fix",
            "removed the issue"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "I missed one comment. \n\nThe tier of existing swapfiles is immutable once assigned at swapon.\nI removed tier reference.\nInstead of reference counting, each operation validates the tier\nrange at operation time to guarantee this invariant.\n\n- add:    Does not change existing swapfiles' tier. New tier may\n          split priority range, but existing assignments stay.\n- remove: Rejected with -EBUSY if any swapfile is attached.\n- modify: Rejected if the change would cause any swapfile to\n          move to a different tier.\n\nSo swapfiles never jump between tiers at runtime.\n\nYoungjun Park",
          "reply_to": "Chris Li",
          "message_date": "2026-02-13"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-20": {
      "report_file": "2026-02-21_ollama_llama3.1-8b.html",
      "developer": "Shakeel Butt",
      "reviews": [
        {
          "author": "Shakeel Butt",
          "summary": "Reviewer requested that further discussion be concluded before a new version of the series is sent, indicating a need for more review and potential changes.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "need for more review"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Please don't send a new version of the series before concluding the discussion\non the previous one.\n\nOn Fri, Feb 13, 2026 at 12:58:40PM +0900, YoungJun Park wrote:",
          "reply_to": "YoungJun Park",
          "message_date": "2026-02-20"
        },
        {
          "author": "Shakeel Butt",
          "summary": "Reviewer Shakeel Butt expressed concerns about introducing a stable interface for swap tiers, suggesting first testing the BPF approach in a production environment before implementing hierarchical control.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "not convinced"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Yes it provides the flexibility but that is not the main reason I am pushing for\nit. The reason I want you to first try the BPF approach without introducing any\nstable interfaces. Show how swap tiers will be used and configured in production\nenvironment and then we can talk if a stable interface is needed. I am still not\nconvinced that swap tiers need to be controlled hierarchically and the non-root\nshould be able to control it.",
          "reply_to": "YoungJun Park",
          "message_date": "2026-02-20"
        },
        {
          "author": "Shakeel Butt",
          "summary": "Reviewer Shakeel Butt noted that while BPF provides more power, its control is limited to administrators who can still make mistakes.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "no clear signal of approval or disapproval"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Yes BPF provides more power but it is controlled by admin and admin can shoot\ntheir foot in multiple ways.",
          "reply_to": "YoungJun Park",
          "message_date": "2026-02-20"
        },
        {
          "author": "Shakeel Butt",
          "summary": "Reviewer Shakeel Butt requested clarification on the patch's use case, specifically asking about ordering between multiple assigned swap devices for a workload and the reason behind using 'tiers' in the name.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "requested clarification",
            "wanted to brainstorm future use-cases"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "No need to constraint anything.\n\nTaking a step back, can you describe your use-case a bit more and share\nrequirements?\n\nYou have multiple swap devices of different properties and you want to assign\nthose swap devices to different workloads. Now couple of questions:\n\n1. If more than one device is assign to a workload, do you want to have\n   some kind of ordering between them for the worklod or do you want option to\n   have round robin kind of policy?\n\n2. What's the reason to use 'tiers' in the name? Is it similar to memory tiers\n   and you want promotion/demotion among the tiers?\n\n3. If a workload has multiple swap devices assigned, can you describe the\n   scenario where such workloads need to partition/divide given devices to their\n   sub-workloads?\n\nLet's start with these questions. Please note that I want us to not just look at\nthe current use-case but brainstorm more future use-cases and then come up with\nthe solution which is more future proof.\n\nthanks,\nShakeel",
          "reply_to": "YoungJun Park",
          "message_date": "2026-02-20"
        },
        {
          "author": "Chris Li",
          "summary": "Reviewer Chris Li expressed concern that Shakeel Butt had not responded to YoungJun Park's previous response in over a week, potentially leading to confusion about whether the discussion was concluded.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "concern",
            "potential confusion"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "In this case I think it is fine.  You haven't responded to YoungJun's\nlast response in over a week. He might have mistaken that the\ndiscussion concluded.\nConsider it is one of the iterations. It is hard enough to contribute\nto the kernel. Relax.\nPlus, much of the discussion on the mailing list always has differing\nopinions. So, it's hard to determine what is truly concluded.\nDifferent people might have different interitations of the same text.",
          "reply_to": "Shakeel Butt",
          "message_date": "2026-02-20"
        },
        {
          "author": "Chris Li",
          "summary": "Reviewer Chris Li expressed concern that the patch's current implementation may not be suitable for production use, suggesting adding a config option to mark it as experimental and allowing more testing before merging.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "concern about production readiness",
            "suggestion to add config option"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Is that your biggest concern? Many different ways exist to solve that\nproblem. e.g. We can put a config option protecting it and mark it as\nexperimental. This will unblock the development allow experiment. We\ncan have more people to try it out and give feedback.",
          "reply_to": "Shakeel Butt",
          "message_date": "2026-02-20"
        },
        {
          "author": "Chris Li",
          "summary": "Reviewer Chris Li confirmed that his company uses a different swap device at different cgroup levels, emphasizing that control at the non-root level is a real need.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "no specific technical concerns raised",
            "emphasized practical use case"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Yes, my company uses a different swap device at different cgroup\nlevel. I did ask my coworker to confirm that usage. Control at the non\nroot level is a real need.",
          "reply_to": "Shakeel Butt",
          "message_date": "2026-02-20"
        },
        {
          "author": "Chris Li",
          "summary": "Chris Li noted that the swap device control introduced in this patch is not generic enough, as it only controls swap devices other than zswap, whereas zswap.writeback has more limited functionality and was previously held back due to concerns about its generality.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "I think this swap device control is a very basic need. All your\nobjections to swapping control in the group can equally apply to\nzswap.writeback. Unlike zswap.writeback, which only control from the\nzswap behavior. This is a more generic version control swap device\nother than zswap as well. BTW, I raised that concern about\nzswap.writeback was not generic enough as swap control was limited\nwhen zswap was proposed. We did hold back zswap.writeback. The\nconsensers is interface can be improved as later iterations. So here\nwe are.",
          "reply_to": "Shakeel Butt",
          "message_date": "2026-02-20"
        },
        {
          "author": "Chris Li",
          "summary": "Chris Li mentioned that he has an internal cgroup swapfile control interface, which could be replaced by the proposed patch, allowing it to be upstreamed instead of maintaining a separate internal interface.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "no clear technical objection or suggestion"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "There is a very long thread on the linux-mm maillist. I'm too lazy to dig it up.\n\nI can share our usage requirement to refresh your memory. We\ninternally use a cgroup swapfile control interface that has not been\nupstreamed. With this we can remove the need of that internal\ninterface and go upstream instead.",
          "reply_to": "Shakeel Butt",
          "message_date": "2026-02-20"
        },
        {
          "author": "Chris Li",
          "summary": "Reviewer Chris Li noted that the swap tier system's performance is dependent on the number of devices within each tier, and suggested using a round-robin algorithm to distribute swap operations across devices within the same tier.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "performance concern",
            "algorithm suggestion"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "It depends on the number of devices in the tiers. Different tiers\nmaintain an order. Within the same tier round robin.",
          "reply_to": "Shakeel Butt",
          "message_date": "2026-02-20"
        },
        {
          "author": "Chris Li",
          "summary": "Reviewer Chris Li suggested alternative names for the 'tier' concept, proposing 'swap.device_speed_classes' and acknowledging that the current name is inspired by memory tiers.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "suggested alternative",
            "acknowledged inspiration"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "I propose the tier name. Guilty. Yes, in was inpired by memory tiers.\nIt just different class of swap speeds. I am not fixed on the name. We\ncan also call it swap.device_speed_classes. You can suggest\nalternatives.\n\nPromotion / demotion is possible in the future. The current state,\nwithout promotion or demotion, already provides value. Our current\ndeployment uses only one class of swap device at a time. However I do\nknow other companies use  more than one class of swap device.",
          "reply_to": "Shakeel Butt",
          "message_date": "2026-02-20"
        },
        {
          "author": "Chris Li",
          "summary": "Reviewer Chris Li noted that their deployment uses multiple swap devices to reduce lock contention and suggested that the patch should consider allowing users to specify multiple swap devices for each tier.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "In our deployment, we always use more than one swap device to reduce\nswap device lock contention.\nThe job config can describe the swap speed it can tolerate. Some jobs\ncan tolerate slower speeds, while others cannot.",
          "reply_to": "Shakeel Butt",
          "message_date": "2026-02-20"
        },
        {
          "author": "Chris Li",
          "summary": "Reviewer Chris Li expressed a nuanced view on the patch, suggesting that incremental improvements are often better than trying to be overly future-proof. He noted that the current need for different cgroup swap speeds is real and should be addressed first, rather than designing for hypothetical future scenarios.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "incremental progress",
            "being too future-proof"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Take zswap.writeback as example. We have a solution that worked for\nthe requirement at that time. Incremental improvement is fine as well.\nUsually, incremental progress is better. At least currently there is a\nreal need to allow different cgroups to select different swap speeds.\nThere is a risk in being too future-proof: we might design things that\npeople in the future don't use as we envisioned. I see that happen too\noften as well.\n\nSo starting from the current need is a solid starting point. It's just\na different design philosophy. Each to their own.\n\nThat is the only usage case I know. YoungJun feel free to add yours\nusage as well.\n\nChris",
          "reply_to": "Shakeel Butt",
          "message_date": "2026-02-20"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-21": {
      "report_file": "2026-02-21_ollama_llama3.1-8b.html",
      "developer": "Shakeel Butt",
      "reviews": [
        {
          "author": "YoungJun Park (author)",
          "summary": "Author acknowledged Shakeel Butt's concern about swapoff path and per-vswap spinlock, agreeing that the spinlock should be dropped before calling try_to_unmap(), but did not commit to a specific fix or timeline.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged",
            "agreed"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Understood. Let's continue the discussion. :D\n\nChris has already provided a thorough response, but I would like to\nadd my perspective as well.",
          "reply_to": "Shakeel Butt",
          "message_date": "2026-02-21"
        },
        {
          "author": "YoungJun Park (author)",
          "summary": "Author acknowledged a concern about committing to a stable interface too early and proposed two possible solutions: guarding the interface behind a build-time config option or marking it as experimental.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "acknowledged a concern",
            "proposed alternative solutions"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "I understand your concern about committing to a stable interface too\nearly. As Chris suggested, we could reduce this concern by guarding\nthe interface behind a build-time config option or marking it as\nexperimental, which I will also touch on further below.\n\nOn that note, if BPF were to become the primary control mechanism,\nI am not sure a memcg interface would still be needed at all, since\nBPF already provides a high degree of freedom. However, that level\nof freedom is also what concerns me -- BPF-driven swap device\nassignments could subtly conflict with memcg hierarchy semantics in\nways that are hard to predict or debug. A more constrained memcg-based\napproach might actually be safer in that regard.",
          "reply_to": "Shakeel Butt",
          "message_date": "2026-02-21"
        },
        {
          "author": "YoungJun Park (author)",
          "summary": "Author acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), but did not explicitly state a plan for addressing this issue in future revisions.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged a concern",
            "did not promise a fix"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "I think this concern is closely tied to your question #3 below about\nconcrete use cases for partitioning devices across sub-workloads.\nI hope my answer there helps clarify this.",
          "reply_to": "Shakeel Butt",
          "message_date": "2026-02-21"
        },
        {
          "author": "YoungJun Park (author)",
          "summary": "Author suggests that enabling the swap tier feature through a build-time config or runtime constraints would improve predictability and usability.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "suggests an alternative approach",
            "acknowledges potential issues"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "As I mentioned above, I think guarding the feature behind a build-time\nconfig or runtime constraints could keep the usage well-defined and\npredictable, while still being useful.",
          "reply_to": "Shakeel Butt",
          "message_date": "2026-02-21"
        },
        {
          "author": "YoungJun Park (author)",
          "summary": "The author is addressing Shakeel Butt's concern about the patch's ability to handle more complex use cases beyond their initial proposal. The author explains that they initially suggested per-cgroup swap device priorities but later pivoted to the 'swap tier' mechanism as it suffices for their simple use case.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Our use case is simple at now. \nWe have two swap devices with different performance\ncharacteristics and want to assign different swap devices to different\nworkloads (cgroups).\n\nFor some background, when I initially proposed this, I suggested allowing\nper-cgroup swap device priorities so that it could also accommodate the\nbroader scenarios you mentioned. However, since even our own use case\ndoes not require reversing swap priorities within a cgroup, we pivoted\nto the \"swap tier\" mechanism that Chris proposed.",
          "reply_to": "Shakeel Butt",
          "message_date": "2026-02-21"
        },
        {
          "author": "YoungJun Park (author)",
          "summary": "Author addressed Shakeel Butt's concern about how swap devices are ordered when they have the same priority within a tier, explaining that round-robin ordering is used in this case and that the current tier structure can satisfy either preference.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Both. If devices are in the same tier with the same priority, round robin.\nIf they are in the same tier with different priorities, or in different\ntiers, ordering applies. The current tier structure should be able to\nsatisfy either preference.",
          "reply_to": "Shakeel Butt",
          "message_date": "2026-02-21"
        },
        {
          "author": "YoungJun Park (author)",
          "summary": "Author acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), but did not provide a fix or explanation for why this was not done in the original patch.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged a technical issue",
            "did not address the problem"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "This was originally Chris's idea. I think he explained the rationale\nwell in his reply.",
          "reply_to": "Shakeel Butt",
          "message_date": "2026-02-21"
        },
        {
          "author": "YoungJun Park (author)",
          "summary": "Author acknowledged a concern about lock contention in the original patch, suggesting that one possible solution is to partition swap devices between parent and child cgroups.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "acknowledged a concern",
            "suggested a possible scenario"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "One possible scenario is reducing lock contention by partitioning swap\ndevices between parent and child cgroups.",
          "reply_to": "Shakeel Butt",
          "message_date": "2026-02-21"
        },
        {
          "author": "YoungJun Park (author)",
          "summary": "The author is addressing Shakeel Butt's concern about the long-term maintainability of the patch by suggesting that it could be replaced by a BPF-based solution in the future, but for now proposes to introduce a CONFIG option to allow moving forward without committing to a stable interface.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledges fix is needed",
            "proposes temporary workaround"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "We have clear production use cases from both us and Chris, and I also\npresented a deployment example in the cover letter.\n\nI think it is hard to design concretely for future use cases at this\npoint. When those needs become clearer, BPF with its flexibility\nwould be a better fit then. I see BPF as a natural extension path\nrather than a starting point.\n\nFor now, guarding the memcg & tier behind a CONFIG option would\nlet us move forward without committing to a stable interface, and\nwe can always pivot to BPF later if needed\n\nThanks,\nYoungJun Park",
          "reply_to": "Shakeel Butt",
          "message_date": "2026-02-21"
        },
        {
          "author": "Shakeel Butt",
          "summary": "Reviewer Shakeel Butt expressed concern that the patch does not address the primary use case of controlling and partitioning swap devices among sub-workloads, and therefore questioned the value of adding a stable API at this stage.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "concern about lack of clear use-case",
            "questioning the value of adding a stable API"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "No, that is secondary because I am not seeing the real use-case of\ncontrolling/partitioning swap devices among sub-workloads. Until that is\nfigured out, adding a stable API is not good.",
          "reply_to": "Chris Li",
          "message_date": "2026-02-21"
        },
        {
          "author": "Shakeel Butt",
          "summary": "Reviewer Shakeel Butt questioned whether the patch's concept of swap tiers is a new innovation or simply reusing an existing interface, specifically referencing Google's prodkernel team's past work on per-cgroup swapfiles and zswap.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "unclear intent",
            "request for clarification"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "I am assuming you meant Google and particularly Prodkernel team and not\nAndroid or ChromeOS. Google's prodkernel used to have per-cgroup\nswapfiles exposed through memory.swapfiles (if I remember correctly\nSuleiman implemented this along with ghost swapfiles). Later this was\ndeprecated (by Yu Zhao) and global (ghost) swapfiles were being used.\nThe memory.swapfiles interface instead of supporting real swapfiles\nstarted having select options among default, ghost/zswap and real\n(something like that). However such interface was used to just disable\nor enable zswap for a workload and never about hierarchically\ncontrolling the swap devices (Google prodkernel only have zswap). Has\nsomething changed?",
          "reply_to": "Chris Li",
          "message_date": "2026-02-21"
        },
        {
          "author": "Shakeel Butt",
          "summary": "Reviewer Shakeel Butt expressed skepticism about the introduction of a new swap tier interface without a clear use case, prompting him to push back against its addition.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "skepticism",
            "pushback"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "This just motivates me to pushback even harder on adding a new interface\nwithout a clear use-case.",
          "reply_to": "Chris Li",
          "message_date": "2026-02-21"
        },
        {
          "author": "Shakeel Butt",
          "summary": "Reviewer Shakeel Butt questioned the practical application of hierarchical swap device control, specifically asking for a real-world use case to justify this feature.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "lack of concrete example"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "I already asked above but let me say it again. What's the actual real\nworld use-case to control/allow/disallow swap devices hierarchically?",
          "reply_to": "Chris Li",
          "message_date": "2026-02-21"
        },
        {
          "author": "Shakeel Butt",
          "summary": "Reviewer Shakeel Butt noted that having multiple swap devices to reduce lock contention does not address the need for hierarchical control of swap devices among sub-workloads, and is unrelated to the concept of Swap Tiers.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "unrelated",
            "does not address"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Having more than one swap devices to reduce lock contention is unrelated\nto hierarchically control swap devices among sub-workloads.",
          "reply_to": "Chris Li",
          "message_date": "2026-02-21"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-22": {
      "report_file": "2026-02-21_ollama_llama3.1-8b.html",
      "developer": "Shakeel Butt",
      "reviews": [
        {
          "author": "YoungJun Park (author)",
          "summary": "Author is addressing concerns about the BPF-first approach, specifically questioning its feasibility in an embedded environment and asking for clarification on precedents of BPF prototypes becoming stable kernel interfaces.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "questioning",
            "asking for clarification"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "....\n\nAfter reading the reply and re-think more of it.\n\nI have a few questions regarding the BPF-first approach you\nsuggested, if you don't mind. Some of them I am re-asking\nbecause I feel they have not been clearly addressed yet.\n\n- We are in an embedded environment where enabling additional\n  kernel compile options is costly. BPF is disabled by\n  default in some of our production configurations. From a\n  trade-off perspective, does it make sense to enable BPF\n  just for swap device control?\n\n- You suggest starting with BPF and discussing a stable\n  interface later. I am genuinely curious, are there actual\n  precedents where a BPF prototype graduated into a stable\n  kernel interface? \n\n- You raised that stable interfaces are hard to remove. Would\n  gating it behind a CONFIG option or marking it experimental\n  be an acceptable compromise?\n\n- You already acknowledged the use-case for assigning\n  different swap devices to different workloads. Your\n  objection is specifically about hierarchical parent-child\n  partitioning. If the interface enforced uniform policy\n  within a subtree, would that be acceptable?\n\n- We already run a modified kernel with internal swap control\n  in production and have real feedback from it. Requiring BPF\n  as a prerequisite to gather production experience seems\n  unnecessary when we are already doing that.\n\nTo be honest, I am having trouble understanding the motivation\nbehind the BPF-first validation approach. If the real point is\nthat BPF enables more flexible swap-out policies than any fixed\ninterface can, that would make much more sense to me. I would\nappreciate it if you could share more on this.\n\nThanks,\nYoungjun Park",
          "reply_to": "Shakeel Butt",
          "message_date": "2026-02-22"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    }
  }
}