{
  "thread_id": "4ce911a475b998ddf76951629ad203e6440ab0ca.1771884128.git.loemra.dev@gmail.com",
  "subject": "[PATCH v3 1/3] btrfs: skip COW for written extent buffers allocated in current transaction",
  "url": "https://lore.kernel.org/all/4ce911a475b998ddf76951629ad203e6440ab0ca.1771884128.git.loemra.dev@gmail.com/",
  "dates": {
    "2026-02-24": {
      "report_file": "2026-02-24_ollama_llama3.1-8b.html",
      "developer": "Leo Martins",
      "reviews": [
        {
          "author": "David Hildenbrand",
          "summary": "The patch looks good, but the author should consider adding a comment to explain why the per-restart-site tracepoint is necessary and how it improves over the existing counter-based approach.",
          "sentiment": "positive",
          "sentiment_signals": [
            "NEEDS_WORK"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "",
          "reply_to": "",
          "message_date": "",
          "message_id": ""
        }
      ],
      "analysis_source": "llm",
      "patch_summary": "This patch adds a new tracepoint to the Btrfs kernel module, allowing for tracking of search slot restarts in btrfs_search_slot(). The tracepoint records the root, tree level, and reason for each restart, enabling more detailed analysis of COW amplification under memory pressure."
    },
    "2026-02-25": {
      "report_file": "2026-02-24.html",
      "developer": "Leo Martins",
      "reviews": [
        {
          "author": "Sun YangKai",
          "summary": "should_cow_block(), making the overall logic easier to follow. And the commit message and comments are quite detailed and helpful.",
          "sentiment": "neutral",
          "sentiment_signals": [],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "heuristic",
          "raw_body": "\n\nOn 2026/2/25 03:22, Leo Martins wrote:\n> When memory pressure causes writeback of a recently COW'd buffer,\n> btrfs sets BTRFS_HEADER_FLAG_WRITTEN on it. Subsequent\n> btrfs_search_slot() restarts then see the WRITTEN flag and re-COW\n> the buffer unnecessarily, causing COW amplification that can exhaust\n> block reservations and degrade throughput.\n> \n> Overwriting in place is crash-safe because the committed superblock\n> does not reference buffers allocated in the current (uncommitted)\n> transaction, so no on-disk tree points to this block yet.\n> \n> When should_cow_block() encounters a WRITTEN buffer whose generation\n> matches the current transaction, instead of requesting a COW, re-dirty\n> the buffer and re-register its range in the transaction's dirty_pages.\n> \n> Both are necessary because btrfs tracks dirty metadata through two\n> independent mechanisms. set_extent_buffer_dirty() sets the\n> EXTENT_BUFFER_DIRTY flag and the buffer_tree xarray PAGECACHE_TAG_DIRTY\n> mark, which is what background writeback (btree_write_cache_pages) uses\n> to find and write dirty buffers. The transaction's dirty_pages io tree\n> is a separate structure used by btrfs_write_and_wait_transaction() at\n> commit time to ensure all buffers allocated during the transaction are\n> persisted. The dirty_pages range was originally registered in\n> btrfs_init_new_buffer() when the block was first allocated. Normally\n> dirty_pages is only cleared at commit time by\n> btrfs_write_and_wait_transaction(), but if qgroups are enabled and\n> snapshots are being created, qgroup_account_snapshot() may have already\n> called btrfs_write_and_wait_transaction() and released the range before\n> the final commit-time call.\n> \n> Keep BTRFS_HEADER_FLAG_WRITTEN set so that btrfs_free_tree_block()\n> correctly pins the block if it is freed later.\n> \n> Relax the lockdep assertion in btrfs_mark_buffer_dirty() from\n> btrfs_assert_tree_write_locked() to lockdep_assert_held() so that it\n> accepts either a read or write lock. should_cow_block() may be called\n> from btrfs_search_slot() when only a read lock is held (nodes above\n> write_lock_level are read-locked). The write lock assertion previously\n> documented the caller convention that buffer content was being modified\n> under exclusive access, but btrfs_mark_buffer_dirty() and\n> set_extent_buffer_dirty() themselves only perform independently\n> synchronized operations: atomic bit ops on bflags, folio_mark_dirty()\n> (kernel-internal folio locking), xarray mark updates (xarray spinlock),\n> and percpu counter updates. The read lock is sufficient because it\n> prevents lock_extent_buffer_for_io() from acquiring the write lock and\n> racing on the dirty state. Since rw_semaphore permits concurrent\n> readers, multiple threads can enter btrfs_mark_buffer_dirty()\n> simultaneously for the same buffer; this is safe because\n> test_and_set_bit(EXTENT_BUFFER_DIRTY) ensures only one thread performs\n> the full dirty state transition.\n> \n> Remove the CONFIG_BTRFS_DEBUG assertion in set_extent_buffer_dirty()\n> that checked folio_test_dirty() after marking the buffer dirty. This\n> assertion assumed exclusive access (only one thread in\n> set_extent_buffer_dirty() at a time), which held when the only caller\n> was btrfs_mark_buffer_dirty() under write lock. With concurrent readers\n> calling through should_cow_block(), a thread that loses the\n> test_and_set_bit race sees was_dirty=true and skips the folio dirty\n> marking, but the winning thread may not have called\n> btrfs_meta_folio_set_dirty() yet, causing the assertion to fire. This\n> is a benign race: the winning thread will complete the folio dirty\n> marking, and no writeback can clear it while readers hold their locks.\n> \n> Hoist the EXTENT_BUFFER_WRITEBACK, BTRFS_HEADER_FLAG_RELOC, and\n> BTRFS_ROOT_FORCE_COW checks before the WRITTEN block since they apply\n> regardless of whether the buffer has been written back. This\n> consolidates the exclusion logic and simplifies the WRITTEN path to\n> only handle log trees and zoned devices. Moving the RELOC checks\n> before the smp_mb__before_atomic() barrier is safe because both\n> btrfs_root_id() (immutable) and BTRFS_HEADER_FLAG_RELOC (set at COW\n> time under tree lock) are stable values not subject to concurrent\n> modification; the barrier is only needed for BTRFS_ROOT_FORCE_COW\n> which is set concurrently by create_pending_snapshot().\n> \n> Exclude cases where in-place overwrite is not safe:\n>   - EXTENT_BUFFER_WRITEBACK: buffer is mid-I/O\n>   - Zoned devices: require sequential writes\n>   - Log trees: log blocks are immediately referenced by a committed\n>     superblock via btrfs_sync_log(), so overwriting could corrupt the\n>     committed log\n>   - BTRFS_ROOT_FORCE_COW: snapshot in progress\n>   - BTRFS_HEADER_FLAG_RELOC: block being relocated\n> \n> Signed-off-by: Leo Martins <loemra.dev@gmail.com>\n\nThanks! Looks good now. Currently we only early return true in \nshould_cow_block(), making the overall logic easier to follow.\n\nAnd the commit message and comments are quite detailed and helpful.\n\nReviewed-by: Sun YangKai <sunk67188@gmail.com>\n\n> ---\n>   fs/btrfs/ctree.c     | 56 ++++++++++++++++++++++++++++++++++++++------\n>   fs/btrfs/disk-io.c   |  2 +-\n>   fs/btrfs/extent_io.c |  4 ----\n>   3 files changed, 50 insertions(+), 12 deletions(-)\n> \n> diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c\n> index 7267b2502665..0e02b7b14adc 100644\n> --- a/fs/btrfs/ctree.c\n> +++ b/fs/btrfs/ctree.c\n> @@ -599,9 +599,9 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,\n>   \treturn ret;\n>   }\n>   \n> -static inline bool should_cow_block(const struct btrfs_trans_handle *trans,\n> +static inline bool should_cow_block(struct btrfs_trans_handle *trans,\n>   \t\t\t\t    const struct btrfs_root *root,\n> -\t\t\t\t    const struct extent_buffer *buf)\n> +\t\t\t\t    struct extent_buffer *buf)\n>   {\n>   \tif (btrfs_is_testing(root->fs_info))\n>   \t\treturn false;\n> @@ -621,7 +621,11 @@ static inline bool should_cow_block(const struct btrfs_trans_handle *trans,\n>   \tif (btrfs_header_generation(buf) != trans->transid)\n>   \t\treturn true;\n>   \n> -\tif (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN))\n> +\tif (test_bit(EXTENT_BUFFER_WRITEBACK, &buf->bflags))\n> +\t\treturn true;\n> +\n> +\tif (btrfs_root_id(root) != BTRFS_TREE_RELOC_OBJECTID &&\n> +\t    btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC))\n>   \t\treturn true;\n>   \n>   \t/* Ensure we can see the FORCE_COW bit. */\n> @@ -629,11 +633,49 @@ static inline bool should_cow_block(const struct btrfs_trans_handle *trans,\n>   \tif (test_bit(BTRFS_ROOT_FORCE_COW, &root->state))\n>   \t\treturn true;\n>   \n> -\tif (btrfs_root_id(root) == BTRFS_TREE_RELOC_OBJECTID)\n> -\t\treturn false;\n> +\tif (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {\n> +\t\t/*\n> +\t\t * The buffer was allocated in this transaction and has been\n> +\t\t * written back to disk (WRITTEN is set). Normally we'd COW\n> +\t\t * it again, but since the committed superblock doesn't\n> +\t\t * reference this buffer (it was allocated in this transaction),\n> +\t\t * we can safely overwrite it in place.\n> +\t\t *\n> +\t\t * We keep BTRFS_HEADER_FLAG_WRITTEN set. The block has been\n> +\t\t * persisted at this bytenr and will be again after the\n> +\t\t * in-place update. This is important so that\n> +\t\t * btrfs_free_tree_block() correctly pins the block if it is\n> +\t\t * freed later (e.g., during tree rebalancing or FORCE_COW).\n> +\t\t *\n> +\t\t * Log trees and zoned devices cannot use this optimization:\n> +\t\t * - Log trees: log blocks are written and immediately\n> +\t\t *   referenced by a committed superblock via\n> +\t\t *   btrfs_sync_log(), bypassing the normal transaction\n> +\t\t *   commit. Overwriting in place could corrupt the\n> +\t\t *   committed log.\n> +\t\t * - Zoned devices: require sequential writes.\n> +\t\t */\n> +\t\tif (btrfs_root_id(root) == BTRFS_TREE_LOG_OBJECTID ||\n> +\t\t    btrfs_is_zoned(root->fs_info))\n> +\t\t\treturn true;\n>   \n> -\tif (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC))\n> -\t\treturn true;\n> +\t\t/*\n> +\t\t * Re-register this block's range in the current transaction's\n> +\t\t * dirty_pages so that btrfs_write_and_wait_transaction()\n> +\t\t * writes it. The range was originally registered when the\n> +\t\t * block was allocated. Normally dirty_pages is only cleared\n> +\t\t * at commit time by btrfs_write_and_wait_transaction(), but\n> +\t\t * if qgroups are enabled and snapshots are being created,\n> +\t\t * qgroup_account_snapshot() may have already called\n> +\t\t * btrfs_write_and_wait_transaction() and released the range\n> +\t\t * before the final commit-time call.\n> +\t\t */\n> +\t\tbtrfs_set_extent_bit(&trans->transaction->dirty_pages,\n> +\t\t\t\t     buf->start,\n> +\t\t\t\t     buf->start + buf->len - 1,\n> +\t\t\t\t     EXTENT_DIRTY, NULL);\n> +\t\tbtrfs_mark_buffer_dirty(trans, buf);\n> +\t}\n>   \n>   \treturn false;\n>   }\n> diff --git a/fs/btrfs/disk-io.c b/fs/btrfs/disk-io.c\n> index 32fffb0557e5..bee8f76fbfea 100644\n> --- a/fs/btrfs/disk-io.c\n> +++ b/fs/btrfs/disk-io.c\n> @@ -4491,7 +4491,7 @@ void btrfs_mark_buffer_dirty(struct btrfs_trans_handle *trans,\n>   #endif\n>   \t/* This is an active transaction (its state < TRANS_STATE_UNBLOCKED). */\n>   \tASSERT(trans->transid == fs_info->generation);\n> -\tbtrfs_assert_tree_write_locked(buf);\n> +\tlockdep_assert_held(&buf->lock);\n>   \tif (unlikely(transid != fs_info->generation)) {\n>   \t\tbtrfs_abort_transaction(trans, -EUCLEAN);\n>   \t\tbtrfs_crit(fs_info,\n> diff --git a/fs/btrfs/extent_io.c b/fs/btrfs/extent_io.c\n> index dfc17c292217..ff1fc699a6ca 100644\n> --- a/fs/btrfs/extent_io.c\n> +++ b/fs/btrfs/extent_io.c\n> @@ -3791,10 +3791,6 @@ void set_extent_buffer_dirty(struct extent_buffer *eb)\n>   \t\t\t\t\t eb->len,\n>   \t\t\t\t\t eb->fs_info->dirty_metadata_batch);\n>   \t}\n> -#ifdef CONFIG_BTRFS_DEBUG\n> -\tfor (int i = 0; i < num_extent_folios(eb); i++)\n> -\t\tASSERT(folio_test_dirty(eb->folios[i]));\n> -#endif\n>   }\n>   \n>   void clear_extent_buffer_uptodate(struct extent_buffer *eb)\n\n",
          "reply_to": "",
          "message_date": "2026-02-25",
          "message_id": ""
        }
      ],
      "analysis_source": "heuristic",
      "patch_summary": "When memory pressure causes writeback of a recently COW'd buffer, btrfs sets BTRFS_HEADER_FLAG_WRITTEN on it. Subsequent btrfs_search_slot() restarts then see the WRITTEN flag and re-COW the buffer unnecessarily, causing COW amplification that can exhaust block reservations and degrade throughput.\n\nOverwriting in place is crash-safe because the committed superblock does not reference buffers allocated in the current (uncommitted) transaction, so no on-disk tree points to this block yet.\n\nWhen should_cow_block() encounters a WRITTEN buffer whose generation matches the current transaction, instead of requesting a COW, re-dirty the buffer and re-register its range in the transaction's dirty_pages."
    }
  }
}