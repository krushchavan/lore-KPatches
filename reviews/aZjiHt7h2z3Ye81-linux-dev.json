{
  "thread_id": "aZjiHt7h2z3Ye81_@linux.dev",
  "subject": "Re: [PATCH] arm64: remove HAVE_CMPXCHG_LOCAL",
  "url": "https://lore.kernel.org/all/aZjiHt7h2z3Ye81_@linux.dev/",
  "dates": {
    "2026-02-20": {
      "report_file": "2026-02-20_ollama_llama3.1-8b.html",
      "developer": "Shakeel Butt",
      "reviews": [
        {
          "author": "Jisheng Zhang (author)",
          "summary": "Author asked for clarification on whether reviewer thinks cmpxchg_local is better than generic disable/enable irq version on newer arm64 systems.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarifying question"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Hi Will,\n\nI read this as an implication that the cmpxchg_local version is better\nthan generic disable/enable irq version on the newer arm64 systems. Is my\nunderstanding correct?",
          "reply_to": "Dev Jain",
          "message_date": "2026-02-20"
        },
        {
          "author": "Jisheng Zhang (author)",
          "summary": "Author acknowledged that removing preempt_disable/enable from _pcp_protect_return improves performance, but the HAVE_CMPXCHG_LOCAL version is still slower than the generic disable/enable irq version on CA55 and CA73 platforms.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged a performance issue",
            "still worse"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Hi Dev,\n\nThanks for the hints. I tried to remove the preempt_disable/enable from\n_pcp_protect_return, it improves, but the HAVE_CMPXCHG_LOCAL version is\nstill worse than generic disable/enable irq version on CA55 and CA73.",
          "reply_to": "Dev Jain",
          "message_date": "2026-02-20"
        },
        {
          "author": "Jisheng Zhang (author)",
          "summary": "Author acknowledged that arm, powerpc, and mips architectures rely on the generic disable/enable irq version of cmpxchg, implying they may not be safe in NMI context, but did not commit to revising the patch or addressing this issue.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "acknowledged a potential issue",
            "did not commit to revising"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "hmm, interesting...\n\nfgrep HAVE_NMI arch/*/Kconfig\nthen\nfgrep HAVE_CMPXCHG_LOCAL arch/*/Kconfig\n\nshows that only x86, arm64, s390 and loongarch are safe, while arm,\npowerpc and mips enable HAVE_NMI but missing HAVE_CMPXCHG_LOCAL, so\nthey rely on generic generic disable/enable irq version, so you imply\nthat these three arch are not safe considering mod_node_page_state()\nin NMI context.",
          "reply_to": "Shakeel Butt",
          "message_date": "2026-02-20"
        },
        {
          "author": "Shakeel Butt",
          "summary": "Reviewer noted that vmstat updates may require NMI-safe cmpxchg operations, similar to memcg stats, and wondered if adding this support would introduce unnecessary complexity for certain architectures.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "complexity",
            "NMI-safe cmpxchg"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Yes it seems like it. For memcg stats, we use ARCH_HAVE_NMI_SAFE_CMPXCHG and\nARCH_HAS_NMI_SAFE_THIS_CPU_OPS config options to correctly handle the updates\nfrom NMI context. Maybe we need something similar for vmstat as well.\n\nSo arm, powerpc and mips does not have ARCH_HAS_NMI_SAFE_THIS_CPU_OPS but\npowerpc does have ARCH_HAVE_NMI_SAFE_CMPXCHG and arm has\nit for CPU_V7, CPU_V7M & CPU_V6K models.\n\nI wonder if we need to add complexity for these archs.",
          "reply_to": "Jisheng Zhang",
          "message_date": "2026-02-20"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-16": {
      "report_file": "2026-02-20_ollama_llama3.1-8b.html",
      "developer": "Shakeel Butt",
      "reviews": [
        {
          "author": "Dev Jain",
          "summary": "Reviewer noted that the issue isn't with LL/SC/LSE but rather preempt_disable()/enable() in this_cpu_* functions, and suggested keeping the code while removing the HAVE_CMPXCHG_LOCAL config selection.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Thanks. This concurs with my investigation on [1]. The problem\nisn't really LL/SC/LSE but preempt_disable()/enable() in\nthis_cpu_* [1, 2].\n\nI think you should only remove the selection of the config,\nbut keep the code? We may want to switch this on again if\nthe real issue gets solved.\n\n[1] https://lore.kernel.org/all/5a6782f3-d758-4d9c-975b-5ae4b5d80d4e@arm.com/\n[2] https://lore.kernel.org/all/CAHbLzkpcN-T8MH6=W3jCxcFj1gVZp8fRqe231yzZT-rV_E_org@mail.gmail.com/",
          "reply_to": "Jisheng Zhang",
          "message_date": "2026-02-16"
        },
        {
          "author": "Will Deacon",
          "summary": "Reviewer Will Deacon expressed concern that the patch is system-dependent and may not be suitable for all architectures, also questioning the priority of micro-optimizing atomic operations.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "system-dependence",
            "micro-optimization"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "That is _entirely_ dependent on the system, so this isn't the right\napproach. I also don't think it's something we particularly want to\nmicro-optimise to accomodate systems that suck at atomics.\n\nWill",
          "reply_to": "Jisheng Zhang",
          "message_date": "2026-02-16"
        },
        {
          "author": "Dev Jain",
          "summary": "Reviewer Dev Jain suspects that the performance regression is caused by preempt_disable() in _pcp_protect_return, not atomics, and requests further testing to confirm this hypothesis.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "further testing"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Hi Will,\n\nAs I mention in the other email, the suspect is not the atomics, but\npreempt_disable(). On Apple M3, the regression reported in [1] resolves\nby removing preempt_disable/enable in _pcp_protect_return. To prove\nthis another way, I disabled CONFIG_ARM64_HAS_LSE_ATOMICS and the\nregression worsened, indicating that at least on Apple M3 the\natomics are faster.\n\nIt may help to confirm this hypothesis on other hardware - perhaps\nJisheng can test with this change on his hardware and confirm\nwhether he gets the same performance improvement.\n\nBy coincidence, Yang Shi has been discussing the this_cpu_* overhead\nat [2].\n\n[1] https://lore.kernel.org/all/1052a452-9ba3-4da7-be47-7d27d27b3d1d@arm.com/\n[2] https://lore.kernel.org/all/CAHbLzkpcN-T8MH6=W3jCxcFj1gVZp8fRqe231yzZT-rV_E_org@mail.gmail.com/",
          "reply_to": "Will Deacon",
          "message_date": "2026-02-16"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-17": {
      "report_file": "2026-02-20_ollama_llama3.1-8b.html",
      "developer": "Shakeel Butt",
      "reviews": [
        {
          "author": "Catalin Marinas",
          "summary": "Reviewer Catalin Marinas suggested replacing preempt disabling with local_irq_save() in the arm64 code to still use LSE atomics, citing a potential issue where another CPU can access and modify the current CPU's variable via per_cpu_ptr(), which is not atomic.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Then why don't we replace the preempt disabling with local_irq_save()\nin the arm64 code and still use the LSE atomics?\n\nIIUC (lots of macro indirection), the generic cmpxchg is not atomic, so\nanother CPU is allowed to mess this up if it accesses current CPU's\nvariable via per_cpu_ptr().\n\n-- \nCatalin",
          "reply_to": "Dev Jain",
          "message_date": "2026-02-17"
        },
        {
          "author": "Will Deacon",
          "summary": "Reviewer Will Deacon suggested that instead of removing HAVE_CMPXCHG_LOCAL, the patch should focus on optimizing preempt_disable() as it is used in many other places.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Even better, work on making preempt_disable() faster as it's used in many\nother places. Of course, if people want to hack the .config, they could\nalso change the preemption mode...\n\nWill",
          "reply_to": "Catalin Marinas",
          "message_date": "2026-02-17"
        },
        {
          "author": "Catalin Marinas",
          "summary": "The reviewer noted that preempt_enable_notrace() in the cmpxchg128_local() macro can unconditionally call __schedule(), which may lead to issues, and suggested a simple change to the preempt_schedule_notrace() function to avoid this.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Yes, that would be good. It's the preempt_enable_notrace() path that\nends up calling preempt_schedule_notrace() -> __schedule() pretty much\nunconditionally. Not sure what would go wrong but some simple change\nlike this (can be done at a higher in the preempt macros to even avoid\ngetting here):\n\ndiff --git a/kernel/sched/core.c b/kernel/sched/core.c\nindex 854984967fe2..d9a5d6438303 100644\n--- a/kernel/sched/core.c\n+++ b/kernel/sched/core.c\n@@ -7119,7 +7119,7 @@ asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)\n \tif (likely(!preemptible()))\n \t\treturn;\n \n-\tdo {\n+\twhile (need_resched()) {\n \t\t/*\n \t\t * Because the function tracer can trace preempt_count_sub()\n \t\t * and it also uses preempt_enable/disable_notrace(), if\n@@ -7146,7 +7146,7 @@ asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)\n \n \t\tpreempt_latency_stop(1);\n \t\tpreempt_enable_no_resched_notrace();\n-\t} while (need_resched());\n+\t}\n }\n EXPORT_SYMBOL_GPL(preempt_schedule_notrace);\n \n\nOf course, changing the preemption model solves this by making the\nmacros no-ops but I assume people want to keep preemption on.\n\n-- \nCatalin",
          "reply_to": "Will Deacon",
          "message_date": "2026-02-17"
        },
        {
          "author": "Christoph (Ampere)",
          "summary": "Reviewer noted that the performance of cmpxchg varies by platform and kernel config, and requested consideration for removing HAVE_CMPXCHG_LOCAL based on measurements from Ampere processors which did not indicate a regression.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Yang Shi is on vacation but we have a patchset that removes\npreempt_enable/disable from this_cpu operations on ARM64.\n\nThe performance of cmpxchg varies by platform in use and with the kernel\nconfig. The measurements that I did 2 years ago indicated that the cmpxchg\nuse with Ampere processors did not cause a regression.\n\nNote that distro kernels often do not enable PREEMPT_FULL and therefore\npreempt_disable/enable overhead is not incurred in production systems.\n\nPREEMPT_VOLUNTARY does not use preemption for this_cpu ops.",
          "reply_to": "Dev Jain",
          "message_date": "2026-02-17"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-18": {
      "report_file": "2026-02-20_ollama_llama3.1-8b.html",
      "developer": "Shakeel Butt",
      "reviews": [
        {
          "author": "K Nayak",
          "summary": "Reviewer K Nayak noted that the patch does not provide a clear justification for removing HAVE_CMPXCHG_LOCAL and requested documentation to explain why this change is necessary.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Hello Catalin,\n\nOn 2/17/2026 10:18 PM, Catalin Marinas wrote:",
          "reply_to": "Catalin Marinas",
          "message_date": "2026-02-18"
        },
        {
          "author": "K Nayak",
          "summary": "Reviewer K Nayak pointed out that the patch description's claim of an 'unconditional' change is incorrect, as both x86 and arm64 implementations check __preempt_count_dec_and_test() before calling into __schedule(), and instead suggested that the arm64 implementation uses a separate 'ti->preempt.need_resched' bit within the 'ti->preempt_count' union.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "What do you mean by unconditionally? We always check\n__preempt_count_dec_and_test() before calling into __schedule().\n\nOn x86, We use MSB of preempt_count to indicate a resched and\nset_preempt_need_resched() would just clear this MSB.\n\nIf the preempt_count() turns 0, we immediately go into schedule\nor  or the next preempt_enable() -> __preempt_count_dec_and_test()\nwould see the entire preempt_count being clear and will call into\nschedule.\n\nThe arm64 implementation seems to be doing something similar too\nwith a separate \"ti->preempt.need_resched\" bit which is part of\nthe \"ti->preempt_count\"'s union so it isn't really unconditional.",
          "reply_to": "Catalin Marinas",
          "message_date": "2026-02-18"
        },
        {
          "author": "K Nayak",
          "summary": "The reviewer pointed out that the patch introduces a redundant check for need_resched() state, which is already handled by __preempt_count_dec_and_test(), and suggested removing it.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "redundant check",
            "suggested removal"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Essentially you are simply checking it twice now on entry since\nneed_resched() state would have already been communicated by\n__preempt_count_dec_and_test().",
          "reply_to": "Catalin Marinas",
          "message_date": "2026-02-18"
        },
        {
          "author": "Catalin Marinas",
          "summary": "reviewer pointed out that the patch does not handle the case where HAVE_CMPXCHG_LOCAL is removed but the cmpxchg_local() function is still used in other parts of the code, which could lead to a build error",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "build error",
            "missing handling"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Hi Prateek,\n\nOn Wed, Feb 18, 2026 at 09:31:19AM +0530, K Prateek Nayak wrote:",
          "reply_to": "K Nayak",
          "message_date": "2026-02-18"
        },
        {
          "author": "Catalin Marinas",
          "summary": "Reviewer noted that the additional pointer chase and preempt_count update contribute to the performance overhead, suggesting a large overhaul or alternative approaches like in-kernel restartable sequences.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Ah, yes, you are right. I got the polarity of need_resched in\nthread_info wrong (we should have named it no_need_to_resched).\n\nSo in the common case, the overhead is caused by the additional\npointer chase and preempt_count update, on top of the cpu offset read.\nNot sure we can squeeze any more cycles out of these without some\nlarge overhaul like:\n\nhttps://git.kernel.org/mark/c/84ee5f23f93d4a650e828f831da9ed29c54623c5\n\nor Yang's per-CPU page tables. Well, there are more ideas like in-kernel\nrestartable sequences but they move the overhead elsewhere.\n\nThanks.\n\n-- \nCatalin",
          "reply_to": "K Nayak",
          "message_date": "2026-02-18"
        },
        {
          "author": "Shakeel Butt",
          "summary": "Reviewer noted that the generic disable/enable irq implementation used by this patch is not safe in NMI context, citing newer arm architectures' support for NMIs.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "lock ordering violation",
            "NMI safety concern"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Please note that mod_node_page_state() can be called in NMI context and\ngeneric disable/enable irq are not safe against NMIs (newer arm arch supports\nNMI).",
          "reply_to": "Jisheng Zhang",
          "message_date": "2026-02-18"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    }
  }
}