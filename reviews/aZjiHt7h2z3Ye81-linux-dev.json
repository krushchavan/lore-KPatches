{
  "thread_id": "aZjiHt7h2z3Ye81_@linux.dev",
  "subject": "Re: [PATCH] arm64: remove HAVE_CMPXCHG_LOCAL",
  "url": "https://lore.kernel.org/all/aZjiHt7h2z3Ye81_@linux.dev/",
  "dates": {
    "2026-02-20": {
      "report_file": "2026-02-20_ollama_llama3.1-8b.html",
      "developer": "Shakeel Butt",
      "reviews": [
        {
          "author": "Jisheng Zhang (author)",
          "summary": "Author asked for clarification on whether reviewer thinks cmpxchg_local is better than generic disable/enable irq version on newer arm64 systems.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarifying question"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Hi Will,\n\nI read this as an implication that the cmpxchg_local version is better\nthan generic disable/enable irq version on the newer arm64 systems. Is my\nunderstanding correct?",
          "reply_to": "Dev Jain",
          "message_date": "2026-02-20"
        },
        {
          "author": "Jisheng Zhang (author)",
          "summary": "Author acknowledged that removing preempt_disable/enable from _pcp_protect_return improves performance, but the HAVE_CMPXCHG_LOCAL version is still slower than the generic implementation on two test platforms.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged a performance issue",
            "planned to re-evaluate"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Hi Dev,\n\nThanks for the hints. I tried to remove the preempt_disable/enable from\n_pcp_protect_return, it improves, but the HAVE_CMPXCHG_LOCAL version is\nstill worse than generic disable/enable irq version on CA55 and CA73.",
          "reply_to": "Dev Jain",
          "message_date": "2026-02-20"
        },
        {
          "author": "Jisheng Zhang (author)",
          "summary": "Author acknowledges that arm, powerpc, and mips architectures may have issues with mod_node_page_state() in NMI context, but does not explicitly address the safety concern or plan a fix.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "acknowledges issue",
            "does not commit to fixing"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "hmm, interesting...\n\nfgrep HAVE_NMI arch/*/Kconfig\nthen\nfgrep HAVE_CMPXCHG_LOCAL arch/*/Kconfig\n\nshows that only x86, arm64, s390 and loongarch are safe, while arm,\npowerpc and mips enable HAVE_NMI but missing HAVE_CMPXCHG_LOCAL, so\nthey rely on generic generic disable/enable irq version, so you imply\nthat these three arch are not safe considering mod_node_page_state()\nin NMI context.",
          "reply_to": "Shakeel Butt",
          "message_date": "2026-02-20"
        },
        {
          "author": "Shakeel Butt",
          "summary": "Reviewer noted that vmstat updates may require NMI-safe cmpxchg operations, similar to memcg stats, and questioned whether adding complexity for certain architectures is necessary.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "complexity",
            "nmi-safe"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Yes it seems like it. For memcg stats, we use ARCH_HAVE_NMI_SAFE_CMPXCHG and\nARCH_HAS_NMI_SAFE_THIS_CPU_OPS config options to correctly handle the updates\nfrom NMI context. Maybe we need something similar for vmstat as well.\n\nSo arm, powerpc and mips does not have ARCH_HAS_NMI_SAFE_THIS_CPU_OPS but\npowerpc does have ARCH_HAVE_NMI_SAFE_CMPXCHG and arm has\nit for CPU_V7, CPU_V7M & CPU_V6K models.\n\nI wonder if we need to add complexity for these archs.",
          "reply_to": "Jisheng Zhang",
          "message_date": "2026-02-20"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-16": {
      "report_file": "2026-02-20_ollama_llama3.1-8b.html",
      "developer": "Shakeel Butt",
      "reviews": [
        {
          "author": "Dev Jain",
          "summary": "Reviewer noted that the issue isn't with LL/SC/LSE but rather preempt_disable()/enable() in this_cpu_* functions, and suggested keeping the code while only removing the HAVE_CMPXCHG_LOCAL config selection.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Thanks. This concurs with my investigation on [1]. The problem\nisn't really LL/SC/LSE but preempt_disable()/enable() in\nthis_cpu_* [1, 2].\n\nI think you should only remove the selection of the config,\nbut keep the code? We may want to switch this on again if\nthe real issue gets solved.\n\n[1] https://lore.kernel.org/all/5a6782f3-d758-4d9c-975b-5ae4b5d80d4e@arm.com/\n[2] https://lore.kernel.org/all/CAHbLzkpcN-T8MH6=W3jCxcFj1gVZp8fRqe231yzZT-rV_E_org@mail.gmail.com/",
          "reply_to": "Jisheng Zhang",
          "message_date": "2026-02-16"
        },
        {
          "author": "Will Deacon",
          "summary": "Reviewer Will Deacon expressed concern that the patch is system-dependent and may not be suitable for all architectures, particularly those that struggle with atomic operations.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "system-dependency",
            "micro-optimization"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "That is _entirely_ dependent on the system, so this isn't the right\napproach. I also don't think it's something we particularly want to\nmicro-optimise to accomodate systems that suck at atomics.\n\nWill",
          "reply_to": "Jisheng Zhang",
          "message_date": "2026-02-16"
        },
        {
          "author": "Dev Jain",
          "summary": "Reviewer Dev Jain suspects that the performance regression is caused by preempt_disable() in _pcp_protect_return, and suggests testing this hypothesis on other hardware to confirm whether disabling preempt_disable/enable improves performance.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "hypothesis"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Hi Will,\n\nAs I mention in the other email, the suspect is not the atomics, but\npreempt_disable(). On Apple M3, the regression reported in [1] resolves\nby removing preempt_disable/enable in _pcp_protect_return. To prove\nthis another way, I disabled CONFIG_ARM64_HAS_LSE_ATOMICS and the\nregression worsened, indicating that at least on Apple M3 the\natomics are faster.\n\nIt may help to confirm this hypothesis on other hardware - perhaps\nJisheng can test with this change on his hardware and confirm\nwhether he gets the same performance improvement.\n\nBy coincidence, Yang Shi has been discussing the this_cpu_* overhead\nat [2].\n\n[1] https://lore.kernel.org/all/1052a452-9ba3-4da7-be47-7d27d27b3d1d@arm.com/\n[2] https://lore.kernel.org/all/CAHbLzkpcN-T8MH6=W3jCxcFj1gVZp8fRqe231yzZT-rV_E_org@mail.gmail.com/",
          "reply_to": "Will Deacon",
          "message_date": "2026-02-16"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-17": {
      "report_file": "2026-02-20_ollama_llama3.1-8b.html",
      "developer": "Shakeel Butt",
      "reviews": [
        {
          "author": "Catalin Marinas",
          "summary": "Reviewer Catalin Marinas suggested replacing preempt disabling with local_irq_save() in the arm64 code to still use LSE atomics, citing a potential issue where another CPU can access and modify the current CPU's variable via per_cpu_ptr().",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Then why don't we replace the preempt disabling with local_irq_save()\nin the arm64 code and still use the LSE atomics?\n\nIIUC (lots of macro indirection), the generic cmpxchg is not atomic, so\nanother CPU is allowed to mess this up if it accesses current CPU's\nvariable via per_cpu_ptr().\n\n-- \nCatalin",
          "reply_to": "Dev Jain",
          "message_date": "2026-02-17"
        },
        {
          "author": "Will Deacon",
          "summary": "Reviewer Will Deacon suggested improving preempt_disable() performance as it is used in multiple places and proposed that users can also modify their .config to change preemption mode",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "performance improvement",
            "alternative solution"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Even better, work on making preempt_disable() faster as it's used in many\nother places. Of course, if people want to hack the .config, they could\nalso change the preemption mode...\n\nWill",
          "reply_to": "Catalin Marinas",
          "message_date": "2026-02-17"
        },
        {
          "author": "Catalin Marinas",
          "summary": "The reviewer noted that preempt_enable_notrace() can unconditionally call __schedule(), and suggested a simple change to the preempt_schedule_notrace() function to avoid this issue, or alternatively changing the preemption model to make the macros no-ops.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Yes, that would be good. It's the preempt_enable_notrace() path that\nends up calling preempt_schedule_notrace() -> __schedule() pretty much\nunconditionally. Not sure what would go wrong but some simple change\nlike this (can be done at a higher in the preempt macros to even avoid\ngetting here):\n\ndiff --git a/kernel/sched/core.c b/kernel/sched/core.c\nindex 854984967fe2..d9a5d6438303 100644\n--- a/kernel/sched/core.c\n+++ b/kernel/sched/core.c\n@@ -7119,7 +7119,7 @@ asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)\n \tif (likely(!preemptible()))\n \t\treturn;\n \n-\tdo {\n+\twhile (need_resched()) {\n \t\t/*\n \t\t * Because the function tracer can trace preempt_count_sub()\n \t\t * and it also uses preempt_enable/disable_notrace(), if\n@@ -7146,7 +7146,7 @@ asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)\n \n \t\tpreempt_latency_stop(1);\n \t\tpreempt_enable_no_resched_notrace();\n-\t} while (need_resched());\n+\t}\n }\n EXPORT_SYMBOL_GPL(preempt_schedule_notrace);\n \n\nOf course, changing the preemption model solves this by making the\nmacros no-ops but I assume people want to keep preemption on.\n\n-- \nCatalin",
          "reply_to": "Will Deacon",
          "message_date": "2026-02-17"
        },
        {
          "author": "Christoph (Ampere)",
          "summary": "Reviewer noted that the performance of cmpxchg varies by platform and kernel config, citing measurements from 2 years ago on Ampere processors where it did not cause a regression, and suggested that preempt_enable/disable overhead is not incurred in production systems due to PREEMPT_FULL not being enabled",
          "sentiment": "neutral",
          "sentiment_signals": [
            "NEEDS_WORK"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Yang Shi is on vacation but we have a patchset that removes\npreempt_enable/disable from this_cpu operations on ARM64.\n\nThe performance of cmpxchg varies by platform in use and with the kernel\nconfig. The measurements that I did 2 years ago indicated that the cmpxchg\nuse with Ampere processors did not cause a regression.\n\nNote that distro kernels often do not enable PREEMPT_FULL and therefore\npreempt_disable/enable overhead is not incurred in production systems.\n\nPREEMPT_VOLUNTARY does not use preemption for this_cpu ops.",
          "reply_to": "Dev Jain",
          "message_date": "2026-02-17"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-18": {
      "report_file": "2026-02-20_ollama_llama3.1-8b.html",
      "developer": "Shakeel Butt",
      "reviews": [
        {
          "author": "K Nayak",
          "summary": "Reviewer K Nayak noted that the patch removes HAVE_CMPXCHG_LOCAL without considering the implications of cmpxchg128_local() on arm64, which is still using this feature.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "patch needs reevaluation",
            "consideration for arm64"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Hello Catalin,\n\nOn 2/17/2026 10:18 PM, Catalin Marinas wrote:",
          "reply_to": "Catalin Marinas",
          "message_date": "2026-02-18"
        },
        {
          "author": "K Nayak",
          "summary": "Reviewer K Nayak pointed out that the arm64 implementation's use of a separate 'need_resched' bit within the 'ti->preempt_count' union is not unconditional, as it still checks __preempt_count_dec_and_test() before calling into __schedule().",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "What do you mean by unconditionally? We always check\n__preempt_count_dec_and_test() before calling into __schedule().\n\nOn x86, We use MSB of preempt_count to indicate a resched and\nset_preempt_need_resched() would just clear this MSB.\n\nIf the preempt_count() turns 0, we immediately go into schedule\nor  or the next preempt_enable() -> __preempt_count_dec_and_test()\nwould see the entire preempt_count being clear and will call into\nschedule.\n\nThe arm64 implementation seems to be doing something similar too\nwith a separate \"ti->preempt.need_resched\" bit which is part of\nthe \"ti->preempt_count\"'s union so it isn't really unconditional.",
          "reply_to": "Catalin Marinas",
          "message_date": "2026-02-18"
        },
        {
          "author": "K Nayak",
          "summary": "Reviewer K Nayak pointed out that the patch introduces a redundant check for need_resched() state, which is already communicated by __preempt_count_dec_and_test(), and requested further investigation.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "redundant check",
            "further investigation"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Essentially you are simply checking it twice now on entry since\nneed_resched() state would have already been communicated by\n__preempt_count_dec_and_test().",
          "reply_to": "Catalin Marinas",
          "message_date": "2026-02-18"
        },
        {
          "author": "Catalin Marinas",
          "summary": "Reviewer Catalin Marinas noted that the patch removes HAVE_CMPXCHG_LOCAL, but this change is not sufficient to enable cmpxchg_local on arm64, as it requires additional changes in the arch/arm64/include/asm/percpu.h file.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "additional changes required"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Hi Prateek,\n\nOn Wed, Feb 18, 2026 at 09:31:19AM +0530, K Prateek Nayak wrote:",
          "reply_to": "K Nayak",
          "message_date": "2026-02-18"
        },
        {
          "author": "Catalin Marinas",
          "summary": "Reviewer Catalin Marinas noted that the patch's performance improvement is due to an additional pointer chase and preempt_count update, not the removal of HAVE_CMPXCHG_LOCAL as claimed, and suggested exploring alternative optimizations.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Ah, yes, you are right. I got the polarity of need_resched in\nthread_info wrong (we should have named it no_need_to_resched).\n\nSo in the common case, the overhead is caused by the additional\npointer chase and preempt_count update, on top of the cpu offset read.\nNot sure we can squeeze any more cycles out of these without some\nlarge overhaul like:\n\nhttps://git.kernel.org/mark/c/84ee5f23f93d4a650e828f831da9ed29c54623c5\n\nor Yang's per-CPU page tables. Well, there are more ideas like in-kernel\nrestartable sequences but they move the overhead elsewhere.\n\nThanks.\n\n-- \nCatalin",
          "reply_to": "K Nayak",
          "message_date": "2026-02-18"
        },
        {
          "author": "Shakeel Butt",
          "summary": "Reviewer noted that mod_node_page_state() can be called in NMI context, but the generic disable/enable irq implementation is not safe against NMIs on newer ARM architectures, and requested further consideration.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "lock ordering violation",
            "NMI context"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Please note that mod_node_page_state() can be called in NMI context and\ngeneric disable/enable irq are not safe against NMIs (newer arm arch supports\nNMI).",
          "reply_to": "Jisheng Zhang",
          "message_date": "2026-02-18"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-23": {
      "report_file": "2026-02-20_ollama_llama3.1-8b.html",
      "developer": "Shakeel Butt",
      "reviews": [
        {
          "author": "Heiko Carstens",
          "summary": "Reviewer noted that the patch's removal of HAVE_CMPXCHG_LOCAL may be problematic due to the enforcement of PREEMPT_LAZY in newer kernels, which can lead to full preempt_disable()/preempt_enable() overhead for this_cpu_* operations.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "PREEMPT_LAZY",
            "PREEMPT_BUILD"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Note that with commit 7dadeaa6e851 (\"sched: Further restrict the preemption\nmodes\") at least PREEMPT_LAZY is enforced, which comes together with\nPREEMPT_BUILD and full preempt_disable()/preempt_enable() overhead for\nthis_cpu_* ops for every \"up-to-date\" architecture (except x86).\n\nPREEMPT_NONE and PREEMPT_VOLUNTARY are gone for those architectures.",
          "reply_to": "Christoph (Ampere)",
          "message_date": "2026-02-23"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    }
  }
}