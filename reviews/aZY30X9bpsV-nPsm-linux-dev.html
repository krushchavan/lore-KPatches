<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Review Comments: Re: [PATCH] arm64: remove HAVE_CMPXCHG_LOCAL</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto,
                         "Helvetica Neue", Arial, sans-serif;
            background: #f5f5f5;
            color: #333;
            line-height: 1.6;
            padding: 20px;
            max-width: 900px;
            margin: 0 auto;
        }
        .home-link { margin-bottom: 12px; display: block; }
        .home-link a { color: #0366d6; text-decoration: none; font-size: 0.9em; }
        .home-link a:hover { text-decoration: underline; }

        h1 { font-size: 1.3em; margin-bottom: 2px; color: #1a1a1a; line-height: 1.3; }

        .lore-link { font-size: 0.85em; margin: 4px 0 6px; display: block; }
        .lore-link a { color: #0366d6; text-decoration: none; }
        .lore-link a:hover { text-decoration: underline; }

        .date-range {
            font-size: 0.8em;
            color: #888;
            margin-bottom: 16px;
        }
        .date-range a { color: #0366d6; text-decoration: none; }
        .date-range a:hover { text-decoration: underline; }

        /* thread-node scroll margin so the card isn't clipped at the top */
        .thread-node { scroll-margin-top: 8px; }

        /* ── Patch summary ──────────────────────────────────────────── */
        .patch-summary-block {
            background: #fff;
            border-radius: 8px;
            padding: 12px 16px;
            margin-bottom: 20px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            border-left: 3px solid #4a90d9;
        }
        .patch-summary-label {
            font-size: 0.72em;
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 0.06em;
            color: #4a90d9;
            margin-bottom: 4px;
        }
        .patch-summary-text {
            font-size: 0.88em;
            color: #444;
            line-height: 1.55;
        }

        /* ── Thread tree ────────────────────────────────────────────── */
        .thread-tree {
            display: flex;
            flex-direction: column;
            gap: 6px;
        }

        /* Depth indentation via left border */
        .thread-node { position: relative; }
        .thread-children {
            margin-left: 20px;
            padding-left: 12px;
            border-left: 2px solid #e0e0e0;
            margin-top: 6px;
            display: flex;
            flex-direction: column;
            gap: 6px;
        }

        /* ── Review comment card ────────────────────────────────────── */
        .review-comment {
            background: #fff;
            border-radius: 6px;
            padding: 10px 14px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            font-size: 0.88em;
        }
        .review-comment-header {
            display: flex;
            flex-wrap: wrap;
            align-items: center;
            gap: 6px;
            margin-bottom: 5px;
        }
        .review-author {
            font-weight: 700;
            color: #1a1a1a;
            font-size: 0.95em;
        }

        /* Date chip — links back to the daily report */
        .date-chip {
            font-size: 0.75em;
            color: #777;
            background: #f0f0f0;
            border-radius: 10px;
            padding: 1px 7px;
            text-decoration: none;
            white-space: nowrap;
        }
        a.date-chip:hover { background: #e0e8f5; color: #0366d6; }

        .badge {
            display: inline-block;
            padding: 1px 8px;
            border-radius: 10px;
            font-size: 0.75em;
            font-weight: 600;
        }
        .inline-review-badge {
            display: inline-block;
            padding: 0 6px;
            border-radius: 8px;
            font-size: 0.78em;
            font-weight: 500;
            background: #e3f2fd;
            color: #1565c0;
        }
        .review-tag-badge {
            display: inline-block;
            padding: 0 6px;
            border-radius: 8px;
            font-size: 0.78em;
            font-weight: 500;
            background: #e8f5e9;
            color: #2e7d32;
        }
        .analysis-source-badge {
            display: inline-block;
            padding: 1px 7px;
            border-radius: 10px;
            font-size: 0.72em;
            font-weight: 600;
            border: 1px solid rgba(0,0,0,0.1);
        }

        .review-comment-text {
            color: #444;
            line-height: 1.55;
            margin-bottom: 4px;
        }
        .review-comment-signals {
            margin-top: 3px;
            font-size: 0.85em;
            color: #aaa;
            font-style: italic;
        }

        /* ── Collapsible raw body ───────────────────────────────────── */
        .raw-body-toggle {
            margin-top: 5px;
            font-size: 0.85em;
        }
        .raw-body-toggle summary {
            cursor: pointer;
            color: #888;
            padding: 2px 0;
            font-weight: 500;
            font-size: 0.9em;
            list-style: none;
        }
        .raw-body-toggle summary::-webkit-details-marker { display: none; }
        .raw-body-toggle summary::before { content: "▶ "; font-size: 0.7em; }
        .raw-body-toggle[open] summary::before { content: "▼ "; }
        .raw-body-toggle summary:hover { color: #555; }
        .raw-body-text {
            white-space: pre-wrap;
            font-size: 0.95em;
            background: #f8f8f8;
            padding: 8px 10px;
            border-radius: 4px;
            max-height: 360px;
            overflow-y: auto;
            margin-top: 4px;
            line-height: 1.5;
            color: #444;
            border: 1px solid #e8e8e8;
        }

        .no-reviews {
            color: #aaa;
            font-size: 0.85em;
            font-style: italic;
            padding: 8px 0;
        }

        footer {
            text-align: center;
            color: #bbb;
            font-size: 0.78em;
            margin-top: 36px;
            padding: 16px;
        }
    </style>
</head>
<body>
    <div class="home-link"><a href="../">&larr; Back to reports</a></div>
    <h1>Re: [PATCH] arm64: remove HAVE_CMPXCHG_LOCAL</h1>
    <div class="lore-link"><a href="https://lore.kernel.org/all/aZY30X9bpsV-nPsm@linux.dev/" target="_blank">View on lore.kernel.org &rarr;</a></div>
    <div class="date-range">Active on: <a href="#2026-02-18">2026-02-18</a></div>
    
    <div class="thread-tree">
<div class="thread-node depth-0" id="2026-02-18">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Dev Jain</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-18">2026-02-18</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Dev Jain suggests that the patch should only remove the selection of the config, but keep the code, as the real issue may be related to preempt_disable() and not LL/SC/LSE or atomics.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">
On 15/02/26 9:09 am, Jisheng Zhang wrote:
&gt; It turns out the generic disable/enable irq this_cpu_cmpxchg
&gt; implementation is faster than LL/SC or lse implementation. Remove
&gt; HAVE_CMPXCHG_LOCAL for better performance on arm64.
&gt;
&gt; Tested on Quad 1.9GHZ CA55 platform:
&gt; average mod_node_page_state() cost decreases from 167ns to 103ns
&gt; the spawn (30 duration) benchmark in unixbench is improved
&gt; from 147494 lps to 150561 lps, improved by 2.1%
&gt;
&gt; Tested on Quad 2.1GHZ CA73 platform:
&gt; average mod_node_page_state() cost decreases from 113ns to 85ns
&gt; the spawn (30 duration) benchmark in unixbench is improved
&gt; from 209844 lps to 212581 lps, improved by 1.3%
&gt;
&gt; Signed-off-by: Jisheng Zhang &lt;jszhang@kernel.org&gt;
&gt; ---

Thanks. This concurs with my investigation on [1]. The problem
isn&#x27;t really LL/SC/LSE but preempt_disable()/enable() in
this_cpu_* [1, 2].

I think you should only remove the selection of the config,
but keep the code? We may want to switch this on again if
the real issue gets solved.

[1] https://lore.kernel.org/all/5a6782f3-d758-4d9c-975b-5ae4b5d80d4e@arm.com/
[2] https://lore.kernel.org/all/CAHbLzkpcN-T8MH6=W3jCxcFj1gVZp8fRqe231yzZT-rV_E_org@mail.gmail.com/

&gt;  arch/arm64/Kconfig              |  1 -
&gt;  arch/arm64/include/asm/percpu.h | 24 ------------------------
&gt;  2 files changed, 25 deletions(-)
&gt;
&gt; diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
&gt; index 38dba5f7e4d2..5e7e2e65d5a5 100644
&gt; --- a/arch/arm64/Kconfig
&gt; +++ b/arch/arm64/Kconfig
&gt; @@ -205,7 +205,6 @@ config ARM64
&gt;  	select HAVE_EBPF_JIT
&gt;  	select HAVE_C_RECORDMCOUNT
&gt;  	select HAVE_CMPXCHG_DOUBLE
&gt; -	select HAVE_CMPXCHG_LOCAL
&gt;  	select HAVE_CONTEXT_TRACKING_USER
&gt;  	select HAVE_DEBUG_KMEMLEAK
&gt;  	select HAVE_DMA_CONTIGUOUS
&gt; diff --git a/arch/arm64/include/asm/percpu.h b/arch/arm64/include/asm/percpu.h
&gt; index b57b2bb00967..70ffe566cb4b 100644
&gt; --- a/arch/arm64/include/asm/percpu.h
&gt; +++ b/arch/arm64/include/asm/percpu.h
&gt; @@ -232,30 +232,6 @@ PERCPU_RET_OP(add, add, ldadd)
&gt;  #define this_cpu_xchg_8(pcp, val)	\
&gt;  	_pcp_protect_return(xchg_relaxed, pcp, val)
&gt;  
&gt; -#define this_cpu_cmpxchg_1(pcp, o, n)	\
&gt; -	_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)
&gt; -#define this_cpu_cmpxchg_2(pcp, o, n)	\
&gt; -	_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)
&gt; -#define this_cpu_cmpxchg_4(pcp, o, n)	\
&gt; -	_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)
&gt; -#define this_cpu_cmpxchg_8(pcp, o, n)	\
&gt; -	_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)
&gt; -
&gt; -#define this_cpu_cmpxchg64(pcp, o, n)	this_cpu_cmpxchg_8(pcp, o, n)
&gt; -
&gt; -#define this_cpu_cmpxchg128(pcp, o, n)					\
&gt; -({									\
&gt; -	typedef typeof(pcp) pcp_op_T__;					\
&gt; -	u128 old__, new__, ret__;					\
&gt; -	pcp_op_T__ *ptr__;						\
&gt; -	old__ = o;							\
&gt; -	new__ = n;							\
&gt; -	preempt_disable_notrace();					\
&gt; -	ptr__ = raw_cpu_ptr(&amp;(pcp));					\
&gt; -	ret__ = cmpxchg128_local((void *)ptr__, old__, new__);		\
&gt; -	preempt_enable_notrace();					\
&gt; -	ret__;								\
&gt; -})
&gt;  
&gt;  #ifdef __KVM_NVHE_HYPERVISOR__
&gt;  extern unsigned long __hyp_per_cpu_offset(unsigned int cpu);


---


On 16/02/26 4:30 pm, Will Deacon wrote:
&gt; On Sun, Feb 15, 2026 at 11:39:44AM +0800, Jisheng Zhang wrote:
&gt;&gt; It turns out the generic disable/enable irq this_cpu_cmpxchg
&gt;&gt; implementation is faster than LL/SC or lse implementation. Remove
&gt;&gt; HAVE_CMPXCHG_LOCAL for better performance on arm64.
&gt;&gt;
&gt;&gt; Tested on Quad 1.9GHZ CA55 platform:
&gt;&gt; average mod_node_page_state() cost decreases from 167ns to 103ns
&gt;&gt; the spawn (30 duration) benchmark in unixbench is improved
&gt;&gt; from 147494 lps to 150561 lps, improved by 2.1%
&gt;&gt;
&gt;&gt; Tested on Quad 2.1GHZ CA73 platform:
&gt;&gt; average mod_node_page_state() cost decreases from 113ns to 85ns
&gt;&gt; the spawn (30 duration) benchmark in unixbench is improved
&gt;&gt; from 209844 lps to 212581 lps, improved by 1.3%
&gt;&gt;
&gt;&gt; Signed-off-by: Jisheng Zhang &lt;jszhang@kernel.org&gt;
&gt;&gt; ---
&gt;&gt;  arch/arm64/Kconfig              |  1 -
&gt;&gt;  arch/arm64/include/asm/percpu.h | 24 ------------------------
&gt;&gt;  2 files changed, 25 deletions(-)
&gt; That is _entirely_ dependent on the system, so this isn&#x27;t the right
&gt; approach. I also don&#x27;t think it&#x27;s something we particularly want to
&gt; micro-optimise to accomodate systems that suck at atomics.

Hi Will,

As I mention in the other email, the suspect is not the atomics, but
preempt_disable(). On Apple M3, the regression reported in [1] resolves
by removing preempt_disable/enable in _pcp_protect_return. To prove
this another way, I disabled CONFIG_ARM64_HAS_LSE_ATOMICS and the
regression worsened, indicating that at least on Apple M3 the
atomics are faster.

It may help to confirm this hypothesis on other hardware - perhaps
Jisheng can test with this change on his hardware and confirm
whether he gets the same performance improvement.

By coincidence, Yang Shi has been discussing the this_cpu_* overhead
at [2].

[1] https://lore.kernel.org/all/1052a452-9ba3-4da7-be47-7d27d27b3d1d@arm.com/
[2] https://lore.kernel.org/all/CAHbLzkpcN-T8MH6=W3jCxcFj1gVZp8fRqe231yzZT-rV_E_org@mail.gmail.com/

&gt;
&gt; Will
&gt;
&gt;&gt; diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
&gt;&gt; index 38dba5f7e4d2..5e7e2e65d5a5 100644
&gt;&gt; --- a/arch/arm64/Kconfig
&gt;&gt; +++ b/arch/arm64/Kconfig
&gt;&gt; @@ -205,7 +205,6 @@ config ARM64
&gt;&gt;  	select HAVE_EBPF_JIT
&gt;&gt;  	select HAVE_C_RECORDMCOUNT
&gt;&gt;  	select HAVE_CMPXCHG_DOUBLE
&gt;&gt; -	select HAVE_CMPXCHG_LOCAL
&gt;&gt;  	select HAVE_CONTEXT_TRACKING_USER
&gt;&gt;  	select HAVE_DEBUG_KMEMLEAK
&gt;&gt;  	select HAVE_DMA_CONTIGUOUS
&gt;&gt; diff --git a/arch/arm64/include/asm/percpu.h b/arch/arm64/include/asm/percpu.h
&gt;&gt; index b57b2bb00967..70ffe566cb4b 100644
&gt;&gt; --- a/arch/arm64/include/asm/percpu.h
&gt;&gt; +++ b/arch/arm64/include/asm/percpu.h
&gt;&gt; @@ -232,30 +232,6 @@ PERCPU_RET_OP(add, add, ldadd)
&gt;&gt;  #define this_cpu_xchg_8(pcp, val)	\
&gt;&gt;  	_pcp_protect_return(xchg_relaxed, pcp, val)
&gt;&gt;  
&gt;&gt; -#define this_cpu_cmpxchg_1(pcp, o, n)	\
&gt;&gt; -	_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)
&gt;&gt; -#define this_cpu_cmpxchg_2(pcp, o, n)	\
&gt;&gt; -	_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)
&gt;&gt; -#define this_cpu_cmpxchg_4(pcp, o, n)	\
&gt;&gt; -	_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)
&gt;&gt; -#define this_cpu_cmpxchg_8(pcp, o, n)	\
&gt;&gt; -	_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)
&gt;&gt; -
&gt;&gt; -#define this_cpu_cmpxchg64(pcp, o, n)	this_cpu_cmpxchg_8(pcp, o, n)
&gt;&gt; -
&gt;&gt; -#define this_cpu_cmpxchg128(pcp, o, n)					\
&gt;&gt; -({									\
&gt;&gt; -	typedef typeof(pcp) pcp_op_T__;					\
&gt;&gt; -	u128 old__, new__, ret__;					\
&gt;&gt; -	pcp_op_T__ *ptr__;						\
&gt;&gt; -	old__ = o;							\
&gt;&gt; -	new__ = n;							\
&gt;&gt; -	preempt_disable_notrace();					\
&gt;&gt; -	ptr__ = raw_cpu_ptr(&amp;(pcp));					\
&gt;&gt; -	ret__ = cmpxchg128_local((void *)ptr__, old__, new__);		\
&gt;&gt; -	preempt_enable_notrace();					\
&gt;&gt; -	ret__;								\
&gt;&gt; -})
&gt;&gt;  
&gt;&gt;  #ifdef __KVM_NVHE_HYPERVISOR__
&gt;&gt;  extern unsigned long __hyp_per_cpu_offset(unsigned int cpu);
&gt;&gt; -- 
&gt;&gt; 2.51.0
&gt;&gt;
</pre>
</details>
<div class="review-comment-signals">Signals: requested changes</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Will Deacon</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-18">2026-02-18</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Will Deacon raised concerns about the patch&#x27;s approach to optimizing atomic operations on arm64, suggesting it is system-dependent and not a good candidate for micro-optimization.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Sun, Feb 15, 2026 at 11:39:44AM +0800, Jisheng Zhang wrote:
&gt; It turns out the generic disable/enable irq this_cpu_cmpxchg
&gt; implementation is faster than LL/SC or lse implementation. Remove
&gt; HAVE_CMPXCHG_LOCAL for better performance on arm64.
&gt; 
&gt; Tested on Quad 1.9GHZ CA55 platform:
&gt; average mod_node_page_state() cost decreases from 167ns to 103ns
&gt; the spawn (30 duration) benchmark in unixbench is improved
&gt; from 147494 lps to 150561 lps, improved by 2.1%
&gt; 
&gt; Tested on Quad 2.1GHZ CA73 platform:
&gt; average mod_node_page_state() cost decreases from 113ns to 85ns
&gt; the spawn (30 duration) benchmark in unixbench is improved
&gt; from 209844 lps to 212581 lps, improved by 1.3%
&gt; 
&gt; Signed-off-by: Jisheng Zhang &lt;jszhang@kernel.org&gt;
&gt; ---
&gt;  arch/arm64/Kconfig              |  1 -
&gt;  arch/arm64/include/asm/percpu.h | 24 ------------------------
&gt;  2 files changed, 25 deletions(-)

That is _entirely_ dependent on the system, so this isn&#x27;t the right
approach. I also don&#x27;t think it&#x27;s something we particularly want to
micro-optimise to accomodate systems that suck at atomics.

Will

&gt; diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
&gt; index 38dba5f7e4d2..5e7e2e65d5a5 100644
&gt; --- a/arch/arm64/Kconfig
&gt; +++ b/arch/arm64/Kconfig
&gt; @@ -205,7 +205,6 @@ config ARM64
&gt;  	select HAVE_EBPF_JIT
&gt;  	select HAVE_C_RECORDMCOUNT
&gt;  	select HAVE_CMPXCHG_DOUBLE
&gt; -	select HAVE_CMPXCHG_LOCAL
&gt;  	select HAVE_CONTEXT_TRACKING_USER
&gt;  	select HAVE_DEBUG_KMEMLEAK
&gt;  	select HAVE_DMA_CONTIGUOUS
&gt; diff --git a/arch/arm64/include/asm/percpu.h b/arch/arm64/include/asm/percpu.h
&gt; index b57b2bb00967..70ffe566cb4b 100644
&gt; --- a/arch/arm64/include/asm/percpu.h
&gt; +++ b/arch/arm64/include/asm/percpu.h
&gt; @@ -232,30 +232,6 @@ PERCPU_RET_OP(add, add, ldadd)
&gt;  #define this_cpu_xchg_8(pcp, val)	\
&gt;  	_pcp_protect_return(xchg_relaxed, pcp, val)
&gt;  
&gt; -#define this_cpu_cmpxchg_1(pcp, o, n)	\
&gt; -	_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)
&gt; -#define this_cpu_cmpxchg_2(pcp, o, n)	\
&gt; -	_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)
&gt; -#define this_cpu_cmpxchg_4(pcp, o, n)	\
&gt; -	_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)
&gt; -#define this_cpu_cmpxchg_8(pcp, o, n)	\
&gt; -	_pcp_protect_return(cmpxchg_relaxed, pcp, o, n)
&gt; -
&gt; -#define this_cpu_cmpxchg64(pcp, o, n)	this_cpu_cmpxchg_8(pcp, o, n)
&gt; -
&gt; -#define this_cpu_cmpxchg128(pcp, o, n)					\
&gt; -({									\
&gt; -	typedef typeof(pcp) pcp_op_T__;					\
&gt; -	u128 old__, new__, ret__;					\
&gt; -	pcp_op_T__ *ptr__;						\
&gt; -	old__ = o;							\
&gt; -	new__ = n;							\
&gt; -	preempt_disable_notrace();					\
&gt; -	ptr__ = raw_cpu_ptr(&amp;(pcp));					\
&gt; -	ret__ = cmpxchg128_local((void *)ptr__, old__, new__);		\
&gt; -	preempt_enable_notrace();					\
&gt; -	ret__;								\
&gt; -})
&gt;  
&gt;  #ifdef __KVM_NVHE_HYPERVISOR__
&gt;  extern unsigned long __hyp_per_cpu_offset(unsigned int cpu);
&gt; -- 
&gt; 2.51.0
&gt; 


---

On Tue, Feb 17, 2026 at 01:53:19PM +0000, Catalin Marinas wrote:
&gt; On Mon, Feb 16, 2026 at 08:59:17PM +0530, Dev Jain wrote:
&gt; &gt; On 16/02/26 4:30 pm, Will Deacon wrote:
&gt; &gt; &gt; On Sun, Feb 15, 2026 at 11:39:44AM +0800, Jisheng Zhang wrote:
&gt; &gt; &gt;&gt; It turns out the generic disable/enable irq this_cpu_cmpxchg
&gt; &gt; &gt;&gt; implementation is faster than LL/SC or lse implementation. Remove
&gt; &gt; &gt;&gt; HAVE_CMPXCHG_LOCAL for better performance on arm64.
&gt; &gt; &gt;&gt;
&gt; &gt; &gt;&gt; Tested on Quad 1.9GHZ CA55 platform:
&gt; &gt; &gt;&gt; average mod_node_page_state() cost decreases from 167ns to 103ns
&gt; &gt; &gt;&gt; the spawn (30 duration) benchmark in unixbench is improved
&gt; &gt; &gt;&gt; from 147494 lps to 150561 lps, improved by 2.1%
&gt; &gt; &gt;&gt;
&gt; &gt; &gt;&gt; Tested on Quad 2.1GHZ CA73 platform:
&gt; &gt; &gt;&gt; average mod_node_page_state() cost decreases from 113ns to 85ns
&gt; &gt; &gt;&gt; the spawn (30 duration) benchmark in unixbench is improved
&gt; &gt; &gt;&gt; from 209844 lps to 212581 lps, improved by 1.3%
&gt; &gt; &gt;&gt;
&gt; &gt; &gt;&gt; Signed-off-by: Jisheng Zhang &lt;jszhang@kernel.org&gt;
&gt; &gt; &gt;&gt; ---
&gt; &gt; &gt;&gt;  arch/arm64/Kconfig              |  1 -
&gt; &gt; &gt;&gt;  arch/arm64/include/asm/percpu.h | 24 ------------------------
&gt; &gt; &gt;&gt;  2 files changed, 25 deletions(-)
&gt; &gt; &gt; That is _entirely_ dependent on the system, so this isn&#x27;t the right
&gt; &gt; &gt; approach. I also don&#x27;t think it&#x27;s something we particularly want to
&gt; &gt; &gt; micro-optimise to accomodate systems that suck at atomics.
&gt; &gt; 
&gt; &gt; Hi Will,
&gt; &gt; 
&gt; &gt; As I mention in the other email, the suspect is not the atomics, but
&gt; &gt; preempt_disable(). On Apple M3, the regression reported in [1] resolves
&gt; &gt; by removing preempt_disable/enable in _pcp_protect_return. To prove
&gt; &gt; this another way, I disabled CONFIG_ARM64_HAS_LSE_ATOMICS and the
&gt; &gt; regression worsened, indicating that at least on Apple M3 the
&gt; &gt; atomics are faster.
&gt; 
&gt; Then why don&#x27;t we replace the preempt disabling with local_irq_save()
&gt; in the arm64 code and still use the LSE atomics?

Even better, work on making preempt_disable() faster as it&#x27;s used in many
other places. Of course, if people want to hack the .config, they could
also change the preemption mode...

Will
</pre>
</details>
<div class="review-comment-signals">Signals: requested changes</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Catalin Marinas</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-18">2026-02-18</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The reviewer raised concerns about the patch removing HAVE_CMPXCHG_LOCAL, suggesting that using local_irq_save() instead of preempt_disable_notrace() would be a better approach to avoid potential issues with atomicity and preemption.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Mon, Feb 16, 2026 at 08:59:17PM +0530, Dev Jain wrote:
&gt; On 16/02/26 4:30 pm, Will Deacon wrote:
&gt; &gt; On Sun, Feb 15, 2026 at 11:39:44AM +0800, Jisheng Zhang wrote:
&gt; &gt;&gt; It turns out the generic disable/enable irq this_cpu_cmpxchg
&gt; &gt;&gt; implementation is faster than LL/SC or lse implementation. Remove
&gt; &gt;&gt; HAVE_CMPXCHG_LOCAL for better performance on arm64.
&gt; &gt;&gt;
&gt; &gt;&gt; Tested on Quad 1.9GHZ CA55 platform:
&gt; &gt;&gt; average mod_node_page_state() cost decreases from 167ns to 103ns
&gt; &gt;&gt; the spawn (30 duration) benchmark in unixbench is improved
&gt; &gt;&gt; from 147494 lps to 150561 lps, improved by 2.1%
&gt; &gt;&gt;
&gt; &gt;&gt; Tested on Quad 2.1GHZ CA73 platform:
&gt; &gt;&gt; average mod_node_page_state() cost decreases from 113ns to 85ns
&gt; &gt;&gt; the spawn (30 duration) benchmark in unixbench is improved
&gt; &gt;&gt; from 209844 lps to 212581 lps, improved by 1.3%
&gt; &gt;&gt;
&gt; &gt;&gt; Signed-off-by: Jisheng Zhang &lt;jszhang@kernel.org&gt;
&gt; &gt;&gt; ---
&gt; &gt;&gt;  arch/arm64/Kconfig              |  1 -
&gt; &gt;&gt;  arch/arm64/include/asm/percpu.h | 24 ------------------------
&gt; &gt;&gt;  2 files changed, 25 deletions(-)
&gt; &gt; That is _entirely_ dependent on the system, so this isn&#x27;t the right
&gt; &gt; approach. I also don&#x27;t think it&#x27;s something we particularly want to
&gt; &gt; micro-optimise to accomodate systems that suck at atomics.
&gt; 
&gt; Hi Will,
&gt; 
&gt; As I mention in the other email, the suspect is not the atomics, but
&gt; preempt_disable(). On Apple M3, the regression reported in [1] resolves
&gt; by removing preempt_disable/enable in _pcp_protect_return. To prove
&gt; this another way, I disabled CONFIG_ARM64_HAS_LSE_ATOMICS and the
&gt; regression worsened, indicating that at least on Apple M3 the
&gt; atomics are faster.

Then why don&#x27;t we replace the preempt disabling with local_irq_save()
in the arm64 code and still use the LSE atomics?

IIUC (lots of macro indirection), the generic cmpxchg is not atomic, so
another CPU is allowed to mess this up if it accesses current CPU&#x27;s
variable via per_cpu_ptr().

-- 
Catalin


---

On Tue, Feb 17, 2026 at 03:00:22PM +0000, Will Deacon wrote:
&gt; On Tue, Feb 17, 2026 at 01:53:19PM +0000, Catalin Marinas wrote:
&gt; &gt; On Mon, Feb 16, 2026 at 08:59:17PM +0530, Dev Jain wrote:
&gt; &gt; &gt; On 16/02/26 4:30 pm, Will Deacon wrote:
&gt; &gt; &gt; &gt; On Sun, Feb 15, 2026 at 11:39:44AM +0800, Jisheng Zhang wrote:
&gt; &gt; &gt; &gt;&gt; It turns out the generic disable/enable irq this_cpu_cmpxchg
&gt; &gt; &gt; &gt;&gt; implementation is faster than LL/SC or lse implementation. Remove
&gt; &gt; &gt; &gt;&gt; HAVE_CMPXCHG_LOCAL for better performance on arm64.
&gt; &gt; &gt; &gt;&gt;
&gt; &gt; &gt; &gt;&gt; Tested on Quad 1.9GHZ CA55 platform:
&gt; &gt; &gt; &gt;&gt; average mod_node_page_state() cost decreases from 167ns to 103ns
&gt; &gt; &gt; &gt;&gt; the spawn (30 duration) benchmark in unixbench is improved
&gt; &gt; &gt; &gt;&gt; from 147494 lps to 150561 lps, improved by 2.1%
&gt; &gt; &gt; &gt;&gt;
&gt; &gt; &gt; &gt;&gt; Tested on Quad 2.1GHZ CA73 platform:
&gt; &gt; &gt; &gt;&gt; average mod_node_page_state() cost decreases from 113ns to 85ns
&gt; &gt; &gt; &gt;&gt; the spawn (30 duration) benchmark in unixbench is improved
&gt; &gt; &gt; &gt;&gt; from 209844 lps to 212581 lps, improved by 1.3%
[...]
&gt; &gt; &gt; &gt; That is _entirely_ dependent on the system, so this isn&#x27;t the right
&gt; &gt; &gt; &gt; approach. I also don&#x27;t think it&#x27;s something we particularly want to
&gt; &gt; &gt; &gt; micro-optimise to accomodate systems that suck at atomics.
&gt; &gt; &gt; 
&gt; &gt; &gt; As I mention in the other email, the suspect is not the atomics, but
&gt; &gt; &gt; preempt_disable(). On Apple M3, the regression reported in [1] resolves
&gt; &gt; &gt; by removing preempt_disable/enable in _pcp_protect_return. To prove
&gt; &gt; &gt; this another way, I disabled CONFIG_ARM64_HAS_LSE_ATOMICS and the
&gt; &gt; &gt; regression worsened, indicating that at least on Apple M3 the
&gt; &gt; &gt; atomics are faster.
&gt; &gt; 
&gt; &gt; Then why don&#x27;t we replace the preempt disabling with local_irq_save()
&gt; &gt; in the arm64 code and still use the LSE atomics?
&gt; 
&gt; Even better, work on making preempt_disable() faster as it&#x27;s used in many
&gt; other places.

Yes, that would be good. It&#x27;s the preempt_enable_notrace() path that
ends up calling preempt_schedule_notrace() -&gt; __schedule() pretty much
unconditionally. Not sure what would go wrong but some simple change
like this (can be done at a higher in the preempt macros to even avoid
getting here):

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 854984967fe2..d9a5d6438303 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -7119,7 +7119,7 @@ asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)
 	if (likely(!preemptible()))
 		return;
 
-	do {
+	while (need_resched()) {
 		/*
 		 * Because the function tracer can trace preempt_count_sub()
 		 * and it also uses preempt_enable/disable_notrace(), if
@@ -7146,7 +7146,7 @@ asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)
 
 		preempt_latency_stop(1);
 		preempt_enable_no_resched_notrace();
-	} while (need_resched());
+	}
 }
 EXPORT_SYMBOL_GPL(preempt_schedule_notrace);
 

Of course, changing the preemption model solves this by making the
macros no-ops but I assume people want to keep preemption on.

-- 
Catalin


---

Hi Prateek,

On Wed, Feb 18, 2026 at 09:31:19AM +0530, K Prateek Nayak wrote:
&gt; On 2/17/2026 10:18 PM, Catalin Marinas wrote:
&gt; &gt; Yes, that would be good. It&#x27;s the preempt_enable_notrace() path that
&gt; &gt; ends up calling preempt_schedule_notrace() -&gt; __schedule() pretty much
&gt; &gt; unconditionally.
&gt; 
&gt; What do you mean by unconditionally? We always check
&gt; __preempt_count_dec_and_test() before calling into __schedule().
&gt; 
&gt; On x86, We use MSB of preempt_count to indicate a resched and
&gt; set_preempt_need_resched() would just clear this MSB.
&gt; 
&gt; If the preempt_count() turns 0, we immediately go into schedule
&gt; or  or the next preempt_enable() -&gt; __preempt_count_dec_and_test()
&gt; would see the entire preempt_count being clear and will call into
&gt; schedule.
&gt; 
&gt; The arm64 implementation seems to be doing something similar too
&gt; with a separate &quot;ti-&gt;preempt.need_resched&quot; bit which is part of
&gt; the &quot;ti-&gt;preempt_count&quot;&#x27;s union so it isn&#x27;t really unconditional.

Ah, yes, you are right. I got the polarity of need_resched in
thread_info wrong (we should have named it no_need_to_resched).

So in the common case, the overhead is caused by the additional
pointer chase and preempt_count update, on top of the cpu offset read.
Not sure we can squeeze any more cycles out of these without some
large overhaul like:

https://git.kernel.org/mark/c/84ee5f23f93d4a650e828f831da9ed29c54623c5

or Yang&#x27;s per-CPU page tables. Well, there are more ideas like in-kernel
restartable sequences but they move the overhead elsewhere.

Thanks.

-- 
Catalin
</pre>
</details>
<div class="review-comment-signals">Signals: requested changes</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Christoph (Ampere)</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-18">2026-02-18</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Christoph raised concerns that the patch may cause regressions on certain platforms, citing his own measurements from 2 years ago, but noted that preempt_disable/enable overhead is not incurred in production systems.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Mon, 16 Feb 2026, Dev Jain wrote:

&gt; By coincidence, Yang Shi has been discussing the this_cpu_* overhead
&gt; at [2].


Yang Shi is on vacation but we have a patchset that removes
preempt_enable/disable from this_cpu operations on ARM64.

The performance of cmpxchg varies by platform in use and with the kernel
config. The measurements that I did 2 years ago indicated that the cmpxchg
use with Ampere processors did not cause a regression.

Note that distro kernels often do not enable PREEMPT_FULL and therefore
preempt_disable/enable overhead is not incurred in production systems.

PREEMPT_VOLUNTARY does not use preemption for this_cpu ops.

</pre>
</details>
<div class="review-comment-signals">Signals: regressions, platform-specific</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">K Nayak</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-18">2026-02-18</a>
<span class="inline-review-badge">Inline Review</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer K Nayak questioned the patch&#x27;s description of removing HAVE_CMPXCHG_LOCAL as &#x27;unconditional&#x27;, pointing out that arm64 implementation checks preempt_count before calling __schedule() and has a similar need_resched bit.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Hello Catalin,

On 2/17/2026 10:18 PM, Catalin Marinas wrote:
&gt; Yes, that would be good. It&#x27;s the preempt_enable_notrace() path that
&gt; ends up calling preempt_schedule_notrace() -&gt; __schedule() pretty much
&gt; unconditionally.

What do you mean by unconditionally? We always check
__preempt_count_dec_and_test() before calling into __schedule().

On x86, We use MSB of preempt_count to indicate a resched and
set_preempt_need_resched() would just clear this MSB.

If the preempt_count() turns 0, we immediately go into schedule
or  or the next preempt_enable() -&gt; __preempt_count_dec_and_test()
would see the entire preempt_count being clear and will call into
schedule.

The arm64 implementation seems to be doing something similar too
with a separate &quot;ti-&gt;preempt.need_resched&quot; bit which is part of
the &quot;ti-&gt;preempt_count&quot;&#x27;s union so it isn&#x27;t really unconditional.

&gt; Not sure what would go wrong but some simple change
&gt; like this (can be done at a higher in the preempt macros to even avoid
&gt; getting here):
&gt; 
&gt; diff --git a/kernel/sched/core.c b/kernel/sched/core.c
&gt; index 854984967fe2..d9a5d6438303 100644
&gt; --- a/kernel/sched/core.c
&gt; +++ b/kernel/sched/core.c
&gt; @@ -7119,7 +7119,7 @@ asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)
&gt;  	if (likely(!preemptible()))
&gt;  		return;
&gt;  
&gt; -	do {
&gt; +	while (need_resched()) {

Essentially you are simply checking it twice now on entry since
need_resched() state would have already been communicated by
__preempt_count_dec_and_test().

&gt;  		/*
&gt;  		 * Because the function tracer can trace preempt_count_sub()
&gt;  		 * and it also uses preempt_enable/disable_notrace(), if
&gt; @@ -7146,7 +7146,7 @@ asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)
&gt;  
&gt;  		preempt_latency_stop(1);
&gt;  		preempt_enable_no_resched_notrace();
&gt; -	} while (need_resched());
&gt; +	}
&gt;  }
&gt;  EXPORT_SYMBOL_GPL(preempt_schedule_notrace);
-- 
Thanks and Regards,
Prateek

</pre>
</details>
<div class="review-comment-signals">Signals: requested changes</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Shakeel Butt</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-18">2026-02-18</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Shakeel Butt raised a concern that mod_node_page_state() can be called in NMI context, and the generic disable/enable irq implementation is not safe against NMIs.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Sun, Feb 15, 2026 at 11:39:44AM +0800, Jisheng Zhang wrote:
&gt; It turns out the generic disable/enable irq this_cpu_cmpxchg
&gt; implementation is faster than LL/SC or lse implementation. Remove
&gt; HAVE_CMPXCHG_LOCAL for better performance on arm64.
&gt; 
&gt; Tested on Quad 1.9GHZ CA55 platform:
&gt; average mod_node_page_state() cost decreases from 167ns to 103ns
&gt; the spawn (30 duration) benchmark in unixbench is improved
&gt; from 147494 lps to 150561 lps, improved by 2.1%
&gt; 
&gt; Tested on Quad 2.1GHZ CA73 platform:
&gt; average mod_node_page_state() cost decreases from 113ns to 85ns
&gt; the spawn (30 duration) benchmark in unixbench is improved
&gt; from 209844 lps to 212581 lps, improved by 1.3%
&gt; 
&gt; Signed-off-by: Jisheng Zhang &lt;jszhang@kernel.org&gt;

Please note that mod_node_page_state() can be called in NMI context and
generic disable/enable irq are not safe against NMIs (newer arm arch supports
NMI).


</pre>
</details>
<div class="review-comment-signals">Signals: concern about NMI safety</div>
</div>
</div>
</div>

    <footer>LKML Daily Activity Tracker</footer>
    <script>
    // When arriving via a date anchor (e.g. #2026-02-15 from a daily report),
    // scroll the anchor into view after a brief delay so layout is complete.
    (function () {
        var hash = window.location.hash;
        if (!hash) return;
        var target = document.getElementById(hash.slice(1));
        if (!target) return;
        setTimeout(function () {
            target.scrollIntoView({behavior: 'smooth', block: 'start'});
        }, 80);
    })();
    </script>
</body>
</html>