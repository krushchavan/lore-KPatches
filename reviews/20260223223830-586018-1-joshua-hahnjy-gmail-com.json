{
  "thread_id": "20260223223830.586018-1-joshua.hahnjy@gmail.com",
  "subject": "[RFC PATCH 0/6] mm/memcontrol: Make memcg limits tier-aware",
  "url": "https://lore.kernel.org/all/20260223223830.586018-1-joshua.hahnjy@gmail.com/",
  "dates": {
    "2026-02-23": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Joshua Hahn",
      "reviews": [
        {
          "author": "Joshua Hahn (author)",
          "summary": "The author addressed a concern about the performance impact of relying on per-memcg-lruvec statistics for limit checking, explaining that introducing a new cacheline in struct page_counter to track tiered memory limits and usage would reduce latency. The author confirmed that this approach is being taken instead of using lruvec stats.",
          "sentiment": "positive",
          "sentiment_signals": [
            "acknowledged a concern",
            "confirmed an alternative solution"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "On systems with tiered memory, there is currently no tracking of memory\nat the tier-memcg granularity. While per-memcg-lruvec serves at a finer\ngranularity that can be accumulated to give us the desired\nper-tier-memcg accounting, relying on these lruvec stats for limit\nchecking can prove touch too many hot paths too frequently and can\nintroduce increased latency for other memcg users.\n\nInstead, add a new cacheline in struct page_counter to track toptier\nmemcg limits and usage, as well as cached capacity values. This\ncacheline is only used by the mem_cgroup->memory page_counter.\n\nAlso, introduce helpers that use these new fields to calculate\nproportional toptier high and low values, based on the system's\ntoptier:total capacity ratio.\n\nSigned-off-by: Joshua Hahn <joshua.hahnjy@gmail.com>\n---\n include/linux/page_counter.h | 22 +++++++++++++++++++++-\n mm/page_counter.c            | 34 ++++++++++++++++++++++++++++++++++\n 2 files changed, 55 insertions(+), 1 deletion(-)\n\ndiff --git a/include/linux/page_counter.h b/include/linux/page_counter.h\nindex d649b6bbbc87..128c1272c88c 100644\n--- a/include/linux/page_counter.h\n+++ b/include/linux/page_counter.h\n@@ -5,6 +5,7 @@\n #include <linux/atomic.h>\n #include <linux/cache.h>\n #include <linux/limits.h>\n+#include <linux/nodemask.h>\n #include <asm/page.h>\n \n struct page_counter {\n@@ -31,9 +32,23 @@ struct page_counter {\n \t/* Latest cg2 reset watermark */\n \tunsigned long local_watermark;\n \n-\t/* Keep all the read most fields in a separete cacheline. */\n+\t/* Keep all the tiered memory fields in a separate cacheline. */\n \tCACHELINE_PADDING(_pad2_);\n \n+\tatomic_long_t toptier_usage;\n+\n+\t/* effective toptier-proportional low protection */\n+\tunsigned long etoptier_low;\n+\tatomic_long_t toptier_low_usage;\n+\tatomic_long_t children_toptier_low_usage;\n+\n+\t/* Cached toptier capacity for proportional limit calculations */\n+\tunsigned long toptier_capacity;\n+\tunsigned long total_capacity;\n+\n+\t/* Keep all the read most fields in a separate cacheline. */\n+\tCACHELINE_PADDING(_pad3_);\n+\n \tbool protection_support;\n \tbool track_failcnt;\n \tunsigned long min;\n@@ -61,6 +76,9 @@ static inline void page_counter_init(struct page_counter *counter,\n \tcounter->parent = parent;\n \tcounter->protection_support = protection_support;\n \tcounter->track_failcnt = false;\n+\tcounter->toptier_usage = (atomic_long_t)ATOMIC_LONG_INIT(0);\n+\tcounter->toptier_capacity = 0;\n+\tcounter->total_capacity = 0;\n }\n \n static inline unsigned long page_counter_read(struct page_counter *counter)\n@@ -103,6 +121,8 @@ static inline void page_counter_reset_watermark(struct page_counter *counter)\n void page_counter_calculate_protection(struct page_counter *root,\n \t\t\t\t       struct page_counter *counter,\n \t\t\t\t       bool recursive_protection);\n+unsigned long page_counter_toptier_high(struct page_counter *counter);\n+unsigned long page_counter_toptier_low(struct page_counter *counter);\n #else\n static inline void page_counter_calculate_protection(struct page_counter *root,\n \t\t\t\t\t\t     struct page_counter *counter,\ndiff --git a/mm/page_counter.c b/mm/page_counter.c\nindex 661e0f2a5127..5ec97811c418 100644\n--- a/mm/page_counter.c\n+++ b/mm/page_counter.c\n@@ -462,4 +462,38 @@ void page_counter_calculate_protection(struct page_counter *root,\n \t\t\tatomic_long_read(&parent->children_low_usage),\n \t\t\trecursive_protection));\n }\n+\n+unsigned long page_counter_toptier_high(struct page_counter *counter)\n+{\n+\tunsigned long high = READ_ONCE(counter->high);\n+\tunsigned long toptier_cap, total_cap;\n+\n+\tif (high == PAGE_COUNTER_MAX)\n+\t\treturn PAGE_COUNTER_MAX;\n+\n+\ttoptier_cap = counter->toptier_capacity;\n+\ttotal_cap = counter->total_capacity;\n+\n+\tif (!total_cap)\n+\t\treturn PAGE_COUNTER_MAX;\n+\n+\treturn mult_frac(high, toptier_cap, total_cap);\n+}\n+\n+unsigned long page_counter_toptier_low(struct page_counter *counter)\n+{\n+\tunsigned long low = READ_ONCE(counter->low);\n+\tunsigned long toptier_cap, total_cap;\n+\n+\tif (!low)\n+\t\treturn 0;\n+\n+\ttoptier_cap = counter->toptier_capacity;\n+\ttotal_cap = counter->total_capacity;\n+\n+\tif (!total_cap)\n+\t\treturn 0;\n+\n+\treturn mult_frac(low, toptier_cap, total_cap);\n+}\n #endif /* CONFIG_MEMCG || CONFIG_CGROUP_DMEM */\n-- \n2.47.3",
          "reply_to": "",
          "message_date": "2026-02-23",
          "message_id": ""
        },
        {
          "author": "Joshua Hahn (author)",
          "summary": "The author addressed a concern about updating toptier statistics after charging or uncharging memory control groups (memcgs). They modified the `charge_memcg` function to update toptier fields only when charging succeeds, and added new functions `memcg_charge_toptier` and `memcg_uncharge_toptier` to handle this. The author also updated other functions like `uncharge_batch`, `uncharge_folio`, and `mem_cgroup_replace_folio` to account for toptier usage.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged a fix is needed",
            "modified code"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Modify memcg charging and uncharging sites to also update toptier\nstatistics.\n\nUnfortunately, try_charge_memcg is unaware of the physical folio being\ncharged; it only deals with nr_pages. Instead of modifying\ntry_charge_memcg, instead adjust the toptier fields once\ntry_charge_memcg succeeds, inside charge_memcg.\n\nSigned-off-by: Joshua Hahn <joshua.hahnjy@gmail.com>\n---\n mm/memcontrol.c | 39 +++++++++++++++++++++++++++++++++++++++\n 1 file changed, 39 insertions(+)\n\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex f3e4a6ce7181..07464f02c529 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -1948,6 +1948,24 @@ static void memcg_uncharge(struct mem_cgroup *memcg, unsigned int nr_pages)\n \t\tpage_counter_uncharge(&memcg->memsw, nr_pages);\n }\n \n+static void memcg_charge_toptier(struct mem_cgroup *memcg,\n+\t\t\t\t unsigned long nr_pages)\n+{\n+\tstruct page_counter *c;\n+\n+\tfor (c = &memcg->memory; c; c = c->parent)\n+\t\tatomic_long_add(nr_pages, &c->toptier_usage);\n+}\n+\n+static void memcg_uncharge_toptier(struct mem_cgroup *memcg,\n+\t\t\t\t   unsigned long nr_pages)\n+{\n+\tstruct page_counter *c;\n+\n+\tfor (c = &memcg->memory; c; c = c->parent)\n+\t\tatomic_long_sub(nr_pages, &c->toptier_usage);\n+}\n+\n /*\n  * Returns stocks cached in percpu and reset cached information.\n  */\n@@ -4830,6 +4848,9 @@ static int charge_memcg(struct folio *folio, struct mem_cgroup *memcg,\n \tif (ret)\n \t\tgoto out;\n \n+\tif (node_is_toptier(folio_nid(folio)))\n+\t\tmemcg_charge_toptier(memcg, folio_nr_pages(folio));\n+\n \tcss_get(&memcg->css);\n \tcommit_charge(folio, memcg);\n \tmemcg1_commit_charge(folio, memcg);\n@@ -4921,6 +4942,7 @@ int mem_cgroup_swapin_charge_folio(struct folio *folio, struct mm_struct *mm,\n struct uncharge_gather {\n \tstruct mem_cgroup *memcg;\n \tunsigned long nr_memory;\n+\tunsigned long nr_toptier;\n \tunsigned long pgpgout;\n \tunsigned long nr_kmem;\n \tint nid;\n@@ -4941,6 +4963,8 @@ static void uncharge_batch(const struct uncharge_gather *ug)\n \t\t}\n \t\tmemcg1_oom_recover(ug->memcg);\n \t}\n+\tif (ug->nr_toptier)\n+\t\tmemcg_uncharge_toptier(ug->memcg, ug->nr_toptier);\n \n \tmemcg1_uncharge_batch(ug->memcg, ug->pgpgout, ug->nr_memory, ug->nid);\n \n@@ -4989,6 +5013,9 @@ static void uncharge_folio(struct folio *folio, struct uncharge_gather *ug)\n \n \tnr_pages = folio_nr_pages(folio);\n \n+\tif (node_is_toptier(folio_nid(folio)))\n+\t\tug->nr_toptier += nr_pages;\n+\n \tif (folio_memcg_kmem(folio)) {\n \t\tug->nr_memory += nr_pages;\n \t\tug->nr_kmem += nr_pages;\n@@ -5072,6 +5099,10 @@ void mem_cgroup_replace_folio(struct folio *old, struct folio *new)\n \t\t\tpage_counter_charge(&memcg->memsw, nr_pages);\n \t}\n \n+\t/* The old folio's toptier_usage will be decremented when it is freed */\n+\tif (node_is_toptier(folio_nid(new)))\n+\t\tmemcg_charge_toptier(memcg, nr_pages);\n+\n \tcss_get(&memcg->css);\n \tcommit_charge(new, memcg);\n \tmemcg1_commit_charge(new, memcg);\n@@ -5091,6 +5122,7 @@ void mem_cgroup_replace_folio(struct folio *old, struct folio *new)\n void mem_cgroup_migrate(struct folio *old, struct folio *new)\n {\n \tstruct mem_cgroup *memcg;\n+\tint old_toptier, new_toptier;\n \n \tVM_BUG_ON_FOLIO(!folio_test_locked(old), old);\n \tVM_BUG_ON_FOLIO(!folio_test_locked(new), new);\n@@ -5111,6 +5143,13 @@ void mem_cgroup_migrate(struct folio *old, struct folio *new)\n \tif (!memcg)\n \t\treturn;\n \n+\told_toptier = node_is_toptier(folio_nid(old));\n+\tnew_toptier = node_is_toptier(folio_nid(new));\n+\tif (old_toptier && !new_toptier)\n+\t\tmemcg_uncharge_toptier(memcg, folio_nr_pages(old));\n+\telse if (!old_toptier && new_toptier)\n+\t\tmemcg_charge_toptier(memcg, folio_nr_pages(old));\n+\n \t/* Transfer the charge and the css ref */\n \tcommit_charge(new, memcg);\n \n-- \n2.47.3",
          "reply_to": "",
          "message_date": "2026-02-23",
          "message_id": ""
        },
        {
          "author": "Joshua Hahn (author)",
          "summary": "Author addressed a concern about the dynamic nature of toptier nodes and introduced functions to calculate and update toptier capacity during cpuset.mems changes and memory hotplug events.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "acknowledged a need for dynamic updates",
            "introduced new functions"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "What a memcg considers to be a valid toptier node is defined by three\ncriteria: (1) The node has CPUs, (2) The node has online memory,\nand (3) The node is within the cgroup's cpuset.mems.\n\nOf the three, the second and third criteria are the only ones that can\nchange dynamically during runtime, via memory hotplug events and\ncpuset.mems changes, respectively.\n\nIntroduce functions to calculate and update toptier capacity, and call\nthem during cpuset.mems changes and memory hotplug events.\n\nSigned-off-by: Joshua Hahn <joshua.hahnjy@gmail.com>\n---\n include/linux/memcontrol.h   |  6 ++++++\n include/linux/memory-tiers.h | 29 +++++++++++++++++++++++++\n include/linux/page_counter.h |  2 ++\n kernel/cgroup/cpuset.c       |  2 +-\n mm/memcontrol.c              | 17 +++++++++++++++\n mm/memory-tiers.c            | 41 ++++++++++++++++++++++++++++++++++++\n mm/page_counter.c            |  8 +++++++\n 7 files changed, 104 insertions(+), 1 deletion(-)\n\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex 5173a9f16721..900a36112b62 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -608,6 +608,8 @@ static inline void mem_cgroup_protection(struct mem_cgroup *root,\n void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n \t\t\t\t     struct mem_cgroup *memcg);\n \n+void update_memcg_toptier_capacity(void);\n+\n static inline bool mem_cgroup_unprotected(struct mem_cgroup *target,\n \t\t\t\t\t  struct mem_cgroup *memcg)\n {\n@@ -1116,6 +1118,10 @@ static inline void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n {\n }\n \n+static inline void update_memcg_toptier_capacity(void)\n+{\n+}\n+\n static inline bool mem_cgroup_unprotected(struct mem_cgroup *target,\n \t\t\t\t\t  struct mem_cgroup *memcg)\n {\ndiff --git a/include/linux/memory-tiers.h b/include/linux/memory-tiers.h\nindex 85440473effb..cf616885e0db 100644\n--- a/include/linux/memory-tiers.h\n+++ b/include/linux/memory-tiers.h\n@@ -53,6 +53,9 @@ int mt_perf_to_adistance(struct access_coordinate *perf, int *adist);\n struct memory_dev_type *mt_find_alloc_memory_type(int adist,\n \t\t\t\t\t\t  struct list_head *memory_types);\n void mt_put_memory_types(struct list_head *memory_types);\n+void mt_get_toptier_nodemask(nodemask_t *mask, const nodemask_t *allowed);\n+unsigned long mt_get_toptier_capacity(const nodemask_t *allowed);\n+unsigned long mt_get_total_capacity(const nodemask_t *allowed);\n #ifdef CONFIG_MIGRATION\n int next_demotion_node(int node, const nodemask_t *allowed_mask);\n void node_get_allowed_targets(pg_data_t *pgdat, nodemask_t *targets);\n@@ -152,5 +155,31 @@ static inline struct memory_dev_type *mt_find_alloc_memory_type(int adist,\n static inline void mt_put_memory_types(struct list_head *memory_types)\n {\n }\n+\n+static inline void mt_get_toptier_nodemask(nodemask_t *mask,\n+\t\t\t\t\t   const nodemask_t *allowed)\n+{\n+\t*mask = node_states[N_MEMORY];\n+\tif (allowed)\n+\t\tnodes_and(*mask, *mask, *allowed);\n+}\n+\n+static inline unsigned long mt_get_toptier_capacity(const nodemask_t *allowed)\n+{\n+\tint nid;\n+\tunsigned long capacity = 0;\n+\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tif (allowed && !node_isset(nid, *allowed))\n+\t\t\tcontinue;\n+\t\tcapacity += NODE_DATA(nid)->node_present_pages;\n+\t}\n+\treturn capacity;\n+}\n+\n+static inline unsigned long mt_get_total_capacity(const nodemask_t *allowed)\n+{\n+\treturn mt_get_toptier_capacity(allowed);\n+}\n #endif\t/* CONFIG_NUMA */\n #endif  /* _LINUX_MEMORY_TIERS_H */\ndiff --git a/include/linux/page_counter.h b/include/linux/page_counter.h\nindex 128c1272c88c..ada5f1dd75d4 100644\n--- a/include/linux/page_counter.h\n+++ b/include/linux/page_counter.h\n@@ -121,6 +121,8 @@ static inline void page_counter_reset_watermark(struct page_counter *counter)\n void page_counter_calculate_protection(struct page_counter *root,\n \t\t\t\t       struct page_counter *counter,\n \t\t\t\t       bool recursive_protection);\n+void page_counter_update_toptier_capacity(struct page_counter *counter,\n+\t\t\t\t\t  const nodemask_t *allowed);\n unsigned long page_counter_toptier_high(struct page_counter *counter);\n unsigned long page_counter_toptier_low(struct page_counter *counter);\n #else\ndiff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c\nindex 7607dfe516e6..e5641dc1af88 100644\n--- a/kernel/cgroup/cpuset.c\n+++ b/kernel/cgroup/cpuset.c\n@@ -2620,7 +2620,6 @@ static void update_nodemasks_hier(struct cpuset *cs, nodemask_t *new_mems)\n \trcu_read_lock();\n \tcpuset_for_each_descendant_pre(cp, pos_css, cs) {\n \t\tstruct cpuset *parent = parent_cs(cp);\n-\n \t\tbool has_mems = nodes_and(*new_mems, cp->mems_allowed, parent->effective_mems);\n \n \t\t/*\n@@ -2701,6 +2700,7 @@ static int update_nodemask(struct cpuset *cs, struct cpuset *trialcs,\n \n \t/* use trialcs->mems_allowed as a temp variable */\n \tupdate_nodemasks_hier(cs, &trialcs->mems_allowed);\n+\tupdate_memcg_toptier_capacity();\n \treturn 0;\n }\n \ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 0be1e823d813..f3e4a6ce7181 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -54,6 +54,7 @@\n #include <linux/seq_file.h>\n #include <linux/vmpressure.h>\n #include <linux/memremap.h>\n+#include <linux/memory-tiers.h>\n #include <linux/mm_inline.h>\n #include <linux/swap_cgroup.h>\n #include <linux/cpu.h>\n@@ -3906,6 +3907,7 @@ mem_cgroup_css_alloc(struct cgroup_subsys_state *parent_css)\n \n \t\tpage_counter_init(&memcg->memory, &parent->memory, memcg_on_dfl);\n \t\tpage_counter_init(&memcg->swap, &parent->swap, false);\n+\t\tpage_counter_update_toptier_capacity(&memcg->memory, NULL);\n #ifdef CONFIG_MEMCG_V1\n \t\tmemcg->memory.track_failcnt = !memcg_on_dfl;\n \t\tWRITE_ONCE(memcg->oom_kill_disable, READ_ONCE(parent->oom_kill_disable));\n@@ -3917,6 +3919,7 @@ mem_cgroup_css_alloc(struct cgroup_subsys_state *parent_css)\n \t\tinit_memcg_events();\n \t\tpage_counter_init(&memcg->memory, NULL, true);\n \t\tpage_counter_init(&memcg->swap, NULL, false);\n+\t\tpage_counter_update_toptier_capacity(&memcg->memory, NULL);\n #ifdef CONFIG_MEMCG_V1\n \t\tpage_counter_init(&memcg->kmem, NULL, false);\n \t\tpage_counter_init(&memcg->tcpmem, NULL, false);\n@@ -4804,6 +4807,20 @@ void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n \tpage_counter_calculate_protection(&root->memory, &memcg->memory, recursive_protection);\n }\n \n+void update_memcg_toptier_capacity(void)\n+{\n+\tstruct mem_cgroup *memcg;\n+\tnodemask_t allowed;\n+\n+\tfor_each_mem_cgroup(memcg) {\n+\t\tif (memcg == root_mem_cgroup)\n+\t\t\tcontinue;\n+\n+\t\tcpuset_nodes_allowed(memcg->css.cgroup, &allowed);\n+\t\tpage_counter_update_toptier_capacity(&memcg->memory, &allowed);\n+\t}\n+}\n+\n static int charge_memcg(struct folio *folio, struct mem_cgroup *memcg,\n \t\t\tgfp_t gfp)\n {\ndiff --git a/mm/memory-tiers.c b/mm/memory-tiers.c\nindex a88256381519..259caaf4be8f 100644\n--- a/mm/memory-tiers.c\n+++ b/mm/memory-tiers.c\n@@ -889,6 +889,7 @@ static int __meminit memtier_hotplug_callback(struct notifier_block *self,\n \t\tmutex_lock(&memory_tier_lock);\n \t\tif (clear_node_memory_tier(nn->nid))\n \t\t\testablish_demotion_targets();\n+\t\tupdate_memcg_toptier_capacity();\n \t\tmutex_unlock(&memory_tier_lock);\n \t\tbreak;\n \tcase NODE_ADDED_FIRST_MEMORY:\n@@ -896,6 +897,7 @@ static int __meminit memtier_hotplug_callback(struct notifier_block *self,\n \t\tmemtier = set_node_memory_tier(nn->nid);\n \t\tif (!IS_ERR(memtier))\n \t\t\testablish_demotion_targets();\n+\t\tupdate_memcg_toptier_capacity();\n \t\tmutex_unlock(&memory_tier_lock);\n \t\tbreak;\n \t}\n@@ -941,6 +943,45 @@ bool numa_demotion_enabled = false;\n \n bool tier_aware_memcg_limits;\n \n+void mt_get_toptier_nodemask(nodemask_t *mask, const nodemask_t *allowed)\n+{\n+\tint nid;\n+\n+\t*mask = NODE_MASK_NONE;\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tif (node_is_toptier(nid))\n+\t\t\tnode_set(nid, *mask);\n+\t}\n+\tif (allowed)\n+\t\tnodes_and(*mask, *mask, *allowed);\n+}\n+\n+unsigned long mt_get_toptier_capacity(const nodemask_t *allowed)\n+{\n+\tint nid;\n+\tunsigned long capacity = 0;\n+\tnodemask_t mask;\n+\n+\tmt_get_toptier_nodemask(&mask, allowed);\n+\tfor_each_node_mask(nid, mask)\n+\t\tcapacity += NODE_DATA(nid)->node_present_pages;\n+\n+\treturn capacity;\n+}\n+\n+unsigned long mt_get_total_capacity(const nodemask_t *allowed)\n+{\n+\tint nid;\n+\tunsigned long capacity = 0;\n+\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tif (allowed && !node_isset(nid, *allowed))\n+\t\t\tcontinue;\n+\t\tcapacity += NODE_DATA(nid)->node_present_pages;\n+\t}\n+\treturn capacity;\n+}\n+\n #ifdef CONFIG_MIGRATION\n #ifdef CONFIG_SYSFS\n static ssize_t demotion_enabled_show(struct kobject *kobj,\ndiff --git a/mm/page_counter.c b/mm/page_counter.c\nindex 5ec97811c418..cf21c72bfd4e 100644\n--- a/mm/page_counter.c\n+++ b/mm/page_counter.c\n@@ -11,6 +11,7 @@\n #include <linux/string.h>\n #include <linux/sched.h>\n #include <linux/bug.h>\n+#include <linux/memory-tiers.h>\n #include <asm/page.h>\n \n static bool track_protection(struct page_counter *c)\n@@ -463,6 +464,13 @@ void page_counter_calculate_protection(struct page_counter *root,\n \t\t\trecursive_protection));\n }\n \n+void page_counter_update_toptier_capacity(struct page_counter *counter,\n+\t\t\t\t\t  const nodemask_t *allowed)\n+{\n+\tcounter->toptier_capacity = mt_get_toptier_capacity(allowed);\n+\tcounter->total_capacity = mt_get_total_capacity(allowed);\n+}\n+\n unsigned long page_counter_toptier_high(struct page_counter *counter)\n {\n \tunsigned long high = READ_ONCE(counter->high);\n-- \n2.47.3",
          "reply_to": "",
          "message_date": "2026-02-23",
          "message_id": ""
        },
        {
          "author": "Joshua Hahn (author)",
          "summary": "The author is addressing a concern about fairness in distributing toptier memory among workloads, which is currently impossible due to the lack of tier-aware memcg limits. The author explains that their patch extends the existing memory.low protection to be tier-aware and provides best-effort attempts at protecting a fair proportion of toptier memory. The enforcement of tier-aware memcg limits is gated behind a sysctl.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "On machines serving multiple workloads whose memory is isolated via\nthe memory cgroup controller, it is currently impossible to enforce a\nfair distribution of toptier memory among the worloads, as the only\nenforcable limits have to do with total memory footprint, but not where\nthat memory resides.\n\nThis makes ensuring a consistent and baseline performance difficult, as\neach workload's performance is heavily impacted by workload-external\nfactors such as which other workloads are co-located in the same host,\nand the order at which different workloads are started.\n\nExtend the existing memory.low protection to be tier-aware in the\ncharging, enforcement, and protection calculation to provide\nbest-effort attempts at protecting a fair proportion of toptier memory.\n\nUpdates to protection and charging are performed in the same path as\nthe standard memcontrol equivalents. Enforcing tier-aware memcg limits\nhowever, are gated behind the sysctl tier_aware_memcg. This is so that\nruntime-enabling of tier aware limits can account for memory already\npresent in the system.\n\nSigned-off-by: Joshua Hahn <joshua.hahnjy@gmail.com>\n---\n include/linux/memcontrol.h   | 15 +++++++++++----\n include/linux/page_counter.h |  7 ++++---\n kernel/cgroup/dmem.c         |  2 +-\n mm/memcontrol.c              | 14 ++++++++++++--\n mm/page_counter.c            | 35 ++++++++++++++++++++++++++++++++++-\n mm/vmscan.c                  | 13 +++++++++----\n 6 files changed, 71 insertions(+), 15 deletions(-)\n\ndiff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h\nindex 900a36112b62..a998a1e3b8b0 100644\n--- a/include/linux/memcontrol.h\n+++ b/include/linux/memcontrol.h\n@@ -606,7 +606,9 @@ static inline void mem_cgroup_protection(struct mem_cgroup *root,\n }\n \n void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n-\t\t\t\t     struct mem_cgroup *memcg);\n+\t\t\t\t     struct mem_cgroup *memcg, bool toptier);\n+\n+unsigned long mem_cgroup_toptier_usage(struct mem_cgroup *memcg);\n \n void update_memcg_toptier_capacity(void);\n \n@@ -623,11 +625,15 @@ static inline bool mem_cgroup_unprotected(struct mem_cgroup *target,\n }\n \n static inline bool mem_cgroup_below_low(struct mem_cgroup *target,\n-\t\t\t\t\tstruct mem_cgroup *memcg)\n+\t\t\t\t\tstruct mem_cgroup *memcg, bool toptier)\n {\n \tif (mem_cgroup_unprotected(target, memcg))\n \t\treturn false;\n \n+\tif (toptier)\n+\t\treturn READ_ONCE(memcg->memory.etoptier_low) >=\n+\t\t\t\t mem_cgroup_toptier_usage(memcg);\n+\n \treturn READ_ONCE(memcg->memory.elow) >=\n \t\tpage_counter_read(&memcg->memory);\n }\n@@ -1114,7 +1120,8 @@ static inline void mem_cgroup_protection(struct mem_cgroup *root,\n }\n \n static inline void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n-\t\t\t\t\t\t   struct mem_cgroup *memcg)\n+\t\t\t\t\t\t   struct mem_cgroup *memcg,\n+\t\t\t\t\t\t   bool toptier)\n {\n }\n \n@@ -1128,7 +1135,7 @@ static inline bool mem_cgroup_unprotected(struct mem_cgroup *target,\n \treturn true;\n }\n static inline bool mem_cgroup_below_low(struct mem_cgroup *target,\n-\t\t\t\t\tstruct mem_cgroup *memcg)\n+\t\t\t\t\tstruct mem_cgroup *memcg, bool toptier)\n {\n \treturn false;\n }\ndiff --git a/include/linux/page_counter.h b/include/linux/page_counter.h\nindex ada5f1dd75d4..6635ee7b9575 100644\n--- a/include/linux/page_counter.h\n+++ b/include/linux/page_counter.h\n@@ -120,15 +120,16 @@ static inline void page_counter_reset_watermark(struct page_counter *counter)\n #if IS_ENABLED(CONFIG_MEMCG) || IS_ENABLED(CONFIG_CGROUP_DMEM)\n void page_counter_calculate_protection(struct page_counter *root,\n \t\t\t\t       struct page_counter *counter,\n-\t\t\t\t       bool recursive_protection);\n+\t\t\t\t       bool recursive_protection, bool toptier);\n void page_counter_update_toptier_capacity(struct page_counter *counter,\n \t\t\t\t\t  const nodemask_t *allowed);\n unsigned long page_counter_toptier_high(struct page_counter *counter);\n unsigned long page_counter_toptier_low(struct page_counter *counter);\n #else\n static inline void page_counter_calculate_protection(struct page_counter *root,\n-\t\t\t\t\t\t     struct page_counter *counter,\n-\t\t\t\t\t\t     bool recursive_protection) {}\n+\t\t\t\t\t\tstruct page_counter *counter,\n+\t\t\t\t\t\tbool recursive_protection,\n+\t\t\t\t\t\tbool toptier) {}\n #endif\n \n #endif /* _LINUX_PAGE_COUNTER_H */\ndiff --git a/kernel/cgroup/dmem.c b/kernel/cgroup/dmem.c\nindex 1ea6afffa985..536d43c42de8 100644\n--- a/kernel/cgroup/dmem.c\n+++ b/kernel/cgroup/dmem.c\n@@ -277,7 +277,7 @@ dmem_cgroup_calculate_protection(struct dmem_cgroup_pool_state *limit_pool,\n \t\t\tcontinue;\n \n \t\tpage_counter_calculate_protection(\n-\t\t\tclimit, &found_pool->cnt, true);\n+\t\t\tclimit, &found_pool->cnt, true, false);\n \n \t\tif (found_pool == test_pool)\n \t\t\tbreak;\ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 07464f02c529..8aa7ae361a73 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -4806,12 +4806,13 @@ struct cgroup_subsys memory_cgrp_subsys = {\n  * mem_cgroup_calculate_protection - check if memory consumption is in the normal range\n  * @root: the top ancestor of the sub-tree being checked\n  * @memcg: the memory cgroup to check\n+ * @toptier: whether the caller is in a toptier node\n  *\n  * WARNING: This function is not stateless! It can only be used as part\n  *          of a top-down tree iteration, not for isolated queries.\n  */\n void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n-\t\t\t\t     struct mem_cgroup *memcg)\n+\t\t\t\t     struct mem_cgroup *memcg, bool toptier)\n {\n \tbool recursive_protection =\n \t\tcgrp_dfl_root.flags & CGRP_ROOT_MEMORY_RECURSIVE_PROT;\n@@ -4822,7 +4823,16 @@ void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n \tif (!root)\n \t\troot = root_mem_cgroup;\n \n-\tpage_counter_calculate_protection(&root->memory, &memcg->memory, recursive_protection);\n+\tpage_counter_calculate_protection(&root->memory, &memcg->memory,\n+\t\t\t\t\t  recursive_protection, toptier);\n+}\n+\n+unsigned long mem_cgroup_toptier_usage(struct mem_cgroup *memcg)\n+{\n+\tif (mem_cgroup_disabled() || !memcg)\n+\t\treturn 0;\n+\n+\treturn atomic_long_read(&memcg->memory.toptier_usage);\n }\n \n void update_memcg_toptier_capacity(void)\ndiff --git a/mm/page_counter.c b/mm/page_counter.c\nindex cf21c72bfd4e..79d46a1c4c0c 100644\n--- a/mm/page_counter.c\n+++ b/mm/page_counter.c\n@@ -410,12 +410,39 @@ static unsigned long effective_protection(unsigned long usage,\n \treturn ep;\n }\n \n+static void calculate_protection_toptier(struct page_counter *counter,\n+\t\t\t\t\t bool recursive_protection)\n+{\n+\tstruct page_counter *parent = counter->parent;\n+\tunsigned long toptier_low;\n+\tunsigned long toptier_usage, parent_toptier_usage;\n+\tunsigned long toptier_protected, old_toptier_protected;\n+\tlong delta;\n+\n+\ttoptier_low = page_counter_toptier_low(counter);\n+\ttoptier_usage = atomic_long_read(&counter->toptier_usage);\n+\tparent_toptier_usage = atomic_long_read(&parent->toptier_usage);\n+\n+\t/* Propagate toptier low usage to parent for sibling distribution */\n+\ttoptier_protected = min(toptier_usage, toptier_low);\n+\told_toptier_protected = atomic_long_xchg(&counter->toptier_low_usage,\n+\t\t\t\t\t\t toptier_protected);\n+\tdelta = toptier_protected - old_toptier_protected;\n+\tatomic_long_add(delta, &parent->children_toptier_low_usage);\n+\n+\tWRITE_ONCE(counter->etoptier_low,\n+\t\t   effective_protection(toptier_usage, parent_toptier_usage,\n+\t\t   toptier_low, READ_ONCE(parent->etoptier_low),\n+\t\t   atomic_long_read(&parent->children_toptier_low_usage),\n+\t\t   recursive_protection));\n+}\n \n /**\n  * page_counter_calculate_protection - check if memory consumption is in the normal range\n  * @root: the top ancestor of the sub-tree being checked\n  * @counter: the page_counter the counter to update\n  * @recursive_protection: Whether to use memory_recursiveprot behavior.\n+ * @toptier: Whether to calculate toptier-proportional protection\n  *\n  * Calculates elow/emin thresholds for given page_counter.\n  *\n@@ -424,7 +451,7 @@ static unsigned long effective_protection(unsigned long usage,\n  */\n void page_counter_calculate_protection(struct page_counter *root,\n \t\t\t\t       struct page_counter *counter,\n-\t\t\t\t       bool recursive_protection)\n+\t\t\t\t       bool recursive_protection, bool toptier)\n {\n \tunsigned long usage, parent_usage;\n \tstruct page_counter *parent = counter->parent;\n@@ -446,6 +473,9 @@ void page_counter_calculate_protection(struct page_counter *root,\n \tif (parent == root) {\n \t\tcounter->emin = READ_ONCE(counter->min);\n \t\tcounter->elow = READ_ONCE(counter->low);\n+\t\tif (toptier)\n+\t\t\tWRITE_ONCE(counter->etoptier_low,\n+\t\t\t\t   page_counter_toptier_low(counter));\n \t\treturn;\n \t}\n \n@@ -462,6 +492,9 @@ void page_counter_calculate_protection(struct page_counter *root,\n \t\t\tREAD_ONCE(parent->elow),\n \t\t\tatomic_long_read(&parent->children_low_usage),\n \t\t\trecursive_protection));\n+\n+\tif (toptier)\n+\t\tcalculate_protection_toptier(counter, recursive_protection);\n }\n \n void page_counter_update_toptier_capacity(struct page_counter *counter,\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 6a87ac7be43c..5b4cb030a477 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -4144,6 +4144,7 @@ static void lru_gen_age_node(struct pglist_data *pgdat, struct scan_control *sc)\n \tstruct mem_cgroup *memcg;\n \tunsigned long min_ttl = READ_ONCE(lru_gen_min_ttl);\n \tbool reclaimable = !min_ttl;\n+\tbool toptier = node_is_toptier(pgdat->node_id);\n \n \tVM_WARN_ON_ONCE(!current_is_kswapd());\n \n@@ -4153,7 +4154,7 @@ static void lru_gen_age_node(struct pglist_data *pgdat, struct scan_control *sc)\n \tdo {\n \t\tstruct lruvec *lruvec = mem_cgroup_lruvec(memcg, pgdat);\n \n-\t\tmem_cgroup_calculate_protection(NULL, memcg);\n+\t\tmem_cgroup_calculate_protection(NULL, memcg, toptier);\n \n \t\tif (!reclaimable)\n \t\t\treclaimable = lruvec_is_reclaimable(lruvec, sc, min_ttl);\n@@ -4905,12 +4906,14 @@ static int shrink_one(struct lruvec *lruvec, struct scan_control *sc)\n \tunsigned long reclaimed = sc->nr_reclaimed;\n \tstruct mem_cgroup *memcg = lruvec_memcg(lruvec);\n \tstruct pglist_data *pgdat = lruvec_pgdat(lruvec);\n+\tbool toptier = tier_aware_memcg_limits &&\n+\t\t       node_is_toptier(pgdat->node_id);\n \n \t/* lru_gen_age_node() called mem_cgroup_calculate_protection() */\n \tif (mem_cgroup_below_min(NULL, memcg))\n \t\treturn MEMCG_LRU_YOUNG;\n \n-\tif (mem_cgroup_below_low(NULL, memcg)) {\n+\tif (mem_cgroup_below_low(NULL, memcg, toptier)) {\n \t\t/* see the comment on MEMCG_NR_GENS */\n \t\tif (READ_ONCE(lruvec->lrugen.seg) != MEMCG_LRU_TAIL)\n \t\t\treturn MEMCG_LRU_TAIL;\n@@ -5960,6 +5963,7 @@ static void shrink_node_memcgs(pg_data_t *pgdat, struct scan_control *sc)\n \t};\n \tstruct mem_cgroup_reclaim_cookie *partial = &reclaim;\n \tstruct mem_cgroup *memcg;\n+\tbool toptier = node_is_toptier(pgdat->node_id);\n \n \t/*\n \t * In most cases, direct reclaimers can do partial walks\n@@ -5987,7 +5991,7 @@ static void shrink_node_memcgs(pg_data_t *pgdat, struct scan_control *sc)\n \t\t */\n \t\tcond_resched();\n \n-\t\tmem_cgroup_calculate_protection(target_memcg, memcg);\n+\t\tmem_cgroup_calculate_protection(target_memcg, memcg, toptier);\n \n \t\tif (mem_cgroup_below_min(target_memcg, memcg)) {\n \t\t\t/*\n@@ -5995,7 +5999,8 @@ static void shrink_node_memcgs(pg_data_t *pgdat, struct scan_control *sc)\n \t\t\t * If there is no reclaimable memory, OOM.\n \t\t\t */\n \t\t\tcontinue;\n-\t\t} else if (mem_cgroup_below_low(target_memcg, memcg)) {\n+\t\t} else if (mem_cgroup_below_low(target_memcg, memcg,\n+\t\t\t\t\ttier_aware_memcg_limits && toptier)) {\n \t\t\t/*\n \t\t\t * Soft protection.\n \t\t\t * Respect the protection only as long as\n-- \n2.47.3",
          "reply_to": "",
          "message_date": "2026-02-23",
          "message_id": ""
        },
        {
          "author": "Joshua Hahn (author)",
          "summary": "The author addressed a concern about the interaction between memory cgroups and toptier usage limits by explaining how they plan to modify the existing high protection mechanism to be tier-aware, adding a new nodemask parameter to try_to_free_mem_cgroup_pages, and introducing a new function to calculate overage for toptier usage. The author confirmed that these changes will address the issue of workload-external factors impacting performance.",
          "sentiment": "positive",
          "sentiment_signals": [
            "acknowledged concern",
            "planned fix"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "On machines serving multiple workloads whose memory is isolated via the\nmemory cgroup controller, it is currently impossible to enforce a fair\ndistribution of toptier memory among the workloads, as the only\nenforcable limits have to do with total memory footprint, but not where\nthat memory resides.\n\nThis makes ensuring a consistent and baseline performance difficult, as\neach workload's performance is heavily impacted by workload-external\nfactors wuch as which other workloads are co-located in the same host,\nand the order at which different workloads are started.\n\nExtend the existing memory.high protection to be tier-aware in the\ncharging and enforcement to limit toptier-hogging for workloads.\n\nAlso, add a new nodemask parameter to try_to_free_mem_cgroup_pages,\nwhich can be used to selectively reclaim from memory at the\nmemcg-tier interection of a cgroup.\n\nSigned-off-by: Joshua Hahn <joshua.hahnjy@gmail.com>\n---\n include/linux/swap.h |  3 +-\n mm/memcontrol-v1.c   |  6 ++--\n mm/memcontrol.c      | 85 +++++++++++++++++++++++++++++++++++++-------\n mm/vmscan.c          | 11 +++---\n 4 files changed, 84 insertions(+), 21 deletions(-)\n\ndiff --git a/include/linux/swap.h b/include/linux/swap.h\nindex 0effe3cc50f5..c6037ac7bf6e 100644\n--- a/include/linux/swap.h\n+++ b/include/linux/swap.h\n@@ -368,7 +368,8 @@ extern unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,\n \t\t\t\t\t\t  unsigned long nr_pages,\n \t\t\t\t\t\t  gfp_t gfp_mask,\n \t\t\t\t\t\t  unsigned int reclaim_options,\n-\t\t\t\t\t\t  int *swappiness);\n+\t\t\t\t\t\t  int *swappiness,\n+\t\t\t\t\t\t  nodemask_t *allowed);\n extern unsigned long mem_cgroup_shrink_node(struct mem_cgroup *mem,\n \t\t\t\t\t\tgfp_t gfp_mask, bool noswap,\n \t\t\t\t\t\tpg_data_t *pgdat,\ndiff --git a/mm/memcontrol-v1.c b/mm/memcontrol-v1.c\nindex 0b39ba608109..29630c7f3567 100644\n--- a/mm/memcontrol-v1.c\n+++ b/mm/memcontrol-v1.c\n@@ -1497,7 +1497,8 @@ static int mem_cgroup_resize_max(struct mem_cgroup *memcg,\n \t\t}\n \n \t\tif (!try_to_free_mem_cgroup_pages(memcg, 1, GFP_KERNEL,\n-\t\t\t\tmemsw ? 0 : MEMCG_RECLAIM_MAY_SWAP, NULL)) {\n+\t\t\t\tmemsw ? 0 : MEMCG_RECLAIM_MAY_SWAP,\n+\t\t\t\tNULL, NULL)) {\n \t\t\tret = -EBUSY;\n \t\t\tbreak;\n \t\t}\n@@ -1529,7 +1530,8 @@ static int mem_cgroup_force_empty(struct mem_cgroup *memcg)\n \t\t\treturn -EINTR;\n \n \t\tif (!try_to_free_mem_cgroup_pages(memcg, 1, GFP_KERNEL,\n-\t\t\t\t\t\t  MEMCG_RECLAIM_MAY_SWAP, NULL))\n+\t\t\t\t\t\t  MEMCG_RECLAIM_MAY_SWAP,\n+\t\t\t\t\t\t  NULL, NULL))\n \t\t\tnr_retries--;\n \t}\n \ndiff --git a/mm/memcontrol.c b/mm/memcontrol.c\nindex 8aa7ae361a73..ebd4a1b73c51 100644\n--- a/mm/memcontrol.c\n+++ b/mm/memcontrol.c\n@@ -2184,18 +2184,30 @@ static unsigned long reclaim_high(struct mem_cgroup *memcg,\n \n \tdo {\n \t\tunsigned long pflags;\n-\n-\t\tif (page_counter_read(&memcg->memory) <=\n-\t\t    READ_ONCE(memcg->memory.high))\n+\t\tnodemask_t toptier_nodes, *reclaim_nodes;\n+\t\tbool mem_high_ok, toptier_high_ok;\n+\n+\t\tmt_get_toptier_nodemask(&toptier_nodes, NULL);\n+\t\tmem_high_ok = page_counter_read(&memcg->memory) <=\n+\t\t\t      READ_ONCE(memcg->memory.high);\n+\t\ttoptier_high_ok = !(tier_aware_memcg_limits &&\n+\t\t\t\t    mem_cgroup_toptier_usage(memcg) >\n+\t\t\t\t    page_counter_toptier_high(&memcg->memory));\n+\t\tif (mem_high_ok && toptier_high_ok)\n \t\t\tcontinue;\n \n+\t\tif (mem_high_ok && !toptier_high_ok)\n+\t\t\treclaim_nodes = &toptier_nodes;\n+\t\telse\n+\t\t\treclaim_nodes = NULL;\n+\n \t\tmemcg_memory_event(memcg, MEMCG_HIGH);\n \n \t\tpsi_memstall_enter(&pflags);\n \t\tnr_reclaimed += try_to_free_mem_cgroup_pages(memcg, nr_pages,\n \t\t\t\t\t\t\tgfp_mask,\n \t\t\t\t\t\t\tMEMCG_RECLAIM_MAY_SWAP,\n-\t\t\t\t\t\t\tNULL);\n+\t\t\t\t\t\t\tNULL, reclaim_nodes);\n \t\tpsi_memstall_leave(&pflags);\n \t} while ((memcg = parent_mem_cgroup(memcg)) &&\n \t\t !mem_cgroup_is_root(memcg));\n@@ -2296,6 +2308,24 @@ static u64 mem_find_max_overage(struct mem_cgroup *memcg)\n \treturn max_overage;\n }\n \n+static u64 toptier_find_max_overage(struct mem_cgroup *memcg)\n+{\n+\tu64 overage, max_overage = 0;\n+\n+\tif (!tier_aware_memcg_limits)\n+\t\treturn 0;\n+\n+\tdo {\n+\t\tunsigned long usage = mem_cgroup_toptier_usage(memcg);\n+\t\tunsigned long high = page_counter_toptier_high(&memcg->memory);\n+\n+\t\toverage = calculate_overage(usage, high);\n+\t\tmax_overage = max(overage, max_overage);\n+\t} while ((memcg = parent_mem_cgroup(memcg)) &&\n+\t\t  !mem_cgroup_is_root(memcg));\n+\n+\treturn max_overage;\n+}\n static u64 swap_find_max_overage(struct mem_cgroup *memcg)\n {\n \tu64 overage, max_overage = 0;\n@@ -2401,6 +2431,14 @@ void __mem_cgroup_handle_over_high(gfp_t gfp_mask)\n \tpenalty_jiffies += calculate_high_delay(memcg, nr_pages,\n \t\t\t\t\t\tswap_find_max_overage(memcg));\n \n+\t/*\n+\t * Don't double-penalize for toptier high overage if system-wide\n+\t * memory.high has already been breached.\n+\t */\n+\tif (!penalty_jiffies)\n+\t\tpenalty_jiffies += calculate_high_delay(memcg, nr_pages,\n+\t\t\t\t\ttoptier_find_max_overage(memcg));\n+\n \t/*\n \t * Clamp the max delay per usermode return so as to still keep the\n \t * application moving forwards and also permit diagnostics, albeit\n@@ -2503,7 +2541,8 @@ static int try_charge_memcg(struct mem_cgroup *memcg, gfp_t gfp_mask,\n \n \tpsi_memstall_enter(&pflags);\n \tnr_reclaimed = try_to_free_mem_cgroup_pages(mem_over_limit, nr_pages,\n-\t\t\t\t\t\t    gfp_mask, reclaim_options, NULL);\n+\t\t\t\t\t\t    gfp_mask, reclaim_options,\n+\t\t\t\t\t\t    NULL, NULL);\n \tpsi_memstall_leave(&pflags);\n \n \tif (mem_cgroup_margin(mem_over_limit) >= nr_pages)\n@@ -2592,23 +2631,26 @@ static int try_charge_memcg(struct mem_cgroup *memcg, gfp_t gfp_mask,\n \t * reclaim, the cost of mismatch is negligible.\n \t */\n \tdo {\n-\t\tbool mem_high, swap_high;\n+\t\tbool mem_high, swap_high, toptier_high = false;\n \n \t\tmem_high = page_counter_read(&memcg->memory) >\n \t\t\tREAD_ONCE(memcg->memory.high);\n \t\tswap_high = page_counter_read(&memcg->swap) >\n \t\t\tREAD_ONCE(memcg->swap.high);\n+\t\ttoptier_high = tier_aware_memcg_limits &&\n+\t\t\t       (mem_cgroup_toptier_usage(memcg) >\n+\t\t\t\tpage_counter_toptier_high(&memcg->memory));\n \n \t\t/* Don't bother a random interrupted task */\n \t\tif (!in_task()) {\n-\t\t\tif (mem_high) {\n+\t\t\tif (mem_high || toptier_high) {\n \t\t\t\tschedule_work(&memcg->high_work);\n \t\t\t\tbreak;\n \t\t\t}\n \t\t\tcontinue;\n \t\t}\n \n-\t\tif (mem_high || swap_high) {\n+\t\tif (mem_high || swap_high || toptier_high) {\n \t\t\t/*\n \t\t\t * The allocating tasks in this cgroup will need to do\n \t\t\t * reclaim or be throttled to prevent further growth\n@@ -4476,7 +4518,7 @@ static ssize_t memory_high_write(struct kernfs_open_file *of,\n \tstruct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));\n \tunsigned int nr_retries = MAX_RECLAIM_RETRIES;\n \tbool drained = false;\n-\tunsigned long high;\n+\tunsigned long high, toptier_high;\n \tint err;\n \n \tbuf = strstrip(buf);\n@@ -4485,15 +4527,22 @@ static ssize_t memory_high_write(struct kernfs_open_file *of,\n \t\treturn err;\n \n \tpage_counter_set_high(&memcg->memory, high);\n+\ttoptier_high = page_counter_toptier_high(&memcg->memory);\n \n \tif (of->file->f_flags & O_NONBLOCK)\n \t\tgoto out;\n \n \tfor (;;) {\n \t\tunsigned long nr_pages = page_counter_read(&memcg->memory);\n+\t\tunsigned long toptier_pages = mem_cgroup_toptier_usage(memcg);\n \t\tunsigned long reclaimed;\n+\t\tunsigned long to_free;\n+\t\tnodemask_t toptier_nodes, *reclaim_nodes;\n+\t\tbool mem_high_ok = nr_pages <= high;\n+\t\tbool toptier_high_ok = !(tier_aware_memcg_limits &&\n+\t\t\t\t\t toptier_pages > toptier_high);\n \n-\t\tif (nr_pages <= high)\n+\t\tif (mem_high_ok && toptier_high_ok)\n \t\t\tbreak;\n \n \t\tif (signal_pending(current))\n@@ -4505,8 +4554,17 @@ static ssize_t memory_high_write(struct kernfs_open_file *of,\n \t\t\tcontinue;\n \t\t}\n \n-\t\treclaimed = try_to_free_mem_cgroup_pages(memcg, nr_pages - high,\n-\t\t\t\t\tGFP_KERNEL, MEMCG_RECLAIM_MAY_SWAP, NULL);\n+\t\tmt_get_toptier_nodemask(&toptier_nodes, NULL);\n+\t\tif (mem_high_ok && !toptier_high_ok) {\n+\t\t\treclaim_nodes = &toptier_nodes;\n+\t\t\tto_free = toptier_pages - toptier_high;\n+\t\t} else {\n+\t\t\treclaim_nodes = NULL;\n+\t\t\tto_free = nr_pages - high;\n+\t\t}\n+\t\treclaimed = try_to_free_mem_cgroup_pages(memcg, to_free,\n+\t\t\t\t\tGFP_KERNEL, MEMCG_RECLAIM_MAY_SWAP,\n+\t\t\t\t\tNULL, reclaim_nodes);\n \n \t\tif (!reclaimed && !nr_retries--)\n \t\t\tbreak;\n@@ -4558,7 +4616,8 @@ static ssize_t memory_max_write(struct kernfs_open_file *of,\n \n \t\tif (nr_reclaims) {\n \t\t\tif (!try_to_free_mem_cgroup_pages(memcg, nr_pages - max,\n-\t\t\t\t\tGFP_KERNEL, MEMCG_RECLAIM_MAY_SWAP, NULL))\n+\t\t\t\t\tGFP_KERNEL, MEMCG_RECLAIM_MAY_SWAP,\n+\t\t\t\t\tNULL, NULL))\n \t\t\t\tnr_reclaims--;\n \t\t\tcontinue;\n \t\t}\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 5b4cb030a477..94498734b4f5 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -6652,7 +6652,7 @@ unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,\n \t\t\t\t\t   unsigned long nr_pages,\n \t\t\t\t\t   gfp_t gfp_mask,\n \t\t\t\t\t   unsigned int reclaim_options,\n-\t\t\t\t\t   int *swappiness)\n+\t\t\t\t\t   int *swappiness, nodemask_t *allowed)\n {\n \tunsigned long nr_reclaimed;\n \tunsigned int noreclaim_flag;\n@@ -6668,6 +6668,7 @@ unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,\n \t\t.may_unmap = 1,\n \t\t.may_swap = !!(reclaim_options & MEMCG_RECLAIM_MAY_SWAP),\n \t\t.proactive = !!(reclaim_options & MEMCG_RECLAIM_PROACTIVE),\n+\t\t.nodemask = allowed,\n \t};\n \t/*\n \t * Traverse the ZONELIST_FALLBACK zonelist of the current node to put\n@@ -6693,7 +6694,7 @@ unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,\n \t\t\t\t\t   unsigned long nr_pages,\n \t\t\t\t\t   gfp_t gfp_mask,\n \t\t\t\t\t   unsigned int reclaim_options,\n-\t\t\t\t\t   int *swappiness)\n+\t\t\t\t\t   int *swappiness, nodemask_t *allowed)\n {\n \treturn 0;\n }\n@@ -7806,9 +7807,9 @@ int user_proactive_reclaim(char *buf,\n \t\t\treclaim_options = MEMCG_RECLAIM_MAY_SWAP |\n \t\t\t\t\t  MEMCG_RECLAIM_PROACTIVE;\n \t\t\treclaimed = try_to_free_mem_cgroup_pages(memcg,\n-\t\t\t\t\t\t batch_size, gfp_mask,\n-\t\t\t\t\t\t reclaim_options,\n-\t\t\t\t\t\t swappiness == -1 ? NULL : &swappiness);\n+\t\t\t\t\tbatch_size, gfp_mask, reclaim_options,\n+\t\t\t\t\tswappiness == -1 ? NULL : &swappiness,\n+\t\t\t\t\tNULL);\n \t\t} else {\n \t\t\tstruct scan_control sc = {\n \t\t\t\t.gfp_mask = current_gfp_context(gfp_mask),\n-- \n2.47.3",
          "reply_to": "",
          "message_date": "2026-02-23",
          "message_id": ""
        },
        {
          "author": "Joshua Hahn (author)",
          "summary": "The author is addressing a concern that the patch does not account for systems where all memory is equal, and explaining how tier-aware memcg limits can provide better quality of service guarantees in systems with tiered memory.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "addressing_concern",
            "providing_explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Memory cgroups provide an interface that allow multiple workloads on a\nhost to co-exist, and establish both weak and strong memory isolation\nguarantees. For large servers and small embedded systems alike, memcgs\nprovide an effective way to provide a baseline quality of service for\nprotected workloads.\n\nThis works, because for the most part, all memory is equal (except for\nzram / zswap). Restricting a cgroup's memory footprint restricts how\nmuch it can hurt other workloads competing for memory. Likewise, setting\nmemory.low or memory.min limits can provide weak and strong guarantees\nto the performance of a cgroup.\n\nHowever, on systems with tiered memory (e.g. CXL / compressed memory),\nthe quality of service guarantees that memcg limits enforced become less\neffective, as memcg has no awareness of the physical location of its\ncharged memory. In other words, a workload that is well-behaved within\nits memcg limits may still be hurting the performance of other\nwell-behaving workloads on the system by hogging more than its\n\"fair share\" of toptier memory.\n\nIntroduce tier-aware memcg limits, which scale memory.low/high to\nreflect the ratio of toptier:total memory the cgroup has access.\n\nTake the following scenario as an example:\nOn a host with 3:1 toptier:lowtier, say 150G toptier, and 50Glowtier,\nsetting a cgroup's limits to:\n\tmemory.min:  15G\n\tmemory.low:  20G\n\tmemory.high: 40G\n\tmemory.max:  50G\n\nWill be enforced at the toptier as:\n\tmemory.min:          15G\n\tmemory.toptier_low:  15G (20 * 150/200)\n\tmemory.toptier_high: 30G (40 * 150/200)\n\tmemory.max:          50G\n\nLet's say that there are 4 such cgroups on the host. Previously, it would\nbe possible for 3 hosts to completely take over all of DRAM, while one\ncgroup could only access the lowtier memory. In the perspective of a\ntier-agnostic memcg limit enforcement, the three cgroups are all\nwell-behaved, consuming within their memory limits.\n\nThis is not to say that the scenario above is incorrect. In fact, for\nletting the hottest cgroups run in DRAM while pushing out colder cgroups\nto lowtier memory lets the system perform the most aggregate work total.\n\nBut for other scenarios, the target might not be maximizing aggregate\nwork, but maximizing the minimum performance guarantee for each\nindividual workload (think hosts shared across different users, such as\nVM hosting services).\n\nTo reflect these two scenarios, introduce a sysctl tier_aware_memcg,\nwhich allows the host to toggle between enforcing and overlooking\ntoptier memcg limit breaches.\n\nThis work is inspired & based off of Kaiyang Zhao's work from 2024 [1],\nwhere he referred to this concept as \"memory tiering fairness\".\nThe biggest difference in the implementations lie in how toptier memory\nis tracked; in his implementation, an lruvec stat aggregation is done on\neach usage check, while in this implementation, a new cacheline is\nintroduced in page_coutner to keep track of toptier usage (Kaiyang also\nintroduces a new cachline in page_counter, but only uses it to cache\ncapacity and thresholds). This implementation also extends the memory\nlimit enforcement to memory.high as well.\n\n[1] https://lore.kernel.org/linux-mm/20240920221202.1734227-1-kaiyang2@cs.cmu.edu/\n\n---\nJoshua Hahn (6):\n  mm/memory-tiers: Introduce tier-aware memcg limit sysfs\n  mm/page_counter: Introduce tiered memory awareness to page_counter\n  mm/memory-tiers, memcontrol: Introduce toptier capacity updates\n  mm/memcontrol: Charge and uncharge from toptier\n  mm/memcontrol, page_counter: Make memory.low tier-aware\n  mm/memcontrol: Make memory.high tier-aware\n\n include/linux/memcontrol.h   |  21 ++++-\n include/linux/memory-tiers.h |  30 +++++++\n include/linux/page_counter.h |  31 ++++++-\n include/linux/swap.h         |   3 +-\n kernel/cgroup/cpuset.c       |   2 +-\n kernel/cgroup/dmem.c         |   2 +-\n mm/memcontrol-v1.c           |   6 +-\n mm/memcontrol.c              | 155 +++++++++++++++++++++++++++++++----\n mm/memory-tiers.c            |  63 ++++++++++++++\n mm/page_counter.c            |  77 ++++++++++++++++-\n mm/vmscan.c                  |  24 ++++--\n 11 files changed, 376 insertions(+), 38 deletions(-)\n\n-- \n2.47.3",
          "reply_to": "",
          "message_date": "2026-02-23",
          "message_id": ""
        }
      ],
      "analysis_source": "llm",
      "patch_summary": "This patch introduces a new sysfs entry to toggle between memory cgroup (memcg) limits that are proportional to the system's top-tier capacity ratio. The goal is to make memcg limits tier-aware, allowing for more efficient resource allocation and utilization. This is achieved by adding a boolean flag `tier_aware_memcg_limits` and two new sysfs attributes: `tier_aware_memcg_show` and `tier_aware_memcg_store`. When enabled, the memcg limits will be adjusted based on the system's top-tier capacity ratio, allowing for more efficient resource allocation. The patch is part of a larger series that aims to make memory management more efficient and scalable."
    },
    "2026-02-24": {
      "report_file": "2026-02-24.html",
      "developer": "Joshua Hahn",
      "reviews": [
        {
          "author": "Michal Hocko",
          "summary": "Reviewer Michal Hocko questioned whether it's typical for active workingset sizes of all workloads to not fit into the top tier, suggesting that promotions would otherwise ensure most active memory is in the top tier.\n\nReviewer Michal Hocko expressed concerns that the current implementation focuses only on the top tier, potentially overlooking similar issues in other tiers, and questioned the need to duplicate limits for each/top tier.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "questioning typical behavior",
            "requesting clarification",
            "focusing only on top tier",
            "potential oversight of other tiers"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "This assumes that the active workingset size of all workloads doesn't\nfit into the top tier right? Otherwise promotions would make sure to\nthat we have the most active memory in the top tier. Is this typical in\nreal life configurations?\n\nOr do you intend to limit memory consumption on particular tier even\nwithout an external pressure?\n\n---\n\nLet's spend some more time with the interface first. You seem to be\nfocusing only on the top tier with this interface, right? Is this really the\nright way to go long term? What makes you believe that we do not really\nhit the same issue with other tiers as well? Also do we want/need to\nduplicate all the limits for each/top tier? What is the reasoning for\nthe switch to be runtime sysctl rather than boot-time or cgroup mount\noption?\n\nI will likely have more questions but these are immediate ones after\nreading the cover. Please note I haven't really looked at the\nimplementation yet. I really want to understand usecases and interface\nfirst.\n-- \nMichal Hocko\nSUSE Labs",
          "reply_to": "Joshua Hahn",
          "message_date": "2026-02-24",
          "message_id": ""
        },
        {
          "author": "Joshua Hahn (author)",
          "summary": "Author is addressing Michal's feedback by explaining their intention to discuss the project scope and scalability in a separate forum, LSFMMBPF, rather than revising the current patch series.\n\nAuthor acknowledged a concern about the impact of a workload violating its fair share of toptier memory, explaining that it mostly hurts other workloads when the aggregate working set size exceeds toptier memory capacity.\n\nAuthor acknowledged that traditional throughput-maximizing approach may not be suitable for all use cases and explained how tier-aware memcg limits can provide a more optimal solution for workloads prioritizing performance guarantees or low latency.\n\nAuthor agrees that the patch's tier-aware memcg limit feature is relevant to cloud providers and hyperscalers, providing two examples of realistic scenarios.\n\nThe author is addressing a concern about the interface for tier-aware memcg limits, specifically whether it should allow workloads to use more memory than their fair share in opportunistic mode. The author proposes two modes: fixed (limiting usage when a workload exceeds its fair share) and opportunistic (allowing workloads to use more memory but restricting them only when the top-tier is pressured). They ask for feedback on these options, suggesting that implementing opportunistic mode would require additional sysctl changes.\n\nAuthor acknowledges that the patch series was sent out prematurely and agrees it would have been better to send a proposal for LSFMMBPF first, indicating no immediate need for further revision.\n\nAuthor acknowledged that the current implementation only supports two-tiered systems and is open to revising it in the future when more complex systems become common.\n\nAuthor asked for clarification on whether reviewer's concern was about multiple nodes within a tier or multiple tiers within a tier, indicating uncertainty and need for further discussion.\n\nAuthor addressed Michal Hocko's concern that allowing cgroups to set their own mount options for tier-aware memcg limits could lead to inconsistent behavior and undermine the purpose of having a performance guarantee. Author agrees that this is not a good idea, but no specific fix or restructuring plan was mentioned.\n\nAuthor acknowledged that the feedback was positive and thanked the reviewer, indicating no further revision is needed.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "clarification",
            "separate discussion",
            "clarifying explanation",
            "no clear resolution signal",
            "acknowledged a different perspective",
            "explained reasoning",
            "agreement",
            "support",
            "asking for clarification",
            "proposing alternative solutions",
            "acknowledges feedback",
            "agrees with alternative approach",
            "open to revision",
            "acknowledged limitation",
            "uncertainty",
            "need_for_further_discussion",
            "acknowledged a problem",
            "agreed with feedback",
            "acknowledged",
            "thanked"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Hello Michal,\n\nI hope that you are doing well! Thank you for taking the time to review my\nwork and leaving your thoughts.\n\nI wanted to note that I hope to bring this discussion to LSFMMBPF as well,\nto discuss what the scope of the project should be, what usecases there\nare (as I will note below), how to make this scalable and sustainable\nfor the future, etc. I'll send out a topic proposal later today. I had\nseparated the series from the proposal because I imagined that this\nseries would go through many versions, so it would be helpful to have\nthe topic as a unified place for pre-conference discussions.\n\n---\n\nYes, for the scenario above, a workload that is violating its fair share\nof toptier memory mostly hurts other workloads if the aggregate working\nset size of all workloads exceeds the size of toptier memory.\n\n---\n\nThis is true. And for a lot of usecases, this is 100% the right thing to do.\nHowever, with this patch I want to encourage a different perspective,\nwhich is to think about things in a per-workload perspective, and not a\nper-system perspective.\n\nHaving hot memory in high tiers and cold memory in low tiers is only\nlogical, since we increase the system's throughput and make the most\noptimal choices for latency. However, what about systems that care about\nobjectives other than simply maximizing throughput?\n\nIn the original cover letter I offered an example of VM hosting services\nthat care less about maximizing host-wide throughput, but more on ensuring\na bottomline performance guarantee for all workloads running on the system.\nFor the users on these services, they don't care that the host their VM is\nrunning on is maximizing throughput; rather, they care that their VM meets\nthe performance guarantees that their provider promised. If there is no\nway to know or enforce which tier of memory their workload lands on, either\nthe bottomline guarantee becomes very underestimated, or users must deal\nwith a high variance in performance.\n\nHere's another example: Let's say there is a host with multiple workloads,\neach serving queries for a database. The host would like to guarantee the\nlowest maximum latency possible, while maximizing the total throughput\nof the system. Once again in this situation, without tier-aware memcg\nlimits the host can maximize throughput, but can only make severely\nunderestimated promises on the bottom line.\n\n---\n\nI would say so. I think that the two examples above are realistic\nscenarios that cloud providers and hyperscalers might face on tiered systems.\n\n---\n\nThis is a great question, and one that I hope to discuss at LSFMMBPF\nto see how people expect an interface like this to work.\n\nOver the past few weeks, I have been discussing this idea during the\nLinux Memory Hotness and Promotion biweekly calls with Gregory Price [1].\nOne of the proposals that we made there (but did not include in this\nseries) is the idea of \"fixed\" vs. \"opportunistic\" reclaim.\n\nFixed mode is what we have here -- start limiting toptier usage whenever\na workload goes above its fair slice of toptier.\nOpportunistic mode would allow workloads to use more toptier memory than\nits fair share, but only be restricted when toptier is pressured.\n\nWhat do you think about these two options? For the stated goal of this\nseries, which is to help maximize the bottom line for workloads, fair\nshare seemed to make sense. Implementing opportunistic mode changes\non top of this work would most likely just be another sysctl.\n\n---\n\nThat sounds good with me, my goal was to bring this out as an RFC patchset\nso folks could look at the code and understand the motivation, and then send\nout the LSFMMBPF topic proposal. In retrospect I think I should have done\nit in the opposite order. I'm sorry if this caused any confusion.\n\n---\n\nYes, that's right. I'm not sure if this is the right way to go long-term\n(say, past the next 5 years). My thinking was that I can stick with doing\nthis for toptier vs. non-toptier memory for now, and deal with having\n3+ tiers in the future, when we start to have systems with that many tiers.\nAFAICT two-tiered systems are still ~relatively new, and I don't think\nthere are a lot of genuine usecases for enforcing mid-tier memory limits\nas of now. Of course, I would be excited to learn about these usecases\nand work this patchset to support them as well if anybody has them.\n\n---\n\nSorry, I'm not sure that I completely understood this question. Are you\nreferring to the case where we have multiple nodes in the toptier?\nIf so, then all of those nodes are treated the same, and don't have\nunique limits. Or are you referring to the case where we have multiple\ntiers in the toptier? If so, I hope the answer above can answer this too.\n\n---\n\nGood point : -) I don't think cgroup mount options are a good idea,\nsince this would mean that we can have a set of cgroups self-policing\ntheir toptier usage, while another cgroup allocates memory unrestricted.\nThis would punish the self-policing cgroup and we would lose the benefit\nof having a bottomline performance guarantee.\n\n---\n\nThat sounds good to me, thank you again for reviewing this work!\nI hope you have a great day : -)\nJoshua\n\n[1] https://lore.kernel.org/linux-mm/c8bc2dce-d4ec-c16e-8df4-2624c48cfc06@google.com/",
          "reply_to": "Michal Hocko",
          "message_date": "2026-02-24",
          "message_id": ""
        },
        {
          "author": "Gregory Price",
          "summary": "Reviewer Gregory Price noted that the patch does not handle the case where tier-aware memcg limits are enabled, but the system has only one memory tier, which could lead to incorrect behavior and requested a fix for this edge case.\n\nReviewer Gregory Price expressed concern that the patch reduces the usefulness of secondary tiers of memory by introducing tier-aware memcg limits, which may lead to performance variance and discourage deployment on such machines.\n\nReviewer Gregory Price noted that tier-awareness is a significant blocker for deploying mixed workloads on large, dense memory systems with multiple tiers (2+), and suggested using the existing nobs (max/high/low/min) to proportionally control coherent memory tiers.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "blocker",
            "significant"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Just injecting a few points here\n(disclosure: I have been in the development loop for this feature)\n\n---\n\nYes / No.  This makes the assumption that you always want this.\n\nBarring a minimum Quality of Service mechanism (as Joshua explains)\nthis reduces the usefulness of a secondary tier of memory.\n\nServices will just prefer not to be deployed to these kinds of\nmachines because the performance variance is too high.\n\n---\n\nThe answer is unequivocally yes.\n\nLacking tier-awareness is actually a huge blocker for deploying mixed\nworkloads on large, dense memory systems with multiple tiers (2+).\n\nTechnically we're already at 4-ish tiers: DDR, CXL, ZSWAP, SWAP.\n\nWe have zswap/swap controls in cgroups already, we just lack that same\ncontrol for coherent memory tiers.  This tries to use the existing nobs\n(max/high/low/min) to do what they already do - just proportionally.\n\n~Gregory",
          "reply_to": "Joshua Hahn",
          "message_date": "2026-02-24",
          "message_id": ""
        },
        {
          "author": "Kaiyang Zhao",
          "summary": "Reviewer Kaiyang Zhao noted that the results of a preprint paper on arXiv confirmed that co-colocated workloads can have working set sizes exceeding top-tier memory capacity, causing contention and significant variations in tail latency and throughput.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "NEUTRAL"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Hello! I'm the author of the RFC in 2024. Just want to add that we've\nrecently released a preprint paper on arXiv that includes case studies\nwith a few of Meta's production workloads using a prototype version of\nthe patches.\n\nThe results confirmed that co-colocated workloads can have working set\nsizes exceeding the limited top-tier memory capacity given today's\nserver memory shapes and workload stacking settings, causing contention\nof top-tier memory. Workloads see significant variations in tail\nlatency and throughput depending on the share of top-tier tier memory\nthey get, which this patch set will alleviate.\n\nBest,\nKaiyang\n\n[1] https://arxiv.org/pdf/2602.08800",
          "reply_to": "Gregory Price",
          "message_date": "2026-02-24",
          "message_id": ""
        }
      ],
      "analysis_source": "llm",
      "patch_summary": "This patch introduces a new sysfs entry to toggle between memory cgroup (memcg) limits that are proportional to the system's top-tier capacity ratio. The goal is to make memcg limits tier-aware, allowing for more efficient resource allocation and utilization. This is achieved by adding a boolean flag `tier_aware_memcg_limits` and two new sysfs attributes: `tier_aware_memcg_show` and `tier_aware_memcg_store`. When enabled, the memcg limits will be adjusted based on the system's top-tier capacity ratio, allowing for more efficient resource allocation. The patch is part of a larger series that aims to make memory management more efficient and scalable."
    },
    "2026-02-26": {
      "report_file": "2026-02-26_ollama_llama3.1-8b.html",
      "developer": "Joshua Hahn",
      "reviews": [
        {
          "author": "Michal Hocko",
          "summary": "Raised concerns about thrashing and reclaim activity, suggesting that the effective toptier memory limits should be exposed as a new sysfs file. Also mentioned scenarios where reclaim activity occurs even when there is available memory.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "thrashing",
            "reclaim activity"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "",
          "reply_to": "",
          "message_date": "",
          "message_id": ""
        },
        {
          "author": "Joshua Hahn",
          "summary": "Acknowledged Michal's concerns and suggested two potential components to address thrashing: throttling promotions when toptier is facing cgroup-local pressure, and background balancing between nodes. Also mentioned that enforcing limits based on capacity could be a feasible solution.",
          "sentiment": "positive",
          "sentiment_signals": [
            "acknowledgment",
            "suggestion"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "",
          "reply_to": "Michal Hocko",
          "message_date": "",
          "message_id": ""
        }
      ],
      "analysis_source": "llm",
      "patch_summary": "The patch aims to make memcg limits tier-aware, addressing concerns around thrashing and reclaim activity."
    }
  }
}