{
  "thread_id": "20260202155634.650837-1-kas@kernel.org",
  "subject": "[PATCHv7 00/17] mm: Eliminate fake head pages from vmemmap optimization",
  "url": "https://lore.kernel.org/all/20260202155634.650837-1-kas@kernel.org/",
  "dates": {
    "2026-02-02": {
      "report_file": "2026-02-27_ollama_llama3.1-8b.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author is addressing a concern about passing down the head and tail page indices, instead proposing to pass the tail and head pages directly along with their order in preparation for changing how the head position is encoded in the tail page.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "preparation",
            "change"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Move MAX_FOLIO_ORDER definition from mm.h to mmzone.h.\n\nThis is preparation for adding the vmemmap_tails array to struct\npglist_data, which requires MAX_FOLIO_ORDER to be available in mmzone.h.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nAcked-by: David Hildenbrand (Red Hat) <david@kernel.org>\nAcked-by: Zi Yan <ziy@nvidia.com>\nAcked-by: Muchun Song <muchun.song@linux.dev>\n---\n include/linux/mm.h     | 31 -------------------------------\n include/linux/mmzone.h | 31 +++++++++++++++++++++++++++++++\n 2 files changed, 31 insertions(+), 31 deletions(-)\n\ndiff --git a/include/linux/mm.h b/include/linux/mm.h\nindex f8a8fd47399c..8d5fa655fea4 100644\n--- a/include/linux/mm.h\n+++ b/include/linux/mm.h\n@@ -27,7 +27,6 @@\n #include <linux/page-flags.h>\n #include <linux/page_ref.h>\n #include <linux/overflow.h>\n-#include <linux/sizes.h>\n #include <linux/sched.h>\n #include <linux/pgtable.h>\n #include <linux/kasan.h>\n@@ -2477,36 +2476,6 @@ static inline unsigned long folio_nr_pages(const struct folio *folio)\n \treturn folio_large_nr_pages(folio);\n }\n \n-#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)\n-/*\n- * We don't expect any folios that exceed buddy sizes (and consequently\n- * memory sections).\n- */\n-#define MAX_FOLIO_ORDER\t\tMAX_PAGE_ORDER\n-#elif defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\n-/*\n- * Only pages within a single memory section are guaranteed to be\n- * contiguous. By limiting folios to a single memory section, all folio\n- * pages are guaranteed to be contiguous.\n- */\n-#define MAX_FOLIO_ORDER\t\tPFN_SECTION_SHIFT\n-#elif defined(CONFIG_HUGETLB_PAGE)\n-/*\n- * There is no real limit on the folio size. We limit them to the maximum we\n- * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect\n- * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.\n- */\n-#define MAX_FOLIO_ORDER\t\tget_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)\n-#else\n-/*\n- * Without hugetlb, gigantic folios that are bigger than a single PUD are\n- * currently impossible.\n- */\n-#define MAX_FOLIO_ORDER\t\tPUD_ORDER\n-#endif\n-\n-#define MAX_FOLIO_NR_PAGES\t(1UL << MAX_FOLIO_ORDER)\n-\n /*\n  * compound_nr() returns the number of pages in this potentially compound\n  * page.  compound_nr() can be called on a tail page, and is defined to\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 3e51190a55e4..be8ce40b5638 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -23,6 +23,7 @@\n #include <linux/page-flags.h>\n #include <linux/local_lock.h>\n #include <linux/zswap.h>\n+#include <linux/sizes.h>\n #include <asm/page.h>\n \n /* Free memory management - zoned buddy allocator.  */\n@@ -61,6 +62,36 @@\n  */\n #define PAGE_ALLOC_COSTLY_ORDER 3\n \n+#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)\n+/*\n+ * We don't expect any folios that exceed buddy sizes (and consequently\n+ * memory sections).\n+ */\n+#define MAX_FOLIO_ORDER\t\tMAX_PAGE_ORDER\n+#elif defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\n+/*\n+ * Only pages within a single memory section are guaranteed to be\n+ * contiguous. By limiting folios to a single memory section, all folio\n+ * pages are guaranteed to be contiguous.\n+ */\n+#define MAX_FOLIO_ORDER\t\tPFN_SECTION_SHIFT\n+#elif defined(CONFIG_HUGETLB_PAGE)\n+/*\n+ * There is no real limit on the folio size. We limit them to the maximum we\n+ * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect\n+ * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.\n+ */\n+#define MAX_FOLIO_ORDER\t\tget_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)\n+#else\n+/*\n+ * Without hugetlb, gigantic folios that are bigger than a single PUD are\n+ * currently impossible.\n+ */\n+#define MAX_FOLIO_ORDER\t\tPUD_ORDER\n+#endif\n+\n+#define MAX_FOLIO_NR_PAGES\t(1UL << MAX_FOLIO_ORDER)\n+\n enum migratetype {\n \tMIGRATE_UNMOVABLE,\n \tMIGRATE_MOVABLE,\n-- \n2.51.2\n\n---\n\nInstead of passing down the head page and tail page index, pass the tail\nand head pages directly, as well as the order of the compound page.\n\nThis is a preparation for changing how the head position is encoded in\nthe tail page.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\n---\n include/linux/page-flags.h |  4 +++-\n mm/hugetlb.c               |  8 +++++---\n mm/internal.h              | 12 ++++++------\n mm/mm_init.c               |  2 +-\n mm/page_alloc.c            |  2 +-\n 5 files changed, 16 insertions(+), 12 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex f7a0e4af0c73..8a3694369e15 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -865,7 +865,9 @@ static inline bool folio_test_large(const struct folio *folio)\n \treturn folio_test_head(folio);\n }\n \n-static __always_inline void set_compound_head(struct page *page, struct page *head)\n+static __always_inline void set_compound_head(struct page *page,\n+\t\t\t\t\t      const struct page *head,\n+\t\t\t\t\t      unsigned int order)\n {\n \tWRITE_ONCE(page->compound_head, (unsigned long)head + 1);\n }\ndiff --git a/mm/hugetlb.c b/mm/hugetlb.c\nindex 6e855a32de3d..54ba7cd05a86 100644\n--- a/mm/hugetlb.c\n+++ b/mm/hugetlb.c\n@@ -3168,6 +3168,7 @@ int __alloc_bootmem_huge_page(struct hstate *h, int nid)\n \n /* Initialize [start_page:end_page_number] tail struct pages of a hugepage */\n static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n+\t\t\t\t\tstruct hstate *h,\n \t\t\t\t\tunsigned long start_page_number,\n \t\t\t\t\tunsigned long end_page_number)\n {\n@@ -3176,6 +3177,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n \tstruct page *page = folio_page(folio, start_page_number);\n \tunsigned long head_pfn = folio_pfn(folio);\n \tunsigned long pfn, end_pfn = head_pfn + end_page_number;\n+\tunsigned int order = huge_page_order(h);\n \n \t/*\n \t * As we marked all tail pages with memblock_reserved_mark_noinit(),\n@@ -3183,7 +3185,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n \t */\n \tfor (pfn = head_pfn + start_page_number; pfn < end_pfn; page++, pfn++) {\n \t\t__init_single_page(page, pfn, zone, nid);\n-\t\tprep_compound_tail((struct page *)folio, pfn - head_pfn);\n+\t\tprep_compound_tail(page, &folio->page, order);\n \t\tset_page_count(page, 0);\n \t}\n }\n@@ -3203,7 +3205,7 @@ static void __init hugetlb_folio_init_vmemmap(struct folio *folio,\n \t__folio_set_head(folio);\n \tret = folio_ref_freeze(folio, 1);\n \tVM_BUG_ON(!ret);\n-\thugetlb_folio_init_tail_vmemmap(folio, 1, nr_pages);\n+\thugetlb_folio_init_tail_vmemmap(folio, h, 1, nr_pages);\n \tprep_compound_head(&folio->page, huge_page_order(h));\n }\n \n@@ -3260,7 +3262,7 @@ static void __init prep_and_add_bootmem_folios(struct hstate *h,\n \t\t\t * time as this is early in boot and there should\n \t\t\t * be no contention.\n \t\t\t */\n-\t\t\thugetlb_folio_init_tail_vmemmap(folio,\n+\t\t\thugetlb_folio_init_tail_vmemmap(folio, h,\n \t\t\t\t\tHUGETLB_VMEMMAP_RESERVE_PAGES,\n \t\t\t\t\tpages_per_huge_page(h));\n \t\t}\ndiff --git a/mm/internal.h b/mm/internal.h\nindex d67e8bb75734..037ddcda25ff 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -879,13 +879,13 @@ static inline void prep_compound_head(struct page *page, unsigned int order)\n \t\tINIT_LIST_HEAD(&folio->_deferred_list);\n }\n \n-static inline void prep_compound_tail(struct page *head, int tail_idx)\n+static inline void prep_compound_tail(struct page *tail,\n+\t\t\t\t      const struct page *head,\n+\t\t\t\t      unsigned int order)\n {\n-\tstruct page *p = head + tail_idx;\n-\n-\tp->mapping = TAIL_MAPPING;\n-\tset_compound_head(p, head);\n-\tset_page_private(p, 0);\n+\ttail->mapping = TAIL_MAPPING;\n+\tset_compound_head(tail, head, order);\n+\tset_page_private(tail, 0);\n }\n \n void post_alloc_hook(struct page *page, unsigned int order, gfp_t gfp_flags);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..ba50f4c4337b 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -1099,7 +1099,7 @@ static void __ref memmap_init_compound(struct page *head,\n \t\tstruct page *page = pfn_to_page(pfn);\n \n \t\t__init_zone_device_page(page, pfn, zone_idx, nid, pgmap);\n-\t\tprep_compound_tail(head, pfn - head_pfn);\n+\t\tprep_compound_tail(page, head, order);\n \t\tset_page_count(page, 0);\n \t}\n \tprep_compound_head(head, order);\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..00c7ea958767 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -744,7 +744,7 @@ void prep_compound_page(struct page *page, unsigned int order)\n \n \t__SetPageHead(page);\n \tfor (i = 1; i < nr_pages; i++)\n-\t\tprep_compound_tail(page, i);\n+\t\tprep_compound_tail(page + i, page, order);\n \n \tprep_compound_head(page, order);\n }\n-- \n2.51.2\n\n---\n\nThe 'compound_head' field in the 'struct page' encodes whether the page\nis a tail and where to locate the head page. Bit 0 is set if the page is\na tail, and the remaining bits in the field point to the head page.\n\nAs preparation for changing how the field encodes information about the\nhead page, rename the field to 'compound_info'.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\n---\n .../admin-guide/kdump/vmcoreinfo.rst          |  2 +-\n Documentation/mm/vmemmap_dedup.rst            |  6 +++---\n include/linux/mm_types.h                      | 20 +++++++++----------\n include/linux/page-flags.h                    | 18 ++++++++---------\n include/linux/types.h                         |  2 +-\n kernel/vmcore_info.c                          |  2 +-\n mm/page_alloc.c                               |  2 +-\n mm/slab.h                                     |  2 +-\n mm/util.c                                     |  2 +-\n 9 files changed, 28 insertions(+), 28 deletions(-)\n\ndiff --git a/Documentation/admin-guide/kdump/vmcoreinfo.rst b/Documentation/admin-guide/kdump/vmcoreinfo.rst\nindex 404a15f6782c..7663c610fe90 100644\n--- a/Documentation/admin-guide/kdump/vmcoreinfo.rst\n+++ b/Documentation/admin-guide/kdump/vmcoreinfo.rst\n@@ -141,7 +141,7 @@ nodemask_t\n The size of a nodemask_t type. Used to compute the number of online\n nodes.\n \n-(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_head)\n+(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_info)\n ----------------------------------------------------------------------------------\n \n User-space tools compute their values based on the offset of these\ndiff --git a/Documentation/mm/vmemmap_dedup.rst b/Documentation/mm/vmemmap_dedup.rst\nindex b4a55b6569fa..1863d88d2dcb 100644\n--- a/Documentation/mm/vmemmap_dedup.rst\n+++ b/Documentation/mm/vmemmap_dedup.rst\n@@ -24,7 +24,7 @@ For each base page, there is a corresponding ``struct page``.\n Within the HugeTLB subsystem, only the first 4 ``struct page`` are used to\n contain unique information about a HugeTLB page. ``__NR_USED_SUBPAGE`` provides\n this upper limit. The only 'useful' information in the remaining ``struct page``\n-is the compound_head field, and this field is the same for all tail pages.\n+is the compound_info field, and this field is the same for all tail pages.\n \n By removing redundant ``struct page`` for HugeTLB pages, memory can be returned\n to the buddy allocator for other uses.\n@@ -124,10 +124,10 @@ Here is how things look before optimization::\n  |           |\n  +-----------+\n \n-The value of page->compound_head is the same for all tail pages. The first\n+The value of page->compound_info is the same for all tail pages. The first\n page of ``struct page`` (page 0) associated with the HugeTLB page contains the 4\n ``struct page`` necessary to describe the HugeTLB. The only use of the remaining\n-pages of ``struct page`` (page 1 to page 7) is to point to page->compound_head.\n+pages of ``struct page`` (page 1 to page 7) is to point to page->compound_info.\n Therefore, we can remap pages 1 to 7 to page 0. Only 1 page of ``struct page``\n will be used for each HugeTLB page. This will allow us to free the remaining\n 7 pages to the buddy allocator.\ndiff --git a/include/linux/mm_types.h b/include/linux/mm_types.h\nindex 3cc8ae722886..7bc82a2b889f 100644\n--- a/include/linux/mm_types.h\n+++ b/include/linux/mm_types.h\n@@ -126,14 +126,14 @@ struct page {\n \t\t\tatomic_long_t pp_ref_count;\n \t\t};\n \t\tstruct {\t/* Tail pages of compound page */\n-\t\t\tunsigned long compound_head;\t/* Bit zero is set */\n+\t\t\tunsigned long compound_info;\t/* Bit zero is set */\n \t\t};\n \t\tstruct {\t/* ZONE_DEVICE pages */\n \t\t\t/*\n-\t\t\t * The first word is used for compound_head or folio\n+\t\t\t * The first word is used for compound_info or folio\n \t\t\t * pgmap\n \t\t\t */\n-\t\t\tvoid *_unused_pgmap_compound_head;\n+\t\t\tvoid *_unused_pgmap_compound_info;\n \t\t\tvoid *zone_device_data;\n \t\t\t/*\n \t\t\t * ZONE_DEVICE private pages are counted as being\n@@ -409,7 +409,7 @@ struct folio {\n \t/* private: avoid cluttering the output */\n \t\t\t\t/* For the Unevictable \"LRU list\" slot */\n \t\t\t\tstruct {\n-\t\t\t\t\t/* Avoid compound_head */\n+\t\t\t\t\t/* Avoid compound_info */\n \t\t\t\t\tvoid *__filler;\n \t/* public: */\n \t\t\t\t\tunsigned int mlock_count;\n@@ -510,7 +510,7 @@ struct folio {\n FOLIO_MATCH(flags, flags);\n FOLIO_MATCH(lru, lru);\n FOLIO_MATCH(mapping, mapping);\n-FOLIO_MATCH(compound_head, lru);\n+FOLIO_MATCH(compound_info, lru);\n FOLIO_MATCH(__folio_index, index);\n FOLIO_MATCH(private, private);\n FOLIO_MATCH(_mapcount, _mapcount);\n@@ -529,7 +529,7 @@ FOLIO_MATCH(_last_cpupid, _last_cpupid);\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + sizeof(struct page))\n FOLIO_MATCH(flags, _flags_1);\n-FOLIO_MATCH(compound_head, _head_1);\n+FOLIO_MATCH(compound_info, _head_1);\n FOLIO_MATCH(_mapcount, _mapcount_1);\n FOLIO_MATCH(_refcount, _refcount_1);\n #undef FOLIO_MATCH\n@@ -537,13 +537,13 @@ FOLIO_MATCH(_refcount, _refcount_1);\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + 2 * sizeof(struct page))\n FOLIO_MATCH(flags, _flags_2);\n-FOLIO_MATCH(compound_head, _head_2);\n+FOLIO_MATCH(compound_info, _head_2);\n #undef FOLIO_MATCH\n #define FOLIO_MATCH(pg, fl)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + 3 * sizeof(struct page))\n FOLIO_MATCH(flags, _flags_3);\n-FOLIO_MATCH(compound_head, _head_3);\n+FOLIO_MATCH(compound_info, _head_3);\n #undef FOLIO_MATCH\n \n /**\n@@ -609,8 +609,8 @@ struct ptdesc {\n #define TABLE_MATCH(pg, pt)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct page, pg) == offsetof(struct ptdesc, pt))\n TABLE_MATCH(flags, pt_flags);\n-TABLE_MATCH(compound_head, pt_list);\n-TABLE_MATCH(compound_head, _pt_pad_1);\n+TABLE_MATCH(compound_info, pt_list);\n+TABLE_MATCH(compound_info, _pt_pad_1);\n TABLE_MATCH(mapping, __page_mapping);\n TABLE_MATCH(__folio_index, pt_index);\n TABLE_MATCH(rcu_head, pt_rcu_head);\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex 8a3694369e15..aa46d49e82f7 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -213,7 +213,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t/*\n \t * Only addresses aligned with PAGE_SIZE of struct page may be fake head\n \t * struct page. The alignment check aims to avoid access the fields (\n-\t * e.g. compound_head) of the @page[1]. It can avoid touch a (possibly)\n+\t * e.g. compound_info) of the @page[1]. It can avoid touch a (possibly)\n \t * cold cacheline in some cases.\n \t */\n \tif (IS_ALIGNED((unsigned long)page, PAGE_SIZE) &&\n@@ -223,7 +223,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t\t * because the @page is a compound page composed with at least\n \t\t * two contiguous pages.\n \t\t */\n-\t\tunsigned long head = READ_ONCE(page[1].compound_head);\n+\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n \n \t\tif (likely(head & 1))\n \t\t\treturn (const struct page *)(head - 1);\n@@ -281,7 +281,7 @@ static __always_inline int page_is_fake_head(const struct page *page)\n \n static __always_inline unsigned long _compound_head(const struct page *page)\n {\n-\tunsigned long head = READ_ONCE(page->compound_head);\n+\tunsigned long head = READ_ONCE(page->compound_info);\n \n \tif (unlikely(head & 1))\n \t\treturn head - 1;\n@@ -320,13 +320,13 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n \n static __always_inline int PageTail(const struct page *page)\n {\n-\treturn READ_ONCE(page->compound_head) & 1 || page_is_fake_head(page);\n+\treturn READ_ONCE(page->compound_info) & 1 || page_is_fake_head(page);\n }\n \n static __always_inline int PageCompound(const struct page *page)\n {\n \treturn test_bit(PG_head, &page->flags.f) ||\n-\t       READ_ONCE(page->compound_head) & 1;\n+\t       READ_ONCE(page->compound_info) & 1;\n }\n \n #define\tPAGE_POISON_PATTERN\t-1l\n@@ -348,7 +348,7 @@ static const unsigned long *const_folio_flags(const struct folio *folio,\n {\n \tconst struct page *page = &folio->page;\n \n-\tVM_BUG_ON_PGFLAGS(page->compound_head & 1, page);\n+\tVM_BUG_ON_PGFLAGS(page->compound_info & 1, page);\n \tVM_BUG_ON_PGFLAGS(n > 0 && !test_bit(PG_head, &page->flags.f), page);\n \treturn &page[n].flags.f;\n }\n@@ -357,7 +357,7 @@ static unsigned long *folio_flags(struct folio *folio, unsigned n)\n {\n \tstruct page *page = &folio->page;\n \n-\tVM_BUG_ON_PGFLAGS(page->compound_head & 1, page);\n+\tVM_BUG_ON_PGFLAGS(page->compound_info & 1, page);\n \tVM_BUG_ON_PGFLAGS(n > 0 && !test_bit(PG_head, &page->flags.f), page);\n \treturn &page[n].flags.f;\n }\n@@ -869,12 +869,12 @@ static __always_inline void set_compound_head(struct page *page,\n \t\t\t\t\t      const struct page *head,\n \t\t\t\t\t      unsigned int order)\n {\n-\tWRITE_ONCE(page->compound_head, (unsigned long)head + 1);\n+\tWRITE_ONCE(page->compound_info, (unsigned long)head + 1);\n }\n \n static __always_inline void clear_compound_head(struct page *page)\n {\n-\tWRITE_ONCE(page->compound_head, 0);\n+\tWRITE_ONCE(page->compound_info, 0);\n }\n \n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\ndiff --git a/include/linux/types.h b/include/linux/types.h\nindex f69be881369f..604697abf151 100644\n--- a/include/linux/types.h\n+++ b/include/linux/types.h\n@@ -234,7 +234,7 @@ struct ustat {\n  *\n  * This guarantee is important for few reasons:\n  *  - future call_rcu_lazy() will make use of lower bits in the pointer;\n- *  - the structure shares storage space in struct page with @compound_head,\n+ *  - the structure shares storage space in struct page with @compound_info,\n  *    which encode PageTail() in bit 0. The guarantee is needed to avoid\n  *    false-positive PageTail().\n  */\ndiff --git a/kernel/vmcore_info.c b/kernel/vmcore_info.c\nindex 46198580373a..0a46df3e3db9 100644\n--- a/kernel/vmcore_info.c\n+++ b/kernel/vmcore_info.c\n@@ -198,7 +198,7 @@ static int __init crash_save_vmcoreinfo_init(void)\n \tVMCOREINFO_OFFSET(page, lru);\n \tVMCOREINFO_OFFSET(page, _mapcount);\n \tVMCOREINFO_OFFSET(page, private);\n-\tVMCOREINFO_OFFSET(page, compound_head);\n+\tVMCOREINFO_OFFSET(page, compound_info);\n \tVMCOREINFO_OFFSET(pglist_data, node_zones);\n \tVMCOREINFO_OFFSET(pglist_data, nr_zones);\n #ifdef CONFIG_FLATMEM\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 00c7ea958767..cb7375eb1713 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -731,7 +731,7 @@ static inline bool pcp_allowed_order(unsigned int order)\n  * The first PAGE_SIZE page is called the \"head page\" and have PG_head set.\n  *\n  * The remaining PAGE_SIZE pages are called \"tail pages\". PageTail() is encoded\n- * in bit 0 of page->compound_head. The rest of bits is pointer to head page.\n+ * in bit 0 of page->compound_info. The rest of bits is pointer to head page.\n  *\n  * The first tail page's ->compound_order holds the order of allocation.\n  * This usage means that zero-order pages may not be compound.\ndiff --git a/mm/slab.h b/mm/slab.h\nindex e767aa7e91b0..8a2a9c6c697b 100644\n--- a/mm/slab.h\n+++ b/mm/slab.h\n@@ -100,7 +100,7 @@ struct slab {\n #define SLAB_MATCH(pg, sl)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct page, pg) == offsetof(struct slab, sl))\n SLAB_MATCH(flags, flags);\n-SLAB_MATCH(compound_head, slab_cache);\t/* Ensure bit 0 is clear */\n+SLAB_MATCH(compound_info, slab_cache);\t/* Ensure bit 0 is clear */\n SLAB_MATCH(_refcount, __page_refcount);\n #ifdef CONFIG_MEMCG\n SLAB_MATCH(memcg_data, obj_exts);\ndiff --git a/mm/util.c b/mm/util.c\nindex b05ab6f97e11..3ebcb9e6035c 100644\n--- a/mm/util.c\n+++ b/mm/util.c\n@@ -1247,7 +1247,7 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n again:\n \tmemset(&ps->folio_snapshot, 0, sizeof(struct folio));\n \tmemcpy(&ps->page_snapshot, page, sizeof(*page));\n-\thead = ps->page_snapshot.compound_head;\n+\thead = ps->page_snapshot.compound_info;\n \tif ((head & 1) == 0) {\n \t\tps->idx = 0;\n \t\tfoliop = (struct folio *)&ps->page_snapshot;\n-- \n2.51.2\n\n---\n\nMove set_compound_head() and clear_compound_head() to be adjacent to the\ncompound_head() function in page-flags.h.\n\nThese functions encode and decode the same compound_info field, so\nkeeping them together makes it easier to verify their logic is\nconsistent, especially when the encoding changes.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\n---\n include/linux/page-flags.h | 24 ++++++++++++------------\n 1 file changed, 12 insertions(+), 12 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex aa46d49e82f7..d14a17ffb55b 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -290,6 +290,18 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n \n #define compound_head(page)\t((typeof(page))_compound_head(page))\n \n+static __always_inline void set_compound_head(struct page *page,\n+\t\t\t\t\t      const struct page *head,\n+\t\t\t\t\t      unsigned int order)\n+{\n+\tWRITE_ONCE(page->compound_info, (unsigned long)head + 1);\n+}\n+\n+static __always_inline void clear_compound_head(struct page *page)\n+{\n+\tWRITE_ONCE(page->compound_info, 0);\n+}\n+\n /**\n  * page_folio - Converts from page to folio.\n  * @p: The page.\n@@ -865,18 +877,6 @@ static inline bool folio_test_large(const struct folio *folio)\n \treturn folio_test_head(folio);\n }\n \n-static __always_inline void set_compound_head(struct page *page,\n-\t\t\t\t\t      const struct page *head,\n-\t\t\t\t\t      unsigned int order)\n-{\n-\tWRITE_ONCE(page->compound_info, (unsigned long)head + 1);\n-}\n-\n-static __always_inline void clear_compound_head(struct page *page)\n-{\n-\tWRITE_ONCE(page->compound_info, 0);\n-}\n-\n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\n static inline void ClearPageCompound(struct page *page)\n {\n-- \n2.51.2\n\n---\n\nThe upcoming change to the HugeTLB vmemmap optimization (HVO) requires\nstruct pages of the head page to be naturally aligned with regard to the\nfolio size.\n\nAlign vmemmap to MAX_FOLIO_NR_PAGES.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\n---\n arch/riscv/mm/init.c | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a/arch/riscv/mm/init.c b/arch/riscv/mm/init.c\nindex 21d534824624..c555b9a4fdce 100644\n--- a/arch/riscv/mm/init.c\n+++ b/arch/riscv/mm/init.c\n@@ -63,7 +63,8 @@ phys_addr_t phys_ram_base __ro_after_init;\n EXPORT_SYMBOL(phys_ram_base);\n \n #ifdef CONFIG_SPARSEMEM_VMEMMAP\n-#define VMEMMAP_ADDR_ALIGN\t(1ULL << SECTION_SIZE_BITS)\n+#define VMEMMAP_ADDR_ALIGN\tmax(1ULL << SECTION_SIZE_BITS, \\\n+\t\t\t\t    MAX_FOLIO_NR_PAGES * sizeof(struct page))\n \n unsigned long vmemmap_start_pfn __ro_after_init;\n EXPORT_SYMBOL(vmemmap_start_pfn);\n-- \n2.51.2",
          "reply_to": "",
          "message_date": "2026-02-02",
          "message_id": ""
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author addressed a concern about the alignment of struct pages in HugeTLB vmemmap optimization, explained that for cases where sizeof(struct page) is power-of-2, they plan to change the encoding of compound_info to store a mask that can be applied to the virtual address of the tail page to access the head page. This modification will allow all tail pages of the same order to have identical 'compound_info', regardless of the compound page they are associated with, paving the way for eliminating fake heads.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\nstruct pages of the head page to be naturally aligned with regard to the\nfolio size.\n\nAlign vmemmap to MAX_FOLIO_NR_PAGES.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\n---\n arch/loongarch/include/asm/pgtable.h | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h\nindex c33b3bcb733e..f9416acb9156 100644\n--- a/arch/loongarch/include/asm/pgtable.h\n+++ b/arch/loongarch/include/asm/pgtable.h\n@@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];\n \t min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)\n #endif\n \n-#define vmemmap\t\t((struct page *)((VMALLOC_END + PMD_SIZE) & PMD_MASK))\n+#define VMEMMAP_ALIGN\tmax(PMD_SIZE, MAX_FOLIO_NR_PAGES * sizeof(struct page))\n+#define vmemmap\t\t((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))\n #define VMEMMAP_END\t((unsigned long)vmemmap + VMEMMAP_SIZE - 1)\n \n #define KFENCE_AREA_START\t(VMEMMAP_END + 1)\n-- \n2.51.2\n\n---\n\nFor tail pages, the kernel uses the 'compound_info' field to get to the\nhead page. The bit 0 of the field indicates whether the page is a\ntail page, and if set, the remaining bits represent a pointer to the\nhead page.\n\nFor cases when size of struct page is power-of-2, change the encoding of\ncompound_info to store a mask that can be applied to the virtual address\nof the tail page in order to access the head page. It is possible\nbecause struct page of the head page is naturally aligned with regards\nto order of the page.\n\nThe significant impact of this modification is that all tail pages of\nthe same order will now have identical 'compound_info', regardless of\nthe compound page they are associated with. This paves the way for\neliminating fake heads.\n\nThe HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only\napplied when the sizeof(struct page) is power-of-2. Having identical\ntail pages allows the same page to be mapped into the vmemmap of all\npages, maintaining memory savings without fake heads.\n\nIf sizeof(struct page) is not power-of-2, there is no functional\nchanges.\n\nLimit mask usage to HugeTLB vmemmap optimization (HVO) where it makes\na difference. The approach with mask would work in the wider set of\nconditions, but it requires validating that struct pages are naturally\naligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\n---\n include/linux/page-flags.h | 81 ++++++++++++++++++++++++++++++++++----\n mm/slab.h                  | 16 ++++++--\n mm/util.c                  | 16 ++++++--\n 3 files changed, 97 insertions(+), 16 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex d14a17ffb55b..8f2c7fbc739b 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -198,6 +198,29 @@ enum pageflags {\n \n #ifndef __GENERATING_BOUNDS_H\n \n+/*\n+ * For tail pages, if the size of struct page is power-of-2 ->compound_info\n+ * encodes the mask that converts the address of the tail page address to\n+ * the head page address.\n+ *\n+ * Otherwise, ->compound_info has direct pointer to head pages.\n+ */\n+static __always_inline bool compound_info_has_mask(void)\n+{\n+\t/*\n+\t * Limit mask usage to HugeTLB vmemmap optimization (HVO) where it\n+\t * makes a difference.\n+\t *\n+\t * The approach with mask would work in the wider set of conditions,\n+\t * but it requires validating that struct pages are naturally aligned\n+\t * for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n+\t */\n+\tif (!IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP))\n+\t\treturn false;\n+\n+\treturn is_power_of_2(sizeof(struct page));\n+}\n+\n #ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP\n DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\n \n@@ -210,6 +233,10 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \tif (!static_branch_unlikely(&hugetlb_optimize_vmemmap_key))\n \t\treturn page;\n \n+\t/* Fake heads only exists if compound_info_has_mask() is true */\n+\tif (!compound_info_has_mask())\n+\t\treturn page;\n+\n \t/*\n \t * Only addresses aligned with PAGE_SIZE of struct page may be fake head\n \t * struct page. The alignment check aims to avoid access the fields (\n@@ -223,10 +250,14 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t\t * because the @page is a compound page composed with at least\n \t\t * two contiguous pages.\n \t\t */\n-\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n+\t\tunsigned long info = READ_ONCE(page[1].compound_info);\n \n-\t\tif (likely(head & 1))\n-\t\t\treturn (const struct page *)(head - 1);\n+\t\t/* See set_compound_head() */\n+\t\tif (likely(info & 1)) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\n+\t\t\treturn (const struct page *)(p & info);\n+\t\t}\n \t}\n \treturn page;\n }\n@@ -281,11 +312,26 @@ static __always_inline int page_is_fake_head(const struct page *page)\n \n static __always_inline unsigned long _compound_head(const struct page *page)\n {\n-\tunsigned long head = READ_ONCE(page->compound_info);\n+\tunsigned long info = READ_ONCE(page->compound_info);\n \n-\tif (unlikely(head & 1))\n-\t\treturn head - 1;\n-\treturn (unsigned long)page_fixed_fake_head(page);\n+\t/* Bit 0 encodes PageTail() */\n+\tif (!(info & 1))\n+\t\treturn (unsigned long)page_fixed_fake_head(page);\n+\n+\t/*\n+\t * If compound_info_has_mask() is false, the rest of compound_info is\n+\t * the pointer to the head page.\n+\t */\n+\tif (!compound_info_has_mask())\n+\t\treturn info - 1;\n+\n+\t/*\n+\t * If compoun_info_has_mask() is true the rest of the info encodes\n+\t * the mask that converts the address of the tail page to the head page.\n+\t *\n+\t * No need to clear bit 0 in the mask as 'page' always has it clear.\n+\t */\n+\treturn (unsigned long)page & info;\n }\n \n #define compound_head(page)\t((typeof(page))_compound_head(page))\n@@ -294,7 +340,26 @@ static __always_inline void set_compound_head(struct page *page,\n \t\t\t\t\t      const struct page *head,\n \t\t\t\t\t      unsigned int order)\n {\n-\tWRITE_ONCE(page->compound_info, (unsigned long)head + 1);\n+\tunsigned int shift;\n+\tunsigned long mask;\n+\n+\tif (!compound_info_has_mask()) {\n+\t\tWRITE_ONCE(page->compound_info, (unsigned long)head | 1);\n+\t\treturn;\n+\t}\n+\n+\t/*\n+\t * If the size of struct page is power-of-2, bits [shift:0] of the\n+\t * virtual address of compound head are zero.\n+\t *\n+\t * Calculate mask that can be applied to the virtual address of\n+\t * the tail page to get address of the head page.\n+\t */\n+\tshift = order + order_base_2(sizeof(struct page));\n+\tmask = GENMASK(BITS_PER_LONG - 1, shift);\n+\n+\t/* Bit 0 encodes PageTail() */\n+\tWRITE_ONCE(page->compound_info, mask | 1);\n }\n \n static __always_inline void clear_compound_head(struct page *page)\ndiff --git a/mm/slab.h b/mm/slab.h\nindex 8a2a9c6c697b..f68c3ac8126f 100644\n--- a/mm/slab.h\n+++ b/mm/slab.h\n@@ -137,11 +137,19 @@ static_assert(IS_ALIGNED(offsetof(struct slab, freelist), sizeof(struct freelist\n  */\n static inline struct slab *page_slab(const struct page *page)\n {\n-\tunsigned long head;\n+\tunsigned long info;\n+\n+\tinfo = READ_ONCE(page->compound_info);\n+\tif (info & 1) {\n+\t\t/* See compound_head() */\n+\t\tif (compound_info_has_mask()) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\t\t\tpage = (struct page *)(p & info);\n+\t\t} else {\n+\t\t\tpage = (struct page *)(info - 1);\n+\t\t}\n+\t}\n \n-\thead = READ_ONCE(page->compound_head);\n-\tif (head & 1)\n-\t\tpage = (struct page *)(head - 1);\n \tif (data_race(page->page_type >> 24) != PGTY_slab)\n \t\tpage = NULL;\n \ndiff --git a/mm/util.c b/mm/util.c\nindex 3ebcb9e6035c..20dccf2881d7 100644\n--- a/mm/util.c\n+++ b/mm/util.c\n@@ -1237,7 +1237,7 @@ static void set_ps_flags(struct page_snapshot *ps, const struct folio *folio,\n  */\n void snapshot_page(struct page_snapshot *ps, const struct page *page)\n {\n-\tunsigned long head, nr_pages = 1;\n+\tunsigned long info, nr_pages = 1;\n \tstruct folio *foliop;\n \tint loops = 5;\n \n@@ -1247,8 +1247,8 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n again:\n \tmemset(&ps->folio_snapshot, 0, sizeof(struct folio));\n \tmemcpy(&ps->page_snapshot, page, sizeof(*page));\n-\thead = ps->page_snapshot.compound_info;\n-\tif ((head & 1) == 0) {\n+\tinfo = ps->page_snapshot.compound_info;\n+\tif (!(info & 1)) {\n \t\tps->idx = 0;\n \t\tfoliop = (struct folio *)&ps->page_snapshot;\n \t\tif (!folio_test_large(foliop)) {\n@@ -1259,7 +1259,15 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n \t\t}\n \t\tfoliop = (struct folio *)page;\n \t} else {\n-\t\tfoliop = (struct folio *)(head - 1);\n+\t\t/* See compound_head() */\n+\t\tif (compound_info_has_mask()) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\n+\t\t\tfoliop = (struct folio *)(p & info);\n+\t\t} else {\n+\t\t\tfoliop = (struct folio *)(info - 1);\n+\t\t}\n+\n \t\tps->idx = folio_page_idx(foliop, page);\n \t}\n \n-- \n2.51.2\n\n---\n\nWith the upcoming changes to HVO, a single page of tail struct pages\nwill be shared across all huge pages of the same order on a node. Since\nhuge pages on the same node may belong to different zones, the zone\ninformation stored in shared tail page flags would be incorrect.\n\nAlways fetch zone information from the head page, which has unique and\ncorrect zone flags for each compound page.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nAcked-by: Zi Yan <ziy@nvidia.com>\n---\n include/linux/mmzone.h | 1 +\n 1 file changed, 1 insertion(+)\n\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex be8ce40b5638..192143b5cdc0 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n \n static inline enum zone_type page_zonenum(const struct page *page)\n {\n+\tpage = compound_head(page);\n \treturn memdesc_zonenum(page->flags);\n }\n \n-- \n2.51.2\n\n---\n\nIf page->compound_info encodes a mask, it is expected that vmemmap to be\nnaturally aligned to the maximum folio size.\n\nAdd a VM_BUG_ON() to check the alignment.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nAcked-by: Zi Yan <ziy@nvidia.com>\n---\n mm/sparse.c | 7 +++++++\n 1 file changed, 7 insertions(+)\n\ndiff --git a/mm/sparse.c b/mm/sparse.c\nindex b5b2b6f7041b..6c9b62607f3f 100644\n--- a/mm/sparse.c\n+++ b/mm/sparse.c\n@@ -600,6 +600,13 @@ void __init sparse_init(void)\n \tBUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));\n \tmemblocks_present();\n \n+\tif (compound_info_has_mask()) {\n+\t\tunsigned long alignment;\n+\n+\t\talignment = MAX_FOLIO_NR_PAGES * sizeof(struct page);\n+\t\tVM_BUG_ON(!IS_ALIGNED((unsigned long) pfn_to_page(0), alignment));\n+\t}\n+\n \tpnum_begin = first_present_section_nr();\n \tnid_begin = sparse_early_nid(__nr_to_section(pnum_begin));\n \n-- \n2.51.2\n\n---\n\nThis series removes \"fake head pages\" from the HugeTLB vmemmap\noptimization (HVO) by changing how tail pages encode their relationship\nto the head page.\n\nIt simplifies compound_head() and page_ref_add_unless(). Both are in the\nhot path.\n\nBackground\n==========\n\nHVO reduces memory overhead by freeing vmemmap pages for HugeTLB pages\nand remapping the freed virtual addresses to a single physical page.\nPreviously, all tail page vmemmap entries were remapped to the first\nvmemmap page (containing the head struct page), creating \"fake heads\" -\ntail pages that appear to have PG_head set when accessed through the\ndeduplicated vmemmap.\n\nThis required special handling in compound_head() to detect and work\naround fake heads, adding complexity and overhead to a very hot path.\n\nNew Approach\n============\n\nFor architectures/configs where sizeof(struct page) is a power of 2 (the\ncommon case), this series changes how position of the head page is encoded\nin the tail pages.\n\nInstead of storing a pointer to the head page, the ->compound_info\n(renamed from ->compound_head) now stores a mask.\n\nThe mask can be applied to any tail page's virtual address to compute\nthe head page address. Critically, all tail pages of the same order now\nhave identical compound_info values, regardless of which compound page\nthey belong to.\n\nThe key insight is that all tail pages of the same order now have\nidentical compound_info values, regardless of which compound page they\nbelong to. This allows a single page of tail struct pages to be shared\nacross all huge pages of the same order on a NUMA node.\n\nBenefits\n========\n\n1. Simplified compound_head(): No fake head detection needed, can be\n   implemented in a branchless manner.\n\n2. Simplified page_ref_add_unless(): RCU protection removed since there's\n   no race with fake head remapping.\n\n3. Cleaner architecture: The shared tail pages are truly read-only and\n   contain valid tail page metadata.\n\nIf sizeof(struct page) is not power-of-2, there are no functional changes.\nHVO is not supported in this configuration.\n\nI had hoped to see performance improvement, but my testing thus far has\nshown either no change or only a slight improvement within the noise.\n\nSeries Organization\n===================\n\nPatch 1: Preparation - move MAX_FOLIO_ORDER to mmzone.h\nPatches 2-4: Refactoring - interface changes, field rename, code movement\nPatches 5-6: Arch fixes - align vmemmap for riscv and LoongArch\nPatch 7: Core change - new mask-based compound_head() encoding\nPatch 8: Correctness fix - page_zonenum() must use head page\nPatch 9: Add memmap alignment check for compound_info_has_mask()\nPatch 10: Refactor vmemmap_walk for new design\nPatch 11: Eliminate fake heads with shared tail pages\nPatches 12-15: Cleanup - remove fake head infrastructure\nPatch 16: Documentation update\nPatch 17: Get rid of opencoded compound_head() in page_slab()\n\nChanges in v6:\n==============\n  - Simplify memmap alignment check in mm/sparse.c: use VM_BUG_ON()\n    (Muchun)\n\n  - Store struct page pointers in vmemmap_tails[] instead of PFNs.\n    (Muchun)\n\n  - Fix build error on powerpc due to negative NR_VMEMMAP_TAILS.\n\nChanges in v5:\n==============\n  - Rebased to mm-everything-2026-01-27-04-35\n\n  - Add arch-specific patches to align vmemmap to maximal folio size\n    for riscv and LoongArch architectures.\n\n  - Strengthen the memmap alignment check in mm/sparse.c: use BUG()\n    for CONFIG_DEBUG_VM, WARN() otherwise. (Muchun)\n\n  - Use cmpxchg() instead of hugetlb_lock to update vmemmap_tails\n    array. (Muchun)\n\n  - Update page_slab().\n\nChanges in v4:\n==============\n  - Fix build issues due to linux/mmzone.h <-> linux/pgtable.h\n    dependency loop by avoiding including linux/pgtable.h into\n    linux/mmzone.h\n\n  - Rework vmemmap_remap_alloc() interface. (Muchun)\n\n  - Use &folio->page instead of folio address for optimization\n    target. (Muchun)\n\nChanges in v3:\n==============\n  - Fixed error recovery path in vmemmap_remap_free() to pass correct start\n    address for TLB flush. (Muchun)\n\n  - Wrapped the mask-based compound_info encoding within CONFIG_SPARSEMEM_VMEMMAP\n    check via compound_info_has_mask(). For other memory models, alignment\n    guarantees are harder to verify. (Muchun)\n\n  - Updated vmemmap_dedup.rst documentation wording: changed \"vmemmap_tail\n    shared for the struct hstate\" to \"A single, per-node page frame shared\n    among all hugepages of the same size\". (Muchun)\n\n  - Fixed build error with MAX_FOLIO_ORDER expanding to undefined PUD_ORDER\n    in certain configurations. (kernel test robot)\n\nChanges in v2:\n==============\n\n- Handle boot-allocated huge pages correctly. (Frank)\n\n- Changed from per-hstate vmemmap_tail to per-node vmemmap_tails[] array\n  in pglist_data. (Muchun)\n\n- Added spin_lock(&hugetlb_lock) protection in vmemmap_get_tail() to fix\n  a race condition where two threads could both allocate tail pages.\n  The losing thread now properly frees its allocated page. (Usama)\n\n- Add warning if memmap is not aligned to MAX_FOLIO_SIZE, which is\n  required for the mask approach. (Muchun)\n\n- Make page_zonenum() use head page - correctness fix since shared\n  tail pages cannot have valid zone information. (Muchun)\n\n- Added 'const' qualifier to head parameter in set_compound_head() and\n  prep_compound_tail(). (Usama)\n\n- Updated commit messages.\n\nKiryl Shutsemau (17):\n  mm: Move MAX_FOLIO_ORDER definition to mmzone.h\n  mm: Change the interface of prep_compound_tail()\n  mm: Rename the 'compound_head' field in the 'struct page' to\n    'compound_info'\n  mm: Move set/clear_compound_head() next to compound_head()\n  riscv/mm: Align vmemmap to maximal folio size\n  LoongArch/mm: Align vmemmap to maximal folio size\n  mm: Rework compound_head() for power-of-2 sizeof(struct page)\n  mm: Make page_zonenum() use head page\n  mm/sparse: Check memmap alignment for compound_info_has_mask()\n  mm/hugetlb: Refactor code around vmemmap_walk\n  mm/hugetlb: Remove fake head pages\n  mm: Drop fake head checks\n  hugetlb: Remove VMEMMAP_SYNCHRONIZE_RCU\n  mm/hugetlb: Remove hugetlb_optimize_vmemmap_key static key\n  mm: Remove the branch from compound_head()\n  hugetlb: Update vmemmap_dedup.rst\n  mm/slab: Use compound_head() in page_slab()\n\n .../admin-guide/kdump/vmcoreinfo.rst          |   2 +-\n Documentation/mm/vmemmap_dedup.rst            |  62 ++--\n arch/loongarch/include/asm/pgtable.h          |   3 +-\n arch/riscv/mm/init.c                          |   3 +-\n include/linux/mm.h                            |  31 --\n include/linux/mm_types.h                      |  20 +-\n include/linux/mmzone.h                        |  47 +++\n include/linux/page-flags.h                    | 167 +++++-----\n include/linux/page_ref.h                      |   8 +-\n include/linux/types.h                         |   2 +-\n kernel/vmcore_info.c                          |   2 +-\n mm/hugetlb.c                                  |   8 +-\n mm/hugetlb_vmemmap.c                          | 288 ++++++++----------\n mm/internal.h                                 |  12 +-\n mm/mm_init.c                                  |   2 +-\n mm/page_alloc.c                               |   4 +-\n mm/slab.h                                     |   8 +-\n mm/sparse-vmemmap.c                           |  43 ++-\n mm/sparse.c                                   |   7 +\n mm/util.c                                     |  16 +-\n 20 files changed, 363 insertions(+), 372 deletions(-)\n\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nMove MAX_FOLIO_ORDER definition from mm.h to mmzone.h.\n\nThis is preparation for adding the vmemmap_tails array to struct\npglist_data, which requires MAX_FOLIO_ORDER to be available in mmzone.h.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nAcked-by: David Hildenbrand (Red Hat) <david@kernel.org>\nAcked-by: Zi Yan <ziy@nvidia.com>\nAcked-by: Muchun Song <muchun.song@linux.dev>\n---\n include/linux/mm.h     | 31 -------------------------------\n include/linux/mmzone.h | 31 +++++++++++++++++++++++++++++++\n 2 files changed, 31 insertions(+), 31 deletions(-)\n\ndiff --git a/include/linux/mm.h b/include/linux/mm.h\nindex f8a8fd47399c..8d5fa655fea4 100644\n--- a/include/linux/mm.h\n+++ b/include/linux/mm.h\n@@ -27,7 +27,6 @@\n #include <linux/page-flags.h>\n #include <linux/page_ref.h>\n #include <linux/overflow.h>\n-#include <linux/sizes.h>\n #include <linux/sched.h>\n #include <linux/pgtable.h>\n #include <linux/kasan.h>\n@@ -2477,36 +2476,6 @@ static inline unsigned long folio_nr_pages(const struct folio *folio)\n \treturn folio_large_nr_pages(folio);\n }\n \n-#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)\n-/*\n- * We don't expect any folios that exceed buddy sizes (and consequently\n- * memory sections).\n- */\n-#define MAX_FOLIO_ORDER\t\tMAX_PAGE_ORDER\n-#elif defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\n-/*\n- * Only pages within a single memory section are guaranteed to be\n- * contiguous. By limiting folios to a single memory section, all folio\n- * pages are guaranteed to be contiguous.\n- */\n-#define MAX_FOLIO_ORDER\t\tPFN_SECTION_SHIFT\n-#elif defined(CONFIG_HUGETLB_PAGE)\n-/*\n- * There is no real limit on the folio size. We limit them to the maximum we\n- * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect\n- * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.\n- */\n-#define MAX_FOLIO_ORDER\t\tget_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)\n-#else\n-/*\n- * Without hugetlb, gigantic folios that are bigger than a single PUD are\n- * currently impossible.\n- */\n-#define MAX_FOLIO_ORDER\t\tPUD_ORDER\n-#endif\n-\n-#define MAX_FOLIO_NR_PAGES\t(1UL << MAX_FOLIO_ORDER)\n-\n /*\n  * compound_nr() returns the number of pages in this potentially compound\n  * page.  compound_nr() can be called on a tail page, and is defined to\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 3e51190a55e4..be8ce40b5638 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -23,6 +23,7 @@\n #include <linux/page-flags.h>\n #include <linux/local_lock.h>\n #include <linux/zswap.h>\n+#include <linux/sizes.h>\n #include <asm/page.h>\n \n /* Free memory management - zoned buddy allocator.  */\n@@ -61,6 +62,36 @@\n  */\n #define PAGE_ALLOC_COSTLY_ORDER 3\n \n+#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)\n+/*\n+ * We don't expect any folios that exceed buddy sizes (and consequently\n+ * memory sections).\n+ */\n+#define MAX_FOLIO_ORDER\t\tMAX_PAGE_ORDER\n+#elif defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\n+/*\n+ * Only pages within a single memory section are guaranteed to be\n+ * contiguous. By limiting folios to a single memory section, all folio\n+ * pages are guaranteed to be contiguous.\n+ */\n+#define MAX_FOLIO_ORDER\t\tPFN_SECTION_SHIFT\n+#elif defined(CONFIG_HUGETLB_PAGE)\n+/*\n+ * There is no real limit on the folio size. We limit them to the maximum we\n+ * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect\n+ * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.\n+ */\n+#define MAX_FOLIO_ORDER\t\tget_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)\n+#else\n+/*\n+ * Without hugetlb, gigantic folios that are bigger than a single PUD are\n+ * currently impossible.\n+ */\n+#define MAX_FOLIO_ORDER\t\tPUD_ORDER\n+#endif\n+\n+#define MAX_FOLIO_NR_PAGES\t(1UL << MAX_FOLIO_ORDER)\n+\n enum migratetype {\n \tMIGRATE_UNMOVABLE,\n \tMIGRATE_MOVABLE,\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-02",
          "message_id": ""
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author addressed a concern about the consistency of set_compound_head() and clear_compound_head() logic by moving these functions adjacent to compound_head() in page-flags.h, making it easier to verify their logic is consistent.",
          "sentiment": "positive",
          "sentiment_signals": [
            "acknowledged fix",
            "improved code organization"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Move set_compound_head() and clear_compound_head() to be adjacent to the\ncompound_head() function in page-flags.h.\n\nThese functions encode and decode the same compound_info field, so\nkeeping them together makes it easier to verify their logic is\nconsistent, especially when the encoding changes.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\n---\n include/linux/page-flags.h | 24 ++++++++++++------------\n 1 file changed, 12 insertions(+), 12 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex aa46d49e82f7..d14a17ffb55b 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -290,6 +290,18 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n \n #define compound_head(page)\t((typeof(page))_compound_head(page))\n \n+static __always_inline void set_compound_head(struct page *page,\n+\t\t\t\t\t      const struct page *head,\n+\t\t\t\t\t      unsigned int order)\n+{\n+\tWRITE_ONCE(page->compound_info, (unsigned long)head + 1);\n+}\n+\n+static __always_inline void clear_compound_head(struct page *page)\n+{\n+\tWRITE_ONCE(page->compound_info, 0);\n+}\n+\n /**\n  * page_folio - Converts from page to folio.\n  * @p: The page.\n@@ -865,18 +877,6 @@ static inline bool folio_test_large(const struct folio *folio)\n \treturn folio_test_head(folio);\n }\n \n-static __always_inline void set_compound_head(struct page *page,\n-\t\t\t\t\t      const struct page *head,\n-\t\t\t\t\t      unsigned int order)\n-{\n-\tWRITE_ONCE(page->compound_info, (unsigned long)head + 1);\n-}\n-\n-static __always_inline void clear_compound_head(struct page *page)\n-{\n-\tWRITE_ONCE(page->compound_info, 0);\n-}\n-\n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\n static inline void ClearPageCompound(struct page *page)\n {\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nInstead of passing down the head page and tail page index, pass the tail\nand head pages directly, as well as the order of the compound page.\n\nThis is a preparation for changing how the head position is encoded in\nthe tail page.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\n---\n include/linux/page-flags.h |  4 +++-\n mm/hugetlb.c               |  8 +++++---\n mm/internal.h              | 12 ++++++------\n mm/mm_init.c               |  2 +-\n mm/page_alloc.c            |  2 +-\n 5 files changed, 16 insertions(+), 12 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex f7a0e4af0c73..8a3694369e15 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -865,7 +865,9 @@ static inline bool folio_test_large(const struct folio *folio)\n \treturn folio_test_head(folio);\n }\n \n-static __always_inline void set_compound_head(struct page *page, struct page *head)\n+static __always_inline void set_compound_head(struct page *page,\n+\t\t\t\t\t      const struct page *head,\n+\t\t\t\t\t      unsigned int order)\n {\n \tWRITE_ONCE(page->compound_head, (unsigned long)head + 1);\n }\ndiff --git a/mm/hugetlb.c b/mm/hugetlb.c\nindex 6e855a32de3d..54ba7cd05a86 100644\n--- a/mm/hugetlb.c\n+++ b/mm/hugetlb.c\n@@ -3168,6 +3168,7 @@ int __alloc_bootmem_huge_page(struct hstate *h, int nid)\n \n /* Initialize [start_page:end_page_number] tail struct pages of a hugepage */\n static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n+\t\t\t\t\tstruct hstate *h,\n \t\t\t\t\tunsigned long start_page_number,\n \t\t\t\t\tunsigned long end_page_number)\n {\n@@ -3176,6 +3177,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n \tstruct page *page = folio_page(folio, start_page_number);\n \tunsigned long head_pfn = folio_pfn(folio);\n \tunsigned long pfn, end_pfn = head_pfn + end_page_number;\n+\tunsigned int order = huge_page_order(h);\n \n \t/*\n \t * As we marked all tail pages with memblock_reserved_mark_noinit(),\n@@ -3183,7 +3185,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n \t */\n \tfor (pfn = head_pfn + start_page_number; pfn < end_pfn; page++, pfn++) {\n \t\t__init_single_page(page, pfn, zone, nid);\n-\t\tprep_compound_tail((struct page *)folio, pfn - head_pfn);\n+\t\tprep_compound_tail(page, &folio->page, order);\n \t\tset_page_count(page, 0);\n \t}\n }\n@@ -3203,7 +3205,7 @@ static void __init hugetlb_folio_init_vmemmap(struct folio *folio,\n \t__folio_set_head(folio);\n \tret = folio_ref_freeze(folio, 1);\n \tVM_BUG_ON(!ret);\n-\thugetlb_folio_init_tail_vmemmap(folio, 1, nr_pages);\n+\thugetlb_folio_init_tail_vmemmap(folio, h, 1, nr_pages);\n \tprep_compound_head(&folio->page, huge_page_order(h));\n }\n \n@@ -3260,7 +3262,7 @@ static void __init prep_and_add_bootmem_folios(struct hstate *h,\n \t\t\t * time as this is early in boot and there should\n \t\t\t * be no contention.\n \t\t\t */\n-\t\t\thugetlb_folio_init_tail_vmemmap(folio,\n+\t\t\thugetlb_folio_init_tail_vmemmap(folio, h,\n \t\t\t\t\tHUGETLB_VMEMMAP_RESERVE_PAGES,\n \t\t\t\t\tpages_per_huge_page(h));\n \t\t}\ndiff --git a/mm/internal.h b/mm/internal.h\nindex d67e8bb75734..037ddcda25ff 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -879,13 +879,13 @@ static inline void prep_compound_head(struct page *page, unsigned int order)\n \t\tINIT_LIST_HEAD(&folio->_deferred_list);\n }\n \n-static inline void prep_compound_tail(struct page *head, int tail_idx)\n+static inline void prep_compound_tail(struct page *tail,\n+\t\t\t\t      const struct page *head,\n+\t\t\t\t      unsigned int order)\n {\n-\tstruct page *p = head + tail_idx;\n-\n-\tp->mapping = TAIL_MAPPING;\n-\tset_compound_head(p, head);\n-\tset_page_private(p, 0);\n+\ttail->mapping = TAIL_MAPPING;\n+\tset_compound_head(tail, head, order);\n+\tset_page_private(tail, 0);\n }\n \n void post_alloc_hook(struct page *page, unsigned int order, gfp_t gfp_flags);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 1a29a719af58..ba50f4c4337b 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -1099,7 +1099,7 @@ static void __ref memmap_init_compound(struct page *head,\n \t\tstruct page *page = pfn_to_page(pfn);\n \n \t\t__init_zone_device_page(page, pfn, zone_idx, nid, pgmap);\n-\t\tprep_compound_tail(head, pfn - head_pfn);\n+\t\tprep_compound_tail(page, head, order);\n \t\tset_page_count(page, 0);\n \t}\n \tprep_compound_head(head, order);\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex e4104973e22f..00c7ea958767 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -744,7 +744,7 @@ void prep_compound_page(struct page *page, unsigned int order)\n \n \t__SetPageHead(page);\n \tfor (i = 1; i < nr_pages; i++)\n-\t\tprep_compound_tail(page, i);\n+\t\tprep_compound_tail(page + i, page, order);\n \n \tprep_compound_head(page, order);\n }\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nThe 'compound_head' field in the 'struct page' encodes whether the page\nis a tail and where to locate the head page. Bit 0 is set if the page is\na tail, and the remaining bits in the field point to the head page.\n\nAs preparation for changing how the field encodes information about the\nhead page, rename the field to 'compound_info'.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\n---\n .../admin-guide/kdump/vmcoreinfo.rst          |  2 +-\n Documentation/mm/vmemmap_dedup.rst            |  6 +++---\n include/linux/mm_types.h                      | 20 +++++++++----------\n include/linux/page-flags.h                    | 18 ++++++++---------\n include/linux/types.h                         |  2 +-\n kernel/vmcore_info.c                          |  2 +-\n mm/page_alloc.c                               |  2 +-\n mm/slab.h                                     |  2 +-\n mm/util.c                                     |  2 +-\n 9 files changed, 28 insertions(+), 28 deletions(-)\n\ndiff --git a/Documentation/admin-guide/kdump/vmcoreinfo.rst b/Documentation/admin-guide/kdump/vmcoreinfo.rst\nindex 404a15f6782c..7663c610fe90 100644\n--- a/Documentation/admin-guide/kdump/vmcoreinfo.rst\n+++ b/Documentation/admin-guide/kdump/vmcoreinfo.rst\n@@ -141,7 +141,7 @@ nodemask_t\n The size of a nodemask_t type. Used to compute the number of online\n nodes.\n \n-(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_head)\n+(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_info)\n ----------------------------------------------------------------------------------\n \n User-space tools compute their values based on the offset of these\ndiff --git a/Documentation/mm/vmemmap_dedup.rst b/Documentation/mm/vmemmap_dedup.rst\nindex b4a55b6569fa..1863d88d2dcb 100644\n--- a/Documentation/mm/vmemmap_dedup.rst\n+++ b/Documentation/mm/vmemmap_dedup.rst\n@@ -24,7 +24,7 @@ For each base page, there is a corresponding ``struct page``.\n Within the HugeTLB subsystem, only the first 4 ``struct page`` are used to\n contain unique information about a HugeTLB page. ``__NR_USED_SUBPAGE`` provides\n this upper limit. The only 'useful' information in the remaining ``struct page``\n-is the compound_head field, and this field is the same for all tail pages.\n+is the compound_info field, and this field is the same for all tail pages.\n \n By removing redundant ``struct page`` for HugeTLB pages, memory can be returned\n to the buddy allocator for other uses.\n@@ -124,10 +124,10 @@ Here is how things look before optimization::\n  |           |\n  +-----------+\n \n-The value of page->compound_head is the same for all tail pages. The first\n+The value of page->compound_info is the same for all tail pages. The first\n page of ``struct page`` (page 0) associated with the HugeTLB page contains the 4\n ``struct page`` necessary to describe the HugeTLB. The only use of the remaining\n-pages of ``struct page`` (page 1 to page 7) is to point to page->compound_head.\n+pages of ``struct page`` (page 1 to page 7) is to point to page->compound_info.\n Therefore, we can remap pages 1 to 7 to page 0. Only 1 page of ``struct page``\n will be used for each HugeTLB page. This will allow us to free the remaining\n 7 pages to the buddy allocator.\ndiff --git a/include/linux/mm_types.h b/include/linux/mm_types.h\nindex 3cc8ae722886..7bc82a2b889f 100644\n--- a/include/linux/mm_types.h\n+++ b/include/linux/mm_types.h\n@@ -126,14 +126,14 @@ struct page {\n \t\t\tatomic_long_t pp_ref_count;\n \t\t};\n \t\tstruct {\t/* Tail pages of compound page */\n-\t\t\tunsigned long compound_head;\t/* Bit zero is set */\n+\t\t\tunsigned long compound_info;\t/* Bit zero is set */\n \t\t};\n \t\tstruct {\t/* ZONE_DEVICE pages */\n \t\t\t/*\n-\t\t\t * The first word is used for compound_head or folio\n+\t\t\t * The first word is used for compound_info or folio\n \t\t\t * pgmap\n \t\t\t */\n-\t\t\tvoid *_unused_pgmap_compound_head;\n+\t\t\tvoid *_unused_pgmap_compound_info;\n \t\t\tvoid *zone_device_data;\n \t\t\t/*\n \t\t\t * ZONE_DEVICE private pages are counted as being\n@@ -409,7 +409,7 @@ struct folio {\n \t/* private: avoid cluttering the output */\n \t\t\t\t/* For the Unevictable \"LRU list\" slot */\n \t\t\t\tstruct {\n-\t\t\t\t\t/* Avoid compound_head */\n+\t\t\t\t\t/* Avoid compound_info */\n \t\t\t\t\tvoid *__filler;\n \t/* public: */\n \t\t\t\t\tunsigned int mlock_count;\n@@ -510,7 +510,7 @@ struct folio {\n FOLIO_MATCH(flags, flags);\n FOLIO_MATCH(lru, lru);\n FOLIO_MATCH(mapping, mapping);\n-FOLIO_MATCH(compound_head, lru);\n+FOLIO_MATCH(compound_info, lru);\n FOLIO_MATCH(__folio_index, index);\n FOLIO_MATCH(private, private);\n FOLIO_MATCH(_mapcount, _mapcount);\n@@ -529,7 +529,7 @@ FOLIO_MATCH(_last_cpupid, _last_cpupid);\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + sizeof(struct page))\n FOLIO_MATCH(flags, _flags_1);\n-FOLIO_MATCH(compound_head, _head_1);\n+FOLIO_MATCH(compound_info, _head_1);\n FOLIO_MATCH(_mapcount, _mapcount_1);\n FOLIO_MATCH(_refcount, _refcount_1);\n #undef FOLIO_MATCH\n@@ -537,13 +537,13 @@ FOLIO_MATCH(_refcount, _refcount_1);\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + 2 * sizeof(struct page))\n FOLIO_MATCH(flags, _flags_2);\n-FOLIO_MATCH(compound_head, _head_2);\n+FOLIO_MATCH(compound_info, _head_2);\n #undef FOLIO_MATCH\n #define FOLIO_MATCH(pg, fl)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + 3 * sizeof(struct page))\n FOLIO_MATCH(flags, _flags_3);\n-FOLIO_MATCH(compound_head, _head_3);\n+FOLIO_MATCH(compound_info, _head_3);\n #undef FOLIO_MATCH\n \n /**\n@@ -609,8 +609,8 @@ struct ptdesc {\n #define TABLE_MATCH(pg, pt)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct page, pg) == offsetof(struct ptdesc, pt))\n TABLE_MATCH(flags, pt_flags);\n-TABLE_MATCH(compound_head, pt_list);\n-TABLE_MATCH(compound_head, _pt_pad_1);\n+TABLE_MATCH(compound_info, pt_list);\n+TABLE_MATCH(compound_info, _pt_pad_1);\n TABLE_MATCH(mapping, __page_mapping);\n TABLE_MATCH(__folio_index, pt_index);\n TABLE_MATCH(rcu_head, pt_rcu_head);\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex 8a3694369e15..aa46d49e82f7 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -213,7 +213,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t/*\n \t * Only addresses aligned with PAGE_SIZE of struct page may be fake head\n \t * struct page. The alignment check aims to avoid access the fields (\n-\t * e.g. compound_head) of the @page[1]. It can avoid touch a (possibly)\n+\t * e.g. compound_info) of the @page[1]. It can avoid touch a (possibly)\n \t * cold cacheline in some cases.\n \t */\n \tif (IS_ALIGNED((unsigned long)page, PAGE_SIZE) &&\n@@ -223,7 +223,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t\t * because the @page is a compound page composed with at least\n \t\t * two contiguous pages.\n \t\t */\n-\t\tunsigned long head = READ_ONCE(page[1].compound_head);\n+\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n \n \t\tif (likely(head & 1))\n \t\t\treturn (const struct page *)(head - 1);\n@@ -281,7 +281,7 @@ static __always_inline int page_is_fake_head(const struct page *page)\n \n static __always_inline unsigned long _compound_head(const struct page *page)\n {\n-\tunsigned long head = READ_ONCE(page->compound_head);\n+\tunsigned long head = READ_ONCE(page->compound_info);\n \n \tif (unlikely(head & 1))\n \t\treturn head - 1;\n@@ -320,13 +320,13 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n \n static __always_inline int PageTail(const struct page *page)\n {\n-\treturn READ_ONCE(page->compound_head) & 1 || page_is_fake_head(page);\n+\treturn READ_ONCE(page->compound_info) & 1 || page_is_fake_head(page);\n }\n \n static __always_inline int PageCompound(const struct page *page)\n {\n \treturn test_bit(PG_head, &page->flags.f) ||\n-\t       READ_ONCE(page->compound_head) & 1;\n+\t       READ_ONCE(page->compound_info) & 1;\n }\n \n #define\tPAGE_POISON_PATTERN\t-1l\n@@ -348,7 +348,7 @@ static const unsigned long *const_folio_flags(const struct folio *folio,\n {\n \tconst struct page *page = &folio->page;\n \n-\tVM_BUG_ON_PGFLAGS(page->compound_head & 1, page);\n+\tVM_BUG_ON_PGFLAGS(page->compound_info & 1, page);\n \tVM_BUG_ON_PGFLAGS(n > 0 && !test_bit(PG_head, &page->flags.f), page);\n \treturn &page[n].flags.f;\n }\n@@ -357,7 +357,7 @@ static unsigned long *folio_flags(struct folio *folio, unsigned n)\n {\n \tstruct page *page = &folio->page;\n \n-\tVM_BUG_ON_PGFLAGS(page->compound_head & 1, page);\n+\tVM_BUG_ON_PGFLAGS(page->compound_info & 1, page);\n \tVM_BUG_ON_PGFLAGS(n > 0 && !test_bit(PG_head, &page->flags.f), page);\n \treturn &page[n].flags.f;\n }\n@@ -869,12 +869,12 @@ static __always_inline void set_compound_head(struct page *page,\n \t\t\t\t\t      const struct page *head,\n \t\t\t\t\t      unsigned int order)\n {\n-\tWRITE_ONCE(page->compound_head, (unsigned long)head + 1);\n+\tWRITE_ONCE(page->compound_info, (unsigned long)head + 1);\n }\n \n static __always_inline void clear_compound_head(struct page *page)\n {\n-\tWRITE_ONCE(page->compound_head, 0);\n+\tWRITE_ONCE(page->compound_info, 0);\n }\n \n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\ndiff --git a/include/linux/types.h b/include/linux/types.h\nindex f69be881369f..604697abf151 100644\n--- a/include/linux/types.h\n+++ b/include/linux/types.h\n@@ -234,7 +234,7 @@ struct ustat {\n  *\n  * This guarantee is important for few reasons:\n  *  - future call_rcu_lazy() will make use of lower bits in the pointer;\n- *  - the structure shares storage space in struct page with @compound_head,\n+ *  - the structure shares storage space in struct page with @compound_info,\n  *    which encode PageTail() in bit 0. The guarantee is needed to avoid\n  *    false-positive PageTail().\n  */\ndiff --git a/kernel/vmcore_info.c b/kernel/vmcore_info.c\nindex 46198580373a..0a46df3e3db9 100644\n--- a/kernel/vmcore_info.c\n+++ b/kernel/vmcore_info.c\n@@ -198,7 +198,7 @@ static int __init crash_save_vmcoreinfo_init(void)\n \tVMCOREINFO_OFFSET(page, lru);\n \tVMCOREINFO_OFFSET(page, _mapcount);\n \tVMCOREINFO_OFFSET(page, private);\n-\tVMCOREINFO_OFFSET(page, compound_head);\n+\tVMCOREINFO_OFFSET(page, compound_info);\n \tVMCOREINFO_OFFSET(pglist_data, node_zones);\n \tVMCOREINFO_OFFSET(pglist_data, nr_zones);\n #ifdef CONFIG_FLATMEM\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex 00c7ea958767..cb7375eb1713 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -731,7 +731,7 @@ static inline bool pcp_allowed_order(unsigned int order)\n  * The first PAGE_SIZE page is called the \"head page\" and have PG_head set.\n  *\n  * The remaining PAGE_SIZE pages are called \"tail pages\". PageTail() is encoded\n- * in bit 0 of page->compound_head. The rest of bits is pointer to head page.\n+ * in bit 0 of page->compound_info. The rest of bits is pointer to head page.\n  *\n  * The first tail page's ->compound_order holds the order of allocation.\n  * This usage means that zero-order pages may not be compound.\ndiff --git a/mm/slab.h b/mm/slab.h\nindex e767aa7e91b0..8a2a9c6c697b 100644\n--- a/mm/slab.h\n+++ b/mm/slab.h\n@@ -100,7 +100,7 @@ struct slab {\n #define SLAB_MATCH(pg, sl)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct page, pg) == offsetof(struct slab, sl))\n SLAB_MATCH(flags, flags);\n-SLAB_MATCH(compound_head, slab_cache);\t/* Ensure bit 0 is clear */\n+SLAB_MATCH(compound_info, slab_cache);\t/* Ensure bit 0 is clear */\n SLAB_MATCH(_refcount, __page_refcount);\n #ifdef CONFIG_MEMCG\n SLAB_MATCH(memcg_data, obj_exts);\ndiff --git a/mm/util.c b/mm/util.c\nindex b05ab6f97e11..3ebcb9e6035c 100644\n--- a/mm/util.c\n+++ b/mm/util.c\n@@ -1247,7 +1247,7 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n again:\n \tmemset(&ps->folio_snapshot, 0, sizeof(struct folio));\n \tmemcpy(&ps->page_snapshot, page, sizeof(*page));\n-\thead = ps->page_snapshot.compound_head;\n+\thead = ps->page_snapshot.compound_info;\n \tif ((head & 1) == 0) {\n \t\tps->idx = 0;\n \t\tfoliop = (struct folio *)&ps->page_snapshot;\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nThe upcoming change to the HugeTLB vmemmap optimization (HVO) requires\nstruct pages of the head page to be naturally aligned with regard to the\nfolio size.\n\nAlign vmemmap to MAX_FOLIO_NR_PAGES.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\n---\n arch/riscv/mm/init.c | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a/arch/riscv/mm/init.c b/arch/riscv/mm/init.c\nindex 21d534824624..c555b9a4fdce 100644\n--- a/arch/riscv/mm/init.c\n+++ b/arch/riscv/mm/init.c\n@@ -63,7 +63,8 @@ phys_addr_t phys_ram_base __ro_after_init;\n EXPORT_SYMBOL(phys_ram_base);\n \n #ifdef CONFIG_SPARSEMEM_VMEMMAP\n-#define VMEMMAP_ADDR_ALIGN\t(1ULL << SECTION_SIZE_BITS)\n+#define VMEMMAP_ADDR_ALIGN\tmax(1ULL << SECTION_SIZE_BITS, \\\n+\t\t\t\t    MAX_FOLIO_NR_PAGES * sizeof(struct page))\n \n unsigned long vmemmap_start_pfn __ro_after_init;\n EXPORT_SYMBOL(vmemmap_start_pfn);\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nThe upcoming change to the HugeTLB vmemmap optimization (HVO) requires\nstruct pages of the head page to be naturally aligned with regard to the\nfolio size.\n\nAlign vmemmap to MAX_FOLIO_NR_PAGES.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\n---\n arch/loongarch/include/asm/pgtable.h | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h\nindex c33b3bcb733e..f9416acb9156 100644\n--- a/arch/loongarch/include/asm/pgtable.h\n+++ b/arch/loongarch/include/asm/pgtable.h\n@@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];\n \t min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)\n #endif\n \n-#define vmemmap\t\t((struct page *)((VMALLOC_END + PMD_SIZE) & PMD_MASK))\n+#define VMEMMAP_ALIGN\tmax(PMD_SIZE, MAX_FOLIO_NR_PAGES * sizeof(struct page))\n+#define vmemmap\t\t((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))\n #define VMEMMAP_END\t((unsigned long)vmemmap + VMEMMAP_SIZE - 1)\n \n #define KFENCE_AREA_START\t(VMEMMAP_END + 1)\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-02",
          "message_id": ""
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author addressed a concern about the mask-based compound_info encoding being too restrictive, agreeing that it should be limited to HugeTLB vmemmap optimization (HVO) where it makes a difference. They also acknowledged that validating struct pages are naturally aligned for all orders up to MAX_FOLIO_ORDER can be tricky and would require additional validation.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "acknowledged a limitation",
            "agreed with a restriction"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "For tail pages, the kernel uses the 'compound_info' field to get to the\nhead page. The bit 0 of the field indicates whether the page is a\ntail page, and if set, the remaining bits represent a pointer to the\nhead page.\n\nFor cases when size of struct page is power-of-2, change the encoding of\ncompound_info to store a mask that can be applied to the virtual address\nof the tail page in order to access the head page. It is possible\nbecause struct page of the head page is naturally aligned with regards\nto order of the page.\n\nThe significant impact of this modification is that all tail pages of\nthe same order will now have identical 'compound_info', regardless of\nthe compound page they are associated with. This paves the way for\neliminating fake heads.\n\nThe HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only\napplied when the sizeof(struct page) is power-of-2. Having identical\ntail pages allows the same page to be mapped into the vmemmap of all\npages, maintaining memory savings without fake heads.\n\nIf sizeof(struct page) is not power-of-2, there is no functional\nchanges.\n\nLimit mask usage to HugeTLB vmemmap optimization (HVO) where it makes\na difference. The approach with mask would work in the wider set of\nconditions, but it requires validating that struct pages are naturally\naligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\n---\n include/linux/page-flags.h | 81 ++++++++++++++++++++++++++++++++++----\n mm/slab.h                  | 16 ++++++--\n mm/util.c                  | 16 ++++++--\n 3 files changed, 97 insertions(+), 16 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex d14a17ffb55b..8f2c7fbc739b 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -198,6 +198,29 @@ enum pageflags {\n \n #ifndef __GENERATING_BOUNDS_H\n \n+/*\n+ * For tail pages, if the size of struct page is power-of-2 ->compound_info\n+ * encodes the mask that converts the address of the tail page address to\n+ * the head page address.\n+ *\n+ * Otherwise, ->compound_info has direct pointer to head pages.\n+ */\n+static __always_inline bool compound_info_has_mask(void)\n+{\n+\t/*\n+\t * Limit mask usage to HugeTLB vmemmap optimization (HVO) where it\n+\t * makes a difference.\n+\t *\n+\t * The approach with mask would work in the wider set of conditions,\n+\t * but it requires validating that struct pages are naturally aligned\n+\t * for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n+\t */\n+\tif (!IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP))\n+\t\treturn false;\n+\n+\treturn is_power_of_2(sizeof(struct page));\n+}\n+\n #ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP\n DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\n \n@@ -210,6 +233,10 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \tif (!static_branch_unlikely(&hugetlb_optimize_vmemmap_key))\n \t\treturn page;\n \n+\t/* Fake heads only exists if compound_info_has_mask() is true */\n+\tif (!compound_info_has_mask())\n+\t\treturn page;\n+\n \t/*\n \t * Only addresses aligned with PAGE_SIZE of struct page may be fake head\n \t * struct page. The alignment check aims to avoid access the fields (\n@@ -223,10 +250,14 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t\t * because the @page is a compound page composed with at least\n \t\t * two contiguous pages.\n \t\t */\n-\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n+\t\tunsigned long info = READ_ONCE(page[1].compound_info);\n \n-\t\tif (likely(head & 1))\n-\t\t\treturn (const struct page *)(head - 1);\n+\t\t/* See set_compound_head() */\n+\t\tif (likely(info & 1)) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\n+\t\t\treturn (const struct page *)(p & info);\n+\t\t}\n \t}\n \treturn page;\n }\n@@ -281,11 +312,26 @@ static __always_inline int page_is_fake_head(const struct page *page)\n \n static __always_inline unsigned long _compound_head(const struct page *page)\n {\n-\tunsigned long head = READ_ONCE(page->compound_info);\n+\tunsigned long info = READ_ONCE(page->compound_info);\n \n-\tif (unlikely(head & 1))\n-\t\treturn head - 1;\n-\treturn (unsigned long)page_fixed_fake_head(page);\n+\t/* Bit 0 encodes PageTail() */\n+\tif (!(info & 1))\n+\t\treturn (unsigned long)page_fixed_fake_head(page);\n+\n+\t/*\n+\t * If compound_info_has_mask() is false, the rest of compound_info is\n+\t * the pointer to the head page.\n+\t */\n+\tif (!compound_info_has_mask())\n+\t\treturn info - 1;\n+\n+\t/*\n+\t * If compoun_info_has_mask() is true the rest of the info encodes\n+\t * the mask that converts the address of the tail page to the head page.\n+\t *\n+\t * No need to clear bit 0 in the mask as 'page' always has it clear.\n+\t */\n+\treturn (unsigned long)page & info;\n }\n \n #define compound_head(page)\t((typeof(page))_compound_head(page))\n@@ -294,7 +340,26 @@ static __always_inline void set_compound_head(struct page *page,\n \t\t\t\t\t      const struct page *head,\n \t\t\t\t\t      unsigned int order)\n {\n-\tWRITE_ONCE(page->compound_info, (unsigned long)head + 1);\n+\tunsigned int shift;\n+\tunsigned long mask;\n+\n+\tif (!compound_info_has_mask()) {\n+\t\tWRITE_ONCE(page->compound_info, (unsigned long)head | 1);\n+\t\treturn;\n+\t}\n+\n+\t/*\n+\t * If the size of struct page is power-of-2, bits [shift:0] of the\n+\t * virtual address of compound head are zero.\n+\t *\n+\t * Calculate mask that can be applied to the virtual address of\n+\t * the tail page to get address of the head page.\n+\t */\n+\tshift = order + order_base_2(sizeof(struct page));\n+\tmask = GENMASK(BITS_PER_LONG - 1, shift);\n+\n+\t/* Bit 0 encodes PageTail() */\n+\tWRITE_ONCE(page->compound_info, mask | 1);\n }\n \n static __always_inline void clear_compound_head(struct page *page)\ndiff --git a/mm/slab.h b/mm/slab.h\nindex 8a2a9c6c697b..f68c3ac8126f 100644\n--- a/mm/slab.h\n+++ b/mm/slab.h\n@@ -137,11 +137,19 @@ static_assert(IS_ALIGNED(offsetof(struct slab, freelist), sizeof(struct freelist\n  */\n static inline struct slab *page_slab(const struct page *page)\n {\n-\tunsigned long head;\n+\tunsigned long info;\n+\n+\tinfo = READ_ONCE(page->compound_info);\n+\tif (info & 1) {\n+\t\t/* See compound_head() */\n+\t\tif (compound_info_has_mask()) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\t\t\tpage = (struct page *)(p & info);\n+\t\t} else {\n+\t\t\tpage = (struct page *)(info - 1);\n+\t\t}\n+\t}\n \n-\thead = READ_ONCE(page->compound_head);\n-\tif (head & 1)\n-\t\tpage = (struct page *)(head - 1);\n \tif (data_race(page->page_type >> 24) != PGTY_slab)\n \t\tpage = NULL;\n \ndiff --git a/mm/util.c b/mm/util.c\nindex 3ebcb9e6035c..20dccf2881d7 100644\n--- a/mm/util.c\n+++ b/mm/util.c\n@@ -1237,7 +1237,7 @@ static void set_ps_flags(struct page_snapshot *ps, const struct folio *folio,\n  */\n void snapshot_page(struct page_snapshot *ps, const struct page *page)\n {\n-\tunsigned long head, nr_pages = 1;\n+\tunsigned long info, nr_pages = 1;\n \tstruct folio *foliop;\n \tint loops = 5;\n \n@@ -1247,8 +1247,8 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n again:\n \tmemset(&ps->folio_snapshot, 0, sizeof(struct folio));\n \tmemcpy(&ps->page_snapshot, page, sizeof(*page));\n-\thead = ps->page_snapshot.compound_info;\n-\tif ((head & 1) == 0) {\n+\tinfo = ps->page_snapshot.compound_info;\n+\tif (!(info & 1)) {\n \t\tps->idx = 0;\n \t\tfoliop = (struct folio *)&ps->page_snapshot;\n \t\tif (!folio_test_large(foliop)) {\n@@ -1259,7 +1259,15 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n \t\t}\n \t\tfoliop = (struct folio *)page;\n \t} else {\n-\t\tfoliop = (struct folio *)(head - 1);\n+\t\t/* See compound_head() */\n+\t\tif (compound_info_has_mask()) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\n+\t\t\tfoliop = (struct folio *)(p & info);\n+\t\t} else {\n+\t\t\tfoliop = (struct folio *)(info - 1);\n+\t\t}\n+\n \t\tps->idx = folio_page_idx(foliop, page);\n \t}\n \n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nWith the upcoming changes to HVO, a single page of tail struct pages\nwill be shared across all huge pages of the same order on a node. Since\nhuge pages on the same node may belong to different zones, the zone\ninformation stored in shared tail page flags would be incorrect.\n\nAlways fetch zone information from the head page, which has unique and\ncorrect zone flags for each compound page.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nAcked-by: Zi Yan <ziy@nvidia.com>\n---\n include/linux/mmzone.h | 1 +\n 1 file changed, 1 insertion(+)\n\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex be8ce40b5638..192143b5cdc0 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n \n static inline enum zone_type page_zonenum(const struct page *page)\n {\n+\tpage = compound_head(page);\n \treturn memdesc_zonenum(page->flags);\n }\n \n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nIf page->compound_info encodes a mask, it is expected that vmemmap to be\nnaturally aligned to the maximum folio size.\n\nAdd a VM_BUG_ON() to check the alignment.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nAcked-by: Zi Yan <ziy@nvidia.com>\n---\n mm/sparse.c | 7 +++++++\n 1 file changed, 7 insertions(+)\n\ndiff --git a/mm/sparse.c b/mm/sparse.c\nindex b5b2b6f7041b..6c9b62607f3f 100644\n--- a/mm/sparse.c\n+++ b/mm/sparse.c\n@@ -600,6 +600,13 @@ void __init sparse_init(void)\n \tBUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));\n \tmemblocks_present();\n \n+\tif (compound_info_has_mask()) {\n+\t\tunsigned long alignment;\n+\n+\t\talignment = MAX_FOLIO_NR_PAGES * sizeof(struct page);\n+\t\tVM_BUG_ON(!IS_ALIGNED((unsigned long) pfn_to_page(0), alignment));\n+\t}\n+\n \tpnum_begin = first_present_section_nr();\n \tnid_begin = sparse_early_nid(__nr_to_section(pnum_begin));\n \n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn Wed, Feb 04, 2026 at 05:14:12PM +0100, David Hildenbrand (arm) wrote:\n> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > Instead of passing down the head page and tail page index, pass the tail\n> > and head pages directly, as well as the order of the compound page.\n> > \n> > This is a preparation for changing how the head position is encoded in\n> > the tail page.\n> > \n> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> > Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> > Reviewed-by: Zi Yan <ziy@nvidia.com>\n> > ---\n> >   include/linux/page-flags.h |  4 +++-\n> >   mm/hugetlb.c               |  8 +++++---\n> >   mm/internal.h              | 12 ++++++------\n> >   mm/mm_init.c               |  2 +-\n> >   mm/page_alloc.c            |  2 +-\n> >   5 files changed, 16 insertions(+), 12 deletions(-)\n> > \n> > diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\n> > index f7a0e4af0c73..8a3694369e15 100644\n> > --- a/include/linux/page-flags.h\n> > +++ b/include/linux/page-flags.h\n> > @@ -865,7 +865,9 @@ static inline bool folio_test_large(const struct folio *folio)\n> >   \treturn folio_test_head(folio);\n> >   }\n> > -static __always_inline void set_compound_head(struct page *page, struct page *head)\n> > +static __always_inline void set_compound_head(struct page *page,\n> > +\t\t\t\t\t      const struct page *head,\n> > +\t\t\t\t\t      unsigned int order)\n> \n> Two tab indents please on second+ parameter list whenever you touch code.\n\nDo we have this coding style preference written down somewhere?\n\n-tip tree wants the opposite. Documentation/process/maintainer-tip.rst:\n\n\tWhen splitting function declarations or function calls, then please align\n\tthe first argument in the second line with the first argument in the first\n\tline::\n\nI want the editor to do The Right Thing\\u2122 without my brain involvement.\nHaving different coding styles in different corners of the kernel makes\nit hard.\n\n> \n> >   {\n> >   \tWRITE_ONCE(page->compound_head, (unsigned long)head + 1);\n> >   }\n> > diff --git a/mm/hugetlb.c b/mm/hugetlb.c\n> > index 6e855a32de3d..54ba7cd05a86 100644\n> \n> \n> [...]\n> \n> > diff --git a/mm/internal.h b/mm/internal.h\n> > index d67e8bb75734..037ddcda25ff 100644\n> > --- a/mm/internal.h\n> > +++ b/mm/internal.h\n> > @@ -879,13 +879,13 @@ static inline void prep_compound_head(struct page *page, unsigned int order)\n> >   \t\tINIT_LIST_HEAD(&folio->_deferred_list);\n> >   }\n> > -static inline void prep_compound_tail(struct page *head, int tail_idx)\n> > +static inline void prep_compound_tail(struct page *tail,\n> \n> Just wondering whether we should call this \"struct page *page\" for\n> consistency with set_compound_head().\n> \n> Or alternatively, call it also \"tail\" in set_compound_head().\n\nI will take the alternative path :)\n\n> \n> > +\t\t\t\t      const struct page *head,\n> > +\t\t\t\t      unsigned int order)\n> \n> Two tab indent, then this fits into two lines in total.\n> \n> >   {\n> > -\tstruct page *p = head + tail_idx;\n> > -\n> > -\tp->mapping = TAIL_MAPPING;\n> > -\tset_compound_head(p, head);\n> > -\tset_page_private(p, 0);\n> > +\ttail->mapping = TAIL_MAPPING;\n> > +\tset_compound_head(tail, head, order);\n> > +\tset_page_private(tail, 0);\n> >   }\n> Only nits, in general LGTM\n> \n> Acked-by: David Hildenbrand (arm) <david@kernel.org>\n\nThanks!\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n---\n\nOn Wed, Feb 04, 2026 at 05:14:12PM +0100, David Hildenbrand (arm) wrote:\n> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > Instead of passing down the head page and tail page index, pass the tail\n> > and head pages directly, as well as the order of the compound page.\n> > \n> > This is a preparation for changing how the head position is encoded in\n> > the tail page.\n> > \n> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> > Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> > Reviewed-by: Zi Yan <ziy@nvidia.com>\n> > ---\n> >   include/linux/page-flags.h |  4 +++-\n> >   mm/hugetlb.c               |  8 +++++---\n> >   mm/internal.h              | 12 ++++++------\n> >   mm/mm_init.c               |  2 +-\n> >   mm/page_alloc.c            |  2 +-\n> >   5 files changed, 16 insertions(+), 12 deletions(-)\n> > \n> > diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\n> > index f7a0e4af0c73..8a3694369e15 100644\n> > --- a/include/linux/page-flags.h\n> > +++ b/include/linux/page-flags.h\n> > @@ -865,7 +865,9 @@ static inline bool folio_test_large(const struct folio *folio)\n> >   \treturn folio_test_head(folio);\n> >   }\n> > -static __always_inline void set_compound_head(struct page *page, struct page *head)\n> > +static __always_inline void set_compound_head(struct page *page,\n> > +\t\t\t\t\t      const struct page *head,\n> > +\t\t\t\t\t      unsigned int order)\n> \n> Two tab indents please on second+ parameter list whenever you touch code.\n\nDo we have this coding style preference written down somewhere?\n\n-tip tree wants the opposite. Documentation/process/maintainer-tip.rst:\n\n\tWhen splitting function declarations or function calls, then please align\n\tthe first argument in the second line with the first argument in the first\n\tline::\n\nI want the editor to do The Right Thing without my brain involvement.\nHaving different coding styles in different corners of the kernel makes\nit hard.\n\n> \n> >   {\n> >   \tWRITE_ONCE(page->compound_head, (unsigned long)head + 1);\n> >   }\n> > diff --git a/mm/hugetlb.c b/mm/hugetlb.c\n> > index 6e855a32de3d..54ba7cd05a86 100644\n> \n> \n> [...]\n> \n> > diff --git a/mm/internal.h b/mm/internal.h\n> > index d67e8bb75734..037ddcda25ff 100644\n> > --- a/mm/internal.h\n> > +++ b/mm/internal.h\n> > @@ -879,13 +879,13 @@ static inline void prep_compound_head(struct page *page, unsigned int order)\n> >   \t\tINIT_LIST_HEAD(&folio->_deferred_list);\n> >   }\n> > -static inline void prep_compound_tail(struct page *head, int tail_idx)\n> > +static inline void prep_compound_tail(struct page *tail,\n> \n> Just wondering whether we should call this \"struct page *page\" for\n> consistency with set_compound_head().\n> \n> Or alternatively, call it also \"tail\" in set_compound_head().\n\nI will take the alternative path :)\n\n> \n> > +\t\t\t\t      const struct page *head,\n> > +\t\t\t\t      unsigned int order)\n> \n> Two tab indent, then this fits into two lines in total.\n> \n> >   {\n> > -\tstruct page *p = head + tail_idx;\n> > -\n> > -\tp->mapping = TAIL_MAPPING;\n> > -\tset_compound_head(p, head);\n> > -\tset_page_private(p, 0);\n> > +\ttail->mapping = TAIL_MAPPING;\n> > +\tset_compound_head(tail, head, order);\n> > +\tset_page_private(tail, 0);\n> >   }\n> Only nits, in general LGTM\n> \n> Acked-by: David Hildenbrand (arm) <david@kernel.org>\n\nThanks!\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn Thu, Feb 05, 2026 at 01:56:36PM +0100, David Hildenbrand (Arm) wrote:\n> On 2/4/26 17:56, David Hildenbrand (arm) wrote:\n> > On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > > The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n> > > struct pages of the head page to be naturally aligned with regard to the\n> > > folio size.\n> > > \n> > > Align vmemmap to MAX_FOLIO_NR_PAGES.\n> > > \n> > > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> > > ---\n> > >  arch/loongarch/include/asm/pgtable.h | 3 ++-\n> > >  1 file changed, 2 insertions(+), 1 deletion(-)\n> > > \n> > > diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/\n> > > include/asm/pgtable.h\n> > > index c33b3bcb733e..f9416acb9156 100644\n> > > --- a/arch/loongarch/include/asm/pgtable.h\n> > > +++ b/arch/loongarch/include/asm/pgtable.h\n> > > @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE /\n> > > sizeof(unsigned long)];\n> > >  min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE\n> > > * PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE -\n> > > KFENCE_AREA_SIZE)\n> > >  #endif\n> > > -#define vmemmap ((struct page *)((VMALLOC_END + PMD_SIZE) &\n> > > PMD_MASK))\n> > > +#define VMEMMAP_ALIGN max(PMD_SIZE, MAX_FOLIO_NR_PAGES *\n> > > sizeof(struct page))\n> > > +#define vmemmap ((struct page *)(ALIGN(VMALLOC_END,\n> > > VMEMMAP_ALIGN)))\n> > \n> > \n> > Same comment, the \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is just\n> > black magic here\n> > and the description of the situation is wrong.\n> > \n> > Maybe you want to pull the magic \"MAX_FOLIO_NR_PAGES * sizeof(struct\n> > page)\" into the core and call it\n> > \n> > #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct\n> > page))\n> > \n> > But then special case it base on (a) HVO being configured in an (b) HVO\n> > being possible\n> > \n> > #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP && is_power_of_2(sizeof(struct page)\n> > /* A very helpful comment explaining the situation. */\n> > #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct\n> > page))\n> > #else\n> > #define MAX_FOLIO_VMEMMAP_ALIGN 0\n> > #endif\n> > \n> > Something like that.\n> > \n> \n> Thinking about this ...\n> \n> the vmemmap start is always struct-page-aligned. Otherwise we'd be in\n> trouble already.\n> \n> Isn't it then sufficient to just align the start to MAX_FOLIO_NR_PAGES?\n> \n> Let's assume sizeof(struct page) == 64 and MAX_FOLIO_NR_PAGES = 512 for\n> simplicity.\n> \n> vmemmap start would be multiples of 512 (0x0010000000).\n> \n> 512, 1024, 1536, 2048 ...\n> \n> Assume we have an 256-pages folio at 1536+256 = 0x111000000\n\ns/0x/0b/, but okay.\n\n> Assume we have the last page of that folio (0x011111111111), we would just\n> get to the start of that folio by AND-ing with ~(256-1).\n> \n> Which case am I ignoring?\n\nIIUC, you are ignoring the actual size of struct page. It is not 1 byte :P\n\nThe last page of this 256-page folio is at 1536+256 + (64 * 255) which\nis 0b100011011000000. There's no mask that you can AND that gets you to\n0b11100000000.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n---\n\nOn Thu, Feb 05, 2026 at 01:56:36PM +0100, David Hildenbrand (Arm) wrote:\n> On 2/4/26 17:56, David Hildenbrand (arm) wrote:\n> > On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > > The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n> > > struct pages of the head page to be naturally aligned with regard to the\n> > > folio size.\n> > > \n> > > Align vmemmap to MAX_FOLIO_NR_PAGES.\n> > > \n> > > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> > > ---\n> > >  arch/loongarch/include/asm/pgtable.h | 3 ++-\n> > >  1 file changed, 2 insertions(+), 1 deletion(-)\n> > > \n> > > diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/\n> > > include/asm/pgtable.h\n> > > index c33b3bcb733e..f9416acb9156 100644\n> > > --- a/arch/loongarch/include/asm/pgtable.h\n> > > +++ b/arch/loongarch/include/asm/pgtable.h\n> > > @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE /\n> > > sizeof(unsigned long)];\n> > >  min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE\n> > > * PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE -\n> > > KFENCE_AREA_SIZE)\n> > >  #endif\n> > > -#define vmemmap ((struct page *)((VMALLOC_END + PMD_SIZE) &\n> > > PMD_MASK))\n> > > +#define VMEMMAP_ALIGN max(PMD_SIZE, MAX_FOLIO_NR_PAGES *\n> > > sizeof(struct page))\n> > > +#define vmemmap ((struct page *)(ALIGN(VMALLOC_END,\n> > > VMEMMAP_ALIGN)))\n> > \n> > \n> > Same comment, the \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is just\n> > black magic here\n> > and the description of the situation is wrong.\n> > \n> > Maybe you want to pull the magic \"MAX_FOLIO_NR_PAGES * sizeof(struct\n> > page)\" into the core and call it\n> > \n> > #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct\n> > page))\n> > \n> > But then special case it base on (a) HVO being configured in an (b) HVO\n> > being possible\n> > \n> > #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP && is_power_of_2(sizeof(struct page)\n> > /* A very helpful comment explaining the situation. */\n> > #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct\n> > page))\n> > #else\n> > #define MAX_FOLIO_VMEMMAP_ALIGN 0\n> > #endif\n> > \n> > Something like that.\n> > \n> \n> Thinking about this ...\n> \n> the vmemmap start is always struct-page-aligned. Otherwise we'd be in\n> trouble already.\n> \n> Isn't it then sufficient to just align the start to MAX_FOLIO_NR_PAGES?\n> \n> Let's assume sizeof(struct page) == 64 and MAX_FOLIO_NR_PAGES = 512 for\n> simplicity.\n> \n> vmemmap start would be multiples of 512 (0x0010000000).\n> \n> 512, 1024, 1536, 2048 ...\n> \n> Assume we have an 256-pages folio at 1536+256 = 0x111000000\n\ns/0x/0b/, but okay.\n\n> Assume we have the last page of that folio (0x011111111111), we would just\n> get to the start of that folio by AND-ing with ~(256-1).\n> \n> Which case am I ignoring?\n\nIIUC, you are ignoring the actual size of struct page. It is not 1 byte :P\n\nThe last page of this 256-page folio is at 1536+256 + (64 * 255) which\nis 0b100011011000000. There's no mask that you can AND that gets you to\n0b11100000000.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-02",
          "message_id": ""
        }
      ],
      "analysis_source": "llm",
      "patch_summary": "This patch series removes 'fake head pages' from the HugeTLB vmemmap optimization by changing how tail pages encode their relationship to the head page, simplifying compound_head() and page_ref_add_unless(). The new approach uses a mask-based encoding for architectures where sizeof(struct page) is a power of 2, allowing shared read-only tail pages across huge pages on a NUMA node. This reduces complexity and overhead in the hot path, but testing has shown either no change or only slight performance improvement."
    },
    "2026-02-05": {
      "report_file": "2026-02-27_ollama_llama3.1-8b.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "Author acknowledged that the calculation for MAX_FOLIO_NR_PAGES * sizeof(struct page) is incorrect and can result in a non-power-of-2 value, which would disable HVO even if it's configured. He suggested using roundup_pow_of_two(sizeof(struct page)) instead, but also mentioned that avoiding sizeof(struct page) altogether might not be possible.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "acknowledged a technical issue",
            "no clear resolution signal"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "On Wed, Feb 04, 2026 at 05:50:23PM +0100, David Hildenbrand (arm) wrote:\n> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n> > struct pages of the head page to be naturally aligned with regard to the\n> > folio size.\n> > \n> > Align vmemmap to MAX_FOLIO_NR_PAGES.\n> \n> I think neither that statement nor the one in the patch description is\n> correct?\n> \n> \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is neither the maximum folio size\n> nor MAX_FOLIO_NR_PAGES.\n> \n> It's the size of the memmap that a large folio could span at maximum.\n> \n> \n> Assuming we have a 16 GiB folio, the calculation would give us\n> \n> \t4194304 * sizeof(struct page)\n> \n> Which could be something like (assuming 80 bytes)\n> \n> \t335544320\n> \n> -> not even a power of 2, weird? (for HVO you wouldn't care as HVO would be\n> disabled, but that aliment is super weird?)\n> \n> \n> Assuming 64 bytes, it would be a power of two (as 64 is a power of two).\n> \n> \t268435456 (1<< 28)\n> \n> \n> Which makes me wonder whether there is a way to avoid sizeof(struct page)\n> here completely.\n\nI don't think we can. See the other thread.\n\nWhat about using roundup_pow_of_two(sizeof(struct page)) here.\n\n> Or limit the alignment to the case where HVO is actually active and\n> sizeof(struct page) makes any sense?\n\nThe annoying part of HVO is that it is unknown at compile-time if it\nwill be used. You can compile kernel with HVO that will no be activated\ndue to non-power-of-2 sizeof(struct page) because of a debug config option.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n---\n\nOn Wed, Feb 04, 2026 at 05:50:23PM +0100, David Hildenbrand (arm) wrote:\n> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n> > struct pages of the head page to be naturally aligned with regard to the\n> > folio size.\n> > \n> > Align vmemmap to MAX_FOLIO_NR_PAGES.\n> \n> I think neither that statement nor the one in the patch description is\n> correct?\n> \n> \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is neither the maximum folio size\n> nor MAX_FOLIO_NR_PAGES.\n> \n> It's the size of the memmap that a large folio could span at maximum.\n> \n> \n> Assuming we have a 16 GiB folio, the calculation would give us\n> \n> \t4194304 * sizeof(struct page)\n> \n> Which could be something like (assuming 80 bytes)\n> \n> \t335544320\n> \n> -> not even a power of 2, weird? (for HVO you wouldn't care as HVO would be\n> disabled, but that aliment is super weird?)\n> \n> \n> Assuming 64 bytes, it would be a power of two (as 64 is a power of two).\n> \n> \t268435456 (1<< 28)\n> \n> \n> Which makes me wonder whether there is a way to avoid sizeof(struct page)\n> here completely.\n\nI don't think we can. See the other thread.\n\nWhat about using roundup_pow_of_two(sizeof(struct page)) here.\n\n> Or limit the alignment to the case where HVO is actually active and\n> sizeof(struct page) makes any sense?\n\nThe annoying part of HVO is that it is unknown at compile-time if it\nwill be used. You can compile kernel with HVO that will no be activated\ndue to non-power-of-2 sizeof(struct page) because of a debug config option.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn Wed, Feb 04, 2026 at 05:56:45PM +0100, David Hildenbrand (arm) wrote:\n> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n> > struct pages of the head page to be naturally aligned with regard to the\n> > folio size.\n> > \n> > Align vmemmap to MAX_FOLIO_NR_PAGES.\n> > \n> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> > ---\n> >   arch/loongarch/include/asm/pgtable.h | 3 ++-\n> >   1 file changed, 2 insertions(+), 1 deletion(-)\n> > \n> > diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h\n> > index c33b3bcb733e..f9416acb9156 100644\n> > --- a/arch/loongarch/include/asm/pgtable.h\n> > +++ b/arch/loongarch/include/asm/pgtable.h\n> > @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];\n> >   \t min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)\n> >   #endif\n> > -#define vmemmap\t\t((struct page *)((VMALLOC_END + PMD_SIZE) & PMD_MASK))\n> > +#define VMEMMAP_ALIGN\tmax(PMD_SIZE, MAX_FOLIO_NR_PAGES * sizeof(struct page))\n> > +#define vmemmap\t\t((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))\n> \n> \n> Same comment, the \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is just black magic here\n> and the description of the situation is wrong.\n> \n> Maybe you want to pull the magic \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" into the core and call it\n> \n> #define MAX_FOLIO_VMEMMAP_ALIGN\t(MAX_FOLIO_NR_PAGES * sizeof(struct page))\n> \n> But then special case it base on (a) HVO being configured in an (b) HVO being possible\n> \n> #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP && is_power_of_2(sizeof(struct page)\n\nThis would require some kind of asm-offsets.c/bounds.c magic to pull the\nstruct page size condition to the preprocessor level.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n---\n\nOn Wed, Feb 04, 2026 at 05:56:45PM +0100, David Hildenbrand (arm) wrote:\n> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n> > struct pages of the head page to be naturally aligned with regard to the\n> > folio size.\n> > \n> > Align vmemmap to MAX_FOLIO_NR_PAGES.\n> > \n> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> > ---\n> >   arch/loongarch/include/asm/pgtable.h | 3 ++-\n> >   1 file changed, 2 insertions(+), 1 deletion(-)\n> > \n> > diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h\n> > index c33b3bcb733e..f9416acb9156 100644\n> > --- a/arch/loongarch/include/asm/pgtable.h\n> > +++ b/arch/loongarch/include/asm/pgtable.h\n> > @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];\n> >   \t min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)\n> >   #endif\n> > -#define vmemmap\t\t((struct page *)((VMALLOC_END + PMD_SIZE) & PMD_MASK))\n> > +#define VMEMMAP_ALIGN\tmax(PMD_SIZE, MAX_FOLIO_NR_PAGES * sizeof(struct page))\n> > +#define vmemmap\t\t((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))\n> \n> \n> Same comment, the \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is just black magic here\n> and the description of the situation is wrong.\n> \n> Maybe you want to pull the magic \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" into the core and call it\n> \n> #define MAX_FOLIO_VMEMMAP_ALIGN\t(MAX_FOLIO_NR_PAGES * sizeof(struct page))\n> \n> But then special case it base on (a) HVO being configured in an (b) HVO being possible\n> \n> #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP && is_power_of_2(sizeof(struct page)\n\nThis would require some kind of asm-offsets.c/bounds.c magic to pull the\nstruct page size condition to the preprocessor level.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:\n> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > With the upcoming changes to HVO, a single page of tail struct pages\n> > will be shared across all huge pages of the same order on a node. Since\n> > huge pages on the same node may belong to different zones, the zone\n> > information stored in shared tail page flags would be incorrect.\n> > \n> > Always fetch zone information from the head page, which has unique and\n> > correct zone flags for each compound page.\n> > \n> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> > Acked-by: Zi Yan <ziy@nvidia.com>\n> > ---\n> >   include/linux/mmzone.h | 1 +\n> >   1 file changed, 1 insertion(+)\n> > \n> > diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n> > index be8ce40b5638..192143b5cdc0 100644\n> > --- a/include/linux/mmzone.h\n> > +++ b/include/linux/mmzone.h\n> > @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n> >   static inline enum zone_type page_zonenum(const struct page *page)\n> >   {\n> > +\tpage = compound_head(page);\n> >   \treturn memdesc_zonenum(page->flags);\n> \n> We end up calling page_zonenum() without holding a reference.\n> \n> Given that _compound_head() does a READ_ONCE(), this should work even if we\n> see concurrent page freeing etc.\n> \n> However, this change implies that we now perform a compound page lookup for\n> every PageHighMem() [meh], page_zone() [quite some users in the buddy,\n> including for pageblock access and page freeing].\n> \n> That's a nasty compromise for making HVO better? :)\n> \n> We should likely limit that special casing to kernels that really rquire it\n> (HVO).\n\nI will add compound_info_has_mask() check.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n---\n\nOn Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:\n> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > With the upcoming changes to HVO, a single page of tail struct pages\n> > will be shared across all huge pages of the same order on a node. Since\n> > huge pages on the same node may belong to different zones, the zone\n> > information stored in shared tail page flags would be incorrect.\n> > \n> > Always fetch zone information from the head page, which has unique and\n> > correct zone flags for each compound page.\n> > \n> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> > Acked-by: Zi Yan <ziy@nvidia.com>\n> > ---\n> >   include/linux/mmzone.h | 1 +\n> >   1 file changed, 1 insertion(+)\n> > \n> > diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n> > index be8ce40b5638..192143b5cdc0 100644\n> > --- a/include/linux/mmzone.h\n> > +++ b/include/linux/mmzone.h\n> > @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n> >   static inline enum zone_type page_zonenum(const struct page *page)\n> >   {\n> > +\tpage = compound_head(page);\n> >   \treturn memdesc_zonenum(page->flags);\n> \n> We end up calling page_zonenum() without holding a reference.\n> \n> Given that _compound_head() does a READ_ONCE(), this should work even if we\n> see concurrent page freeing etc.\n> \n> However, this change implies that we now perform a compound page lookup for\n> every PageHighMem() [meh], page_zone() [quite some users in the buddy,\n> including for pageblock access and page freeing].\n> \n> That's a nasty compromise for making HVO better? :)\n> \n> We should likely limit that special casing to kernels that really rquire it\n> (HVO).\n\nI will add compound_info_has_mask() check.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn Tue, Feb 10, 2026 at 04:57:55PM +0100, Vlastimil Babka wrote:\n> On 2/9/26 12:52, Kiryl Shutsemau wrote:\n> > On Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:\n> >> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> >> > With the upcoming changes to HVO, a single page of tail struct pages\n> >> > will be shared across all huge pages of the same order on a node. Since\n> >> > huge pages on the same node may belong to different zones, the zone\n> >> > information stored in shared tail page flags would be incorrect.\n> >> > \n> >> > Always fetch zone information from the head page, which has unique and\n> >> > correct zone flags for each compound page.\n> >> > \n> >> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> >> > Acked-by: Zi Yan <ziy@nvidia.com>\n> >> > ---\n> >> >   include/linux/mmzone.h | 1 +\n> >> >   1 file changed, 1 insertion(+)\n> >> > \n> >> > diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n> >> > index be8ce40b5638..192143b5cdc0 100644\n> >> > --- a/include/linux/mmzone.h\n> >> > +++ b/include/linux/mmzone.h\n> >> > @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n> >> >   static inline enum zone_type page_zonenum(const struct page *page)\n> >> >   {\n> >> > +\tpage = compound_head(page);\n> >> >   \treturn memdesc_zonenum(page->flags);\n> >> \n> >> We end up calling page_zonenum() without holding a reference.\n> >> \n> >> Given that _compound_head() does a READ_ONCE(), this should work even if we\n> >> see concurrent page freeing etc.\n> >> \n> >> However, this change implies that we now perform a compound page lookup for\n> >> every PageHighMem() [meh], page_zone() [quite some users in the buddy,\n> >> including for pageblock access and page freeing].\n> >> \n> >> That's a nasty compromise for making HVO better? :)\n> >> \n> >> We should likely limit that special casing to kernels that really rquire it\n> >> (HVO).\n> > \n> > I will add compound_info_has_mask() check.\n> \n> Not thrilled by this indeed. Would it be a problem to have the shared tail\n> pages per node+zone instead of just per node?\n\nI thought it would be overkill. It likely is going to be unused for most\nnodes. But sure, move it to per-zone.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n---\n\nOn Tue, Feb 10, 2026 at 04:57:55PM +0100, Vlastimil Babka wrote:\n> On 2/9/26 12:52, Kiryl Shutsemau wrote:\n> > On Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:\n> >> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> >> > With the upcoming changes to HVO, a single page of tail struct pages\n> >> > will be shared across all huge pages of the same order on a node. Since\n> >> > huge pages on the same node may belong to different zones, the zone\n> >> > information stored in shared tail page flags would be incorrect.\n> >> > \n> >> > Always fetch zone information from the head page, which has unique and\n> >> > correct zone flags for each compound page.\n> >> > \n> >> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> >> > Acked-by: Zi Yan <ziy@nvidia.com>\n> >> > ---\n> >> >   include/linux/mmzone.h | 1 +\n> >> >   1 file changed, 1 insertion(+)\n> >> > \n> >> > diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n> >> > index be8ce40b5638..192143b5cdc0 100644\n> >> > --- a/include/linux/mmzone.h\n> >> > +++ b/include/linux/mmzone.h\n> >> > @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n> >> >   static inline enum zone_type page_zonenum(const struct page *page)\n> >> >   {\n> >> > +\tpage = compound_head(page);\n> >> >   \treturn memdesc_zonenum(page->flags);\n> >> \n> >> We end up calling page_zonenum() without holding a reference.\n> >> \n> >> Given that _compound_head() does a READ_ONCE(), this should work even if we\n> >> see concurrent page freeing etc.\n> >> \n> >> However, this change implies that we now perform a compound page lookup for\n> >> every PageHighMem() [meh], page_zone() [quite some users in the buddy,\n> >> including for pageblock access and page freeing].\n> >> \n> >> That's a nasty compromise for making HVO better? :)\n> >> \n> >> We should likely limit that special casing to kernels that really rquire it\n> >> (HVO).\n> > \n> > I will add compound_info_has_mask() check.\n> \n> Not thrilled by this indeed. Would it be a problem to have the shared tail\n> pages per node+zone instead of just per node?\n\nI thought it would be overkill. It likely is going to be unused for most\nnodes. But sure, move it to per-zone.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn Mon, Feb 16, 2026 at 11:30:22AM +0000, Kiryl Shutsemau wrote:\n> On Tue, Feb 10, 2026 at 04:57:55PM +0100, Vlastimil Babka wrote:\n> > On 2/9/26 12:52, Kiryl Shutsemau wrote:\n> > > On Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:\n> > >> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > >> > With the upcoming changes to HVO, a single page of tail struct pages\n> > >> > will be shared across all huge pages of the same order on a node. Since\n> > >> > huge pages on the same node may belong to different zones, the zone\n> > >> > information stored in shared tail page flags would be incorrect.\n> > >> > \n> > >> > Always fetch zone information from the head page, which has unique and\n> > >> > correct zone flags for each compound page.\n> > >> > \n> > >> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> > >> > Acked-by: Zi Yan <ziy@nvidia.com>\n> > >> > ---\n> > >> >   include/linux/mmzone.h | 1 +\n> > >> >   1 file changed, 1 insertion(+)\n> > >> > \n> > >> > diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n> > >> > index be8ce40b5638..192143b5cdc0 100644\n> > >> > --- a/include/linux/mmzone.h\n> > >> > +++ b/include/linux/mmzone.h\n> > >> > @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n> > >> >   static inline enum zone_type page_zonenum(const struct page *page)\n> > >> >   {\n> > >> > +\tpage = compound_head(page);\n> > >> >   \treturn memdesc_zonenum(page->flags);\n> > >> \n> > >> We end up calling page_zonenum() without holding a reference.\n> > >> \n> > >> Given that _compound_head() does a READ_ONCE(), this should work even if we\n> > >> see concurrent page freeing etc.\n> > >> \n> > >> However, this change implies that we now perform a compound page lookup for\n> > >> every PageHighMem() [meh], page_zone() [quite some users in the buddy,\n> > >> including for pageblock access and page freeing].\n> > >> \n> > >> That's a nasty compromise for making HVO better? :)\n> > >> \n> > >> We should likely limit that special casing to kernels that really rquire it\n> > >> (HVO).\n> > > \n> > > I will add compound_info_has_mask() check.\n> > \n> > Not thrilled by this indeed. Would it be a problem to have the shared tail\n> > pages per node+zone instead of just per node?\n> \n> I thought it would be overkill. It likely is going to be unused for most\n> nodes. But sure, move it to per-zone.\n\nI gave it a try, but stumbled on a problem.\n\nWe need to know the zone in hugetlb_vmemmap_init_early(), but zones are\nnot yet defined.\n\nhugetlb_vmemmap_init_early() is called from within sparse_init(), but\nspan of zones is defined in free_area_init() after sparse_init().\n\nAny ideas, how get past this? :/\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n---\n\nOn Mon, Feb 16, 2026 at 11:30:22AM +0000, Kiryl Shutsemau wrote:\n> On Tue, Feb 10, 2026 at 04:57:55PM +0100, Vlastimil Babka wrote:\n> > On 2/9/26 12:52, Kiryl Shutsemau wrote:\n> > > On Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:\n> > >> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> > >> > With the upcoming changes to HVO, a single page of tail struct pages\n> > >> > will be shared across all huge pages of the same order on a node. Since\n> > >> > huge pages on the same node may belong to different zones, the zone\n> > >> > information stored in shared tail page flags would be incorrect.\n> > >> > \n> > >> > Always fetch zone information from the head page, which has unique and\n> > >> > correct zone flags for each compound page.\n> > >> > \n> > >> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> > >> > Acked-by: Zi Yan <ziy@nvidia.com>\n> > >> > ---\n> > >> >   include/linux/mmzone.h | 1 +\n> > >> >   1 file changed, 1 insertion(+)\n> > >> > \n> > >> > diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n> > >> > index be8ce40b5638..192143b5cdc0 100644\n> > >> > --- a/include/linux/mmzone.h\n> > >> > +++ b/include/linux/mmzone.h\n> > >> > @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n> > >> >   static inline enum zone_type page_zonenum(const struct page *page)\n> > >> >   {\n> > >> > +\tpage = compound_head(page);\n> > >> >   \treturn memdesc_zonenum(page->flags);\n> > >> \n> > >> We end up calling page_zonenum() without holding a reference.\n> > >> \n> > >> Given that _compound_head() does a READ_ONCE(), this should work even if we\n> > >> see concurrent page freeing etc.\n> > >> \n> > >> However, this change implies that we now perform a compound page lookup for\n> > >> every PageHighMem() [meh], page_zone() [quite some users in the buddy,\n> > >> including for pageblock access and page freeing].\n> > >> \n> > >> That's a nasty compromise for making HVO better? :)\n> > >> \n> > >> We should likely limit that special casing to kernels that really rquire it\n> > >> (HVO).\n> > > \n> > > I will add compound_info_has_mask() check.\n> > \n> > Not thrilled by this indeed. Would it be a problem to have the shared tail\n> > pages per node+zone instead of just per node?\n> \n> I thought it would be overkill. It likely is going to be unused for most\n> nodes. But sure, move it to per-zone.\n\nI gave it a try, but stumbled on a problem.\n\nWe need to know the zone in hugetlb_vmemmap_init_early(), but zones are\nnot yet defined.\n\nhugetlb_vmemmap_init_early() is called from within sparse_init(), but\nspan of zones is defined in free_area_init() after sparse_init().\n\nAny ideas, how get past this? :/\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-05",
          "message_id": ""
        },
        {
          "author": "David (arm)",
          "summary": "Reviewer David (arm) noted that the patch introduces a new alignment requirement for vmemmap, which is not strictly necessary because vmemmap start is always struct-page-aligned. He suggested pulling the magic number 'MAX_FOLIO_NR_PAGES * sizeof(struct page)' into the core and defining it conditionally based on HVO configuration and the power-of-2 property of sizeof(struct page).",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "suggested improvement"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On 2/4/26 17:56, David Hildenbrand (arm) wrote:\n> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n>> The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n>> struct pages of the head page to be naturally aligned with regard to the\n>> folio size.\n>>\n>> Align vmemmap to MAX_FOLIO_NR_PAGES.\n>>\n>> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n>> ---\n>>  arch/loongarch/include/asm/pgtable.h | 3 ++-\n>>  1 file changed, 2 insertions(+), 1 deletion(-)\n>>\n>> diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/ \n>> include/asm/pgtable.h\n>> index c33b3bcb733e..f9416acb9156 100644\n>> --- a/arch/loongarch/include/asm/pgtable.h\n>> +++ b/arch/loongarch/include/asm/pgtable.h\n>> @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / \n>> sizeof(unsigned long)];\n>>  min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * \n>> PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - \n>> KFENCE_AREA_SIZE)\n>>  #endif\n>> -#define vmemmap ((struct page *)((VMALLOC_END + PMD_SIZE) & \n>> PMD_MASK))\n>> +#define VMEMMAP_ALIGN max(PMD_SIZE, MAX_FOLIO_NR_PAGES * \n>> sizeof(struct page))\n>> +#define vmemmap ((struct page *)(ALIGN(VMALLOC_END, \n>> VMEMMAP_ALIGN)))\n> \n> \n> Same comment, the \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is just \n> black magic here\n> and the description of the situation is wrong.\n> \n> Maybe you want to pull the magic \"MAX_FOLIO_NR_PAGES * sizeof(struct \n> page)\" into the core and call it\n> \n> #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct \n> page))\n> \n> But then special case it base on (a) HVO being configured in an (b) HVO \n> being possible\n> \n> #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP && is_power_of_2(sizeof(struct page)\n> /* A very helpful comment explaining the situation. */\n> #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct \n> page))\n> #else\n> #define MAX_FOLIO_VMEMMAP_ALIGN 0\n> #endif\n> \n> Something like that.\n> \n\nThinking about this ...\n\nthe vmemmap start is always struct-page-aligned. Otherwise we'd be in \ntrouble already.\n\nIsn't it then sufficient to just align the start to MAX_FOLIO_NR_PAGES?\n\nLet's assume sizeof(struct page) == 64 and MAX_FOLIO_NR_PAGES = 512 for \nsimplicity.\n\nvmemmap start would be multiples of 512 (0x0010000000).\n\n512, 1024, 1536, 2048 ...\n\nAssume we have an 256-pages folio at 1536+256 = 0x111000000\n\nAssume we have the last page of that folio (0x011111111111), we would \njust get to the start of that folio by AND-ing with ~(256-1).\n\nWhich case am I ignoring?\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> With the upcoming changes to HVO, a single page of tail struct pages\n> will be shared across all huge pages of the same order on a node. Since\n> huge pages on the same node may belong to different zones, the zone\n> information stored in shared tail page flags would be incorrect.\n> \n> Always fetch zone information from the head page, which has unique and\n> correct zone flags for each compound page.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Acked-by: Zi Yan <ziy@nvidia.com>\n> ---\n>   include/linux/mmzone.h | 1 +\n>   1 file changed, 1 insertion(+)\n> \n> diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n> index be8ce40b5638..192143b5cdc0 100644\n> --- a/include/linux/mmzone.h\n> +++ b/include/linux/mmzone.h\n> @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n>   \n>   static inline enum zone_type page_zonenum(const struct page *page)\n>   {\n> +\tpage = compound_head(page);\n>   \treturn memdesc_zonenum(page->flags);\n\nWe end up calling page_zonenum() without holding a reference.\n\nGiven that _compound_head() does a READ_ONCE(), this should work even if \nwe see concurrent page freeing etc.\n\nHowever, this change implies that we now perform a compound page lookup \nfor every PageHighMem() [meh], page_zone() [quite some users in the \nbuddy, including for pageblock access and page freeing].\n\nThat's a nasty compromise for making HVO better? :)\n\nWe should likely limit that special casing to kernels that really rquire \nit (HVO).\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> With the upcoming changes to HVO, a single page of tail struct pages\n> will be shared across all huge pages of the same order on a node. Since\n> huge pages on the same node may belong to different zones, the zone\n> information stored in shared tail page flags would be incorrect.\n> \n> Always fetch zone information from the head page, which has unique and\n> correct zone flags for each compound page.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Acked-by: Zi Yan <ziy@nvidia.com>\n> ---\n>   include/linux/mmzone.h | 1 +\n>   1 file changed, 1 insertion(+)\n> \n> diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n> index be8ce40b5638..192143b5cdc0 100644\n> --- a/include/linux/mmzone.h\n> +++ b/include/linux/mmzone.h\n> @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n>   \n>   static inline enum zone_type page_zonenum(const struct page *page)\n>   {\n> +\tpage = compound_head(page);\n>   \treturn memdesc_zonenum(page->flags);\n\nWe end up calling page_zonenum() without holding a reference.\n\nGiven that _compound_head() does a READ_ONCE(), this should work even if \nwe see concurrent page freeing etc.\n\nHowever, this change implies that we now perform a compound page lookup \nfor every PageHighMem() [meh], page_zone() [quite some users in the \nbuddy, including for pageblock access and page freeing].\n\nThat's a nasty compromise for making HVO better? :)\n\nWe should likely limit that special casing to kernels that really rquire \nit (HVO).\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> If page->compound_info encodes a mask, it is expected that vmemmap to be\n> naturally aligned to the maximum folio size.\n> \n> Add a VM_BUG_ON() to check the alignment.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Acked-by: Zi Yan <ziy@nvidia.com>\n> ---\n>   mm/sparse.c | 7 +++++++\n>   1 file changed, 7 insertions(+)\n> \n> diff --git a/mm/sparse.c b/mm/sparse.c\n> index b5b2b6f7041b..6c9b62607f3f 100644\n> --- a/mm/sparse.c\n> +++ b/mm/sparse.c\n> @@ -600,6 +600,13 @@ void __init sparse_init(void)\n>   \tBUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));\n>   \tmemblocks_present();\n>   \n> +\tif (compound_info_has_mask()) {\n> +\t\tunsigned long alignment;\n> +\n> +\t\talignment = MAX_FOLIO_NR_PAGES * sizeof(struct page);\n> +\t\tVM_BUG_ON(!IS_ALIGNED((unsigned long) pfn_to_page(0), alignment));\n\nNo VM_BUG_ON. VM_WARN_ON_ONCE() should be good enough, no?\n\nAs discussed in the other thread, is checking for MAX_FOLIO_NR_PAGES \nalignment sufficient?\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> If page->compound_info encodes a mask, it is expected that vmemmap to be\n> naturally aligned to the maximum folio size.\n> \n> Add a VM_BUG_ON() to check the alignment.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Acked-by: Zi Yan <ziy@nvidia.com>\n> ---\n>   mm/sparse.c | 7 +++++++\n>   1 file changed, 7 insertions(+)\n> \n> diff --git a/mm/sparse.c b/mm/sparse.c\n> index b5b2b6f7041b..6c9b62607f3f 100644\n> --- a/mm/sparse.c\n> +++ b/mm/sparse.c\n> @@ -600,6 +600,13 @@ void __init sparse_init(void)\n>   \tBUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));\n>   \tmemblocks_present();\n>   \n> +\tif (compound_info_has_mask()) {\n> +\t\tunsigned long alignment;\n> +\n> +\t\talignment = MAX_FOLIO_NR_PAGES * sizeof(struct page);\n> +\t\tVM_BUG_ON(!IS_ALIGNED((unsigned long) pfn_to_page(0), alignment));\n\nNo VM_BUG_ON. VM_WARN_ON_ONCE() should be good enough, no?\n\nAs discussed in the other thread, is checking for MAX_FOLIO_NR_PAGES \nalignment sufficient?\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/5/26 14:43, Kiryl Shutsemau wrote:\n> On Thu, Feb 05, 2026 at 01:56:36PM +0100, David Hildenbrand (Arm) wrote:\n>> On 2/4/26 17:56, David Hildenbrand (arm) wrote:\n>>>\n>>>\n>>> Same comment, the \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is just\n>>> black magic here\n>>> and the description of the situation is wrong.\n>>>\n>>> Maybe you want to pull the magic \"MAX_FOLIO_NR_PAGES * sizeof(struct\n>>> page)\" into the core and call it\n>>>\n>>> #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct\n>>> page))\n>>>\n>>> But then special case it base on (a) HVO being configured in an (b) HVO\n>>> being possible\n>>>\n>>> #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP && is_power_of_2(sizeof(struct page)\n>>> /* A very helpful comment explaining the situation. */\n>>> #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct\n>>> page))\n>>> #else\n>>> #define MAX_FOLIO_VMEMMAP_ALIGN 0\n>>> #endif\n>>>\n>>> Something like that.\n>>>\n>>\n>> Thinking about this ...\n>>\n>> the vmemmap start is always struct-page-aligned. Otherwise we'd be in\n>> trouble already.\n>>\n>> Isn't it then sufficient to just align the start to MAX_FOLIO_NR_PAGES?\n>>\n>> Let's assume sizeof(struct page) == 64 and MAX_FOLIO_NR_PAGES = 512 for\n>> simplicity.\n>>\n>> vmemmap start would be multiples of 512 (0x0010000000).\n>>\n>> 512, 1024, 1536, 2048 ...\n>>\n>> Assume we have an 256-pages folio at 1536+256 = 0x111000000\n> \n> s/0x/0b/, but okay.\n\n:)\n\n> \n>> Assume we have the last page of that folio (0x011111111111), we would just\n>> get to the start of that folio by AND-ing with ~(256-1).\n>>\n>> Which case am I ignoring?\n> \n> IIUC, you are ignoring the actual size of struct page. It is not 1 byte :P\n\nI thought it wouldn't matter but, yeah, that's it.\n\n\"Align the vmemmap to the maximum folio metadata size\" it is.\n\nThen you can explain the situation also alongside \nMAX_FOLIO_VMEMMAP_ALIGN, and that we expect this to be a power of 2.\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/5/26 14:43, Kiryl Shutsemau wrote:\n> On Thu, Feb 05, 2026 at 01:56:36PM +0100, David Hildenbrand (Arm) wrote:\n>> On 2/4/26 17:56, David Hildenbrand (arm) wrote:\n>>>\n>>>\n>>> Same comment, the \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is just\n>>> black magic here\n>>> and the description of the situation is wrong.\n>>>\n>>> Maybe you want to pull the magic \"MAX_FOLIO_NR_PAGES * sizeof(struct\n>>> page)\" into the core and call it\n>>>\n>>> #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct\n>>> page))\n>>>\n>>> But then special case it base on (a) HVO being configured in an (b) HVO\n>>> being possible\n>>>\n>>> #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP && is_power_of_2(sizeof(struct page)\n>>> /* A very helpful comment explaining the situation. */\n>>> #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct\n>>> page))\n>>> #else\n>>> #define MAX_FOLIO_VMEMMAP_ALIGN 0\n>>> #endif\n>>>\n>>> Something like that.\n>>>\n>>\n>> Thinking about this ...\n>>\n>> the vmemmap start is always struct-page-aligned. Otherwise we'd be in\n>> trouble already.\n>>\n>> Isn't it then sufficient to just align the start to MAX_FOLIO_NR_PAGES?\n>>\n>> Let's assume sizeof(struct page) == 64 and MAX_FOLIO_NR_PAGES = 512 for\n>> simplicity.\n>>\n>> vmemmap start would be multiples of 512 (0x0010000000).\n>>\n>> 512, 1024, 1536, 2048 ...\n>>\n>> Assume we have an 256-pages folio at 1536+256 = 0x111000000\n> \n> s/0x/0b/, but okay.\n\n:)\n\n> \n>> Assume we have the last page of that folio (0x011111111111), we would just\n>> get to the start of that folio by AND-ing with ~(256-1).\n>>\n>> Which case am I ignoring?\n> \n> IIUC, you are ignoring the actual size of struct page. It is not 1 byte :P\n\nI thought it wouldn't matter but, yeah, that's it.\n\n\"Align the vmemmap to the maximum folio metadata size\" it is.\n\nThen you can explain the situation also alongside \nMAX_FOLIO_VMEMMAP_ALIGN, and that we expect this to be a power of 2.\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/5/26 14:50, Kiryl Shutsemau wrote:\n> On Wed, Feb 04, 2026 at 05:50:23PM +0100, David Hildenbrand (arm) wrote:\n>> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n>>> The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n>>> struct pages of the head page to be naturally aligned with regard to the\n>>> folio size.\n>>>\n>>> Align vmemmap to MAX_FOLIO_NR_PAGES.\n>>\n>> I think neither that statement nor the one in the patch description is\n>> correct?\n>>\n>> \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is neither the maximum folio size\n>> nor MAX_FOLIO_NR_PAGES.\n>>\n>> It's the size of the memmap that a large folio could span at maximum.\n>>\n>>\n>> Assuming we have a 16 GiB folio, the calculation would give us\n>>\n>> \t4194304 * sizeof(struct page)\n>>\n>> Which could be something like (assuming 80 bytes)\n>>\n>> \t335544320\n>>\n>> -> not even a power of 2, weird? (for HVO you wouldn't care as HVO would be\n>> disabled, but that aliment is super weird?)\n>>\n>>\n>> Assuming 64 bytes, it would be a power of two (as 64 is a power of two).\n>>\n>> \t268435456 (1<< 28)\n>>\n>>\n>> Which makes me wonder whether there is a way to avoid sizeof(struct page)\n>> here completely.\n> \n> I don't think we can. See the other thread.\n\nAgreed. You could only go for something larger (like PAGE_SIZE).\n\n> \n> What about using roundup_pow_of_two(sizeof(struct page)) here.\n\nBetter I think.\n\n> \n>> Or limit the alignment to the case where HVO is actually active and\n>> sizeof(struct page) makes any sense?\n> \n> The annoying part of HVO is that it is unknown at compile-time if it\n> will be used. You can compile kernel with HVO that will no be activated\n> due to non-power-of-2 sizeof(struct page) because of a debug config option.\nAh, and now I remember that sizeof cannot be used in macros, damnit.\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/5/26 14:50, Kiryl Shutsemau wrote:\n> On Wed, Feb 04, 2026 at 05:50:23PM +0100, David Hildenbrand (arm) wrote:\n>> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n>>> The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n>>> struct pages of the head page to be naturally aligned with regard to the\n>>> folio size.\n>>>\n>>> Align vmemmap to MAX_FOLIO_NR_PAGES.\n>>\n>> I think neither that statement nor the one in the patch description is\n>> correct?\n>>\n>> \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is neither the maximum folio size\n>> nor MAX_FOLIO_NR_PAGES.\n>>\n>> It's the size of the memmap that a large folio could span at maximum.\n>>\n>>\n>> Assuming we have a 16 GiB folio, the calculation would give us\n>>\n>> \t4194304 * sizeof(struct page)\n>>\n>> Which could be something like (assuming 80 bytes)\n>>\n>> \t335544320\n>>\n>> -> not even a power of 2, weird? (for HVO you wouldn't care as HVO would be\n>> disabled, but that aliment is super weird?)\n>>\n>>\n>> Assuming 64 bytes, it would be a power of two (as 64 is a power of two).\n>>\n>> \t268435456 (1<< 28)\n>>\n>>\n>> Which makes me wonder whether there is a way to avoid sizeof(struct page)\n>> here completely.\n> \n> I don't think we can. See the other thread.\n\nAgreed. You could only go for something larger (like PAGE_SIZE).\n\n> \n> What about using roundup_pow_of_two(sizeof(struct page)) here.\n\nBetter I think.\n\n> \n>> Or limit the alignment to the case where HVO is actually active and\n>> sizeof(struct page) makes any sense?\n> \n> The annoying part of HVO is that it is unknown at compile-time if it\n> will be used. You can compile kernel with HVO that will no be activated\n> due to non-power-of-2 sizeof(struct page) because of a debug config option.\nAh, and now I remember that sizeof cannot be used in macros, damnit.\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/5/26 14:52, Kiryl Shutsemau wrote:\n> On Wed, Feb 04, 2026 at 05:56:45PM +0100, David Hildenbrand (arm) wrote:\n>> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n>>> The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n>>> struct pages of the head page to be naturally aligned with regard to the\n>>> folio size.\n>>>\n>>> Align vmemmap to MAX_FOLIO_NR_PAGES.\n>>>\n>>> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n>>> ---\n>>>    arch/loongarch/include/asm/pgtable.h | 3 ++-\n>>>    1 file changed, 2 insertions(+), 1 deletion(-)\n>>>\n>>> diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h\n>>> index c33b3bcb733e..f9416acb9156 100644\n>>> --- a/arch/loongarch/include/asm/pgtable.h\n>>> +++ b/arch/loongarch/include/asm/pgtable.h\n>>> @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];\n>>>    \t min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)\n>>>    #endif\n>>> -#define vmemmap\t\t((struct page *)((VMALLOC_END + PMD_SIZE) & PMD_MASK))\n>>> +#define VMEMMAP_ALIGN\tmax(PMD_SIZE, MAX_FOLIO_NR_PAGES * sizeof(struct page))\n>>> +#define vmemmap\t\t((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))\n>>\n>>\n>> Same comment, the \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is just black magic here\n>> and the description of the situation is wrong.\n>>\n>> Maybe you want to pull the magic \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" into the core and call it\n>>\n>> #define MAX_FOLIO_VMEMMAP_ALIGN\t(MAX_FOLIO_NR_PAGES * sizeof(struct page))\n>>\n>> But then special case it base on (a) HVO being configured in an (b) HVO being possible\n>>\n>> #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP && is_power_of_2(sizeof(struct page)\n> \n> This would require some kind of asm-offsets.c/bounds.c magic to pull the\n> struct page size condition to the preprocessor level.\n> \n\nRight.\n\nI guess you could move that into the macro and let the compiler handle it.\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/5/26 14:52, Kiryl Shutsemau wrote:\n> On Wed, Feb 04, 2026 at 05:56:45PM +0100, David Hildenbrand (arm) wrote:\n>> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n>>> The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n>>> struct pages of the head page to be naturally aligned with regard to the\n>>> folio size.\n>>>\n>>> Align vmemmap to MAX_FOLIO_NR_PAGES.\n>>>\n>>> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n>>> ---\n>>>    arch/loongarch/include/asm/pgtable.h | 3 ++-\n>>>    1 file changed, 2 insertions(+), 1 deletion(-)\n>>>\n>>> diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h\n>>> index c33b3bcb733e..f9416acb9156 100644\n>>> --- a/arch/loongarch/include/asm/pgtable.h\n>>> +++ b/arch/loongarch/include/asm/pgtable.h\n>>> @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];\n>>>    \t min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)\n>>>    #endif\n>>> -#define vmemmap\t\t((struct page *)((VMALLOC_END + PMD_SIZE) & PMD_MASK))\n>>> +#define VMEMMAP_ALIGN\tmax(PMD_SIZE, MAX_FOLIO_NR_PAGES * sizeof(struct page))\n>>> +#define vmemmap\t\t((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))\n>>\n>>\n>> Same comment, the \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is just black magic here\n>> and the description of the situation is wrong.\n>>\n>> Maybe you want to pull the magic \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" into the core and call it\n>>\n>> #define MAX_FOLIO_VMEMMAP_ALIGN\t(MAX_FOLIO_NR_PAGES * sizeof(struct page))\n>>\n>> But then special case it base on (a) HVO being configured in an (b) HVO being possible\n>>\n>> #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP && is_power_of_2(sizeof(struct page)\n> \n> This would require some kind of asm-offsets.c/bounds.c magic to pull the\n> struct page size condition to the preprocessor level.\n> \n\nRight.\n\nI guess you could move that into the macro and let the compiler handle it.\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/5/26 14:31, David Hildenbrand (Arm) wrote:\n> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n>> If page->compound_info encodes a mask, it is expected that vmemmap to be\n>> naturally aligned to the maximum folio size.\n>>\n>> Add a VM_BUG_ON() to check the alignment.\n>>\n>> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n>> Acked-by: Zi Yan <ziy@nvidia.com>\n>> ---\n>>  mm/sparse.c | 7 +++++++\n>>  1 file changed, 7 insertions(+)\n>>\n>> diff --git a/mm/sparse.c b/mm/sparse.c\n>> index b5b2b6f7041b..6c9b62607f3f 100644\n>> --- a/mm/sparse.c\n>> +++ b/mm/sparse.c\n>> @@ -600,6 +600,13 @@ void __init sparse_init(void)\n>>  BUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));\n>>  memblocks_present();\n>> + if (compound_info_has_mask()) {\n>> + unsigned long alignment;\n>> +\n>> + alignment = MAX_FOLIO_NR_PAGES * sizeof(struct page);\n>> + VM_BUG_ON(!IS_ALIGNED((unsigned long) pfn_to_page(0), \n>> alignment));\n> \n> No VM_BUG_ON. VM_WARN_ON_ONCE() should be good enough, no?\n> \n> As discussed in the other thread, is checking for MAX_FOLIO_NR_PAGES \n> alignment sufficient?\n\nAnd after further discussions, we could use MAX_FOLIO_VMEMMAP_ALIGN \nmacro once we have that.\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/5/26 14:31, David Hildenbrand (Arm) wrote:\n> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n>> If page->compound_info encodes a mask, it is expected that vmemmap to be\n>> naturally aligned to the maximum folio size.\n>>\n>> Add a VM_BUG_ON() to check the alignment.\n>>\n>> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n>> Acked-by: Zi Yan <ziy@nvidia.com>\n>> ---\n>>  mm/sparse.c | 7 +++++++\n>>  1 file changed, 7 insertions(+)\n>>\n>> diff --git a/mm/sparse.c b/mm/sparse.c\n>> index b5b2b6f7041b..6c9b62607f3f 100644\n>> --- a/mm/sparse.c\n>> +++ b/mm/sparse.c\n>> @@ -600,6 +600,13 @@ void __init sparse_init(void)\n>>  BUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));\n>>  memblocks_present();\n>> + if (compound_info_has_mask()) {\n>> + unsigned long alignment;\n>> +\n>> + alignment = MAX_FOLIO_NR_PAGES * sizeof(struct page);\n>> + VM_BUG_ON(!IS_ALIGNED((unsigned long) pfn_to_page(0), \n>> alignment));\n> \n> No VM_BUG_ON. VM_WARN_ON_ONCE() should be good enough, no?\n> \n> As discussed in the other thread, is checking for MAX_FOLIO_NR_PAGES \n> alignment sufficient?\n\nAnd after further discussions, we could use MAX_FOLIO_VMEMMAP_ALIGN \nmacro once we have that.\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-05",
          "message_id": ""
        },
        {
          "author": "David (arm)",
          "summary": "Reviewer David noted that clearing bit 0 before applying the mask to get the head page address is unnecessary, as the page pointer should not have set it in the first place.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "signal1",
            "signal2"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Acked-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> For tail pages, the kernel uses the 'compound_info' field to get to the\n> head page. The bit 0 of the field indicates whether the page is a\n> tail page, and if set, the remaining bits represent a pointer to the\n> head page.\n> \n> For cases when size of struct page is power-of-2, change the encoding of\n> compound_info to store a mask that can be applied to the virtual address\n> of the tail page in order to access the head page. It is possible\n> because struct page of the head page is naturally aligned with regards\n> to order of the page.\n> \n> The significant impact of this modification is that all tail pages of\n> the same order will now have identical 'compound_info', regardless of\n> the compound page they are associated with. This paves the way for\n> eliminating fake heads.\n> \n> The HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only\n> applied when the sizeof(struct page) is power-of-2. Having identical\n> tail pages allows the same page to be mapped into the vmemmap of all\n> pages, maintaining memory savings without fake heads.\n> \n> If sizeof(struct page) is not power-of-2, there is no functional\n> changes.\n> \n> Limit mask usage to HugeTLB vmemmap optimization (HVO) where it makes\n> a difference. The approach with mask would work in the wider set of\n> conditions, but it requires validating that struct pages are naturally\n> aligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n\n[...]\n\n>   \tstruct folio *foliop;\n>   \tint loops = 5;\n>   \n> @@ -1247,8 +1247,8 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n>   again:\n>   \tmemset(&ps->folio_snapshot, 0, sizeof(struct folio));\n>   \tmemcpy(&ps->page_snapshot, page, sizeof(*page));\n> -\thead = ps->page_snapshot.compound_info;\n> -\tif ((head & 1) == 0) {\n> +\tinfo = ps->page_snapshot.compound_info;\n> +\tif (!(info & 1)) {\n>   \t\tps->idx = 0;\n>   \t\tfoliop = (struct folio *)&ps->page_snapshot;\n>   \t\tif (!folio_test_large(foliop)) {\n> @@ -1259,7 +1259,15 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n>   \t\t}\n>   \t\tfoliop = (struct folio *)page;\n>   \t} else {\n> -\t\tfoliop = (struct folio *)(head - 1);\n> +\t\t/* See compound_head() */\n> +\t\tif (compound_info_has_mask()) {\n> +\t\t\tunsigned long p = (unsigned long)page;\n> +\n> +\t\t\tfoliop = (struct folio *)(p & info);\n\nIIUC, we don't care about clearing bit0 before the & as the page pointer \nshouldn't have set it in the first page.\n\nPretty neat\n\nAcked-by: David Hildenbrand (Arm) <david@kernel.org>\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> For tail pages, the kernel uses the 'compound_info' field to get to the\n> head page. The bit 0 of the field indicates whether the page is a\n> tail page, and if set, the remaining bits represent a pointer to the\n> head page.\n> \n> For cases when size of struct page is power-of-2, change the encoding of\n> compound_info to store a mask that can be applied to the virtual address\n> of the tail page in order to access the head page. It is possible\n> because struct page of the head page is naturally aligned with regards\n> to order of the page.\n> \n> The significant impact of this modification is that all tail pages of\n> the same order will now have identical 'compound_info', regardless of\n> the compound page they are associated with. This paves the way for\n> eliminating fake heads.\n> \n> The HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only\n> applied when the sizeof(struct page) is power-of-2. Having identical\n> tail pages allows the same page to be mapped into the vmemmap of all\n> pages, maintaining memory savings without fake heads.\n> \n> If sizeof(struct page) is not power-of-2, there is no functional\n> changes.\n> \n> Limit mask usage to HugeTLB vmemmap optimization (HVO) where it makes\n> a difference. The approach with mask would work in the wider set of\n> conditions, but it requires validating that struct pages are naturally\n> aligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n\n[...]\n\n>   \tstruct folio *foliop;\n>   \tint loops = 5;\n>   \n> @@ -1247,8 +1247,8 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n>   again:\n>   \tmemset(&ps->folio_snapshot, 0, sizeof(struct folio));\n>   \tmemcpy(&ps->page_snapshot, page, sizeof(*page));\n> -\thead = ps->page_snapshot.compound_info;\n> -\tif ((head & 1) == 0) {\n> +\tinfo = ps->page_snapshot.compound_info;\n> +\tif (!(info & 1)) {\n>   \t\tps->idx = 0;\n>   \t\tfoliop = (struct folio *)&ps->page_snapshot;\n>   \t\tif (!folio_test_large(foliop)) {\n> @@ -1259,7 +1259,15 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n>   \t\t}\n>   \t\tfoliop = (struct folio *)page;\n>   \t} else {\n> -\t\tfoliop = (struct folio *)(head - 1);\n> +\t\t/* See compound_head() */\n> +\t\tif (compound_info_has_mask()) {\n> +\t\t\tunsigned long p = (unsigned long)page;\n> +\n> +\t\t\tfoliop = (struct folio *)(p & info);\n\nIIUC, we don't care about clearing bit0 before the & as the page pointer \nshouldn't have set it in the first page.\n\nPretty neat\n\nAcked-by: David Hildenbrand (Arm) <david@kernel.org>\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/16/26 00:13, Matthew Wilcox wrote:\n> On Mon, Feb 02, 2026 at 03:56:24PM +0000, Kiryl Shutsemau wrote:\n>> With the upcoming changes to HVO, a single page of tail struct pages\n>> will be shared across all huge pages of the same order on a node. Since\n>> huge pages on the same node may belong to different zones, the zone\n>> information stored in shared tail page flags would be incorrect.\n>>\n>> Always fetch zone information from the head page, which has unique and\n>> correct zone flags for each compound page.\n> \n> You're right that different pages in the same folio can have different\n> zone number.  But does it matter ... or to put it another way, why is\n> returning the zone number of the head page the correct way to resolve\n> this?\n\nHow can a folio cross zones?\n\nRuntime allocated hugetlb folios from the CMA/buddy (alloc_contig_range) \ndefinitely fall into a single zone.\n\nSo is it about ones allocated early during boot, where, by chance, we \nmanage to cross ZONE_NORMAL + ZONE_MOVABLE etc?\n\nI thought that it's also not allowed there, and I wonder whether we \nshould disallow it if it's possible.\n\n> \n> Arguably, the caller is asking for the zone number of _this page_, and\n> does not care about the zone number of the head page.  It would be good\n> to have a short discussion of this in the commit message (but probably\n> not worth putting this in a comment).\n\nAgreed, in particular, if there would be a functional change. So far I \nassumed there would be no such change.\n\nThings like shrink_zone_span() really need to know the zone of that \npage, not the one of the head; unless both fall into the same zone.\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/16/26 00:13, Matthew Wilcox wrote:\n> On Mon, Feb 02, 2026 at 03:56:24PM +0000, Kiryl Shutsemau wrote:\n>> With the upcoming changes to HVO, a single page of tail struct pages\n>> will be shared across all huge pages of the same order on a node. Since\n>> huge pages on the same node may belong to different zones, the zone\n>> information stored in shared tail page flags would be incorrect.\n>>\n>> Always fetch zone information from the head page, which has unique and\n>> correct zone flags for each compound page.\n> \n> You're right that different pages in the same folio can have different\n> zone number.  But does it matter ... or to put it another way, why is\n> returning the zone number of the head page the correct way to resolve\n> this?\n\nHow can a folio cross zones?\n\nRuntime allocated hugetlb folios from the CMA/buddy (alloc_contig_range) \ndefinitely fall into a single zone.\n\nSo is it about ones allocated early during boot, where, by chance, we \nmanage to cross ZONE_NORMAL + ZONE_MOVABLE etc?\n\nI thought that it's also not allowed there, and I wonder whether we \nshould disallow it if it's possible.\n\n> \n> Arguably, the caller is asking for the zone number of _this page_, and\n> does not care about the zone number of the head page.  It would be good\n> to have a short discussion of this in the commit message (but probably\n> not worth putting this in a comment).\n\nAgreed, in particular, if there would be a functional change. So far I \nassumed there would be no such change.\n\nThings like shrink_zone_span() really need to know the zone of that \npage, not the one of the head; unless both fall into the same zone.\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/23/26 19:18, Matthew Wilcox wrote:\n> On Mon, Feb 16, 2026 at 10:06:57AM +0100, David Hildenbrand (Arm) wrote:\n>> On 2/16/26 00:13, Matthew Wilcox wrote:\n>>>\n>>> You're right that different pages in the same folio can have different\n>>> zone number.  But does it matter ... or to put it another way, why is\n>>> returning the zone number of the head page the correct way to resolve\n>>> this?\n>>\n>> How can a folio cross zones?\n> \n> I thought 1GB pages in hugetlb could cross zones?  Maybe that used to be\n> true and isn't any more, or maybe it was never true and I was just\n> confused.\n\nI recall that 1G folios could end up in ZONE_MOVABLE (comment in\npage_is_unmovable()), but my memory is fuzzy when it comes to crossing\nzones (ZONE_NORMAL -> ZONE_MOVABLE).\n\nFreeing+reinitializing the vmemmap for HVO with such folios would\nalready be problematic I suppose: we would silently switch the zone for\nsome of these pages.\n\nWhen freeing such (boottime) hugetlb folios to the buddy, we use\nfree_frozen_pages(). In there we lookup the zone once.\n\nLikely also problematic :)\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/23/26 19:18, Matthew Wilcox wrote:\n> On Mon, Feb 16, 2026 at 10:06:57AM +0100, David Hildenbrand (Arm) wrote:\n>> On 2/16/26 00:13, Matthew Wilcox wrote:\n>>>\n>>> You're right that different pages in the same folio can have different\n>>> zone number.  But does it matter ... or to put it another way, why is\n>>> returning the zone number of the head page the correct way to resolve\n>>> this?\n>>\n>> How can a folio cross zones?\n> \n> I thought 1GB pages in hugetlb could cross zones?  Maybe that used to be\n> true and isn't any more, or maybe it was never true and I was just\n> confused.\n\nI recall that 1G folios could end up in ZONE_MOVABLE (comment in\npage_is_unmovable()), but my memory is fuzzy when it comes to crossing\nzones (ZONE_NORMAL -> ZONE_MOVABLE).\n\nFreeing+reinitializing the vmemmap for HVO with such folios would\nalready be problematic I suppose: we would silently switch the zone for\nsome of these pages.\n\nWhen freeing such (boottime) hugetlb folios to the buddy, we use\nfree_frozen_pages(). In there we lookup the zone once.\n\nLikely also problematic :)\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-05",
          "message_id": ""
        }
      ],
      "analysis_source": "llm",
      "patch_summary": "This patch series removes 'fake head pages' from the HugeTLB vmemmap optimization by changing how tail pages encode their relationship to the head page, simplifying compound_head() and page_ref_add_unless(). The new approach uses a mask-based encoding for architectures where sizeof(struct page) is a power of 2, allowing shared read-only tail pages across huge pages on a NUMA node. This reduces complexity and overhead in the hot path, but testing has shown either no change or only slight performance improvement."
    },
    "2026-02-27": {
      "report_file": "2026-02-27_ollama_llama3.1-8b.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author is addressing a concern about passing down the head page and tail page index, which was deemed too complex. The author instead proposes to pass the tail and head pages directly, along with the compound page order, as a preparation for changing how the head position is encoded in the tail page.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "preparation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Kiryl Shutsemau <kas@kernel.org>\n\nMove MAX_FOLIO_ORDER definition from mm.h to mmzone.h.\n\nThis is preparation for adding the vmemmap_tails array to struct\nzone, which requires MAX_FOLIO_ORDER to be available in mmzone.h.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nAcked-by: David Hildenbrand (Red Hat) <david@kernel.org>\nAcked-by: Zi Yan <ziy@nvidia.com>\nAcked-by: Muchun Song <muchun.song@linux.dev>\nAcked-by: Usama Arif <usamaarif642@gmail.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n---\n include/linux/mm.h     | 31 -------------------------------\n include/linux/mmzone.h | 31 +++++++++++++++++++++++++++++++\n 2 files changed, 31 insertions(+), 31 deletions(-)\n\ndiff --git a/include/linux/mm.h b/include/linux/mm.h\nindex 5be3d8a8f806..7f4dbbb9d783 100644\n--- a/include/linux/mm.h\n+++ b/include/linux/mm.h\n@@ -27,7 +27,6 @@\n #include <linux/page-flags.h>\n #include <linux/page_ref.h>\n #include <linux/overflow.h>\n-#include <linux/sizes.h>\n #include <linux/sched.h>\n #include <linux/pgtable.h>\n #include <linux/kasan.h>\n@@ -2479,36 +2478,6 @@ static inline unsigned long folio_nr_pages(const struct folio *folio)\n \treturn folio_large_nr_pages(folio);\n }\n \n-#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)\n-/*\n- * We don't expect any folios that exceed buddy sizes (and consequently\n- * memory sections).\n- */\n-#define MAX_FOLIO_ORDER\t\tMAX_PAGE_ORDER\n-#elif defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\n-/*\n- * Only pages within a single memory section are guaranteed to be\n- * contiguous. By limiting folios to a single memory section, all folio\n- * pages are guaranteed to be contiguous.\n- */\n-#define MAX_FOLIO_ORDER\t\tPFN_SECTION_SHIFT\n-#elif defined(CONFIG_HUGETLB_PAGE)\n-/*\n- * There is no real limit on the folio size. We limit them to the maximum we\n- * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect\n- * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.\n- */\n-#define MAX_FOLIO_ORDER\t\tget_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)\n-#else\n-/*\n- * Without hugetlb, gigantic folios that are bigger than a single PUD are\n- * currently impossible.\n- */\n-#define MAX_FOLIO_ORDER\t\tPUD_ORDER\n-#endif\n-\n-#define MAX_FOLIO_NR_PAGES\t(1UL << MAX_FOLIO_ORDER)\n-\n /*\n  * compound_nr() returns the number of pages in this potentially compound\n  * page.  compound_nr() can be called on a tail page, and is defined to\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 3e51190a55e4..be8ce40b5638 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -23,6 +23,7 @@\n #include <linux/page-flags.h>\n #include <linux/local_lock.h>\n #include <linux/zswap.h>\n+#include <linux/sizes.h>\n #include <asm/page.h>\n \n /* Free memory management - zoned buddy allocator.  */\n@@ -61,6 +62,36 @@\n  */\n #define PAGE_ALLOC_COSTLY_ORDER 3\n \n+#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)\n+/*\n+ * We don't expect any folios that exceed buddy sizes (and consequently\n+ * memory sections).\n+ */\n+#define MAX_FOLIO_ORDER\t\tMAX_PAGE_ORDER\n+#elif defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\n+/*\n+ * Only pages within a single memory section are guaranteed to be\n+ * contiguous. By limiting folios to a single memory section, all folio\n+ * pages are guaranteed to be contiguous.\n+ */\n+#define MAX_FOLIO_ORDER\t\tPFN_SECTION_SHIFT\n+#elif defined(CONFIG_HUGETLB_PAGE)\n+/*\n+ * There is no real limit on the folio size. We limit them to the maximum we\n+ * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect\n+ * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.\n+ */\n+#define MAX_FOLIO_ORDER\t\tget_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)\n+#else\n+/*\n+ * Without hugetlb, gigantic folios that are bigger than a single PUD are\n+ * currently impossible.\n+ */\n+#define MAX_FOLIO_ORDER\t\tPUD_ORDER\n+#endif\n+\n+#define MAX_FOLIO_NR_PAGES\t(1UL << MAX_FOLIO_ORDER)\n+\n enum migratetype {\n \tMIGRATE_UNMOVABLE,\n \tMIGRATE_MOVABLE,\n-- \n2.51.2\n\n---\n\nFrom: Kiryl Shutsemau <kas@kernel.org>\n\nInstead of passing down the head page and tail page index, pass the tail\nand head pages directly, as well as the order of the compound page.\n\nThis is a preparation for changing how the head position is encoded in\nthe tail page.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\nAcked-by: David Hildenbrand (arm) <david@kernel.org>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n---\n include/linux/page-flags.h |  5 +++--\n mm/hugetlb.c               |  8 +++++---\n mm/internal.h              | 11 +++++------\n mm/mm_init.c               |  2 +-\n mm/page_alloc.c            |  2 +-\n 5 files changed, 15 insertions(+), 13 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex f7a0e4af0c73..5e7687ccccf8 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -865,9 +865,10 @@ static inline bool folio_test_large(const struct folio *folio)\n \treturn folio_test_head(folio);\n }\n \n-static __always_inline void set_compound_head(struct page *page, struct page *head)\n+static __always_inline void set_compound_head(struct page *tail,\n+\t\tconst struct page *head, unsigned int order)\n {\n-\tWRITE_ONCE(page->compound_head, (unsigned long)head + 1);\n+\tWRITE_ONCE(tail->compound_head, (unsigned long)head + 1);\n }\n \n static __always_inline void clear_compound_head(struct page *page)\ndiff --git a/mm/hugetlb.c b/mm/hugetlb.c\nindex 0beb6e22bc26..fc55f22c9e41 100644\n--- a/mm/hugetlb.c\n+++ b/mm/hugetlb.c\n@@ -3168,6 +3168,7 @@ int __alloc_bootmem_huge_page(struct hstate *h, int nid)\n \n /* Initialize [start_page:end_page_number] tail struct pages of a hugepage */\n static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n+\t\t\t\t\tstruct hstate *h,\n \t\t\t\t\tunsigned long start_page_number,\n \t\t\t\t\tunsigned long end_page_number)\n {\n@@ -3176,6 +3177,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n \tstruct page *page = folio_page(folio, start_page_number);\n \tunsigned long head_pfn = folio_pfn(folio);\n \tunsigned long pfn, end_pfn = head_pfn + end_page_number;\n+\tunsigned int order = huge_page_order(h);\n \n \t/*\n \t * As we marked all tail pages with memblock_reserved_mark_noinit(),\n@@ -3183,7 +3185,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n \t */\n \tfor (pfn = head_pfn + start_page_number; pfn < end_pfn; page++, pfn++) {\n \t\t__init_single_page(page, pfn, zone, nid);\n-\t\tprep_compound_tail((struct page *)folio, pfn - head_pfn);\n+\t\tprep_compound_tail(page, &folio->page, order);\n \t\tset_page_count(page, 0);\n \t}\n }\n@@ -3203,7 +3205,7 @@ static void __init hugetlb_folio_init_vmemmap(struct folio *folio,\n \t__folio_set_head(folio);\n \tret = folio_ref_freeze(folio, 1);\n \tVM_BUG_ON(!ret);\n-\thugetlb_folio_init_tail_vmemmap(folio, 1, nr_pages);\n+\thugetlb_folio_init_tail_vmemmap(folio, h, 1, nr_pages);\n \tprep_compound_head(&folio->page, huge_page_order(h));\n }\n \n@@ -3260,7 +3262,7 @@ static void __init prep_and_add_bootmem_folios(struct hstate *h,\n \t\t\t * time as this is early in boot and there should\n \t\t\t * be no contention.\n \t\t\t */\n-\t\t\thugetlb_folio_init_tail_vmemmap(folio,\n+\t\t\thugetlb_folio_init_tail_vmemmap(folio, h,\n \t\t\t\t\tHUGETLB_VMEMMAP_RESERVE_PAGES,\n \t\t\t\t\tpages_per_huge_page(h));\n \t\t}\ndiff --git a/mm/internal.h b/mm/internal.h\nindex cb0af847d7d9..c76122f22294 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -878,13 +878,12 @@ static inline void prep_compound_head(struct page *page, unsigned int order)\n \t\tINIT_LIST_HEAD(&folio->_deferred_list);\n }\n \n-static inline void prep_compound_tail(struct page *head, int tail_idx)\n+static inline void prep_compound_tail(struct page *tail,\n+\t\tconst struct page *head, unsigned int order)\n {\n-\tstruct page *p = head + tail_idx;\n-\n-\tp->mapping = TAIL_MAPPING;\n-\tset_compound_head(p, head);\n-\tset_page_private(p, 0);\n+\ttail->mapping = TAIL_MAPPING;\n+\tset_compound_head(tail, head, order);\n+\tset_page_private(tail, 0);\n }\n \n void post_alloc_hook(struct page *page, unsigned int order, gfp_t gfp_flags);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 61d983d23f55..0a12a9be0bcc 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -1099,7 +1099,7 @@ static void __ref memmap_init_compound(struct page *head,\n \t\tstruct page *page = pfn_to_page(pfn);\n \n \t\t__init_zone_device_page(page, pfn, zone_idx, nid, pgmap);\n-\t\tprep_compound_tail(head, pfn - head_pfn);\n+\t\tprep_compound_tail(page, head, order);\n \t\tset_page_count(page, 0);\n \t}\n \tprep_compound_head(head, order);\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex fcc32737f451..aa657e4a99e8 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -744,7 +744,7 @@ void prep_compound_page(struct page *page, unsigned int order)\n \n \t__SetPageHead(page);\n \tfor (i = 1; i < nr_pages; i++)\n-\t\tprep_compound_tail(page, i);\n+\t\tprep_compound_tail(page + i, page, order);\n \n \tprep_compound_head(page, order);\n }\n-- \n2.51.2\n\n---\n\nFrom: Kiryl Shutsemau <kas@kernel.org>\n\nThe 'compound_head' field in the 'struct page' encodes whether the page\nis a tail and where to locate the head page. Bit 0 is set if the page is\na tail, and the remaining bits in the field point to the head page.\n\nAs preparation for changing how the field encodes information about the\nhead page, rename the field to 'compound_info'.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\nAcked-by: David Hildenbrand (arm) <david@kernel.org>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n---\n .../admin-guide/kdump/vmcoreinfo.rst          |  2 +-\n Documentation/mm/vmemmap_dedup.rst            |  6 +++---\n include/linux/mm_types.h                      | 20 +++++++++----------\n include/linux/page-flags.h                    | 18 ++++++++---------\n include/linux/types.h                         |  2 +-\n kernel/vmcore_info.c                          |  2 +-\n mm/page_alloc.c                               |  2 +-\n mm/slab.h                                     |  2 +-\n mm/util.c                                     |  2 +-\n 9 files changed, 28 insertions(+), 28 deletions(-)\n\ndiff --git a/Documentation/admin-guide/kdump/vmcoreinfo.rst b/Documentation/admin-guide/kdump/vmcoreinfo.rst\nindex 404a15f6782c..7663c610fe90 100644\n--- a/Documentation/admin-guide/kdump/vmcoreinfo.rst\n+++ b/Documentation/admin-guide/kdump/vmcoreinfo.rst\n@@ -141,7 +141,7 @@ nodemask_t\n The size of a nodemask_t type. Used to compute the number of online\n nodes.\n \n-(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_head)\n+(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_info)\n ----------------------------------------------------------------------------------\n \n User-space tools compute their values based on the offset of these\ndiff --git a/Documentation/mm/vmemmap_dedup.rst b/Documentation/mm/vmemmap_dedup.rst\nindex b4a55b6569fa..1863d88d2dcb 100644\n--- a/Documentation/mm/vmemmap_dedup.rst\n+++ b/Documentation/mm/vmemmap_dedup.rst\n@@ -24,7 +24,7 @@ For each base page, there is a corresponding ``struct page``.\n Within the HugeTLB subsystem, only the first 4 ``struct page`` are used to\n contain unique information about a HugeTLB page. ``__NR_USED_SUBPAGE`` provides\n this upper limit. The only 'useful' information in the remaining ``struct page``\n-is the compound_head field, and this field is the same for all tail pages.\n+is the compound_info field, and this field is the same for all tail pages.\n \n By removing redundant ``struct page`` for HugeTLB pages, memory can be returned\n to the buddy allocator for other uses.\n@@ -124,10 +124,10 @@ Here is how things look before optimization::\n  |           |\n  +-----------+\n \n-The value of page->compound_head is the same for all tail pages. The first\n+The value of page->compound_info is the same for all tail pages. The first\n page of ``struct page`` (page 0) associated with the HugeTLB page contains the 4\n ``struct page`` necessary to describe the HugeTLB. The only use of the remaining\n-pages of ``struct page`` (page 1 to page 7) is to point to page->compound_head.\n+pages of ``struct page`` (page 1 to page 7) is to point to page->compound_info.\n Therefore, we can remap pages 1 to 7 to page 0. Only 1 page of ``struct page``\n will be used for each HugeTLB page. This will allow us to free the remaining\n 7 pages to the buddy allocator.\ndiff --git a/include/linux/mm_types.h b/include/linux/mm_types.h\nindex 3cc8ae722886..7bc82a2b889f 100644\n--- a/include/linux/mm_types.h\n+++ b/include/linux/mm_types.h\n@@ -126,14 +126,14 @@ struct page {\n \t\t\tatomic_long_t pp_ref_count;\n \t\t};\n \t\tstruct {\t/* Tail pages of compound page */\n-\t\t\tunsigned long compound_head;\t/* Bit zero is set */\n+\t\t\tunsigned long compound_info;\t/* Bit zero is set */\n \t\t};\n \t\tstruct {\t/* ZONE_DEVICE pages */\n \t\t\t/*\n-\t\t\t * The first word is used for compound_head or folio\n+\t\t\t * The first word is used for compound_info or folio\n \t\t\t * pgmap\n \t\t\t */\n-\t\t\tvoid *_unused_pgmap_compound_head;\n+\t\t\tvoid *_unused_pgmap_compound_info;\n \t\t\tvoid *zone_device_data;\n \t\t\t/*\n \t\t\t * ZONE_DEVICE private pages are counted as being\n@@ -409,7 +409,7 @@ struct folio {\n \t/* private: avoid cluttering the output */\n \t\t\t\t/* For the Unevictable \"LRU list\" slot */\n \t\t\t\tstruct {\n-\t\t\t\t\t/* Avoid compound_head */\n+\t\t\t\t\t/* Avoid compound_info */\n \t\t\t\t\tvoid *__filler;\n \t/* public: */\n \t\t\t\t\tunsigned int mlock_count;\n@@ -510,7 +510,7 @@ struct folio {\n FOLIO_MATCH(flags, flags);\n FOLIO_MATCH(lru, lru);\n FOLIO_MATCH(mapping, mapping);\n-FOLIO_MATCH(compound_head, lru);\n+FOLIO_MATCH(compound_info, lru);\n FOLIO_MATCH(__folio_index, index);\n FOLIO_MATCH(private, private);\n FOLIO_MATCH(_mapcount, _mapcount);\n@@ -529,7 +529,7 @@ FOLIO_MATCH(_last_cpupid, _last_cpupid);\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + sizeof(struct page))\n FOLIO_MATCH(flags, _flags_1);\n-FOLIO_MATCH(compound_head, _head_1);\n+FOLIO_MATCH(compound_info, _head_1);\n FOLIO_MATCH(_mapcount, _mapcount_1);\n FOLIO_MATCH(_refcount, _refcount_1);\n #undef FOLIO_MATCH\n@@ -537,13 +537,13 @@ FOLIO_MATCH(_refcount, _refcount_1);\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + 2 * sizeof(struct page))\n FOLIO_MATCH(flags, _flags_2);\n-FOLIO_MATCH(compound_head, _head_2);\n+FOLIO_MATCH(compound_info, _head_2);\n #undef FOLIO_MATCH\n #define FOLIO_MATCH(pg, fl)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + 3 * sizeof(struct page))\n FOLIO_MATCH(flags, _flags_3);\n-FOLIO_MATCH(compound_head, _head_3);\n+FOLIO_MATCH(compound_info, _head_3);\n #undef FOLIO_MATCH\n \n /**\n@@ -609,8 +609,8 @@ struct ptdesc {\n #define TABLE_MATCH(pg, pt)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct page, pg) == offsetof(struct ptdesc, pt))\n TABLE_MATCH(flags, pt_flags);\n-TABLE_MATCH(compound_head, pt_list);\n-TABLE_MATCH(compound_head, _pt_pad_1);\n+TABLE_MATCH(compound_info, pt_list);\n+TABLE_MATCH(compound_info, _pt_pad_1);\n TABLE_MATCH(mapping, __page_mapping);\n TABLE_MATCH(__folio_index, pt_index);\n TABLE_MATCH(rcu_head, pt_rcu_head);\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex 5e7687ccccf8..70c4e43f2d9a 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -213,7 +213,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t/*\n \t * Only addresses aligned with PAGE_SIZE of struct page may be fake head\n \t * struct page. The alignment check aims to avoid access the fields (\n-\t * e.g. compound_head) of the @page[1]. It can avoid touch a (possibly)\n+\t * e.g. compound_info) of the @page[1]. It can avoid touch a (possibly)\n \t * cold cacheline in some cases.\n \t */\n \tif (IS_ALIGNED((unsigned long)page, PAGE_SIZE) &&\n@@ -223,7 +223,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t\t * because the @page is a compound page composed with at least\n \t\t * two contiguous pages.\n \t\t */\n-\t\tunsigned long head = READ_ONCE(page[1].compound_head);\n+\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n \n \t\tif (likely(head & 1))\n \t\t\treturn (const struct page *)(head - 1);\n@@ -281,7 +281,7 @@ static __always_inline int page_is_fake_head(const struct page *page)\n \n static __always_inline unsigned long _compound_head(const struct page *page)\n {\n-\tunsigned long head = READ_ONCE(page->compound_head);\n+\tunsigned long head = READ_ONCE(page->compound_info);\n \n \tif (unlikely(head & 1))\n \t\treturn head - 1;\n@@ -320,13 +320,13 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n \n static __always_inline int PageTail(const struct page *page)\n {\n-\treturn READ_ONCE(page->compound_head) & 1 || page_is_fake_head(page);\n+\treturn READ_ONCE(page->compound_info) & 1 || page_is_fake_head(page);\n }\n \n static __always_inline int PageCompound(const struct page *page)\n {\n \treturn test_bit(PG_head, &page->flags.f) ||\n-\t       READ_ONCE(page->compound_head) & 1;\n+\t       READ_ONCE(page->compound_info) & 1;\n }\n \n #define\tPAGE_POISON_PATTERN\t-1l\n@@ -348,7 +348,7 @@ static const unsigned long *const_folio_flags(const struct folio *folio,\n {\n \tconst struct page *page = &folio->page;\n \n-\tVM_BUG_ON_PGFLAGS(page->compound_head & 1, page);\n+\tVM_BUG_ON_PGFLAGS(page->compound_info & 1, page);\n \tVM_BUG_ON_PGFLAGS(n > 0 && !test_bit(PG_head, &page->flags.f), page);\n \treturn &page[n].flags.f;\n }\n@@ -357,7 +357,7 @@ static unsigned long *folio_flags(struct folio *folio, unsigned n)\n {\n \tstruct page *page = &folio->page;\n \n-\tVM_BUG_ON_PGFLAGS(page->compound_head & 1, page);\n+\tVM_BUG_ON_PGFLAGS(page->compound_info & 1, page);\n \tVM_BUG_ON_PGFLAGS(n > 0 && !test_bit(PG_head, &page->flags.f), page);\n \treturn &page[n].flags.f;\n }\n@@ -868,12 +868,12 @@ static inline bool folio_test_large(const struct folio *folio)\n static __always_inline void set_compound_head(struct page *tail,\n \t\tconst struct page *head, unsigned int order)\n {\n-\tWRITE_ONCE(tail->compound_head, (unsigned long)head + 1);\n+\tWRITE_ONCE(tail->compound_info, (unsigned long)head + 1);\n }\n \n static __always_inline void clear_compound_head(struct page *page)\n {\n-\tWRITE_ONCE(page->compound_head, 0);\n+\tWRITE_ONCE(page->compound_info, 0);\n }\n \n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\ndiff --git a/include/linux/types.h b/include/linux/types.h\nindex 7e71d260763c..608050dbca6a 100644\n--- a/include/linux/types.h\n+++ b/include/linux/types.h\n@@ -239,7 +239,7 @@ struct ustat {\n  *\n  * This guarantee is important for few reasons:\n  *  - future call_rcu_lazy() will make use of lower bits in the pointer;\n- *  - the structure shares storage space in struct page with @compound_head,\n+ *  - the structure shares storage space in struct page with @compound_info,\n  *    which encode PageTail() in bit 0. The guarantee is needed to avoid\n  *    false-positive PageTail().\n  */\ndiff --git a/kernel/vmcore_info.c b/kernel/vmcore_info.c\nindex 8d82913223a1..94e4ef75b1b2 100644\n--- a/kernel/vmcore_info.c\n+++ b/kernel/vmcore_info.c\n@@ -198,7 +198,7 @@ static int __init crash_save_vmcoreinfo_init(void)\n \tVMCOREINFO_OFFSET(page, lru);\n \tVMCOREINFO_OFFSET(page, _mapcount);\n \tVMCOREINFO_OFFSET(page, private);\n-\tVMCOREINFO_OFFSET(page, compound_head);\n+\tVMCOREINFO_OFFSET(page, compound_info);\n \tVMCOREINFO_OFFSET(pglist_data, node_zones);\n \tVMCOREINFO_OFFSET(pglist_data, nr_zones);\n #ifdef CONFIG_FLATMEM\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex aa657e4a99e8..e83f67fbbf07 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -731,7 +731,7 @@ static inline bool pcp_allowed_order(unsigned int order)\n  * The first PAGE_SIZE page is called the \"head page\" and have PG_head set.\n  *\n  * The remaining PAGE_SIZE pages are called \"tail pages\". PageTail() is encoded\n- * in bit 0 of page->compound_head. The rest of bits is pointer to head page.\n+ * in bit 0 of page->compound_info. The rest of bits is pointer to head page.\n  *\n  * The first tail page's ->compound_order holds the order of allocation.\n  * This usage means that zero-order pages may not be compound.\ndiff --git a/mm/slab.h b/mm/slab.h\nindex 71c7261bf822..62dfa50c1f01 100644\n--- a/mm/slab.h\n+++ b/mm/slab.h\n@@ -94,7 +94,7 @@ struct slab {\n #define SLAB_MATCH(pg, sl)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct page, pg) == offsetof(struct slab, sl))\n SLAB_MATCH(flags, flags);\n-SLAB_MATCH(compound_head, slab_cache);\t/* Ensure bit 0 is clear */\n+SLAB_MATCH(compound_info, slab_cache);\t/* Ensure bit 0 is clear */\n SLAB_MATCH(_refcount, __page_refcount);\n #ifdef CONFIG_MEMCG\n SLAB_MATCH(memcg_data, obj_exts);\ndiff --git a/mm/util.c b/mm/util.c\nindex b05ab6f97e11..3ebcb9e6035c 100644\n--- a/mm/util.c\n+++ b/mm/util.c\n@@ -1247,7 +1247,7 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n again:\n \tmemset(&ps->folio_snapshot, 0, sizeof(struct folio));\n \tmemcpy(&ps->page_snapshot, page, sizeof(*page));\n-\thead = ps->page_snapshot.compound_head;\n+\thead = ps->page_snapshot.compound_info;\n \tif ((head & 1) == 0) {\n \t\tps->idx = 0;\n \t\tfoliop = (struct folio *)&ps->page_snapshot;\n-- \n2.51.2\n\n---\n\nFrom: Kiryl Shutsemau <kas@kernel.org>\n\nMove set_compound_head() and clear_compound_head() to be adjacent to the\ncompound_head() function in page-flags.h.\n\nThese functions encode and decode the same compound_info field, so\nkeeping them together makes it easier to verify their logic is\nconsistent, especially when the encoding changes.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\nAcked-by: David Hildenbrand (arm) <david@kernel.org>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n---\n include/linux/page-flags.h | 22 +++++++++++-----------\n 1 file changed, 11 insertions(+), 11 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex 70c4e43f2d9a..42bf8ed02a29 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -290,6 +290,17 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n \n #define compound_head(page)\t((typeof(page))_compound_head(page))\n \n+static __always_inline void set_compound_head(struct page *tail,\n+\t\tconst struct page *head, unsigned int order)\n+{\n+\tWRITE_ONCE(tail->compound_info, (unsigned long)head + 1);\n+}\n+\n+static __always_inline void clear_compound_head(struct page *page)\n+{\n+\tWRITE_ONCE(page->compound_info, 0);\n+}\n+\n /**\n  * page_folio - Converts from page to folio.\n  * @p: The page.\n@@ -865,17 +876,6 @@ static inline bool folio_test_large(const struct folio *folio)\n \treturn folio_test_head(folio);\n }\n \n-static __always_inline void set_compound_head(struct page *tail,\n-\t\tconst struct page *head, unsigned int order)\n-{\n-\tWRITE_ONCE(tail->compound_info, (unsigned long)head + 1);\n-}\n-\n-static __always_inline void clear_compound_head(struct page *page)\n-{\n-\tWRITE_ONCE(page->compound_info, 0);\n-}\n-\n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\n static inline void ClearPageCompound(struct page *page)\n {\n-- \n2.51.2",
          "reply_to": "",
          "message_date": "2026-02-27",
          "message_id": ""
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author addressed a concern about aligning vmemmap to the newly introduced MAX_FOLIO_VMEMMAP_ALIGN, explaining that it's required for HugeTLB Vmemmap Optimization (HVO) and providing patch updates for riscv and loongarch architectures. The author also described how changing the encoding of compound_info from a pointer to a mask will allow identical tail pages to be mapped into the vmemmap of all pages, eliminating fake heads.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "addressed_concern",
            "provided_explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Kiryl Shutsemau <kas@kernel.org>\n\nThe upcoming change to the HugeTLB vmemmap optimization (HVO) requires\nstruct pages of the head page to be naturally aligned with regard to the\nfolio size.\n\nAlign vmemmap to the newly introduced MAX_FOLIO_VMEMMAP_ALIGN.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\n---\n arch/riscv/mm/init.c   |  3 ++-\n include/linux/mmzone.h | 11 +++++++++++\n 2 files changed, 13 insertions(+), 1 deletion(-)\n\ndiff --git a/arch/riscv/mm/init.c b/arch/riscv/mm/init.c\nindex 811e03786c56..e8fb2239a0b5 100644\n--- a/arch/riscv/mm/init.c\n+++ b/arch/riscv/mm/init.c\n@@ -63,7 +63,8 @@ phys_addr_t phys_ram_base __ro_after_init;\n EXPORT_SYMBOL(phys_ram_base);\n \n #ifdef CONFIG_SPARSEMEM_VMEMMAP\n-#define VMEMMAP_ADDR_ALIGN\t(1ULL << SECTION_SIZE_BITS)\n+#define VMEMMAP_ADDR_ALIGN\tmax(1ULL << SECTION_SIZE_BITS, \\\n+\t\t\t\t    MAX_FOLIO_VMEMMAP_ALIGN)\n \n unsigned long vmemmap_start_pfn __ro_after_init;\n EXPORT_SYMBOL(vmemmap_start_pfn);\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex be8ce40b5638..492a5be1090f 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -92,6 +92,17 @@\n \n #define MAX_FOLIO_NR_PAGES\t(1UL << MAX_FOLIO_ORDER)\n \n+/*\n+ * HugeTLB Vmemmap Optimization (HVO) requires struct pages of the head page to\n+ * be naturally aligned with regard to the folio size.\n+ *\n+ * HVO which is only active if the size of struct page is a power of 2.\n+ */\n+#define MAX_FOLIO_VMEMMAP_ALIGN \\\n+\t(IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP) && \\\n+\t is_power_of_2(sizeof(struct page)) ? \\\n+\t MAX_FOLIO_NR_PAGES * sizeof(struct page) : 0)\n+\n enum migratetype {\n \tMIGRATE_UNMOVABLE,\n \tMIGRATE_MOVABLE,\n-- \n2.51.2\n\n---\n\nFrom: Kiryl Shutsemau <kas@kernel.org>\n\nThe upcoming change to the HugeTLB vmemmap optimization (HVO) requires\nstruct pages of the head page to be naturally aligned with regard to the\nfolio size.\n\nAlign vmemmap to MAX_FOLIO_VMEMMAP_ALIGN.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\n---\n arch/loongarch/include/asm/pgtable.h | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h\nindex c33b3bcb733e..ea6c09eed2e7 100644\n--- a/arch/loongarch/include/asm/pgtable.h\n+++ b/arch/loongarch/include/asm/pgtable.h\n@@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];\n \t min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)\n #endif\n \n-#define vmemmap\t\t((struct page *)((VMALLOC_END + PMD_SIZE) & PMD_MASK))\n+#define VMEMMAP_ALIGN\tmax(PMD_SIZE, MAX_FOLIO_VMEMMAP_ALIGN)\n+#define vmemmap\t\t((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))\n #define VMEMMAP_END\t((unsigned long)vmemmap + VMEMMAP_SIZE - 1)\n \n #define KFENCE_AREA_START\t(VMEMMAP_END + 1)\n-- \n2.51.2\n\n---\n\nFrom: Kiryl Shutsemau <kas@kernel.org>\n\nFor tail pages, the kernel uses the 'compound_info' field to get to the\nhead page. The bit 0 of the field indicates whether the page is a\ntail page, and if set, the remaining bits represent a pointer to the\nhead page.\n\nFor cases when size of struct page is power-of-2, change the encoding of\ncompound_info to store a mask that can be applied to the virtual address\nof the tail page in order to access the head page. It is possible\nbecause struct page of the head page is naturally aligned with regards\nto order of the page.\n\nThe significant impact of this modification is that all tail pages of\nthe same order will now have identical 'compound_info', regardless of\nthe compound page they are associated with. This paves the way for\neliminating fake heads.\n\nThe HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only\napplied when the sizeof(struct page) is power-of-2. Having identical\ntail pages allows the same page to be mapped into the vmemmap of all\npages, maintaining memory savings without fake heads.\n\nIf sizeof(struct page) is not power-of-2, there is no functional\nchanges.\n\nLimit mask usage to HugeTLB vmemmap optimization (HVO) where it makes\na difference. The approach with mask would work in the wider set of\nconditions, but it requires validating that struct pages are naturally\naligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\nAcked-by: David Hildenbrand (Arm) <david@kernel.org>\nAcked-by: Usama Arif <usamaarif642@gmail.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n---\n include/linux/page-flags.h | 81 ++++++++++++++++++++++++++++++++++----\n mm/slab.h                  | 16 ++++++--\n mm/util.c                  | 16 ++++++--\n 3 files changed, 97 insertions(+), 16 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex 42bf8ed02a29..01970bd38bff 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -198,6 +198,29 @@ enum pageflags {\n \n #ifndef __GENERATING_BOUNDS_H\n \n+/*\n+ * For tail pages, if the size of struct page is power-of-2 ->compound_info\n+ * encodes the mask that converts the address of the tail page address to\n+ * the head page address.\n+ *\n+ * Otherwise, ->compound_info has direct pointer to head pages.\n+ */\n+static __always_inline bool compound_info_has_mask(void)\n+{\n+\t/*\n+\t * Limit mask usage to HugeTLB vmemmap optimization (HVO) where it\n+\t * makes a difference.\n+\t *\n+\t * The approach with mask would work in the wider set of conditions,\n+\t * but it requires validating that struct pages are naturally aligned\n+\t * for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n+\t */\n+\tif (!IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP))\n+\t\treturn false;\n+\n+\treturn is_power_of_2(sizeof(struct page));\n+}\n+\n #ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP\n DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\n \n@@ -207,6 +230,10 @@ DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\n  */\n static __always_inline const struct page *page_fixed_fake_head(const struct page *page)\n {\n+\t/* Fake heads only exists if compound_info_has_mask() is true */\n+\tif (!compound_info_has_mask())\n+\t\treturn page;\n+\n \tif (!static_branch_unlikely(&hugetlb_optimize_vmemmap_key))\n \t\treturn page;\n \n@@ -223,10 +250,14 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t\t * because the @page is a compound page composed with at least\n \t\t * two contiguous pages.\n \t\t */\n-\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n+\t\tunsigned long info = READ_ONCE(page[1].compound_info);\n \n-\t\tif (likely(head & 1))\n-\t\t\treturn (const struct page *)(head - 1);\n+\t\t/* See set_compound_head() */\n+\t\tif (likely(info & 1)) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\n+\t\t\treturn (const struct page *)(p & info);\n+\t\t}\n \t}\n \treturn page;\n }\n@@ -281,11 +312,26 @@ static __always_inline int page_is_fake_head(const struct page *page)\n \n static __always_inline unsigned long _compound_head(const struct page *page)\n {\n-\tunsigned long head = READ_ONCE(page->compound_info);\n+\tunsigned long info = READ_ONCE(page->compound_info);\n \n-\tif (unlikely(head & 1))\n-\t\treturn head - 1;\n-\treturn (unsigned long)page_fixed_fake_head(page);\n+\t/* Bit 0 encodes PageTail() */\n+\tif (!(info & 1))\n+\t\treturn (unsigned long)page_fixed_fake_head(page);\n+\n+\t/*\n+\t * If compound_info_has_mask() is false, the rest of compound_info is\n+\t * the pointer to the head page.\n+\t */\n+\tif (!compound_info_has_mask())\n+\t\treturn info - 1;\n+\n+\t/*\n+\t * If compound_info_has_mask() is true the rest of the info encodes\n+\t * the mask that converts the address of the tail page to the head page.\n+\t *\n+\t * No need to clear bit 0 in the mask as 'page' always has it clear.\n+\t */\n+\treturn (unsigned long)page & info;\n }\n \n #define compound_head(page)\t((typeof(page))_compound_head(page))\n@@ -293,7 +339,26 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n static __always_inline void set_compound_head(struct page *tail,\n \t\tconst struct page *head, unsigned int order)\n {\n-\tWRITE_ONCE(tail->compound_info, (unsigned long)head + 1);\n+\tunsigned int shift;\n+\tunsigned long mask;\n+\n+\tif (!compound_info_has_mask()) {\n+\t\tWRITE_ONCE(tail->compound_info, (unsigned long)head | 1);\n+\t\treturn;\n+\t}\n+\n+\t/*\n+\t * If the size of struct page is power-of-2, bits [shift:0] of the\n+\t * virtual address of compound head are zero.\n+\t *\n+\t * Calculate mask that can be applied to the virtual address of\n+\t * the tail page to get address of the head page.\n+\t */\n+\tshift = order + order_base_2(sizeof(struct page));\n+\tmask = GENMASK(BITS_PER_LONG - 1, shift);\n+\n+\t/* Bit 0 encodes PageTail() */\n+\tWRITE_ONCE(tail->compound_info, mask | 1);\n }\n \n static __always_inline void clear_compound_head(struct page *page)\ndiff --git a/mm/slab.h b/mm/slab.h\nindex 62dfa50c1f01..1a1b3758df05 100644\n--- a/mm/slab.h\n+++ b/mm/slab.h\n@@ -131,11 +131,19 @@ static_assert(IS_ALIGNED(offsetof(struct slab, freelist), sizeof(struct freelist\n  */\n static inline struct slab *page_slab(const struct page *page)\n {\n-\tunsigned long head;\n+\tunsigned long info;\n+\n+\tinfo = READ_ONCE(page->compound_info);\n+\tif (info & 1) {\n+\t\t/* See compound_head() */\n+\t\tif (compound_info_has_mask()) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\t\t\tpage = (struct page *)(p & info);\n+\t\t} else {\n+\t\t\tpage = (struct page *)(info - 1);\n+\t\t}\n+\t}\n \n-\thead = READ_ONCE(page->compound_head);\n-\tif (head & 1)\n-\t\tpage = (struct page *)(head - 1);\n \tif (data_race(page->page_type >> 24) != PGTY_slab)\n \t\tpage = NULL;\n \ndiff --git a/mm/util.c b/mm/util.c\nindex 3ebcb9e6035c..20dccf2881d7 100644\n--- a/mm/util.c\n+++ b/mm/util.c\n@@ -1237,7 +1237,7 @@ static void set_ps_flags(struct page_snapshot *ps, const struct folio *folio,\n  */\n void snapshot_page(struct page_snapshot *ps, const struct page *page)\n {\n-\tunsigned long head, nr_pages = 1;\n+\tunsigned long info, nr_pages = 1;\n \tstruct folio *foliop;\n \tint loops = 5;\n \n@@ -1247,8 +1247,8 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n again:\n \tmemset(&ps->folio_snapshot, 0, sizeof(struct folio));\n \tmemcpy(&ps->page_snapshot, page, sizeof(*page));\n-\thead = ps->page_snapshot.compound_info;\n-\tif ((head & 1) == 0) {\n+\tinfo = ps->page_snapshot.compound_info;\n+\tif (!(info & 1)) {\n \t\tps->idx = 0;\n \t\tfoliop = (struct folio *)&ps->page_snapshot;\n \t\tif (!folio_test_large(foliop)) {\n@@ -1259,7 +1259,15 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n \t\t}\n \t\tfoliop = (struct folio *)page;\n \t} else {\n-\t\tfoliop = (struct folio *)(head - 1);\n+\t\t/* See compound_head() */\n+\t\tif (compound_info_has_mask()) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\n+\t\t\tfoliop = (struct folio *)(p & info);\n+\t\t} else {\n+\t\t\tfoliop = (struct folio *)(info - 1);\n+\t\t}\n+\n \t\tps->idx = folio_page_idx(foliop, page);\n \t}\n \n-- \n2.51.2\n\n---\n\nFrom: Kiryl Shutsemau <kas@kernel.org>\n\nIf page->compound_info encodes a mask, it is expected that vmemmap to be\nnaturally aligned to the maximum folio size.\n\nAdd a VM_WARN_ON_ONCE() to check the alignment.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nAcked-by: Zi Yan <ziy@nvidia.com>\n---\n mm/sparse.c | 5 +++++\n 1 file changed, 5 insertions(+)\n\ndiff --git a/mm/sparse.c b/mm/sparse.c\nindex b5b2b6f7041b..dfabe554adf8 100644\n--- a/mm/sparse.c\n+++ b/mm/sparse.c\n@@ -600,6 +600,11 @@ void __init sparse_init(void)\n \tBUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));\n \tmemblocks_present();\n \n+\tif (compound_info_has_mask()) {\n+\t\tVM_WARN_ON_ONCE(!IS_ALIGNED((unsigned long) pfn_to_page(0),\n+\t\t\t\t    MAX_FOLIO_VMEMMAP_ALIGN));\n+\t}\n+\n \tpnum_begin = first_present_section_nr();\n \tnid_begin = sparse_early_nid(__nr_to_section(pnum_begin));\n \n-- \n2.51.2\n\n---\n\nCurrently, the vmemmap for bootmem-allocated gigantic pages is populated\nearly in hugetlb_vmemmap_init_early(). However, the zone information is\nonly available after zones are initialized. If it is later discovered\nthat a page spans multiple zones, the HVO mapping must be undone and\nreplaced with a normal mapping using vmemmap_undo_hvo().\n\nDefer the actual vmemmap population to hugetlb_vmemmap_init_late(). At\nthis stage, zones are already initialized, so it can be checked if the\npage is valid for HVO before deciding how to populate the vmemmap.\n\nThis allows us to remove vmemmap_undo_hvo() and the complex logic\nrequired to rollback HVO mappings.\n\nIn hugetlb_vmemmap_init_late(), if HVO population fails or if the zones\nare invalid, fall back to a normal vmemmap population.\n\nPostponing population until hugetlb_vmemmap_init_late() also makes zone\ninformation available from within vmemmap_populate_hvo().\n\nSigned-off-by: Kiryl Shutsemau (Meta) <kas@kernel.org>\n---\n include/linux/mm.h   |  2 --\n mm/hugetlb_vmemmap.c | 37 +++++++++++++++----------------\n mm/sparse-vmemmap.c  | 53 --------------------------------------------\n 3 files changed, 18 insertions(+), 74 deletions(-)\n\ndiff --git a/include/linux/mm.h b/include/linux/mm.h\nindex 7f4dbbb9d783..0e2d45008ff4 100644\n--- a/include/linux/mm.h\n+++ b/include/linux/mm.h\n@@ -4484,8 +4484,6 @@ int vmemmap_populate(unsigned long start, unsigned long end, int node,\n \t\tstruct vmem_altmap *altmap);\n int vmemmap_populate_hvo(unsigned long start, unsigned long end, int node,\n \t\t\t unsigned long headsize);\n-int vmemmap_undo_hvo(unsigned long start, unsigned long end, int node,\n-\t\t     unsigned long headsize);\n void vmemmap_wrprotect_hvo(unsigned long start, unsigned long end, int node,\n \t\t\t  unsigned long headsize);\n void vmemmap_populate_print_last(void);\ndiff --git a/mm/hugetlb_vmemmap.c b/mm/hugetlb_vmemmap.c\nindex a9280259e12a..935ec5829be9 100644\n--- a/mm/hugetlb_vmemmap.c\n+++ b/mm/hugetlb_vmemmap.c\n@@ -790,7 +790,6 @@ void __init hugetlb_vmemmap_init_early(int nid)\n {\n \tunsigned long psize, paddr, section_size;\n \tunsigned long ns, i, pnum, pfn, nr_pages;\n-\tunsigned long start, end;\n \tstruct huge_bootmem_page *m = NULL;\n \tvoid *map;\n \n@@ -808,14 +807,6 @@ void __init hugetlb_vmemmap_init_early(int nid)\n \t\tpaddr = virt_to_phys(m);\n \t\tpfn = PHYS_PFN(paddr);\n \t\tmap = pfn_to_page(pfn);\n-\t\tstart = (unsigned long)map;\n-\t\tend = start + nr_pages * sizeof(struct page);\n-\n-\t\tif (vmemmap_populate_hvo(start, end, nid,\n-\t\t\t\t\tHUGETLB_VMEMMAP_RESERVE_SIZE) < 0)\n-\t\t\tcontinue;\n-\n-\t\tmemmap_boot_pages_add(HUGETLB_VMEMMAP_RESERVE_SIZE / PAGE_SIZE);\n \n \t\tpnum = pfn_to_section_nr(pfn);\n \t\tns = psize / section_size;\n@@ -850,28 +841,36 @@ void __init hugetlb_vmemmap_init_late(int nid)\n \t\th = m->hstate;\n \t\tpfn = PHYS_PFN(phys);\n \t\tnr_pages = pages_per_huge_page(h);\n+\t\tmap = pfn_to_page(pfn);\n+\t\tstart = (unsigned long)map;\n+\t\tend = start + nr_pages * sizeof(struct page);\n \n \t\tif (!hugetlb_bootmem_page_zones_valid(nid, m)) {\n \t\t\t/*\n \t\t\t * Oops, the hugetlb page spans multiple zones.\n-\t\t\t * Remove it from the list, and undo HVO.\n+\t\t\t * Remove it from the list, and populate it normally.\n \t\t\t */\n \t\t\tlist_del(&m->list);\n \n-\t\t\tmap = pfn_to_page(pfn);\n-\n-\t\t\tstart = (unsigned long)map;\n-\t\t\tend = start + nr_pages * sizeof(struct page);\n-\n-\t\t\tvmemmap_undo_hvo(start, end, nid,\n-\t\t\t\t\t HUGETLB_VMEMMAP_RESERVE_SIZE);\n-\t\t\tnr_mmap = end - start - HUGETLB_VMEMMAP_RESERVE_SIZE;\n+\t\t\tvmemmap_populate(start, end, nid, NULL);\n+\t\t\tnr_mmap = end - start;\n \t\t\tmemmap_boot_pages_add(DIV_ROUND_UP(nr_mmap, PAGE_SIZE));\n \n \t\t\tmemblock_phys_free(phys, huge_page_size(h));\n \t\t\tcontinue;\n-\t\t} else\n+\t\t}\n+\n+\t\tif (vmemmap_populate_hvo(start, end, nid,\n+\t\t\t\t\t HUGETLB_VMEMMAP_RESERVE_SIZE) < 0) {\n+\t\t\t/* Fallback if HVO population fails */\n+\t\t\tvmemmap_populate(start, end, nid, NULL);\n+\t\t\tnr_mmap = end - start;\n+\t\t} else {\n \t\t\tm->flags |= HUGE_BOOTMEM_ZONES_VALID;\n+\t\t\tnr_mmap = HUGETLB_VMEMMAP_RESERVE_SIZE;\n+\t\t}\n+\n+\t\tmemmap_boot_pages_add(DIV_ROUND_UP(nr_mmap, PAGE_SIZE));\n \t}\n }\n #endif\ndiff --git a/mm/sparse-vmemmap.c b/mm/sparse-vmemmap.c\nindex 37522d6cb398..032a81450838 100644\n--- a/mm/sparse-vmemmap.c\n+++ b/mm/sparse-vmemmap.c\n@@ -302,59 +302,6 @@ int __meminit vmemmap_populate_basepages(unsigned long start, unsigned long end,\n \treturn vmemmap_populate_range(start, end, node, altmap, -1, 0);\n }\n \n-/*\n- * Undo populate_hvo, and replace it with a normal base page mapping.\n- * Used in memory init in case a HVO mapping needs to be undone.\n- *\n- * This can happen when it is discovered that a memblock allocated\n- * hugetlb page spans multiple zones, which can only be verified\n- * after zones have been initialized.\n- *\n- * We know that:\n- * 1) The first @headsize / PAGE_SIZE vmemmap pages were individually\n- *    allocated through memblock, and mapped.\n- *\n- * 2) The rest of the vmemmap pages are mirrors of the last head page.\n- */\n-int __meminit vmemmap_undo_hvo(unsigned long addr, unsigned long end,\n-\t\t\t\t      int node, unsigned long headsize)\n-{\n-\tunsigned long maddr, pfn;\n-\tpte_t *pte;\n-\tint headpages;\n-\n-\t/*\n-\t * Should only be called early in boot, so nothing will\n-\t * be accessing these page structures.\n-\t */\n-\tWARN_ON(!early_boot_irqs_disabled);\n-\n-\theadpages = headsize >> PAGE_SHIFT;\n-\n-\t/*\n-\t * Clear mirrored mappings for tail page structs.\n-\t */\n-\tfor (maddr = addr + headsize; maddr < end; maddr += PAGE_SIZE) {\n-\t\tpte = virt_to_kpte(maddr);\n-\t\tpte_clear(&init_mm, maddr, pte);\n-\t}\n-\n-\t/*\n-\t * Clear and free mappings for head page and first tail page\n-\t * structs.\n-\t */\n-\tfor (maddr = addr; headpages-- > 0; maddr += PAGE_SIZE) {\n-\t\tpte = virt_to_kpte(maddr);\n-\t\tpfn = pte_pfn(ptep_get(pte));\n-\t\tpte_clear(&init_mm, maddr, pte);\n-\t\tmemblock_phys_free(PFN_PHYS(pfn), PAGE_SIZE);\n-\t}\n-\n-\tflush_tlb_kernel_range(addr, end);\n-\n-\treturn vmemmap_populate(addr, end, node, NULL);\n-}\n-\n /*\n  * Write protect the mirrored tail page structs for HVO. This will be\n  * called from the hugetlb code when gathering and initializing the\n-- \n2.51.2",
          "reply_to": "",
          "message_date": "2026-02-27",
          "message_id": ""
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author addressed a concern about zone information being correct for shared tail pages, explaining that in v7, these pages are allocated per-zone and the vmemmap population is deferred until zones are initialized.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "This series removes \"fake head pages\" from the HugeTLB vmemmap\noptimization (HVO) by changing how tail pages encode their relationship\nto the head page.\n\nIt simplifies compound_head() and page_ref_add_unless(). Both are in the\nhot path.\n\nBackground\n==========\n\nHVO reduces memory overhead by freeing vmemmap pages for HugeTLB pages\nand remapping the freed virtual addresses to a single physical page.\nPreviously, all tail page vmemmap entries were remapped to the first\nvmemmap page (containing the head struct page), creating \"fake heads\" -\ntail pages that appear to have PG_head set when accessed through the\ndeduplicated vmemmap.\n\nThis required special handling in compound_head() to detect and work\naround fake heads, adding complexity and overhead to a very hot path.\n\nNew Approach\n============\n\nFor architectures/configs where sizeof(struct page) is a power of 2 (the\ncommon case), this series changes how position of the head page is encoded\nin the tail pages.\n\nInstead of storing a pointer to the head page, the ->compound_info\n(renamed from ->compound_head) now stores a mask.\n\nThe mask can be applied to any tail page's virtual address to compute\nthe head page address. Critically, all tail pages of the same order now\nhave identical compound_info values, regardless of which compound page\nthey belong to.\n\nThe key insight is that all tail pages of the same order now have\nidentical compound_info values, regardless of which compound page they\nbelong to.\n\nIn v7, these shared tail pages are allocated per-zone. This ensures \nthat zone information (stored in page->flags) is correct even for \nshared tail pages, removing the need for the special-casing in \npage_zonenum() proposed in earlier versions.\n\nTo support per-zone shared pages for boot-allocated gigantic pages, \nthe vmemmap population is deferred until zones are initialized. This \nsimplifies the logic significantly and allows the removal of \nvmemmap_undo_hvo().\n\nBenefits\n========\n\n1. Simplified compound_head(): No fake head detection needed, can be\n   implemented in a branchless manner.\n\n2. Simplified page_ref_add_unless(): RCU protection removed since there's\n   no race with fake head remapping.\n\n3. Cleaner architecture: The shared tail pages are truly read-only and\n   contain valid tail page metadata.\n\nIf sizeof(struct page) is not power-of-2, there are no functional changes.\nHVO is not supported in this configuration.\n\nI had hoped to see performance improvement, but my testing thus far has\nshown either no change or only a slight improvement within the noise.\n\nSeries Organization\n===================\n\nPatch 1: Move MAX_FOLIO_ORDER definition to mmzone.h.\nPatches 2-4: Refactoring of field names and interfaces.\nPatches 5-6: Architecture alignment for LoongArch and RISC-V.\nPatch 7: Mask-based compound_head() implementation.\nPatch 8: Add memmap alignment checks.\nPatch 9: Branchless compound_head() optimization.\nPatch 10: Defer vmemmap population for bootmem hugepages.\nPatch 11: Refactor vmemmap_walk.\nPatch 12: x86 vDSO build fix.\nPatch 13: Eliminate fake heads with per-zone shared tail pages.\nPatches 14-16: Cleanup of fake head infrastructure.\nPatch 17: Documentation update.\nPatch 18: Use compound_head() in page_slab().\n\nChanges in v7:\n==============\n\n  - Move vmemmap_tails from per-node to per-zone. This ensures tail\n    pages have correct zone information.\n\n  - Defer vmemmap population for boot-allocated huge pages to \n    hugetlb_vmemmap_init_late(). This makes zone information available \n    during population and allows removing vmemmap_undo_hvo().\n\n  - Undefine CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP for x86 vdso32 to \n    fix build issues.\n\n  - Remove the patch that modified page_zonenum(), as per-zone \n    shared pages make it unnecessary.\n\nChanges in v6:\n==============\n  - Simplify memmap alignment check in mm/sparse.c: use VM_BUG_ON()\n    (Muchun)\n\n  - Store struct page pointers in vmemmap_tails[] instead of PFNs.\n    (Muchun)\n\n  - Fix build error on powerpc due to negative NR_VMEMMAP_TAILS.\n\nChanges in v5:\n==============\n  - Rebased to mm-everything-2026-01-27-04-35\n\n  - Add arch-specific patches to align vmemmap to maximal folio size\n    for riscv and LoongArch architectures.\n\n  - Strengthen the memmap alignment check in mm/sparse.c: use BUG()\n    for CONFIG_DEBUG_VM, WARN() otherwise. (Muchun)\n\n  - Use cmpxchg() instead of hugetlb_lock to update vmemmap_tails\n    array. (Muchun)\n\n  - Update page_slab().\n\nChanges in v4:\n==============\n  - Fix build issues due to linux/mmzone.h <-> linux/pgtable.h\n    dependency loop by avoiding including linux/pgtable.h into\n    linux/mmzone.h\n\n  - Rework vmemmap_remap_alloc() interface. (Muchun)\n\n  - Use &folio->page instead of folio address for optimization\n    target. (Muchun)\n\nChanges in v3:\n==============\n  - Fixed error recovery path in vmemmap_remap_free() to pass correct start\n    address for TLB flush. (Muchun)\n\n  - Wrapped the mask-based compound_info encoding within CONFIG_SPARSEMEM_VMEMMAP\n    check via compound_info_has_mask(). For other memory models, alignment\n    guarantees are harder to verify. (Muchun)\n\n  - Updated vmemmap_dedup.rst documentation wording: changed \"vmemmap_tail\n    shared for the struct hstate\" to \"A single, per-node page frame shared\n    among all hugepages of the same size\". (Muchun)\n\n  - Fixed build error with MAX_FOLIO_ORDER expanding to undefined PUD_ORDER\n    in certain configurations. (kernel test robot)\n\nChanges in v2:\n==============\n\n- Handle boot-allocated huge pages correctly. (Frank)\n\n- Changed from per-hstate vmemmap_tail to per-node vmemmap_tails[] array\n  in pglist_data. (Muchun)\n\n- Added spin_lock(&hugetlb_lock) protection in vmemmap_get_tail() to fix\n  a race condition where two threads could both allocate tail pages.\n  The losing thread now properly frees its allocated page. (Usama)\n\n- Add warning if memmap is not aligned to MAX_FOLIO_SIZE, which is\n  required for the mask approach. (Muchun)\n\n- Make page_zonenum() use head page - correctness fix since shared\n  tail pages cannot have valid zone information. (Muchun)\n\n- Added 'const' qualifier to head parameter in set_compound_head() and\n  prep_compound_tail(). (Usama)\n\n- Updated commit messages.\n\nKiryl Shutsemau (16):\n  mm: Move MAX_FOLIO_ORDER definition to mmzone.h\n  mm: Change the interface of prep_compound_tail()\n  mm: Rename the 'compound_head' field in the 'struct page' to\n    'compound_info'\n  mm: Move set/clear_compound_head() next to compound_head()\n  riscv/mm: Align vmemmap to maximal folio size\n  LoongArch/mm: Align vmemmap to maximal folio size\n  mm: Rework compound_head() for power-of-2 sizeof(struct page)\n  mm/sparse: Check memmap alignment for compound_info_has_mask()\n  mm/hugetlb: Refactor code around vmemmap_walk\n  mm/hugetlb: Remove fake head pages\n  mm: Drop fake head checks\n  hugetlb: Remove VMEMMAP_SYNCHRONIZE_RCU\n  mm/hugetlb: Remove hugetlb_optimize_vmemmap_key static key\n  mm: Remove the branch from compound_head()\n  hugetlb: Update vmemmap_dedup.rst\n  mm/slab: Use compound_head() in page_slab()\n\nKiryl Shutsemau (Meta) (2):\n  mm/hugetlb: Defer vmemmap population for bootmem hugepages\n  x86/vdso: Undefine CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP for vdso32\n\n .../admin-guide/kdump/vmcoreinfo.rst          |   2 +-\n Documentation/mm/vmemmap_dedup.rst            |  62 ++-\n arch/loongarch/include/asm/pgtable.h          |   3 +-\n arch/riscv/mm/init.c                          |   3 +-\n arch/x86/entry/vdso/vdso32/fake_32bit_build.h |   1 +\n include/linux/mm.h                            |  36 +-\n include/linux/mm_types.h                      |  20 +-\n include/linux/mmzone.h                        |  57 +++\n include/linux/page-flags.h                    | 166 ++++----\n include/linux/page_ref.h                      |   8 +-\n include/linux/types.h                         |   2 +-\n kernel/vmcore_info.c                          |   2 +-\n mm/hugetlb.c                                  |   8 +-\n mm/hugetlb_vmemmap.c                          | 362 +++++++++---------\n mm/internal.h                                 |  18 +-\n mm/mm_init.c                                  |   2 +-\n mm/page_alloc.c                               |   4 +-\n mm/slab.h                                     |   8 +-\n mm/sparse-vmemmap.c                           | 110 +++---\n mm/sparse.c                                   |   5 +\n mm/util.c                                     |  16 +-\n 21 files changed, 448 insertions(+), 447 deletions(-)\n\n-- \n2.51.2\n\n---\n\nFrom: Kiryl Shutsemau <kas@kernel.org>\n\nInstead of passing down the head page and tail page index, pass the tail\nand head pages directly, as well as the order of the compound page.\n\nThis is a preparation for changing how the head position is encoded in\nthe tail page.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\nAcked-by: David Hildenbrand (arm) <david@kernel.org>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n---\n include/linux/page-flags.h |  5 +++--\n mm/hugetlb.c               |  8 +++++---\n mm/internal.h              | 11 +++++------\n mm/mm_init.c               |  2 +-\n mm/page_alloc.c            |  2 +-\n 5 files changed, 15 insertions(+), 13 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex f7a0e4af0c73..5e7687ccccf8 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -865,9 +865,10 @@ static inline bool folio_test_large(const struct folio *folio)\n \treturn folio_test_head(folio);\n }\n \n-static __always_inline void set_compound_head(struct page *page, struct page *head)\n+static __always_inline void set_compound_head(struct page *tail,\n+\t\tconst struct page *head, unsigned int order)\n {\n-\tWRITE_ONCE(page->compound_head, (unsigned long)head + 1);\n+\tWRITE_ONCE(tail->compound_head, (unsigned long)head + 1);\n }\n \n static __always_inline void clear_compound_head(struct page *page)\ndiff --git a/mm/hugetlb.c b/mm/hugetlb.c\nindex 0beb6e22bc26..fc55f22c9e41 100644\n--- a/mm/hugetlb.c\n+++ b/mm/hugetlb.c\n@@ -3168,6 +3168,7 @@ int __alloc_bootmem_huge_page(struct hstate *h, int nid)\n \n /* Initialize [start_page:end_page_number] tail struct pages of a hugepage */\n static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n+\t\t\t\t\tstruct hstate *h,\n \t\t\t\t\tunsigned long start_page_number,\n \t\t\t\t\tunsigned long end_page_number)\n {\n@@ -3176,6 +3177,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n \tstruct page *page = folio_page(folio, start_page_number);\n \tunsigned long head_pfn = folio_pfn(folio);\n \tunsigned long pfn, end_pfn = head_pfn + end_page_number;\n+\tunsigned int order = huge_page_order(h);\n \n \t/*\n \t * As we marked all tail pages with memblock_reserved_mark_noinit(),\n@@ -3183,7 +3185,7 @@ static void __init hugetlb_folio_init_tail_vmemmap(struct folio *folio,\n \t */\n \tfor (pfn = head_pfn + start_page_number; pfn < end_pfn; page++, pfn++) {\n \t\t__init_single_page(page, pfn, zone, nid);\n-\t\tprep_compound_tail((struct page *)folio, pfn - head_pfn);\n+\t\tprep_compound_tail(page, &folio->page, order);\n \t\tset_page_count(page, 0);\n \t}\n }\n@@ -3203,7 +3205,7 @@ static void __init hugetlb_folio_init_vmemmap(struct folio *folio,\n \t__folio_set_head(folio);\n \tret = folio_ref_freeze(folio, 1);\n \tVM_BUG_ON(!ret);\n-\thugetlb_folio_init_tail_vmemmap(folio, 1, nr_pages);\n+\thugetlb_folio_init_tail_vmemmap(folio, h, 1, nr_pages);\n \tprep_compound_head(&folio->page, huge_page_order(h));\n }\n \n@@ -3260,7 +3262,7 @@ static void __init prep_and_add_bootmem_folios(struct hstate *h,\n \t\t\t * time as this is early in boot and there should\n \t\t\t * be no contention.\n \t\t\t */\n-\t\t\thugetlb_folio_init_tail_vmemmap(folio,\n+\t\t\thugetlb_folio_init_tail_vmemmap(folio, h,\n \t\t\t\t\tHUGETLB_VMEMMAP_RESERVE_PAGES,\n \t\t\t\t\tpages_per_huge_page(h));\n \t\t}\ndiff --git a/mm/internal.h b/mm/internal.h\nindex cb0af847d7d9..c76122f22294 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -878,13 +878,12 @@ static inline void prep_compound_head(struct page *page, unsigned int order)\n \t\tINIT_LIST_HEAD(&folio->_deferred_list);\n }\n \n-static inline void prep_compound_tail(struct page *head, int tail_idx)\n+static inline void prep_compound_tail(struct page *tail,\n+\t\tconst struct page *head, unsigned int order)\n {\n-\tstruct page *p = head + tail_idx;\n-\n-\tp->mapping = TAIL_MAPPING;\n-\tset_compound_head(p, head);\n-\tset_page_private(p, 0);\n+\ttail->mapping = TAIL_MAPPING;\n+\tset_compound_head(tail, head, order);\n+\tset_page_private(tail, 0);\n }\n \n void post_alloc_hook(struct page *page, unsigned int order, gfp_t gfp_flags);\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex 61d983d23f55..0a12a9be0bcc 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -1099,7 +1099,7 @@ static void __ref memmap_init_compound(struct page *head,\n \t\tstruct page *page = pfn_to_page(pfn);\n \n \t\t__init_zone_device_page(page, pfn, zone_idx, nid, pgmap);\n-\t\tprep_compound_tail(head, pfn - head_pfn);\n+\t\tprep_compound_tail(page, head, order);\n \t\tset_page_count(page, 0);\n \t}\n \tprep_compound_head(head, order);\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex fcc32737f451..aa657e4a99e8 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -744,7 +744,7 @@ void prep_compound_page(struct page *page, unsigned int order)\n \n \t__SetPageHead(page);\n \tfor (i = 1; i < nr_pages; i++)\n-\t\tprep_compound_tail(page, i);\n+\t\tprep_compound_tail(page + i, page, order);\n \n \tprep_compound_head(page, order);\n }\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-27",
          "message_id": ""
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author addressed a concern about the naming of the 'compound_head' field in struct page, explaining that it encodes whether the page is a tail and where to locate the head page. The author renamed the field to 'compound_info', which will be used for the new mask-based encoding.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Kiryl Shutsemau <kas@kernel.org>\n\nThe 'compound_head' field in the 'struct page' encodes whether the page\nis a tail and where to locate the head page. Bit 0 is set if the page is\na tail, and the remaining bits in the field point to the head page.\n\nAs preparation for changing how the field encodes information about the\nhead page, rename the field to 'compound_info'.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\nAcked-by: David Hildenbrand (arm) <david@kernel.org>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n---\n .../admin-guide/kdump/vmcoreinfo.rst          |  2 +-\n Documentation/mm/vmemmap_dedup.rst            |  6 +++---\n include/linux/mm_types.h                      | 20 +++++++++----------\n include/linux/page-flags.h                    | 18 ++++++++---------\n include/linux/types.h                         |  2 +-\n kernel/vmcore_info.c                          |  2 +-\n mm/page_alloc.c                               |  2 +-\n mm/slab.h                                     |  2 +-\n mm/util.c                                     |  2 +-\n 9 files changed, 28 insertions(+), 28 deletions(-)\n\ndiff --git a/Documentation/admin-guide/kdump/vmcoreinfo.rst b/Documentation/admin-guide/kdump/vmcoreinfo.rst\nindex 404a15f6782c..7663c610fe90 100644\n--- a/Documentation/admin-guide/kdump/vmcoreinfo.rst\n+++ b/Documentation/admin-guide/kdump/vmcoreinfo.rst\n@@ -141,7 +141,7 @@ nodemask_t\n The size of a nodemask_t type. Used to compute the number of online\n nodes.\n \n-(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_head)\n+(page, flags|_refcount|mapping|lru|_mapcount|private|compound_order|compound_info)\n ----------------------------------------------------------------------------------\n \n User-space tools compute their values based on the offset of these\ndiff --git a/Documentation/mm/vmemmap_dedup.rst b/Documentation/mm/vmemmap_dedup.rst\nindex b4a55b6569fa..1863d88d2dcb 100644\n--- a/Documentation/mm/vmemmap_dedup.rst\n+++ b/Documentation/mm/vmemmap_dedup.rst\n@@ -24,7 +24,7 @@ For each base page, there is a corresponding ``struct page``.\n Within the HugeTLB subsystem, only the first 4 ``struct page`` are used to\n contain unique information about a HugeTLB page. ``__NR_USED_SUBPAGE`` provides\n this upper limit. The only 'useful' information in the remaining ``struct page``\n-is the compound_head field, and this field is the same for all tail pages.\n+is the compound_info field, and this field is the same for all tail pages.\n \n By removing redundant ``struct page`` for HugeTLB pages, memory can be returned\n to the buddy allocator for other uses.\n@@ -124,10 +124,10 @@ Here is how things look before optimization::\n  |           |\n  +-----------+\n \n-The value of page->compound_head is the same for all tail pages. The first\n+The value of page->compound_info is the same for all tail pages. The first\n page of ``struct page`` (page 0) associated with the HugeTLB page contains the 4\n ``struct page`` necessary to describe the HugeTLB. The only use of the remaining\n-pages of ``struct page`` (page 1 to page 7) is to point to page->compound_head.\n+pages of ``struct page`` (page 1 to page 7) is to point to page->compound_info.\n Therefore, we can remap pages 1 to 7 to page 0. Only 1 page of ``struct page``\n will be used for each HugeTLB page. This will allow us to free the remaining\n 7 pages to the buddy allocator.\ndiff --git a/include/linux/mm_types.h b/include/linux/mm_types.h\nindex 3cc8ae722886..7bc82a2b889f 100644\n--- a/include/linux/mm_types.h\n+++ b/include/linux/mm_types.h\n@@ -126,14 +126,14 @@ struct page {\n \t\t\tatomic_long_t pp_ref_count;\n \t\t};\n \t\tstruct {\t/* Tail pages of compound page */\n-\t\t\tunsigned long compound_head;\t/* Bit zero is set */\n+\t\t\tunsigned long compound_info;\t/* Bit zero is set */\n \t\t};\n \t\tstruct {\t/* ZONE_DEVICE pages */\n \t\t\t/*\n-\t\t\t * The first word is used for compound_head or folio\n+\t\t\t * The first word is used for compound_info or folio\n \t\t\t * pgmap\n \t\t\t */\n-\t\t\tvoid *_unused_pgmap_compound_head;\n+\t\t\tvoid *_unused_pgmap_compound_info;\n \t\t\tvoid *zone_device_data;\n \t\t\t/*\n \t\t\t * ZONE_DEVICE private pages are counted as being\n@@ -409,7 +409,7 @@ struct folio {\n \t/* private: avoid cluttering the output */\n \t\t\t\t/* For the Unevictable \"LRU list\" slot */\n \t\t\t\tstruct {\n-\t\t\t\t\t/* Avoid compound_head */\n+\t\t\t\t\t/* Avoid compound_info */\n \t\t\t\t\tvoid *__filler;\n \t/* public: */\n \t\t\t\t\tunsigned int mlock_count;\n@@ -510,7 +510,7 @@ struct folio {\n FOLIO_MATCH(flags, flags);\n FOLIO_MATCH(lru, lru);\n FOLIO_MATCH(mapping, mapping);\n-FOLIO_MATCH(compound_head, lru);\n+FOLIO_MATCH(compound_info, lru);\n FOLIO_MATCH(__folio_index, index);\n FOLIO_MATCH(private, private);\n FOLIO_MATCH(_mapcount, _mapcount);\n@@ -529,7 +529,7 @@ FOLIO_MATCH(_last_cpupid, _last_cpupid);\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + sizeof(struct page))\n FOLIO_MATCH(flags, _flags_1);\n-FOLIO_MATCH(compound_head, _head_1);\n+FOLIO_MATCH(compound_info, _head_1);\n FOLIO_MATCH(_mapcount, _mapcount_1);\n FOLIO_MATCH(_refcount, _refcount_1);\n #undef FOLIO_MATCH\n@@ -537,13 +537,13 @@ FOLIO_MATCH(_refcount, _refcount_1);\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + 2 * sizeof(struct page))\n FOLIO_MATCH(flags, _flags_2);\n-FOLIO_MATCH(compound_head, _head_2);\n+FOLIO_MATCH(compound_info, _head_2);\n #undef FOLIO_MATCH\n #define FOLIO_MATCH(pg, fl)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n \t\t\toffsetof(struct page, pg) + 3 * sizeof(struct page))\n FOLIO_MATCH(flags, _flags_3);\n-FOLIO_MATCH(compound_head, _head_3);\n+FOLIO_MATCH(compound_info, _head_3);\n #undef FOLIO_MATCH\n \n /**\n@@ -609,8 +609,8 @@ struct ptdesc {\n #define TABLE_MATCH(pg, pt)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct page, pg) == offsetof(struct ptdesc, pt))\n TABLE_MATCH(flags, pt_flags);\n-TABLE_MATCH(compound_head, pt_list);\n-TABLE_MATCH(compound_head, _pt_pad_1);\n+TABLE_MATCH(compound_info, pt_list);\n+TABLE_MATCH(compound_info, _pt_pad_1);\n TABLE_MATCH(mapping, __page_mapping);\n TABLE_MATCH(__folio_index, pt_index);\n TABLE_MATCH(rcu_head, pt_rcu_head);\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex 5e7687ccccf8..70c4e43f2d9a 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -213,7 +213,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t/*\n \t * Only addresses aligned with PAGE_SIZE of struct page may be fake head\n \t * struct page. The alignment check aims to avoid access the fields (\n-\t * e.g. compound_head) of the @page[1]. It can avoid touch a (possibly)\n+\t * e.g. compound_info) of the @page[1]. It can avoid touch a (possibly)\n \t * cold cacheline in some cases.\n \t */\n \tif (IS_ALIGNED((unsigned long)page, PAGE_SIZE) &&\n@@ -223,7 +223,7 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t\t * because the @page is a compound page composed with at least\n \t\t * two contiguous pages.\n \t\t */\n-\t\tunsigned long head = READ_ONCE(page[1].compound_head);\n+\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n \n \t\tif (likely(head & 1))\n \t\t\treturn (const struct page *)(head - 1);\n@@ -281,7 +281,7 @@ static __always_inline int page_is_fake_head(const struct page *page)\n \n static __always_inline unsigned long _compound_head(const struct page *page)\n {\n-\tunsigned long head = READ_ONCE(page->compound_head);\n+\tunsigned long head = READ_ONCE(page->compound_info);\n \n \tif (unlikely(head & 1))\n \t\treturn head - 1;\n@@ -320,13 +320,13 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n \n static __always_inline int PageTail(const struct page *page)\n {\n-\treturn READ_ONCE(page->compound_head) & 1 || page_is_fake_head(page);\n+\treturn READ_ONCE(page->compound_info) & 1 || page_is_fake_head(page);\n }\n \n static __always_inline int PageCompound(const struct page *page)\n {\n \treturn test_bit(PG_head, &page->flags.f) ||\n-\t       READ_ONCE(page->compound_head) & 1;\n+\t       READ_ONCE(page->compound_info) & 1;\n }\n \n #define\tPAGE_POISON_PATTERN\t-1l\n@@ -348,7 +348,7 @@ static const unsigned long *const_folio_flags(const struct folio *folio,\n {\n \tconst struct page *page = &folio->page;\n \n-\tVM_BUG_ON_PGFLAGS(page->compound_head & 1, page);\n+\tVM_BUG_ON_PGFLAGS(page->compound_info & 1, page);\n \tVM_BUG_ON_PGFLAGS(n > 0 && !test_bit(PG_head, &page->flags.f), page);\n \treturn &page[n].flags.f;\n }\n@@ -357,7 +357,7 @@ static unsigned long *folio_flags(struct folio *folio, unsigned n)\n {\n \tstruct page *page = &folio->page;\n \n-\tVM_BUG_ON_PGFLAGS(page->compound_head & 1, page);\n+\tVM_BUG_ON_PGFLAGS(page->compound_info & 1, page);\n \tVM_BUG_ON_PGFLAGS(n > 0 && !test_bit(PG_head, &page->flags.f), page);\n \treturn &page[n].flags.f;\n }\n@@ -868,12 +868,12 @@ static inline bool folio_test_large(const struct folio *folio)\n static __always_inline void set_compound_head(struct page *tail,\n \t\tconst struct page *head, unsigned int order)\n {\n-\tWRITE_ONCE(tail->compound_head, (unsigned long)head + 1);\n+\tWRITE_ONCE(tail->compound_info, (unsigned long)head + 1);\n }\n \n static __always_inline void clear_compound_head(struct page *page)\n {\n-\tWRITE_ONCE(page->compound_head, 0);\n+\tWRITE_ONCE(page->compound_info, 0);\n }\n \n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\ndiff --git a/include/linux/types.h b/include/linux/types.h\nindex 7e71d260763c..608050dbca6a 100644\n--- a/include/linux/types.h\n+++ b/include/linux/types.h\n@@ -239,7 +239,7 @@ struct ustat {\n  *\n  * This guarantee is important for few reasons:\n  *  - future call_rcu_lazy() will make use of lower bits in the pointer;\n- *  - the structure shares storage space in struct page with @compound_head,\n+ *  - the structure shares storage space in struct page with @compound_info,\n  *    which encode PageTail() in bit 0. The guarantee is needed to avoid\n  *    false-positive PageTail().\n  */\ndiff --git a/kernel/vmcore_info.c b/kernel/vmcore_info.c\nindex 8d82913223a1..94e4ef75b1b2 100644\n--- a/kernel/vmcore_info.c\n+++ b/kernel/vmcore_info.c\n@@ -198,7 +198,7 @@ static int __init crash_save_vmcoreinfo_init(void)\n \tVMCOREINFO_OFFSET(page, lru);\n \tVMCOREINFO_OFFSET(page, _mapcount);\n \tVMCOREINFO_OFFSET(page, private);\n-\tVMCOREINFO_OFFSET(page, compound_head);\n+\tVMCOREINFO_OFFSET(page, compound_info);\n \tVMCOREINFO_OFFSET(pglist_data, node_zones);\n \tVMCOREINFO_OFFSET(pglist_data, nr_zones);\n #ifdef CONFIG_FLATMEM\ndiff --git a/mm/page_alloc.c b/mm/page_alloc.c\nindex aa657e4a99e8..e83f67fbbf07 100644\n--- a/mm/page_alloc.c\n+++ b/mm/page_alloc.c\n@@ -731,7 +731,7 @@ static inline bool pcp_allowed_order(unsigned int order)\n  * The first PAGE_SIZE page is called the \"head page\" and have PG_head set.\n  *\n  * The remaining PAGE_SIZE pages are called \"tail pages\". PageTail() is encoded\n- * in bit 0 of page->compound_head. The rest of bits is pointer to head page.\n+ * in bit 0 of page->compound_info. The rest of bits is pointer to head page.\n  *\n  * The first tail page's ->compound_order holds the order of allocation.\n  * This usage means that zero-order pages may not be compound.\ndiff --git a/mm/slab.h b/mm/slab.h\nindex 71c7261bf822..62dfa50c1f01 100644\n--- a/mm/slab.h\n+++ b/mm/slab.h\n@@ -94,7 +94,7 @@ struct slab {\n #define SLAB_MATCH(pg, sl)\t\t\t\t\t\t\\\n \tstatic_assert(offsetof(struct page, pg) == offsetof(struct slab, sl))\n SLAB_MATCH(flags, flags);\n-SLAB_MATCH(compound_head, slab_cache);\t/* Ensure bit 0 is clear */\n+SLAB_MATCH(compound_info, slab_cache);\t/* Ensure bit 0 is clear */\n SLAB_MATCH(_refcount, __page_refcount);\n #ifdef CONFIG_MEMCG\n SLAB_MATCH(memcg_data, obj_exts);\ndiff --git a/mm/util.c b/mm/util.c\nindex b05ab6f97e11..3ebcb9e6035c 100644\n--- a/mm/util.c\n+++ b/mm/util.c\n@@ -1247,7 +1247,7 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n again:\n \tmemset(&ps->folio_snapshot, 0, sizeof(struct folio));\n \tmemcpy(&ps->page_snapshot, page, sizeof(*page));\n-\thead = ps->page_snapshot.compound_head;\n+\thead = ps->page_snapshot.compound_info;\n \tif ((head & 1) == 0) {\n \t\tps->idx = 0;\n \t\tfoliop = (struct folio *)&ps->page_snapshot;\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nFrom: Kiryl Shutsemau <kas@kernel.org>\n\nMove set_compound_head() and clear_compound_head() to be adjacent to the\ncompound_head() function in page-flags.h.\n\nThese functions encode and decode the same compound_info field, so\nkeeping them together makes it easier to verify their logic is\nconsistent, especially when the encoding changes.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\nAcked-by: David Hildenbrand (arm) <david@kernel.org>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n---\n include/linux/page-flags.h | 22 +++++++++++-----------\n 1 file changed, 11 insertions(+), 11 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex 70c4e43f2d9a..42bf8ed02a29 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -290,6 +290,17 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n \n #define compound_head(page)\t((typeof(page))_compound_head(page))\n \n+static __always_inline void set_compound_head(struct page *tail,\n+\t\tconst struct page *head, unsigned int order)\n+{\n+\tWRITE_ONCE(tail->compound_info, (unsigned long)head + 1);\n+}\n+\n+static __always_inline void clear_compound_head(struct page *page)\n+{\n+\tWRITE_ONCE(page->compound_info, 0);\n+}\n+\n /**\n  * page_folio - Converts from page to folio.\n  * @p: The page.\n@@ -865,17 +876,6 @@ static inline bool folio_test_large(const struct folio *folio)\n \treturn folio_test_head(folio);\n }\n \n-static __always_inline void set_compound_head(struct page *tail,\n-\t\tconst struct page *head, unsigned int order)\n-{\n-\tWRITE_ONCE(tail->compound_info, (unsigned long)head + 1);\n-}\n-\n-static __always_inline void clear_compound_head(struct page *page)\n-{\n-\tWRITE_ONCE(page->compound_info, 0);\n-}\n-\n #ifdef CONFIG_TRANSPARENT_HUGEPAGE\n static inline void ClearPageCompound(struct page *page)\n {\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nFrom: Kiryl Shutsemau <kas@kernel.org>\n\nThe upcoming change to the HugeTLB vmemmap optimization (HVO) requires\nstruct pages of the head page to be naturally aligned with regard to the\nfolio size.\n\nAlign vmemmap to the newly introduced MAX_FOLIO_VMEMMAP_ALIGN.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\n---\n arch/riscv/mm/init.c   |  3 ++-\n include/linux/mmzone.h | 11 +++++++++++\n 2 files changed, 13 insertions(+), 1 deletion(-)\n\ndiff --git a/arch/riscv/mm/init.c b/arch/riscv/mm/init.c\nindex 811e03786c56..e8fb2239a0b5 100644\n--- a/arch/riscv/mm/init.c\n+++ b/arch/riscv/mm/init.c\n@@ -63,7 +63,8 @@ phys_addr_t phys_ram_base __ro_after_init;\n EXPORT_SYMBOL(phys_ram_base);\n \n #ifdef CONFIG_SPARSEMEM_VMEMMAP\n-#define VMEMMAP_ADDR_ALIGN\t(1ULL << SECTION_SIZE_BITS)\n+#define VMEMMAP_ADDR_ALIGN\tmax(1ULL << SECTION_SIZE_BITS, \\\n+\t\t\t\t    MAX_FOLIO_VMEMMAP_ALIGN)\n \n unsigned long vmemmap_start_pfn __ro_after_init;\n EXPORT_SYMBOL(vmemmap_start_pfn);\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex be8ce40b5638..492a5be1090f 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -92,6 +92,17 @@\n \n #define MAX_FOLIO_NR_PAGES\t(1UL << MAX_FOLIO_ORDER)\n \n+/*\n+ * HugeTLB Vmemmap Optimization (HVO) requires struct pages of the head page to\n+ * be naturally aligned with regard to the folio size.\n+ *\n+ * HVO which is only active if the size of struct page is a power of 2.\n+ */\n+#define MAX_FOLIO_VMEMMAP_ALIGN \\\n+\t(IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP) && \\\n+\t is_power_of_2(sizeof(struct page)) ? \\\n+\t MAX_FOLIO_NR_PAGES * sizeof(struct page) : 0)\n+\n enum migratetype {\n \tMIGRATE_UNMOVABLE,\n \tMIGRATE_MOVABLE,\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nFrom: Kiryl Shutsemau <kas@kernel.org>\n\nThe upcoming change to the HugeTLB vmemmap optimization (HVO) requires\nstruct pages of the head page to be naturally aligned with regard to the\nfolio size.\n\nAlign vmemmap to MAX_FOLIO_VMEMMAP_ALIGN.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\n---\n arch/loongarch/include/asm/pgtable.h | 3 ++-\n 1 file changed, 2 insertions(+), 1 deletion(-)\n\ndiff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h\nindex c33b3bcb733e..ea6c09eed2e7 100644\n--- a/arch/loongarch/include/asm/pgtable.h\n+++ b/arch/loongarch/include/asm/pgtable.h\n@@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];\n \t min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)\n #endif\n \n-#define vmemmap\t\t((struct page *)((VMALLOC_END + PMD_SIZE) & PMD_MASK))\n+#define VMEMMAP_ALIGN\tmax(PMD_SIZE, MAX_FOLIO_VMEMMAP_ALIGN)\n+#define vmemmap\t\t((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))\n #define VMEMMAP_END\t((unsigned long)vmemmap + VMEMMAP_SIZE - 1)\n \n #define KFENCE_AREA_START\t(VMEMMAP_END + 1)\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nFrom: Kiryl Shutsemau <kas@kernel.org>\n\nMove MAX_FOLIO_ORDER definition from mm.h to mmzone.h.\n\nThis is preparation for adding the vmemmap_tails array to struct\nzone, which requires MAX_FOLIO_ORDER to be available in mmzone.h.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nAcked-by: David Hildenbrand (Red Hat) <david@kernel.org>\nAcked-by: Zi Yan <ziy@nvidia.com>\nAcked-by: Muchun Song <muchun.song@linux.dev>\nAcked-by: Usama Arif <usamaarif642@gmail.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n---\n include/linux/mm.h     | 31 -------------------------------\n include/linux/mmzone.h | 31 +++++++++++++++++++++++++++++++\n 2 files changed, 31 insertions(+), 31 deletions(-)\n\ndiff --git a/include/linux/mm.h b/include/linux/mm.h\nindex 5be3d8a8f806..7f4dbbb9d783 100644\n--- a/include/linux/mm.h\n+++ b/include/linux/mm.h\n@@ -27,7 +27,6 @@\n #include <linux/page-flags.h>\n #include <linux/page_ref.h>\n #include <linux/overflow.h>\n-#include <linux/sizes.h>\n #include <linux/sched.h>\n #include <linux/pgtable.h>\n #include <linux/kasan.h>\n@@ -2479,36 +2478,6 @@ static inline unsigned long folio_nr_pages(const struct folio *folio)\n \treturn folio_large_nr_pages(folio);\n }\n \n-#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)\n-/*\n- * We don't expect any folios that exceed buddy sizes (and consequently\n- * memory sections).\n- */\n-#define MAX_FOLIO_ORDER\t\tMAX_PAGE_ORDER\n-#elif defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\n-/*\n- * Only pages within a single memory section are guaranteed to be\n- * contiguous. By limiting folios to a single memory section, all folio\n- * pages are guaranteed to be contiguous.\n- */\n-#define MAX_FOLIO_ORDER\t\tPFN_SECTION_SHIFT\n-#elif defined(CONFIG_HUGETLB_PAGE)\n-/*\n- * There is no real limit on the folio size. We limit them to the maximum we\n- * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect\n- * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.\n- */\n-#define MAX_FOLIO_ORDER\t\tget_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)\n-#else\n-/*\n- * Without hugetlb, gigantic folios that are bigger than a single PUD are\n- * currently impossible.\n- */\n-#define MAX_FOLIO_ORDER\t\tPUD_ORDER\n-#endif\n-\n-#define MAX_FOLIO_NR_PAGES\t(1UL << MAX_FOLIO_ORDER)\n-\n /*\n  * compound_nr() returns the number of pages in this potentially compound\n  * page.  compound_nr() can be called on a tail page, and is defined to\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 3e51190a55e4..be8ce40b5638 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -23,6 +23,7 @@\n #include <linux/page-flags.h>\n #include <linux/local_lock.h>\n #include <linux/zswap.h>\n+#include <linux/sizes.h>\n #include <asm/page.h>\n \n /* Free memory management - zoned buddy allocator.  */\n@@ -61,6 +62,36 @@\n  */\n #define PAGE_ALLOC_COSTLY_ORDER 3\n \n+#if !defined(CONFIG_HAVE_GIGANTIC_FOLIOS)\n+/*\n+ * We don't expect any folios that exceed buddy sizes (and consequently\n+ * memory sections).\n+ */\n+#define MAX_FOLIO_ORDER\t\tMAX_PAGE_ORDER\n+#elif defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\n+/*\n+ * Only pages within a single memory section are guaranteed to be\n+ * contiguous. By limiting folios to a single memory section, all folio\n+ * pages are guaranteed to be contiguous.\n+ */\n+#define MAX_FOLIO_ORDER\t\tPFN_SECTION_SHIFT\n+#elif defined(CONFIG_HUGETLB_PAGE)\n+/*\n+ * There is no real limit on the folio size. We limit them to the maximum we\n+ * currently expect (see CONFIG_HAVE_GIGANTIC_FOLIOS): with hugetlb, we expect\n+ * no folios larger than 16 GiB on 64bit and 1 GiB on 32bit.\n+ */\n+#define MAX_FOLIO_ORDER\t\tget_order(IS_ENABLED(CONFIG_64BIT) ? SZ_16G : SZ_1G)\n+#else\n+/*\n+ * Without hugetlb, gigantic folios that are bigger than a single PUD are\n+ * currently impossible.\n+ */\n+#define MAX_FOLIO_ORDER\t\tPUD_ORDER\n+#endif\n+\n+#define MAX_FOLIO_NR_PAGES\t(1UL << MAX_FOLIO_ORDER)\n+\n enum migratetype {\n \tMIGRATE_UNMOVABLE,\n \tMIGRATE_MOVABLE,\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-27",
          "message_id": ""
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author addressed a concern about the mask-based encoding of compound_info for tail pages, explaining that it only works when sizeof(struct page) is power-of-2 and struct pages are naturally aligned for all orders up to MAX_FOLIO_ORDER. The author acknowledged that this approach would work in more conditions but requires additional validation.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "acknowledged a limitation",
            "explained the condition under which the mask-based encoding works"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Kiryl Shutsemau <kas@kernel.org>\n\nFor tail pages, the kernel uses the 'compound_info' field to get to the\nhead page. The bit 0 of the field indicates whether the page is a\ntail page, and if set, the remaining bits represent a pointer to the\nhead page.\n\nFor cases when size of struct page is power-of-2, change the encoding of\ncompound_info to store a mask that can be applied to the virtual address\nof the tail page in order to access the head page. It is possible\nbecause struct page of the head page is naturally aligned with regards\nto order of the page.\n\nThe significant impact of this modification is that all tail pages of\nthe same order will now have identical 'compound_info', regardless of\nthe compound page they are associated with. This paves the way for\neliminating fake heads.\n\nThe HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only\napplied when the sizeof(struct page) is power-of-2. Having identical\ntail pages allows the same page to be mapped into the vmemmap of all\npages, maintaining memory savings without fake heads.\n\nIf sizeof(struct page) is not power-of-2, there is no functional\nchanges.\n\nLimit mask usage to HugeTLB vmemmap optimization (HVO) where it makes\na difference. The approach with mask would work in the wider set of\nconditions, but it requires validating that struct pages are naturally\naligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nReviewed-by: Muchun Song <muchun.song@linux.dev>\nReviewed-by: Zi Yan <ziy@nvidia.com>\nAcked-by: David Hildenbrand (Arm) <david@kernel.org>\nAcked-by: Usama Arif <usamaarif642@gmail.com>\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n---\n include/linux/page-flags.h | 81 ++++++++++++++++++++++++++++++++++----\n mm/slab.h                  | 16 ++++++--\n mm/util.c                  | 16 ++++++--\n 3 files changed, 97 insertions(+), 16 deletions(-)\n\ndiff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\nindex 42bf8ed02a29..01970bd38bff 100644\n--- a/include/linux/page-flags.h\n+++ b/include/linux/page-flags.h\n@@ -198,6 +198,29 @@ enum pageflags {\n \n #ifndef __GENERATING_BOUNDS_H\n \n+/*\n+ * For tail pages, if the size of struct page is power-of-2 ->compound_info\n+ * encodes the mask that converts the address of the tail page address to\n+ * the head page address.\n+ *\n+ * Otherwise, ->compound_info has direct pointer to head pages.\n+ */\n+static __always_inline bool compound_info_has_mask(void)\n+{\n+\t/*\n+\t * Limit mask usage to HugeTLB vmemmap optimization (HVO) where it\n+\t * makes a difference.\n+\t *\n+\t * The approach with mask would work in the wider set of conditions,\n+\t * but it requires validating that struct pages are naturally aligned\n+\t * for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n+\t */\n+\tif (!IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP))\n+\t\treturn false;\n+\n+\treturn is_power_of_2(sizeof(struct page));\n+}\n+\n #ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP\n DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\n \n@@ -207,6 +230,10 @@ DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\n  */\n static __always_inline const struct page *page_fixed_fake_head(const struct page *page)\n {\n+\t/* Fake heads only exists if compound_info_has_mask() is true */\n+\tif (!compound_info_has_mask())\n+\t\treturn page;\n+\n \tif (!static_branch_unlikely(&hugetlb_optimize_vmemmap_key))\n \t\treturn page;\n \n@@ -223,10 +250,14 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n \t\t * because the @page is a compound page composed with at least\n \t\t * two contiguous pages.\n \t\t */\n-\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n+\t\tunsigned long info = READ_ONCE(page[1].compound_info);\n \n-\t\tif (likely(head & 1))\n-\t\t\treturn (const struct page *)(head - 1);\n+\t\t/* See set_compound_head() */\n+\t\tif (likely(info & 1)) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\n+\t\t\treturn (const struct page *)(p & info);\n+\t\t}\n \t}\n \treturn page;\n }\n@@ -281,11 +312,26 @@ static __always_inline int page_is_fake_head(const struct page *page)\n \n static __always_inline unsigned long _compound_head(const struct page *page)\n {\n-\tunsigned long head = READ_ONCE(page->compound_info);\n+\tunsigned long info = READ_ONCE(page->compound_info);\n \n-\tif (unlikely(head & 1))\n-\t\treturn head - 1;\n-\treturn (unsigned long)page_fixed_fake_head(page);\n+\t/* Bit 0 encodes PageTail() */\n+\tif (!(info & 1))\n+\t\treturn (unsigned long)page_fixed_fake_head(page);\n+\n+\t/*\n+\t * If compound_info_has_mask() is false, the rest of compound_info is\n+\t * the pointer to the head page.\n+\t */\n+\tif (!compound_info_has_mask())\n+\t\treturn info - 1;\n+\n+\t/*\n+\t * If compound_info_has_mask() is true the rest of the info encodes\n+\t * the mask that converts the address of the tail page to the head page.\n+\t *\n+\t * No need to clear bit 0 in the mask as 'page' always has it clear.\n+\t */\n+\treturn (unsigned long)page & info;\n }\n \n #define compound_head(page)\t((typeof(page))_compound_head(page))\n@@ -293,7 +339,26 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n static __always_inline void set_compound_head(struct page *tail,\n \t\tconst struct page *head, unsigned int order)\n {\n-\tWRITE_ONCE(tail->compound_info, (unsigned long)head + 1);\n+\tunsigned int shift;\n+\tunsigned long mask;\n+\n+\tif (!compound_info_has_mask()) {\n+\t\tWRITE_ONCE(tail->compound_info, (unsigned long)head | 1);\n+\t\treturn;\n+\t}\n+\n+\t/*\n+\t * If the size of struct page is power-of-2, bits [shift:0] of the\n+\t * virtual address of compound head are zero.\n+\t *\n+\t * Calculate mask that can be applied to the virtual address of\n+\t * the tail page to get address of the head page.\n+\t */\n+\tshift = order + order_base_2(sizeof(struct page));\n+\tmask = GENMASK(BITS_PER_LONG - 1, shift);\n+\n+\t/* Bit 0 encodes PageTail() */\n+\tWRITE_ONCE(tail->compound_info, mask | 1);\n }\n \n static __always_inline void clear_compound_head(struct page *page)\ndiff --git a/mm/slab.h b/mm/slab.h\nindex 62dfa50c1f01..1a1b3758df05 100644\n--- a/mm/slab.h\n+++ b/mm/slab.h\n@@ -131,11 +131,19 @@ static_assert(IS_ALIGNED(offsetof(struct slab, freelist), sizeof(struct freelist\n  */\n static inline struct slab *page_slab(const struct page *page)\n {\n-\tunsigned long head;\n+\tunsigned long info;\n+\n+\tinfo = READ_ONCE(page->compound_info);\n+\tif (info & 1) {\n+\t\t/* See compound_head() */\n+\t\tif (compound_info_has_mask()) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\t\t\tpage = (struct page *)(p & info);\n+\t\t} else {\n+\t\t\tpage = (struct page *)(info - 1);\n+\t\t}\n+\t}\n \n-\thead = READ_ONCE(page->compound_head);\n-\tif (head & 1)\n-\t\tpage = (struct page *)(head - 1);\n \tif (data_race(page->page_type >> 24) != PGTY_slab)\n \t\tpage = NULL;\n \ndiff --git a/mm/util.c b/mm/util.c\nindex 3ebcb9e6035c..20dccf2881d7 100644\n--- a/mm/util.c\n+++ b/mm/util.c\n@@ -1237,7 +1237,7 @@ static void set_ps_flags(struct page_snapshot *ps, const struct folio *folio,\n  */\n void snapshot_page(struct page_snapshot *ps, const struct page *page)\n {\n-\tunsigned long head, nr_pages = 1;\n+\tunsigned long info, nr_pages = 1;\n \tstruct folio *foliop;\n \tint loops = 5;\n \n@@ -1247,8 +1247,8 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n again:\n \tmemset(&ps->folio_snapshot, 0, sizeof(struct folio));\n \tmemcpy(&ps->page_snapshot, page, sizeof(*page));\n-\thead = ps->page_snapshot.compound_info;\n-\tif ((head & 1) == 0) {\n+\tinfo = ps->page_snapshot.compound_info;\n+\tif (!(info & 1)) {\n \t\tps->idx = 0;\n \t\tfoliop = (struct folio *)&ps->page_snapshot;\n \t\tif (!folio_test_large(foliop)) {\n@@ -1259,7 +1259,15 @@ void snapshot_page(struct page_snapshot *ps, const struct page *page)\n \t\t}\n \t\tfoliop = (struct folio *)page;\n \t} else {\n-\t\tfoliop = (struct folio *)(head - 1);\n+\t\t/* See compound_head() */\n+\t\tif (compound_info_has_mask()) {\n+\t\t\tunsigned long p = (unsigned long)page;\n+\n+\t\t\tfoliop = (struct folio *)(p & info);\n+\t\t} else {\n+\t\t\tfoliop = (struct folio *)(info - 1);\n+\t\t}\n+\n \t\tps->idx = folio_page_idx(foliop, page);\n \t}\n \n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nFrom: Kiryl Shutsemau <kas@kernel.org>\n\nIf page->compound_info encodes a mask, it is expected that vmemmap to be\nnaturally aligned to the maximum folio size.\n\nAdd a VM_WARN_ON_ONCE() to check the alignment.\n\nSigned-off-by: Kiryl Shutsemau <kas@kernel.org>\nAcked-by: Zi Yan <ziy@nvidia.com>\n---\n mm/sparse.c | 5 +++++\n 1 file changed, 5 insertions(+)\n\ndiff --git a/mm/sparse.c b/mm/sparse.c\nindex b5b2b6f7041b..dfabe554adf8 100644\n--- a/mm/sparse.c\n+++ b/mm/sparse.c\n@@ -600,6 +600,11 @@ void __init sparse_init(void)\n \tBUILD_BUG_ON(!is_power_of_2(sizeof(struct mem_section)));\n \tmemblocks_present();\n \n+\tif (compound_info_has_mask()) {\n+\t\tVM_WARN_ON_ONCE(!IS_ALIGNED((unsigned long) pfn_to_page(0),\n+\t\t\t\t    MAX_FOLIO_VMEMMAP_ALIGN));\n+\t}\n+\n \tpnum_begin = first_present_section_nr();\n \tnid_begin = sparse_early_nid(__nr_to_section(pnum_begin));\n \n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nCurrently, the vmemmap for bootmem-allocated gigantic pages is populated\nearly in hugetlb_vmemmap_init_early(). However, the zone information is\nonly available after zones are initialized. If it is later discovered\nthat a page spans multiple zones, the HVO mapping must be undone and\nreplaced with a normal mapping using vmemmap_undo_hvo().\n\nDefer the actual vmemmap population to hugetlb_vmemmap_init_late(). At\nthis stage, zones are already initialized, so it can be checked if the\npage is valid for HVO before deciding how to populate the vmemmap.\n\nThis allows us to remove vmemmap_undo_hvo() and the complex logic\nrequired to rollback HVO mappings.\n\nIn hugetlb_vmemmap_init_late(), if HVO population fails or if the zones\nare invalid, fall back to a normal vmemmap population.\n\nPostponing population until hugetlb_vmemmap_init_late() also makes zone\ninformation available from within vmemmap_populate_hvo().\n\nSigned-off-by: Kiryl Shutsemau (Meta) <kas@kernel.org>\n---\n include/linux/mm.h   |  2 --\n mm/hugetlb_vmemmap.c | 37 +++++++++++++++----------------\n mm/sparse-vmemmap.c  | 53 --------------------------------------------\n 3 files changed, 18 insertions(+), 74 deletions(-)\n\ndiff --git a/include/linux/mm.h b/include/linux/mm.h\nindex 7f4dbbb9d783..0e2d45008ff4 100644\n--- a/include/linux/mm.h\n+++ b/include/linux/mm.h\n@@ -4484,8 +4484,6 @@ int vmemmap_populate(unsigned long start, unsigned long end, int node,\n \t\tstruct vmem_altmap *altmap);\n int vmemmap_populate_hvo(unsigned long start, unsigned long end, int node,\n \t\t\t unsigned long headsize);\n-int vmemmap_undo_hvo(unsigned long start, unsigned long end, int node,\n-\t\t     unsigned long headsize);\n void vmemmap_wrprotect_hvo(unsigned long start, unsigned long end, int node,\n \t\t\t  unsigned long headsize);\n void vmemmap_populate_print_last(void);\ndiff --git a/mm/hugetlb_vmemmap.c b/mm/hugetlb_vmemmap.c\nindex a9280259e12a..935ec5829be9 100644\n--- a/mm/hugetlb_vmemmap.c\n+++ b/mm/hugetlb_vmemmap.c\n@@ -790,7 +790,6 @@ void __init hugetlb_vmemmap_init_early(int nid)\n {\n \tunsigned long psize, paddr, section_size;\n \tunsigned long ns, i, pnum, pfn, nr_pages;\n-\tunsigned long start, end;\n \tstruct huge_bootmem_page *m = NULL;\n \tvoid *map;\n \n@@ -808,14 +807,6 @@ void __init hugetlb_vmemmap_init_early(int nid)\n \t\tpaddr = virt_to_phys(m);\n \t\tpfn = PHYS_PFN(paddr);\n \t\tmap = pfn_to_page(pfn);\n-\t\tstart = (unsigned long)map;\n-\t\tend = start + nr_pages * sizeof(struct page);\n-\n-\t\tif (vmemmap_populate_hvo(start, end, nid,\n-\t\t\t\t\tHUGETLB_VMEMMAP_RESERVE_SIZE) < 0)\n-\t\t\tcontinue;\n-\n-\t\tmemmap_boot_pages_add(HUGETLB_VMEMMAP_RESERVE_SIZE / PAGE_SIZE);\n \n \t\tpnum = pfn_to_section_nr(pfn);\n \t\tns = psize / section_size;\n@@ -850,28 +841,36 @@ void __init hugetlb_vmemmap_init_late(int nid)\n \t\th = m->hstate;\n \t\tpfn = PHYS_PFN(phys);\n \t\tnr_pages = pages_per_huge_page(h);\n+\t\tmap = pfn_to_page(pfn);\n+\t\tstart = (unsigned long)map;\n+\t\tend = start + nr_pages * sizeof(struct page);\n \n \t\tif (!hugetlb_bootmem_page_zones_valid(nid, m)) {\n \t\t\t/*\n \t\t\t * Oops, the hugetlb page spans multiple zones.\n-\t\t\t * Remove it from the list, and undo HVO.\n+\t\t\t * Remove it from the list, and populate it normally.\n \t\t\t */\n \t\t\tlist_del(&m->list);\n \n-\t\t\tmap = pfn_to_page(pfn);\n-\n-\t\t\tstart = (unsigned long)map;\n-\t\t\tend = start + nr_pages * sizeof(struct page);\n-\n-\t\t\tvmemmap_undo_hvo(start, end, nid,\n-\t\t\t\t\t HUGETLB_VMEMMAP_RESERVE_SIZE);\n-\t\t\tnr_mmap = end - start - HUGETLB_VMEMMAP_RESERVE_SIZE;\n+\t\t\tvmemmap_populate(start, end, nid, NULL);\n+\t\t\tnr_mmap = end - start;\n \t\t\tmemmap_boot_pages_add(DIV_ROUND_UP(nr_mmap, PAGE_SIZE));\n \n \t\t\tmemblock_phys_free(phys, huge_page_size(h));\n \t\t\tcontinue;\n-\t\t} else\n+\t\t}\n+\n+\t\tif (vmemmap_populate_hvo(start, end, nid,\n+\t\t\t\t\t HUGETLB_VMEMMAP_RESERVE_SIZE) < 0) {\n+\t\t\t/* Fallback if HVO population fails */\n+\t\t\tvmemmap_populate(start, end, nid, NULL);\n+\t\t\tnr_mmap = end - start;\n+\t\t} else {\n \t\t\tm->flags |= HUGE_BOOTMEM_ZONES_VALID;\n+\t\t\tnr_mmap = HUGETLB_VMEMMAP_RESERVE_SIZE;\n+\t\t}\n+\n+\t\tmemmap_boot_pages_add(DIV_ROUND_UP(nr_mmap, PAGE_SIZE));\n \t}\n }\n #endif\ndiff --git a/mm/sparse-vmemmap.c b/mm/sparse-vmemmap.c\nindex 37522d6cb398..032a81450838 100644\n--- a/mm/sparse-vmemmap.c\n+++ b/mm/sparse-vmemmap.c\n@@ -302,59 +302,6 @@ int __meminit vmemmap_populate_basepages(unsigned long start, unsigned long end,\n \treturn vmemmap_populate_range(start, end, node, altmap, -1, 0);\n }\n \n-/*\n- * Undo populate_hvo, and replace it with a normal base page mapping.\n- * Used in memory init in case a HVO mapping needs to be undone.\n- *\n- * This can happen when it is discovered that a memblock allocated\n- * hugetlb page spans multiple zones, which can only be verified\n- * after zones have been initialized.\n- *\n- * We know that:\n- * 1) The first @headsize / PAGE_SIZE vmemmap pages were individually\n- *    allocated through memblock, and mapped.\n- *\n- * 2) The rest of the vmemmap pages are mirrors of the last head page.\n- */\n-int __meminit vmemmap_undo_hvo(unsigned long addr, unsigned long end,\n-\t\t\t\t      int node, unsigned long headsize)\n-{\n-\tunsigned long maddr, pfn;\n-\tpte_t *pte;\n-\tint headpages;\n-\n-\t/*\n-\t * Should only be called early in boot, so nothing will\n-\t * be accessing these page structures.\n-\t */\n-\tWARN_ON(!early_boot_irqs_disabled);\n-\n-\theadpages = headsize >> PAGE_SHIFT;\n-\n-\t/*\n-\t * Clear mirrored mappings for tail page structs.\n-\t */\n-\tfor (maddr = addr + headsize; maddr < end; maddr += PAGE_SIZE) {\n-\t\tpte = virt_to_kpte(maddr);\n-\t\tpte_clear(&init_mm, maddr, pte);\n-\t}\n-\n-\t/*\n-\t * Clear and free mappings for head page and first tail page\n-\t * structs.\n-\t */\n-\tfor (maddr = addr; headpages-- > 0; maddr += PAGE_SIZE) {\n-\t\tpte = virt_to_kpte(maddr);\n-\t\tpfn = pte_pfn(ptep_get(pte));\n-\t\tpte_clear(&init_mm, maddr, pte);\n-\t\tmemblock_phys_free(PFN_PHYS(pfn), PAGE_SIZE);\n-\t}\n-\n-\tflush_tlb_kernel_range(addr, end);\n-\n-\treturn vmemmap_populate(addr, end, node, NULL);\n-}\n-\n /*\n  * Write protect the mirrored tail page structs for HVO. This will be\n  * called from the hugetlb code when gathering and initializing the\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nThis series removes \"fake head pages\" from the HugeTLB vmemmap\noptimization (HVO) by changing how tail pages encode their relationship\nto the head page.\n\nIt simplifies compound_head() and page_ref_add_unless(). Both are in the\nhot path.\n\nBackground\n==========\n\nHVO reduces memory overhead by freeing vmemmap pages for HugeTLB pages\nand remapping the freed virtual addresses to a single physical page.\nPreviously, all tail page vmemmap entries were remapped to the first\nvmemmap page (containing the head struct page), creating \"fake heads\" -\ntail pages that appear to have PG_head set when accessed through the\ndeduplicated vmemmap.\n\nThis required special handling in compound_head() to detect and work\naround fake heads, adding complexity and overhead to a very hot path.\n\nNew Approach\n============\n\nFor architectures/configs where sizeof(struct page) is a power of 2 (the\ncommon case), this series changes how position of the head page is encoded\nin the tail pages.\n\nInstead of storing a pointer to the head page, the ->compound_info\n(renamed from ->compound_head) now stores a mask.\n\nThe mask can be applied to any tail page's virtual address to compute\nthe head page address. Critically, all tail pages of the same order now\nhave identical compound_info values, regardless of which compound page\nthey belong to.\n\nThe key insight is that all tail pages of the same order now have\nidentical compound_info values, regardless of which compound page they\nbelong to.\n\nIn v7, these shared tail pages are allocated per-zone. This ensures \nthat zone information (stored in page->flags) is correct even for \nshared tail pages, removing the need for the special-casing in \npage_zonenum() proposed in earlier versions.\n\nTo support per-zone shared pages for boot-allocated gigantic pages, \nthe vmemmap population is deferred until zones are initialized. This \nsimplifies the logic significantly and allows the removal of \nvmemmap_undo_hvo().\n\nBenefits\n========\n\n1. Simplified compound_head(): No fake head detection needed, can be\n   implemented in a branchless manner.\n\n2. Simplified page_ref_add_unless(): RCU protection removed since there's\n   no race with fake head remapping.\n\n3. Cleaner architecture: The shared tail pages are truly read-only and\n   contain valid tail page metadata.\n\nIf sizeof(struct page) is not power-of-2, there are no functional changes.\nHVO is not supported in this configuration.\n\nI had hoped to see performance improvement, but my testing thus far has\nshown either no change or only a slight improvement within the noise.\n\nSeries Organization\n===================\n\nPatch 1: Move MAX_FOLIO_ORDER definition to mmzone.h.\nPatches 2-4: Refactoring of field names and interfaces.\nPatches 5-6: Architecture alignment for LoongArch and RISC-V.\nPatch 7: Mask-based compound_head() implementation.\nPatch 8: Add memmap alignment checks.\nPatch 9: Branchless compound_head() optimization.\nPatch 10: Defer vmemmap population for bootmem hugepages.\nPatch 11: Refactor vmemmap_walk.\nPatch 12: x86 vDSO build fix.\nPatch 13: Eliminate fake heads with per-zone shared tail pages.\nPatches 14-16: Cleanup of fake head infrastructure.\nPatch 17: Documentation update.\nPatch 18: Use compound_head() in page_slab().\n\nChanges in v7:\n==============\n\n  - Move vmemmap_tails from per-node to per-zone. This ensures tail\n    pages have correct zone information.\n\n  - Defer vmemmap population for boot-allocated huge pages to \n    hugetlb_vmemmap_init_late(). This makes zone information available \n    during population and allows removing vmemmap_undo_hvo().\n\n  - Undefine CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP for x86 vdso32 to \n    fix build issues.\n\n  - Remove the patch that modified page_zonenum(), as per-zone \n    shared pages make it unnecessary.\n\nChanges in v6:\n==============\n  - Simplify memmap alignment check in mm/sparse.c: use VM_BUG_ON()\n    (Muchun)\n\n  - Store struct page pointers in vmemmap_tails[] instead of PFNs.\n    (Muchun)\n\n  - Fix build error on powerpc due to negative NR_VMEMMAP_TAILS.\n\nChanges in v5:\n==============\n  - Rebased to mm-everything-2026-01-27-04-35\n\n  - Add arch-specific patches to align vmemmap to maximal folio size\n    for riscv and LoongArch architectures.\n\n  - Strengthen the memmap alignment check in mm/sparse.c: use BUG()\n    for CONFIG_DEBUG_VM, WARN() otherwise. (Muchun)\n\n  - Use cmpxchg() instead of hugetlb_lock to update vmemmap_tails\n    array. (Muchun)\n\n  - Update page_slab().\n\nChanges in v4:\n==============\n  - Fix build issues due to linux/mmzone.h <-> linux/pgtable.h\n    dependency loop by avoiding including linux/pgtable.h into\n    linux/mmzone.h\n\n  - Rework vmemmap_remap_alloc() interface. (Muchun)\n\n  - Use &folio->page instead of folio address for optimization\n    target. (Muchun)\n\nChanges in v3:\n==============\n  - Fixed error recovery path in vmemmap_remap_free() to pass correct start\n    address for TLB flush. (Muchun)\n\n  - Wrapped the mask-based compound_info encoding within CONFIG_SPARSEMEM_VMEMMAP\n    check via compound_info_has_mask(). For other memory models, alignment\n    guarantees are harder to verify. (Muchun)\n\n  - Updated vmemmap_dedup.rst documentation wording: changed \"vmemmap_tail\n    shared for the struct hstate\" to \"A single, per-node page frame shared\n    among all hugepages of the same size\". (Muchun)\n\n  - Fixed build error with MAX_FOLIO_ORDER expanding to undefined PUD_ORDER\n    in certain configurations. (kernel test robot)\n\nChanges in v2:\n==============\n\n- Handle boot-allocated huge pages correctly. (Frank)\n\n- Changed from per-hstate vmemmap_tail to per-node vmemmap_tails[] array\n  in pglist_data. (Muchun)\n\n- Added spin_lock(&hugetlb_lock) protection in vmemmap_get_tail() to fix\n  a race condition where two threads could both allocate tail pages.\n  The losing thread now properly frees its allocated page. (Usama)\n\n- Add warning if memmap is not aligned to MAX_FOLIO_SIZE, which is\n  required for the mask approach. (Muchun)\n\n- Make page_zonenum() use head page - correctness fix since shared\n  tail pages cannot have valid zone information. (Muchun)\n\n- Added 'const' qualifier to head parameter in set_compound_head() and\n  prep_compound_tail(). (Usama)\n\n- Updated commit messages.\n\nKiryl Shutsemau (16):\n  mm: Move MAX_FOLIO_ORDER definition to mmzone.h\n  mm: Change the interface of prep_compound_tail()\n  mm: Rename the 'compound_head' field in the 'struct page' to\n    'compound_info'\n  mm: Move set/clear_compound_head() next to compound_head()\n  riscv/mm: Align vmemmap to maximal folio size\n  LoongArch/mm: Align vmemmap to maximal folio size\n  mm: Rework compound_head() for power-of-2 sizeof(struct page)\n  mm/sparse: Check memmap alignment for compound_info_has_mask()\n  mm/hugetlb: Refactor code around vmemmap_walk\n  mm/hugetlb: Remove fake head pages\n  mm: Drop fake head checks\n  hugetlb: Remove VMEMMAP_SYNCHRONIZE_RCU\n  mm/hugetlb: Remove hugetlb_optimize_vmemmap_key static key\n  mm: Remove the branch from compound_head()\n  hugetlb: Update vmemmap_dedup.rst\n  mm/slab: Use compound_head() in page_slab()\n\nKiryl Shutsemau (Meta) (2):\n  mm/hugetlb: Defer vmemmap population for bootmem hugepages\n  x86/vdso: Undefine CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP for vdso32\n\n .../admin-guide/kdump/vmcoreinfo.rst          |   2 +-\n Documentation/mm/vmemmap_dedup.rst            |  62 ++-\n arch/loongarch/include/asm/pgtable.h          |   3 +-\n arch/riscv/mm/init.c                          |   3 +-\n arch/x86/entry/vdso/vdso32/fake_32bit_build.h |   1 +\n include/linux/mm.h                            |  36 +-\n include/linux/mm_types.h                      |  20 +-\n include/linux/mmzone.h                        |  57 +++\n include/linux/page-flags.h                    | 166 ++++----\n include/linux/page_ref.h                      |   8 +-\n include/linux/types.h                         |   2 +-\n kernel/vmcore_info.c                          |   2 +-\n mm/hugetlb.c                                  |   8 +-\n mm/hugetlb_vmemmap.c                          | 362 +++++++++---------\n mm/internal.h                                 |  18 +-\n mm/mm_init.c                                  |   2 +-\n mm/page_alloc.c                               |   4 +-\n mm/slab.h                                     |   8 +-\n mm/sparse-vmemmap.c                           | 110 +++---\n mm/sparse.c                                   |   5 +\n mm/util.c                                     |  16 +-\n 21 files changed, 448 insertions(+), 447 deletions(-)\n\n-- \n2.51.2\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-27",
          "message_id": ""
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author is apologizing for a mistake in the submission thread, stating that they will resend the patches properly and acknowledging their own error.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "apology",
            "acknowledgment of error"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "I've screwed up threading in the submission, please ignore.\nI will resend properly.\n\nSorry for this mess. I should have figured out mail by now :/\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n---\n\nI've screwed up threading in the submission, please ignore.\nI will resend properly.\n\nSorry for this mess. I should have figured out mail by now :/\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-27",
          "message_id": ""
        }
      ],
      "analysis_source": "llm",
      "patch_summary": "This patch series removes 'fake head pages' from the HugeTLB vmemmap optimization by changing how tail pages encode their relationship to the head page, simplifying compound_head() and page_ref_add_unless(). The new approach uses a mask-based encoding for architectures where sizeof(struct page) is a power of 2, allowing shared read-only tail pages across huge pages on a NUMA node. This reduces complexity and overhead in the hot path, but testing has shown either no change or only slight performance improvement."
    },
    "2026-02-03": {
      "report_file": "2026-02-27_ollama_llama3.1-8b.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "Muchun Song",
          "summary": "Reviewer Muchun Song pointed out that the shared tail pages will have incorrect zone information because huge pages on the same node may belong to different zones, and suggested always fetching zone information from the head page.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "> On Feb 2, 2026, at 23:56, Kiryl Shutsemau <kas@kernel.org> wrote:\n> \n> If page->compound_info encodes a mask, it is expected that vmemmap to be\n> naturally aligned to the maximum folio size.\n> \n> Add a VM_BUG_ON() to check the alignment.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Acked-by: Zi Yan <ziy@nvidia.com>\n\nReviewed-by: Muchun Song <muchun.song@linux.dev>\n\n---\n\n> On Feb 2, 2026, at 23:56, Kiryl Shutsemau <kas@kernel.org> wrote:\n> \n> If page->compound_info encodes a mask, it is expected that vmemmap to be\n> naturally aligned to the maximum folio size.\n> \n> Add a VM_BUG_ON() to check the alignment.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Acked-by: Zi Yan <ziy@nvidia.com>\n\nReviewed-by: Muchun Song <muchun.song@linux.dev>\n\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\n> On Feb 2, 2026, at 23:56, Kiryl Shutsemau <kas@kernel.org> wrote:\n> \n> With the upcoming changes to HVO, a single page of tail struct pages\n> will be shared across all huge pages of the same order on a node. Since\n> huge pages on the same node may belong to different zones, the zone\n> information stored in shared tail page flags would be incorrect.\n> \n> Always fetch zone information from the head page, which has unique and\n> correct zone flags for each compound page.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Acked-by: Zi Yan <ziy@nvidia.com>\n\nAcked-by: Muchun Song <muchun.song@linux.dev>\n\n---\n\n> On Feb 2, 2026, at 23:56, Kiryl Shutsemau <kas@kernel.org> wrote:\n> \n> With the upcoming changes to HVO, a single page of tail struct pages\n> will be shared across all huge pages of the same order on a node. Since\n> huge pages on the same node may belong to different zones, the zone\n> information stored in shared tail page flags would be incorrect.\n> \n> Always fetch zone information from the head page, which has unique and\n> correct zone flags for each compound page.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Acked-by: Zi Yan <ziy@nvidia.com>\n\nAcked-by: Muchun Song <muchun.song@linux.dev>\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-03",
          "message_id": ""
        }
      ],
      "analysis_source": "llm",
      "patch_summary": "This patch series removes 'fake head pages' from the HugeTLB vmemmap optimization by changing how tail pages encode their relationship to the head page, simplifying compound_head() and page_ref_add_unless(). The new approach uses a mask-based encoding for architectures where sizeof(struct page) is a power of 2, allowing shared read-only tail pages across huge pages on a NUMA node. This reduces complexity and overhead in the hot path, but testing has shown either no change or only slight performance improvement."
    },
    "2026-02-04": {
      "report_file": "2026-02-27_ollama_llama3.1-8b.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "David (arm)",
          "summary": "Reviewer David (arm) suggested two minor code style improvements: adding two tab indents to the parameter list in set_compound_head() and renaming a function parameter from 'page' to 'tail' for consistency.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "LGTM",
            "Only nits"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Acked-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> Instead of passing down the head page and tail page index, pass the tail\n> and head pages directly, as well as the order of the compound page.\n> \n> This is a preparation for changing how the head position is encoded in\n> the tail page.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n> ---\n>   include/linux/page-flags.h |  4 +++-\n>   mm/hugetlb.c               |  8 +++++---\n>   mm/internal.h              | 12 ++++++------\n>   mm/mm_init.c               |  2 +-\n>   mm/page_alloc.c            |  2 +-\n>   5 files changed, 16 insertions(+), 12 deletions(-)\n> \n> diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\n> index f7a0e4af0c73..8a3694369e15 100644\n> --- a/include/linux/page-flags.h\n> +++ b/include/linux/page-flags.h\n> @@ -865,7 +865,9 @@ static inline bool folio_test_large(const struct folio *folio)\n>   \treturn folio_test_head(folio);\n>   }\n>   \n> -static __always_inline void set_compound_head(struct page *page, struct page *head)\n> +static __always_inline void set_compound_head(struct page *page,\n> +\t\t\t\t\t      const struct page *head,\n> +\t\t\t\t\t      unsigned int order)\n\nTwo tab indents please on second+ parameter list whenever you touch code.\n\n>   {\n>   \tWRITE_ONCE(page->compound_head, (unsigned long)head + 1);\n>   }\n> diff --git a/mm/hugetlb.c b/mm/hugetlb.c\n> index 6e855a32de3d..54ba7cd05a86 100644\n\n\n[...]\n\n> diff --git a/mm/internal.h b/mm/internal.h\n> index d67e8bb75734..037ddcda25ff 100644\n> --- a/mm/internal.h\n> +++ b/mm/internal.h\n> @@ -879,13 +879,13 @@ static inline void prep_compound_head(struct page *page, unsigned int order)\n>   \t\tINIT_LIST_HEAD(&folio->_deferred_list);\n>   }\n>   \n> -static inline void prep_compound_tail(struct page *head, int tail_idx)\n> +static inline void prep_compound_tail(struct page *tail,\n\nJust wondering whether we should call this \"struct page *page\" for \nconsistency with set_compound_head().\n\nOr alternatively, call it also \"tail\" in set_compound_head().\n\n> +\t\t\t\t      const struct page *head,\n> +\t\t\t\t      unsigned int order)\n\nTwo tab indent, then this fits into two lines in total.\n\n>   {\n> -\tstruct page *p = head + tail_idx;\n> -\n> -\tp->mapping = TAIL_MAPPING;\n> -\tset_compound_head(p, head);\n> -\tset_page_private(p, 0);\n> +\ttail->mapping = TAIL_MAPPING;\n> +\tset_compound_head(tail, head, order);\n> +\tset_page_private(tail, 0);\n>   }\nOnly nits, in general LGTM\n\nAcked-by: David Hildenbrand (arm) <david@kernel.org>\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> The 'compound_head' field in the 'struct page' encodes whether the page\n> is a tail and where to locate the head page. Bit 0 is set if the page is\n> a tail, and the remaining bits in the field point to the head page.\n> \n> As preparation for changing how the field encodes information about the\n> head page, rename the field to 'compound_info'.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n> ---\n\nAcked-by: David Hildenbrand (arm) <david@kernel.org>\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> Instead of passing down the head page and tail page index, pass the tail\n> and head pages directly, as well as the order of the compound page.\n> \n> This is a preparation for changing how the head position is encoded in\n> the tail page.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n> ---\n>   include/linux/page-flags.h |  4 +++-\n>   mm/hugetlb.c               |  8 +++++---\n>   mm/internal.h              | 12 ++++++------\n>   mm/mm_init.c               |  2 +-\n>   mm/page_alloc.c            |  2 +-\n>   5 files changed, 16 insertions(+), 12 deletions(-)\n> \n> diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\n> index f7a0e4af0c73..8a3694369e15 100644\n> --- a/include/linux/page-flags.h\n> +++ b/include/linux/page-flags.h\n> @@ -865,7 +865,9 @@ static inline bool folio_test_large(const struct folio *folio)\n>   \treturn folio_test_head(folio);\n>   }\n>   \n> -static __always_inline void set_compound_head(struct page *page, struct page *head)\n> +static __always_inline void set_compound_head(struct page *page,\n> +\t\t\t\t\t      const struct page *head,\n> +\t\t\t\t\t      unsigned int order)\n\nTwo tab indents please on second+ parameter list whenever you touch code.\n\n>   {\n>   \tWRITE_ONCE(page->compound_head, (unsigned long)head + 1);\n>   }\n> diff --git a/mm/hugetlb.c b/mm/hugetlb.c\n> index 6e855a32de3d..54ba7cd05a86 100644\n\n\n[...]\n\n> diff --git a/mm/internal.h b/mm/internal.h\n> index d67e8bb75734..037ddcda25ff 100644\n> --- a/mm/internal.h\n> +++ b/mm/internal.h\n> @@ -879,13 +879,13 @@ static inline void prep_compound_head(struct page *page, unsigned int order)\n>   \t\tINIT_LIST_HEAD(&folio->_deferred_list);\n>   }\n>   \n> -static inline void prep_compound_tail(struct page *head, int tail_idx)\n> +static inline void prep_compound_tail(struct page *tail,\n\nJust wondering whether we should call this \"struct page *page\" for \nconsistency with set_compound_head().\n\nOr alternatively, call it also \"tail\" in set_compound_head().\n\n> +\t\t\t\t      const struct page *head,\n> +\t\t\t\t      unsigned int order)\n\nTwo tab indent, then this fits into two lines in total.\n\n>   {\n> -\tstruct page *p = head + tail_idx;\n> -\n> -\tp->mapping = TAIL_MAPPING;\n> -\tset_compound_head(p, head);\n> -\tset_page_private(p, 0);\n> +\ttail->mapping = TAIL_MAPPING;\n> +\tset_compound_head(tail, head, order);\n> +\tset_page_private(tail, 0);\n>   }\nOnly nits, in general LGTM\n\nAcked-by: David Hildenbrand (arm) <david@kernel.org>\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> The 'compound_head' field in the 'struct page' encodes whether the page\n> is a tail and where to locate the head page. Bit 0 is set if the page is\n> a tail, and the remaining bits in the field point to the head page.\n> \n> As preparation for changing how the field encodes information about the\n> head page, rename the field to 'compound_info'.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n> ---\n\nAcked-by: David Hildenbrand (arm) <david@kernel.org>\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> Move set_compound_head() and clear_compound_head() to be adjacent to the\n> compound_head() function in page-flags.h.\n> \n> These functions encode and decode the same compound_info field, so\n> keeping them together makes it easier to verify their logic is\n> consistent, especially when the encoding changes.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n> ---\n>   include/linux/page-flags.h | 24 ++++++++++++------------\n>   1 file changed, 12 insertions(+), 12 deletions(-)\n> \n> diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\n> index aa46d49e82f7..d14a17ffb55b 100644\n> --- a/include/linux/page-flags.h\n> +++ b/include/linux/page-flags.h\n> @@ -290,6 +290,18 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n>   \n>   #define compound_head(page)\t((typeof(page))_compound_head(page))\n>   \n> +static __always_inline void set_compound_head(struct page *page,\n> +\t\t\t\t\t      const struct page *head,\n> +\t\t\t\t\t      unsigned int order)\n\n^ :)\n\nAcked-by: David Hildenbrand (arm) <david@kernel.org>\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> Move set_compound_head() and clear_compound_head() to be adjacent to the\n> compound_head() function in page-flags.h.\n> \n> These functions encode and decode the same compound_info field, so\n> keeping them together makes it easier to verify their logic is\n> consistent, especially when the encoding changes.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n> ---\n>   include/linux/page-flags.h | 24 ++++++++++++------------\n>   1 file changed, 12 insertions(+), 12 deletions(-)\n> \n> diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\n> index aa46d49e82f7..d14a17ffb55b 100644\n> --- a/include/linux/page-flags.h\n> +++ b/include/linux/page-flags.h\n> @@ -290,6 +290,18 @@ static __always_inline unsigned long _compound_head(const struct page *page)\n>   \n>   #define compound_head(page)\t((typeof(page))_compound_head(page))\n>   \n> +static __always_inline void set_compound_head(struct page *page,\n> +\t\t\t\t\t      const struct page *head,\n> +\t\t\t\t\t      unsigned int order)\n\n^ :)\n\nAcked-by: David Hildenbrand (arm) <david@kernel.org>\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n> struct pages of the head page to be naturally aligned with regard to the\n> folio size.\n> \n> Align vmemmap to MAX_FOLIO_NR_PAGES.\n\nI think neither that statement nor the one in the patch description is \ncorrect?\n\n\"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is neither the maximum folio \nsize nor MAX_FOLIO_NR_PAGES.\n\nIt's the size of the memmap that a large folio could span at maximum.\n\n\nAssuming we have a 16 GiB folio, the calculation would give us\n\n\t4194304 * sizeof(struct page)\n\nWhich could be something like (assuming 80 bytes)\n\n\t335544320\n\n-> not even a power of 2, weird? (for HVO you wouldn't care as HVO would \nbe disabled, but that aliment is super weird?)\n\n\nAssuming 64 bytes, it would be a power of two (as 64 is a power of two).\n\n\t268435456 (1<< 28)\n\n\nWhich makes me wonder whether there is a way to avoid sizeof(struct \npage) here completely.\n\nOr limit the alignment to the case where HVO is actually active and \nsizeof(struct page) makes any sense?\n\n\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> ---\n>   arch/riscv/mm/init.c | 3 ++-\n>   1 file changed, 2 insertions(+), 1 deletion(-)\n> \n> diff --git a/arch/riscv/mm/init.c b/arch/riscv/mm/init.c\n> index 21d534824624..c555b9a4fdce 100644\n> --- a/arch/riscv/mm/init.c\n> +++ b/arch/riscv/mm/init.c\n> @@ -63,7 +63,8 @@ phys_addr_t phys_ram_base __ro_after_init;\n>   EXPORT_SYMBOL(phys_ram_base);\n>   \n>   #ifdef CONFIG_SPARSEMEM_VMEMMAP\n> -#define VMEMMAP_ADDR_ALIGN\t(1ULL << SECTION_SIZE_BITS)\n> +#define VMEMMAP_ADDR_ALIGN\tmax(1ULL << SECTION_SIZE_BITS, \\\n> +\t\t\t\t    MAX_FOLIO_NR_PAGES * sizeof(struct page))\n>   \n>   unsigned long vmemmap_start_pfn __ro_after_init;\n>   EXPORT_SYMBOL(vmemmap_start_pfn);\n\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n> struct pages of the head page to be naturally aligned with regard to the\n> folio size.\n> \n> Align vmemmap to MAX_FOLIO_NR_PAGES.\n\nI think neither that statement nor the one in the patch description is \ncorrect?\n\n\"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is neither the maximum folio \nsize nor MAX_FOLIO_NR_PAGES.\n\nIt's the size of the memmap that a large folio could span at maximum.\n\n\nAssuming we have a 16 GiB folio, the calculation would give us\n\n\t4194304 * sizeof(struct page)\n\nWhich could be something like (assuming 80 bytes)\n\n\t335544320\n\n-> not even a power of 2, weird? (for HVO you wouldn't care as HVO would \nbe disabled, but that aliment is super weird?)\n\n\nAssuming 64 bytes, it would be a power of two (as 64 is a power of two).\n\n\t268435456 (1<< 28)\n\n\nWhich makes me wonder whether there is a way to avoid sizeof(struct \npage) here completely.\n\nOr limit the alignment to the case where HVO is actually active and \nsizeof(struct page) makes any sense?\n\n\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> ---\n>   arch/riscv/mm/init.c | 3 ++-\n>   1 file changed, 2 insertions(+), 1 deletion(-)\n> \n> diff --git a/arch/riscv/mm/init.c b/arch/riscv/mm/init.c\n> index 21d534824624..c555b9a4fdce 100644\n> --- a/arch/riscv/mm/init.c\n> +++ b/arch/riscv/mm/init.c\n> @@ -63,7 +63,8 @@ phys_addr_t phys_ram_base __ro_after_init;\n>   EXPORT_SYMBOL(phys_ram_base);\n>   \n>   #ifdef CONFIG_SPARSEMEM_VMEMMAP\n> -#define VMEMMAP_ADDR_ALIGN\t(1ULL << SECTION_SIZE_BITS)\n> +#define VMEMMAP_ADDR_ALIGN\tmax(1ULL << SECTION_SIZE_BITS, \\\n> +\t\t\t\t    MAX_FOLIO_NR_PAGES * sizeof(struct page))\n>   \n>   unsigned long vmemmap_start_pfn __ro_after_init;\n>   EXPORT_SYMBOL(vmemmap_start_pfn);\n\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n> struct pages of the head page to be naturally aligned with regard to the\n> folio size.\n> \n> Align vmemmap to MAX_FOLIO_NR_PAGES.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> ---\n>   arch/loongarch/include/asm/pgtable.h | 3 ++-\n>   1 file changed, 2 insertions(+), 1 deletion(-)\n> \n> diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h\n> index c33b3bcb733e..f9416acb9156 100644\n> --- a/arch/loongarch/include/asm/pgtable.h\n> +++ b/arch/loongarch/include/asm/pgtable.h\n> @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];\n>   \t min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)\n>   #endif\n>   \n> -#define vmemmap\t\t((struct page *)((VMALLOC_END + PMD_SIZE) & PMD_MASK))\n> +#define VMEMMAP_ALIGN\tmax(PMD_SIZE, MAX_FOLIO_NR_PAGES * sizeof(struct page))\n> +#define vmemmap\t\t((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))\n\n\nSame comment, the \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is just black magic here\nand the description of the situation is wrong.\n\nMaybe you want to pull the magic \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" into the core and call it\n\n#define MAX_FOLIO_VMEMMAP_ALIGN\t(MAX_FOLIO_NR_PAGES * sizeof(struct page))\n\nBut then special case it base on (a) HVO being configured in an (b) HVO being possible\n\n#ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP && is_power_of_2(sizeof(struct page)\n/* A very helpful comment explaining the situation. */\n#define MAX_FOLIO_VMEMMAP_ALIGN\t(MAX_FOLIO_NR_PAGES * sizeof(struct page))\n#else\n#define MAX_FOLIO_VMEMMAP_ALIGN\t0\n#endif\n\nSomething like that.\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n> struct pages of the head page to be naturally aligned with regard to the\n> folio size.\n> \n> Align vmemmap to MAX_FOLIO_NR_PAGES.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> ---\n>   arch/loongarch/include/asm/pgtable.h | 3 ++-\n>   1 file changed, 2 insertions(+), 1 deletion(-)\n> \n> diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h\n> index c33b3bcb733e..f9416acb9156 100644\n> --- a/arch/loongarch/include/asm/pgtable.h\n> +++ b/arch/loongarch/include/asm/pgtable.h\n> @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];\n>   \t min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - KFENCE_AREA_SIZE)\n>   #endif\n>   \n> -#define vmemmap\t\t((struct page *)((VMALLOC_END + PMD_SIZE) & PMD_MASK))\n> +#define VMEMMAP_ALIGN\tmax(PMD_SIZE, MAX_FOLIO_NR_PAGES * sizeof(struct page))\n> +#define vmemmap\t\t((struct page *)(ALIGN(VMALLOC_END, VMEMMAP_ALIGN)))\n\n\nSame comment, the \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is just black magic here\nand the description of the situation is wrong.\n\nMaybe you want to pull the magic \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" into the core and call it\n\n#define MAX_FOLIO_VMEMMAP_ALIGN\t(MAX_FOLIO_NR_PAGES * sizeof(struct page))\n\nBut then special case it base on (a) HVO being configured in an (b) HVO being possible\n\n#ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP && is_power_of_2(sizeof(struct page)\n/* A very helpful comment explaining the situation. */\n#define MAX_FOLIO_VMEMMAP_ALIGN\t(MAX_FOLIO_NR_PAGES * sizeof(struct page))\n#else\n#define MAX_FOLIO_VMEMMAP_ALIGN\t0\n#endif\n\nSomething like that.\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/5/26 12:35, Kiryl Shutsemau wrote:\n> On Wed, Feb 04, 2026 at 05:14:12PM +0100, David Hildenbrand (arm) wrote:\n>> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n>>> Instead of passing down the head page and tail page index, pass the tail\n>>> and head pages directly, as well as the order of the compound page.\n>>>\n>>> This is a preparation for changing how the head position is encoded in\n>>> the tail page.\n>>>\n>>> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n>>> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n>>> Reviewed-by: Zi Yan <ziy@nvidia.com>\n>>> ---\n>>>    include/linux/page-flags.h |  4 +++-\n>>>    mm/hugetlb.c               |  8 +++++---\n>>>    mm/internal.h              | 12 ++++++------\n>>>    mm/mm_init.c               |  2 +-\n>>>    mm/page_alloc.c            |  2 +-\n>>>    5 files changed, 16 insertions(+), 12 deletions(-)\n>>>\n>>> diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\n>>> index f7a0e4af0c73..8a3694369e15 100644\n>>> --- a/include/linux/page-flags.h\n>>> +++ b/include/linux/page-flags.h\n>>> @@ -865,7 +865,9 @@ static inline bool folio_test_large(const struct folio *folio)\n>>>    \treturn folio_test_head(folio);\n>>>    }\n>>> -static __always_inline void set_compound_head(struct page *page, struct page *head)\n>>> +static __always_inline void set_compound_head(struct page *page,\n>>> +\t\t\t\t\t      const struct page *head,\n>>> +\t\t\t\t\t      unsigned int order)\n>>\n>> Two tab indents please on second+ parameter list whenever you touch code.\n> \n> Do we have this coding style preference written down somewhere?\n\nGood question. I assume not. But it's what we do in MM :)\n\n> \n> -tip tree wants the opposite. Documentation/process/maintainer-tip.rst:\n> \n> \tWhen splitting function declarations or function calls, then please align\n> \tthe first argument in the second line with the first argument in the first\n> \tline::\n> \n> I want the editor to do The Right Thing\\u2122 without my brain involvement.\n> Having different coding styles in different corners of the kernel makes\n> it hard.\n\nYeah, but unavoidable. :)\n\n-- \nCheers,\n\nDavid\n\n---\n\nOn 2/5/26 12:35, Kiryl Shutsemau wrote:\n> On Wed, Feb 04, 2026 at 05:14:12PM +0100, David Hildenbrand (arm) wrote:\n>> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n>>> Instead of passing down the head page and tail page index, pass the tail\n>>> and head pages directly, as well as the order of the compound page.\n>>>\n>>> This is a preparation for changing how the head position is encoded in\n>>> the tail page.\n>>>\n>>> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n>>> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n>>> Reviewed-by: Zi Yan <ziy@nvidia.com>\n>>> ---\n>>>    include/linux/page-flags.h |  4 +++-\n>>>    mm/hugetlb.c               |  8 +++++---\n>>>    mm/internal.h              | 12 ++++++------\n>>>    mm/mm_init.c               |  2 +-\n>>>    mm/page_alloc.c            |  2 +-\n>>>    5 files changed, 16 insertions(+), 12 deletions(-)\n>>>\n>>> diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\n>>> index f7a0e4af0c73..8a3694369e15 100644\n>>> --- a/include/linux/page-flags.h\n>>> +++ b/include/linux/page-flags.h\n>>> @@ -865,7 +865,9 @@ static inline bool folio_test_large(const struct folio *folio)\n>>>    \treturn folio_test_head(folio);\n>>>    }\n>>> -static __always_inline void set_compound_head(struct page *page, struct page *head)\n>>> +static __always_inline void set_compound_head(struct page *page,\n>>> +\t\t\t\t\t      const struct page *head,\n>>> +\t\t\t\t\t      unsigned int order)\n>>\n>> Two tab indents please on second+ parameter list whenever you touch code.\n> \n> Do we have this coding style preference written down somewhere?\n\nGood question. I assume not. But it's what we do in MM :)\n\n> \n> -tip tree wants the opposite. Documentation/process/maintainer-tip.rst:\n> \n> \tWhen splitting function declarations or function calls, then please align\n> \tthe first argument in the second line with the first argument in the first\n> \tline::\n> \n> I want the editor to do The Right Thing without my brain involvement.\n> Having different coding styles in different corners of the kernel makes\n> it hard.\n\nYeah, but unavoidable. :)\n\n-- \nCheers,\n\nDavid\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/4/26 17:56, David Hildenbrand (arm) wrote:\n> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n>> The upcoming change to the HugeTLB vmemmap optimization (HVO) requires\n>> struct pages of the head page to be naturally aligned with regard to the\n>> folio size.\n>>\n>> Align vmemmap to MAX_FOLIO_NR_PAGES.\n>>\n>> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n>> ---\n>>  arch/loongarch/include/asm/pgtable.h | 3 ++-\n>>  1 file changed, 2 insertions(+), 1 deletion(-)\n>>\n>> diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/ \n>> include/asm/pgtable.h\n>> index c33b3bcb733e..f9416acb9156 100644\n>> --- a/arch/loongarch/include/asm/pgtable.h\n>> +++ b/arch/loongarch/include/asm/pgtable.h\n>> @@ -113,7 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / \n>> sizeof(unsigned long)];\n>>  min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * \n>> PAGE_SIZE, (1UL << cpu_vabits) / 2) - PMD_SIZE - VMEMMAP_SIZE - \n>> KFENCE_AREA_SIZE)\n>>  #endif\n>> -#define vmemmap ((struct page *)((VMALLOC_END + PMD_SIZE) & \n>> PMD_MASK))\n>> +#define VMEMMAP_ALIGN max(PMD_SIZE, MAX_FOLIO_NR_PAGES * \n>> sizeof(struct page))\n>> +#define vmemmap ((struct page *)(ALIGN(VMALLOC_END, \n>> VMEMMAP_ALIGN)))\n> \n> \n> Same comment, the \"MAX_FOLIO_NR_PAGES * sizeof(struct page)\" is just \n> black magic here\n> and the description of the situation is wrong.\n> \n> Maybe you want to pull the magic \"MAX_FOLIO_NR_PAGES * sizeof(struct \n> page)\" into the core and call it\n> \n> #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct \n> page))\n> \n> But then special case it base on (a) HVO being configured in an (b) HVO \n> being possible\n> \n> #ifdef HUGETLB_PAGE_OPTIMIZE_VMEMMAP && is_power_of_2(sizeof(struct page)\n> /* A very helpful comment explaining the situation. */\n> #define MAX_FOLIO_VMEMMAP_ALIGN (MAX_FOLIO_NR_PAGES * sizeof(struct \n> page))\n> #else\n> #define MAX_FOLIO_VMEMMAP_ALIGN 0\n> #endif\n> \n> Something like that.\n> \n\nThinking about this ...\n\nthe vmemmap start is always struct-page-aligned. Otherwise we'd be in \ntrouble already.\n\nIsn't it then sufficient to just align the start to MAX_FOLIO_NR_PAGES?\n\nLet's assume sizeof(struct page) == 64 and MAX_FOLIO_NR_PAGES = 512 for \nsimplicity.\n\nvmemmap start would be multiples of 512 (0x0010000000).\n\n512, 1024, 1536, 2048 ...\n\nAssume we have an 256-pages folio at 1536+256 = 0x111000000\n\nAssume we have the last page of that folio (0x011111111111), we would \njust get to the start of that folio by AND-ing with ~(256-1).\n\nWhich case am I ignoring?\n\n-- \nCheers,\n\nDavid",
          "reply_to": "",
          "message_date": "2026-02-04",
          "message_id": ""
        }
      ],
      "analysis_source": "llm",
      "patch_summary": "This patch series removes 'fake head pages' from the HugeTLB vmemmap optimization by changing how tail pages encode their relationship to the head page, simplifying compound_head() and page_ref_add_unless(). The new approach uses a mask-based encoding for architectures where sizeof(struct page) is a power of 2, allowing shared read-only tail pages across huge pages on a NUMA node. This reduces complexity and overhead in the hot path, but testing has shown either no change or only slight performance improvement."
    },
    "2026-02-07": {
      "report_file": "2026-02-27_ollama_llama3.1-8b.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "Usama Arif",
          "summary": "Reviewer Usama Arif pointed out that the new mask-based compound_head() encoding has a potential issue: it requires validating that struct pages are naturally aligned for all orders up to MAX_FOLIO_ORDER, which can be tricky. He suggested limiting the usage of this approach to HugeTLB vmemmap optimization (HVO) where it makes a difference.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "limiting mask usage",
            "potential issue with alignment"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Acked-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On 02/02/2026 15:56, Kiryl Shutsemau wrote:\n> For tail pages, the kernel uses the 'compound_info' field to get to the\n> head page. The bit 0 of the field indicates whether the page is a\n> tail page, and if set, the remaining bits represent a pointer to the\n> head page.\n> \n> For cases when size of struct page is power-of-2, change the encoding of\n> compound_info to store a mask that can be applied to the virtual address\n> of the tail page in order to access the head page. It is possible\n> because struct page of the head page is naturally aligned with regards\n> to order of the page.\n> \n> The significant impact of this modification is that all tail pages of\n> the same order will now have identical 'compound_info', regardless of\n> the compound page they are associated with. This paves the way for\n> eliminating fake heads.\n> \n> The HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only\n> applied when the sizeof(struct page) is power-of-2. Having identical\n> tail pages allows the same page to be mapped into the vmemmap of all\n> pages, maintaining memory savings without fake heads.\n> \n> If sizeof(struct page) is not power-of-2, there is no functional\n> changes.\n> \n> Limit mask usage to HugeTLB vmemmap optimization (HVO) where it makes\n> a difference. The approach with mask would work in the wider set of\n> conditions, but it requires validating that struct pages are naturally\n> aligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n> ---\n\nAcked-by: Usama Arif <usamaarif642@gmail.com>\n\nSmall nit below:\n\n>  include/linux/page-flags.h | 81 ++++++++++++++++++++++++++++++++++----\n>  mm/slab.h                  | 16 ++++++--\n>  mm/util.c                  | 16 ++++++--\n>  3 files changed, 97 insertions(+), 16 deletions(-)\n> \n> diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\n> index d14a17ffb55b..8f2c7fbc739b 100644\n> --- a/include/linux/page-flags.h\n> +++ b/include/linux/page-flags.h\n> @@ -198,6 +198,29 @@ enum pageflags {\n>  \n>  #ifndef __GENERATING_BOUNDS_H\n>  \n> +/*\n> + * For tail pages, if the size of struct page is power-of-2 ->compound_info\n> + * encodes the mask that converts the address of the tail page address to\n> + * the head page address.\n> + *\n> + * Otherwise, ->compound_info has direct pointer to head pages.\n> + */\n> +static __always_inline bool compound_info_has_mask(void)\n> +{\n> +\t/*\n> +\t * Limit mask usage to HugeTLB vmemmap optimization (HVO) where it\n> +\t * makes a difference.\n> +\t *\n> +\t * The approach with mask would work in the wider set of conditions,\n> +\t * but it requires validating that struct pages are naturally aligned\n> +\t * for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n> +\t */\n> +\tif (!IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP))\n> +\t\treturn false;\n> +\n> +\treturn is_power_of_2(sizeof(struct page));\n> +}\n> +\n>  #ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP\n>  DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\n>  \n> @@ -210,6 +233,10 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n>  \tif (!static_branch_unlikely(&hugetlb_optimize_vmemmap_key))\n>  \t\treturn page;\n>  \n> +\t/* Fake heads only exists if compound_info_has_mask() is true */\n> +\tif (!compound_info_has_mask())\n> +\t\treturn page;\n> +\n>  \t/*\n>  \t * Only addresses aligned with PAGE_SIZE of struct page may be fake head\n>  \t * struct page. The alignment check aims to avoid access the fields (\n> @@ -223,10 +250,14 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n>  \t\t * because the @page is a compound page composed with at least\n>  \t\t * two contiguous pages.\n>  \t\t */\n> -\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n> +\t\tunsigned long info = READ_ONCE(page[1].compound_info);\n>  \n> -\t\tif (likely(head & 1))\n> -\t\t\treturn (const struct page *)(head - 1);\n> +\t\t/* See set_compound_head() */\n> +\t\tif (likely(info & 1)) {\n> +\t\t\tunsigned long p = (unsigned long)page;\n> +\n> +\t\t\treturn (const struct page *)(p & info);\n> +\t\t}\n>  \t}\n>  \treturn page;\n>  }\n> @@ -281,11 +312,26 @@ static __always_inline int page_is_fake_head(const struct page *page)\n>  \n>  static __always_inline unsigned long _compound_head(const struct page *page)\n>  {\n> -\tunsigned long head = READ_ONCE(page->compound_info);\n> +\tunsigned long info = READ_ONCE(page->compound_info);\n>  \n> -\tif (unlikely(head & 1))\n> -\t\treturn head - 1;\n> -\treturn (unsigned long)page_fixed_fake_head(page);\n> +\t/* Bit 0 encodes PageTail() */\n> +\tif (!(info & 1))\n> +\t\treturn (unsigned long)page_fixed_fake_head(page);\n> +\n> +\t/*\n> +\t * If compound_info_has_mask() is false, the rest of compound_info is\n> +\t * the pointer to the head page.\n> +\t */\n> +\tif (!compound_info_has_mask())\n> +\t\treturn info - 1;\n> +\n> +\t/*\n> +\t * If compoun_info_has_mask() is true the rest of the info encodes\n\ns/compoun_info_has_mask/compound_info_has_mask/\n\n---\n\nOn 02/02/2026 15:56, Kiryl Shutsemau wrote:\n> For tail pages, the kernel uses the 'compound_info' field to get to the\n> head page. The bit 0 of the field indicates whether the page is a\n> tail page, and if set, the remaining bits represent a pointer to the\n> head page.\n> \n> For cases when size of struct page is power-of-2, change the encoding of\n> compound_info to store a mask that can be applied to the virtual address\n> of the tail page in order to access the head page. It is possible\n> because struct page of the head page is naturally aligned with regards\n> to order of the page.\n> \n> The significant impact of this modification is that all tail pages of\n> the same order will now have identical 'compound_info', regardless of\n> the compound page they are associated with. This paves the way for\n> eliminating fake heads.\n> \n> The HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only\n> applied when the sizeof(struct page) is power-of-2. Having identical\n> tail pages allows the same page to be mapped into the vmemmap of all\n> pages, maintaining memory savings without fake heads.\n> \n> If sizeof(struct page) is not power-of-2, there is no functional\n> changes.\n> \n> Limit mask usage to HugeTLB vmemmap optimization (HVO) where it makes\n> a difference. The approach with mask would work in the wider set of\n> conditions, but it requires validating that struct pages are naturally\n> aligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n> ---\n\nAcked-by: Usama Arif <usamaarif642@gmail.com>\n\nSmall nit below:\n\n>  include/linux/page-flags.h | 81 ++++++++++++++++++++++++++++++++++----\n>  mm/slab.h                  | 16 ++++++--\n>  mm/util.c                  | 16 ++++++--\n>  3 files changed, 97 insertions(+), 16 deletions(-)\n> \n> diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\n> index d14a17ffb55b..8f2c7fbc739b 100644\n> --- a/include/linux/page-flags.h\n> +++ b/include/linux/page-flags.h\n> @@ -198,6 +198,29 @@ enum pageflags {\n>  \n>  #ifndef __GENERATING_BOUNDS_H\n>  \n> +/*\n> + * For tail pages, if the size of struct page is power-of-2 ->compound_info\n> + * encodes the mask that converts the address of the tail page address to\n> + * the head page address.\n> + *\n> + * Otherwise, ->compound_info has direct pointer to head pages.\n> + */\n> +static __always_inline bool compound_info_has_mask(void)\n> +{\n> +\t/*\n> +\t * Limit mask usage to HugeTLB vmemmap optimization (HVO) where it\n> +\t * makes a difference.\n> +\t *\n> +\t * The approach with mask would work in the wider set of conditions,\n> +\t * but it requires validating that struct pages are naturally aligned\n> +\t * for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n> +\t */\n> +\tif (!IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP))\n> +\t\treturn false;\n> +\n> +\treturn is_power_of_2(sizeof(struct page));\n> +}\n> +\n>  #ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP\n>  DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\n>  \n> @@ -210,6 +233,10 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n>  \tif (!static_branch_unlikely(&hugetlb_optimize_vmemmap_key))\n>  \t\treturn page;\n>  \n> +\t/* Fake heads only exists if compound_info_has_mask() is true */\n> +\tif (!compound_info_has_mask())\n> +\t\treturn page;\n> +\n>  \t/*\n>  \t * Only addresses aligned with PAGE_SIZE of struct page may be fake head\n>  \t * struct page. The alignment check aims to avoid access the fields (\n> @@ -223,10 +250,14 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n>  \t\t * because the @page is a compound page composed with at least\n>  \t\t * two contiguous pages.\n>  \t\t */\n> -\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n> +\t\tunsigned long info = READ_ONCE(page[1].compound_info);\n>  \n> -\t\tif (likely(head & 1))\n> -\t\t\treturn (const struct page *)(head - 1);\n> +\t\t/* See set_compound_head() */\n> +\t\tif (likely(info & 1)) {\n> +\t\t\tunsigned long p = (unsigned long)page;\n> +\n> +\t\t\treturn (const struct page *)(p & info);\n> +\t\t}\n>  \t}\n>  \treturn page;\n>  }\n> @@ -281,11 +312,26 @@ static __always_inline int page_is_fake_head(const struct page *page)\n>  \n>  static __always_inline unsigned long _compound_head(const struct page *page)\n>  {\n> -\tunsigned long head = READ_ONCE(page->compound_info);\n> +\tunsigned long info = READ_ONCE(page->compound_info);\n>  \n> -\tif (unlikely(head & 1))\n> -\t\treturn head - 1;\n> -\treturn (unsigned long)page_fixed_fake_head(page);\n> +\t/* Bit 0 encodes PageTail() */\n> +\tif (!(info & 1))\n> +\t\treturn (unsigned long)page_fixed_fake_head(page);\n> +\n> +\t/*\n> +\t * If compound_info_has_mask() is false, the rest of compound_info is\n> +\t * the pointer to the head page.\n> +\t */\n> +\tif (!compound_info_has_mask())\n> +\t\treturn info - 1;\n> +\n> +\t/*\n> +\t * If compoun_info_has_mask() is true the rest of the info encodes\n\ns/compoun_info_has_mask/compound_info_has_mask/\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 02/02/2026 15:56, Kiryl Shutsemau wrote:\n> Move MAX_FOLIO_ORDER definition from mm.h to mmzone.h.\n> \n> This is preparation for adding the vmemmap_tails array to struct\n> pglist_data, which requires MAX_FOLIO_ORDER to be available in mmzone.h.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Acked-by: David Hildenbrand (Red Hat) <david@kernel.org>\n> Acked-by: Zi Yan <ziy@nvidia.com>\n> Acked-by: Muchun Song <muchun.song@linux.dev>\n\nAcked-by: Usama Arif <usamaarif642@gmail.com>\n\n---\n\nOn 02/02/2026 15:56, Kiryl Shutsemau wrote:\n> Move MAX_FOLIO_ORDER definition from mm.h to mmzone.h.\n> \n> This is preparation for adding the vmemmap_tails array to struct\n> pglist_data, which requires MAX_FOLIO_ORDER to be available in mmzone.h.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Acked-by: David Hildenbrand (Red Hat) <david@kernel.org>\n> Acked-by: Zi Yan <ziy@nvidia.com>\n> Acked-by: Muchun Song <muchun.song@linux.dev>\n\nAcked-by: Usama Arif <usamaarif642@gmail.com>\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-07",
          "message_id": ""
        }
      ],
      "analysis_source": "llm",
      "patch_summary": "This patch series removes 'fake head pages' from the HugeTLB vmemmap optimization by changing how tail pages encode their relationship to the head page, simplifying compound_head() and page_ref_add_unless(). The new approach uses a mask-based encoding for architectures where sizeof(struct page) is a power of 2, allowing shared read-only tail pages across huge pages on a NUMA node. This reduces complexity and overhead in the hot path, but testing has shown either no change or only slight performance improvement."
    },
    "2026-02-10": {
      "report_file": "2026-02-27_ollama_llama3.1-8b.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "Vlastimil Babka",
          "summary": "Vlastimil Babka noted that for tail pages, the kernel uses the 'compound_info' field to get to the head page, and requested that the bit 0 of the field be checked before accessing it to prevent potential issues.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "potential issue",
            "requested change"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On 2/2/26 16:56, Kiryl Shutsemau wrote:\n> Move MAX_FOLIO_ORDER definition from mm.h to mmzone.h.\n> \n> This is preparation for adding the vmemmap_tails array to struct\n> pglist_data, which requires MAX_FOLIO_ORDER to be available in mmzone.h.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Acked-by: David Hildenbrand (Red Hat) <david@kernel.org>\n> Acked-by: Zi Yan <ziy@nvidia.com>\n> Acked-by: Muchun Song <muchun.song@linux.dev>\n\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> Move MAX_FOLIO_ORDER definition from mm.h to mmzone.h.\n> \n> This is preparation for adding the vmemmap_tails array to struct\n> pglist_data, which requires MAX_FOLIO_ORDER to be available in mmzone.h.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Acked-by: David Hildenbrand (Red Hat) <david@kernel.org>\n> Acked-by: Zi Yan <ziy@nvidia.com>\n> Acked-by: Muchun Song <muchun.song@linux.dev>\n\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> Instead of passing down the head page and tail page index, pass the tail\n> and head pages directly, as well as the order of the compound page.\n> \n> This is a preparation for changing how the head position is encoded in\n> the tail page.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> Instead of passing down the head page and tail page index, pass the tail\n> and head pages directly, as well as the order of the compound page.\n> \n> This is a preparation for changing how the head position is encoded in\n> the tail page.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> The 'compound_head' field in the 'struct page' encodes whether the page\n> is a tail and where to locate the head page. Bit 0 is set if the page is\n> a tail, and the remaining bits in the field point to the head page.\n> \n> As preparation for changing how the field encodes information about the\n> head page, rename the field to 'compound_info'.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> The 'compound_head' field in the 'struct page' encodes whether the page\n> is a tail and where to locate the head page. Bit 0 is set if the page is\n> a tail, and the remaining bits in the field point to the head page.\n> \n> As preparation for changing how the field encodes information about the\n> head page, rename the field to 'compound_info'.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> Move set_compound_head() and clear_compound_head() to be adjacent to the\n> compound_head() function in page-flags.h.\n> \n> These functions encode and decode the same compound_info field, so\n> keeping them together makes it easier to verify their logic is\n> consistent, especially when the encoding changes.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> Move set_compound_head() and clear_compound_head() to be adjacent to the\n> compound_head() function in page-flags.h.\n> \n> These functions encode and decode the same compound_info field, so\n> keeping them together makes it easier to verify their logic is\n> consistent, especially when the encoding changes.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> For tail pages, the kernel uses the 'compound_info' field to get to the\n> head page. The bit 0 of the field indicates whether the page is a\n> tail page, and if set, the remaining bits represent a pointer to the\n> head page.\n> \n> For cases when size of struct page is power-of-2, change the encoding of\n> compound_info to store a mask that can be applied to the virtual address\n> of the tail page in order to access the head page. It is possible\n> because struct page of the head page is naturally aligned with regards\n> to order of the page.\n> \n> The significant impact of this modification is that all tail pages of\n> the same order will now have identical 'compound_info', regardless of\n> the compound page they are associated with. This paves the way for\n> eliminating fake heads.\n> \n> The HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only\n> applied when the sizeof(struct page) is power-of-2. Having identical\n> tail pages allows the same page to be mapped into the vmemmap of all\n> pages, maintaining memory savings without fake heads.\n> \n> If sizeof(struct page) is not power-of-2, there is no functional\n> changes.\n> \n> Limit mask usage to HugeTLB vmemmap optimization (HVO) where it makes\n> a difference. The approach with mask would work in the wider set of\n> conditions, but it requires validating that struct pages are naturally\n> aligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\nnit:\n\n> ---\n>  include/linux/page-flags.h | 81 ++++++++++++++++++++++++++++++++++----\n>  mm/slab.h                  | 16 ++++++--\n>  mm/util.c                  | 16 ++++++--\n>  3 files changed, 97 insertions(+), 16 deletions(-)\n> \n> diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\n> index d14a17ffb55b..8f2c7fbc739b 100644\n> --- a/include/linux/page-flags.h\n> +++ b/include/linux/page-flags.h\n> @@ -198,6 +198,29 @@ enum pageflags {\n>  \n>  #ifndef __GENERATING_BOUNDS_H\n>  \n> +/*\n> + * For tail pages, if the size of struct page is power-of-2 ->compound_info\n> + * encodes the mask that converts the address of the tail page address to\n> + * the head page address.\n> + *\n> + * Otherwise, ->compound_info has direct pointer to head pages.\n> + */\n> +static __always_inline bool compound_info_has_mask(void)\n> +{\n> +\t/*\n> +\t * Limit mask usage to HugeTLB vmemmap optimization (HVO) where it\n> +\t * makes a difference.\n> +\t *\n> +\t * The approach with mask would work in the wider set of conditions,\n> +\t * but it requires validating that struct pages are naturally aligned\n> +\t * for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n> +\t */\n> +\tif (!IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP))\n> +\t\treturn false;\n> +\n> +\treturn is_power_of_2(sizeof(struct page));\n> +}\n> +\n>  #ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP\n>  DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\n>  \n> @@ -210,6 +233,10 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n>  \tif (!static_branch_unlikely(&hugetlb_optimize_vmemmap_key))\n>  \t\treturn page;\n>  \n> +\t/* Fake heads only exists if compound_info_has_mask() is true */\n> +\tif (!compound_info_has_mask())\n> +\t\treturn page;\n> +\n\nCould we move this compile-time-constant test above the static branch test?\n\n>  \t/*\n>  \t * Only addresses aligned with PAGE_SIZE of struct page may be fake head\n>  \t * struct page. The alignment check aims to avoid access the fields (\n> @@ -223,10 +250,14 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n>  \t\t * because the @page is a compound page composed with at least\n>  \t\t * two contiguous pages.\n>  \t\t */\n> -\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n> +\t\tunsigned long info = READ_ONCE(page[1].compound_info);\n>  \n> -\t\tif (likely(head & 1))\n> -\t\t\treturn (const struct page *)(head - 1);\n> +\t\t/* See set_compound_head() */\n> +\t\tif (likely(info & 1)) {\n> +\t\t\tunsigned long p = (unsigned long)page;\n> +\n> +\t\t\treturn (const struct page *)(p & info);\n> +\t\t}\n>  \t}\n>  \treturn page;\n>  }\n\n---\n\nOn 2/2/26 16:56, Kiryl Shutsemau wrote:\n> For tail pages, the kernel uses the 'compound_info' field to get to the\n> head page. The bit 0 of the field indicates whether the page is a\n> tail page, and if set, the remaining bits represent a pointer to the\n> head page.\n> \n> For cases when size of struct page is power-of-2, change the encoding of\n> compound_info to store a mask that can be applied to the virtual address\n> of the tail page in order to access the head page. It is possible\n> because struct page of the head page is naturally aligned with regards\n> to order of the page.\n> \n> The significant impact of this modification is that all tail pages of\n> the same order will now have identical 'compound_info', regardless of\n> the compound page they are associated with. This paves the way for\n> eliminating fake heads.\n> \n> The HugeTLB Vmemmap Optimization (HVO) creates fake heads and it is only\n> applied when the sizeof(struct page) is power-of-2. Having identical\n> tail pages allows the same page to be mapped into the vmemmap of all\n> pages, maintaining memory savings without fake heads.\n> \n> If sizeof(struct page) is not power-of-2, there is no functional\n> changes.\n> \n> Limit mask usage to HugeTLB vmemmap optimization (HVO) where it makes\n> a difference. The approach with mask would work in the wider set of\n> conditions, but it requires validating that struct pages are naturally\n> aligned for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n> \n> Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n> Reviewed-by: Muchun Song <muchun.song@linux.dev>\n> Reviewed-by: Zi Yan <ziy@nvidia.com>\n\nReviewed-by: Vlastimil Babka <vbabka@suse.cz>\n\nnit:\n\n> ---\n>  include/linux/page-flags.h | 81 ++++++++++++++++++++++++++++++++++----\n>  mm/slab.h                  | 16 ++++++--\n>  mm/util.c                  | 16 ++++++--\n>  3 files changed, 97 insertions(+), 16 deletions(-)\n> \n> diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h\n> index d14a17ffb55b..8f2c7fbc739b 100644\n> --- a/include/linux/page-flags.h\n> +++ b/include/linux/page-flags.h\n> @@ -198,6 +198,29 @@ enum pageflags {\n>  \n>  #ifndef __GENERATING_BOUNDS_H\n>  \n> +/*\n> + * For tail pages, if the size of struct page is power-of-2 ->compound_info\n> + * encodes the mask that converts the address of the tail page address to\n> + * the head page address.\n> + *\n> + * Otherwise, ->compound_info has direct pointer to head pages.\n> + */\n> +static __always_inline bool compound_info_has_mask(void)\n> +{\n> +\t/*\n> +\t * Limit mask usage to HugeTLB vmemmap optimization (HVO) where it\n> +\t * makes a difference.\n> +\t *\n> +\t * The approach with mask would work in the wider set of conditions,\n> +\t * but it requires validating that struct pages are naturally aligned\n> +\t * for all orders up to the MAX_FOLIO_ORDER, which can be tricky.\n> +\t */\n> +\tif (!IS_ENABLED(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP))\n> +\t\treturn false;\n> +\n> +\treturn is_power_of_2(sizeof(struct page));\n> +}\n> +\n>  #ifdef CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP\n>  DECLARE_STATIC_KEY_FALSE(hugetlb_optimize_vmemmap_key);\n>  \n> @@ -210,6 +233,10 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n>  \tif (!static_branch_unlikely(&hugetlb_optimize_vmemmap_key))\n>  \t\treturn page;\n>  \n> +\t/* Fake heads only exists if compound_info_has_mask() is true */\n> +\tif (!compound_info_has_mask())\n> +\t\treturn page;\n> +\n\nCould we move this compile-time-constant test above the static branch test?\n\n>  \t/*\n>  \t * Only addresses aligned with PAGE_SIZE of struct page may be fake head\n>  \t * struct page. The alignment check aims to avoid access the fields (\n> @@ -223,10 +250,14 @@ static __always_inline const struct page *page_fixed_fake_head(const struct page\n>  \t\t * because the @page is a compound page composed with at least\n>  \t\t * two contiguous pages.\n>  \t\t */\n> -\t\tunsigned long head = READ_ONCE(page[1].compound_info);\n> +\t\tunsigned long info = READ_ONCE(page[1].compound_info);\n>  \n> -\t\tif (likely(head & 1))\n> -\t\t\treturn (const struct page *)(head - 1);\n> +\t\t/* See set_compound_head() */\n> +\t\tif (likely(info & 1)) {\n> +\t\t\tunsigned long p = (unsigned long)page;\n> +\n> +\t\t\treturn (const struct page *)(p & info);\n> +\t\t}\n>  \t}\n>  \treturn page;\n>  }\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/9/26 12:52, Kiryl Shutsemau wrote:\n> On Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:\n>> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n>> > With the upcoming changes to HVO, a single page of tail struct pages\n>> > will be shared across all huge pages of the same order on a node. Since\n>> > huge pages on the same node may belong to different zones, the zone\n>> > information stored in shared tail page flags would be incorrect.\n>> > \n>> > Always fetch zone information from the head page, which has unique and\n>> > correct zone flags for each compound page.\n>> > \n>> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n>> > Acked-by: Zi Yan <ziy@nvidia.com>\n>> > ---\n>> >   include/linux/mmzone.h | 1 +\n>> >   1 file changed, 1 insertion(+)\n>> > \n>> > diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n>> > index be8ce40b5638..192143b5cdc0 100644\n>> > --- a/include/linux/mmzone.h\n>> > +++ b/include/linux/mmzone.h\n>> > @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n>> >   static inline enum zone_type page_zonenum(const struct page *page)\n>> >   {\n>> > +\tpage = compound_head(page);\n>> >   \treturn memdesc_zonenum(page->flags);\n>> \n>> We end up calling page_zonenum() without holding a reference.\n>> \n>> Given that _compound_head() does a READ_ONCE(), this should work even if we\n>> see concurrent page freeing etc.\n>> \n>> However, this change implies that we now perform a compound page lookup for\n>> every PageHighMem() [meh], page_zone() [quite some users in the buddy,\n>> including for pageblock access and page freeing].\n>> \n>> That's a nasty compromise for making HVO better? :)\n>> \n>> We should likely limit that special casing to kernels that really rquire it\n>> (HVO).\n> \n> I will add compound_info_has_mask() check.\n\nNot thrilled by this indeed. Would it be a problem to have the shared tail\npages per node+zone instead of just per node?\n\n---\n\nOn 2/9/26 12:52, Kiryl Shutsemau wrote:\n> On Thu, Feb 05, 2026 at 02:10:40PM +0100, David Hildenbrand (Arm) wrote:\n>> On 2/2/26 16:56, Kiryl Shutsemau wrote:\n>> > With the upcoming changes to HVO, a single page of tail struct pages\n>> > will be shared across all huge pages of the same order on a node. Since\n>> > huge pages on the same node may belong to different zones, the zone\n>> > information stored in shared tail page flags would be incorrect.\n>> > \n>> > Always fetch zone information from the head page, which has unique and\n>> > correct zone flags for each compound page.\n>> > \n>> > Signed-off-by: Kiryl Shutsemau <kas@kernel.org>\n>> > Acked-by: Zi Yan <ziy@nvidia.com>\n>> > ---\n>> >   include/linux/mmzone.h | 1 +\n>> >   1 file changed, 1 insertion(+)\n>> > \n>> > diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n>> > index be8ce40b5638..192143b5cdc0 100644\n>> > --- a/include/linux/mmzone.h\n>> > +++ b/include/linux/mmzone.h\n>> > @@ -1219,6 +1219,7 @@ static inline enum zone_type memdesc_zonenum(memdesc_flags_t flags)\n>> >   static inline enum zone_type page_zonenum(const struct page *page)\n>> >   {\n>> > +\tpage = compound_head(page);\n>> >   \treturn memdesc_zonenum(page->flags);\n>> \n>> We end up calling page_zonenum() without holding a reference.\n>> \n>> Given that _compound_head() does a READ_ONCE(), this should work even if we\n>> see concurrent page freeing etc.\n>> \n>> However, this change implies that we now perform a compound page lookup for\n>> every PageHighMem() [meh], page_zone() [quite some users in the buddy,\n>> including for pageblock access and page freeing].\n>> \n>> That's a nasty compromise for making HVO better? :)\n>> \n>> We should likely limit that special casing to kernels that really rquire it\n>> (HVO).\n> \n> I will add compound_info_has_mask() check.\n\nNot thrilled by this indeed. Would it be a problem to have the shared tail\npages per node+zone instead of just per node?\n\n\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn 2/16/26 10:06, David Hildenbrand (Arm) wrote:\n> On 2/16/26 00:13, Matthew Wilcox wrote:\n>> On Mon, Feb 02, 2026 at 03:56:24PM +0000, Kiryl Shutsemau wrote:\n>>> With the upcoming changes to HVO, a single page of tail struct pages\n>>> will be shared across all huge pages of the same order on a node. Since\n>>> huge pages on the same node may belong to different zones, the zone\n>>> information stored in shared tail page flags would be incorrect.\n>>>\n>>> Always fetch zone information from the head page, which has unique and\n>>> correct zone flags for each compound page.\n>> \n>> You're right that different pages in the same folio can have different\n>> zone number.  But does it matter ... or to put it another way, why is\n>> returning the zone number of the head page the correct way to resolve\n>> this?\n> \n> How can a folio cross zones?\n> \n> Runtime allocated hugetlb folios from the CMA/buddy (alloc_contig_range) \n> definitely fall into a single zone.\n> \n> So is it about ones allocated early during boot, where, by chance, we \n> manage to cross ZONE_NORMAL + ZONE_MOVABLE etc?\n> \n> I thought that it's also not allowed there, and I wonder whether we \n> should disallow it if it's possible.\n\nI would be surprised if things didn't break horribly if we allowed crossing\nzones in a single folio. I'd rather not allow it.\n\n(And I still don't like how this patch solves the issue)\n\n>> \n>> Arguably, the caller is asking for the zone number of _this page_, and\n>> does not care about the zone number of the head page.  It would be good\n>> to have a short discussion of this in the commit message (but probably\n>> not worth putting this in a comment).\n> \n> Agreed, in particular, if there would be a functional change. So far I \n> assumed there would be no such change.\n> \n> Things like shrink_zone_span() really need to know the zone of that \n> page, not the one of the head; unless both fall into the same zone.\n>\n\n---\n\nOn 2/16/26 10:06, David Hildenbrand (Arm) wrote:\n> On 2/16/26 00:13, Matthew Wilcox wrote:\n>> On Mon, Feb 02, 2026 at 03:56:24PM +0000, Kiryl Shutsemau wrote:\n>>> With the upcoming changes to HVO, a single page of tail struct pages\n>>> will be shared across all huge pages of the same order on a node. Since\n>>> huge pages on the same node may belong to different zones, the zone\n>>> information stored in shared tail page flags would be incorrect.\n>>>\n>>> Always fetch zone information from the head page, which has unique and\n>>> correct zone flags for each compound page.\n>> \n>> You're right that different pages in the same folio can have different\n>> zone number.  But does it matter ... or to put it another way, why is\n>> returning the zone number of the head page the correct way to resolve\n>> this?\n> \n> How can a folio cross zones?\n> \n> Runtime allocated hugetlb folios from the CMA/buddy (alloc_contig_range) \n> definitely fall into a single zone.\n> \n> So is it about ones allocated early during boot, where, by chance, we \n> manage to cross ZONE_NORMAL + ZONE_MOVABLE etc?\n> \n> I thought that it's also not allowed there, and I wonder whether we \n> should disallow it if it's possible.\n\nI would be surprised if things didn't break horribly if we allowed crossing\nzones in a single folio. I'd rather not allow it.\n\n(And I still don't like how this patch solves the issue)\n\n>> \n>> Arguably, the caller is asking for the zone number of _this page_, and\n>> does not care about the zone number of the head page.  It would be good\n>> to have a short discussion of this in the commit message (but probably\n>> not worth putting this in a comment).\n> \n> Agreed, in particular, if there would be a functional change. So far I \n> assumed there would be no such change.\n> \n> Things like shrink_zone_span() really need to know the zone of that \n> page, not the one of the head; unless both fall into the same zone.\n> \n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-10",
          "message_id": ""
        }
      ],
      "analysis_source": "llm",
      "patch_summary": "This patch series removes 'fake head pages' from the HugeTLB vmemmap optimization by changing how tail pages encode their relationship to the head page, simplifying compound_head() and page_ref_add_unless(). The new approach uses a mask-based encoding for architectures where sizeof(struct page) is a power of 2, allowing shared read-only tail pages across huge pages on a NUMA node. This reduces complexity and overhead in the hot path, but testing has shown either no change or only slight performance improvement."
    },
    "2026-02-15": {
      "report_file": "2026-02-27_ollama_llama3.1-8b.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "Matthew Wilcox",
          "summary": "Matthew Wilcox questioned the correctness of always returning the zone number of the head page, suggesting that the caller may be asking for the zone number of 'this page', and proposed a discussion in the commit message to clarify this.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "questioning correctness",
            "requested clarification"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Mon, Feb 02, 2026 at 03:56:24PM +0000, Kiryl Shutsemau wrote:\n> With the upcoming changes to HVO, a single page of tail struct pages\n> will be shared across all huge pages of the same order on a node. Since\n> huge pages on the same node may belong to different zones, the zone\n> information stored in shared tail page flags would be incorrect.\n> \n> Always fetch zone information from the head page, which has unique and\n> correct zone flags for each compound page.\n\nYou're right that different pages in the same folio can have different\nzone number.  But does it matter ... or to put it another way, why is\nreturning the zone number of the head page the correct way to resolve\nthis?\n\nArguably, the caller is asking for the zone number of _this page_, and\ndoes not care about the zone number of the head page.  It would be good\nto have a short discussion of this in the commit message (but probably\nnot worth putting this in a comment).\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv\n\n---\n\nOn Mon, Feb 02, 2026 at 03:56:24PM +0000, Kiryl Shutsemau wrote:\n> With the upcoming changes to HVO, a single page of tail struct pages\n> will be shared across all huge pages of the same order on a node. Since\n> huge pages on the same node may belong to different zones, the zone\n> information stored in shared tail page flags would be incorrect.\n> \n> Always fetch zone information from the head page, which has unique and\n> correct zone flags for each compound page.\n\nYou're right that different pages in the same folio can have different\nzone number.  But does it matter ... or to put it another way, why is\nreturning the zone number of the head page the correct way to resolve\nthis?\n\nArguably, the caller is asking for the zone number of _this page_, and\ndoes not care about the zone number of the head page.  It would be good\nto have a short discussion of this in the commit message (but probably\nnot worth putting this in a comment).\n\n---\n\nOn Mon, Feb 16, 2026 at 10:06:57AM +0100, David Hildenbrand (Arm) wrote:\n> On 2/16/26 00:13, Matthew Wilcox wrote:\n> > On Mon, Feb 02, 2026 at 03:56:24PM +0000, Kiryl Shutsemau wrote:\n> > > With the upcoming changes to HVO, a single page of tail struct pages\n> > > will be shared across all huge pages of the same order on a node. Since\n> > > huge pages on the same node may belong to different zones, the zone\n> > > information stored in shared tail page flags would be incorrect.\n> > > \n> > > Always fetch zone information from the head page, which has unique and\n> > > correct zone flags for each compound page.\n> > \n> > You're right that different pages in the same folio can have different\n> > zone number.  But does it matter ... or to put it another way, why is\n> > returning the zone number of the head page the correct way to resolve\n> > this?\n> \n> How can a folio cross zones?\n\nI thought 1GB pages in hugetlb could cross zones?  Maybe that used to be\ntrue and isn't any more, or maybe it was never true and I was just\nconfused.\n\n---\n\nOn Mon, Feb 16, 2026 at 10:06:57AM +0100, David Hildenbrand (Arm) wrote:\n> On 2/16/26 00:13, Matthew Wilcox wrote:\n> > On Mon, Feb 02, 2026 at 03:56:24PM +0000, Kiryl Shutsemau wrote:\n> > > With the upcoming changes to HVO, a single page of tail struct pages\n> > > will be shared across all huge pages of the same order on a node. Since\n> > > huge pages on the same node may belong to different zones, the zone\n> > > information stored in shared tail page flags would be incorrect.\n> > > \n> > > Always fetch zone information from the head page, which has unique and\n> > > correct zone flags for each compound page.\n> > \n> > You're right that different pages in the same folio can have different\n> > zone number.  But does it matter ... or to put it another way, why is\n> > returning the zone number of the head page the correct way to resolve\n> > this?\n> \n> How can a folio cross zones?\n\nI thought 1GB pages in hugetlb could cross zones?  Maybe that used to be\ntrue and isn't any more, or maybe it was never true and I was just\nconfused.\n\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-15",
          "message_id": ""
        }
      ],
      "analysis_source": "llm",
      "patch_summary": "This patch series removes 'fake head pages' from the HugeTLB vmemmap optimization by changing how tail pages encode their relationship to the head page, simplifying compound_head() and page_ref_add_unless(). The new approach uses a mask-based encoding for architectures where sizeof(struct page) is a power of 2, allowing shared read-only tail pages across huge pages on a NUMA node. This reduces complexity and overhead in the hot path, but testing has shown either no change or only slight performance improvement."
    },
    "2026-02-23": {
      "report_file": "2026-02-27_ollama_llama3.1-8b.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "Frank Linden",
          "summary": "Frank Linden noted that HugeTLB folios could cross zones due to bootmem (memblock) allocated pages, which would cause issues when freeing and reinitializing the vmemmap for HVO. He recalled a patch (14ed3a595fa4) that fixed this issue by checking for zone intersections in hugetlb allocation.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "acknowledgment"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Mon, Feb 23, 2026 at 11:32AM David Hildenbrand (Arm)\n<david@kernel.org> wrote:\n>\n> On 2/23/26 19:18, Matthew Wilcox wrote:\n> > On Mon, Feb 16, 2026 at 10:06:57AM +0100, David Hildenbrand (Arm) wrote:\n> >> On 2/16/26 00:13, Matthew Wilcox wrote:\n> >>>\n> >>> You're right that different pages in the same folio can have different\n> >>> zone number.  But does it matter ... or to put it another way, why is\n> >>> returning the zone number of the head page the correct way to resolve\n> >>> this?\n> >>\n> >> How can a folio cross zones?\n> >\n> > I thought 1GB pages in hugetlb could cross zones?  Maybe that used to be\n> > true and isn't any more, or maybe it was never true and I was just\n> > confused.\n>\n> I recall that 1G folios could end up in ZONE_MOVABLE (comment in\n> page_is_unmovable()), but my memory is fuzzy when it comes to crossing\n> zones (ZONE_NORMAL -> ZONE_MOVABLE).\n>\n> Freeing+reinitializing the vmemmap for HVO with such folios would\n> already be problematic I suppose: we would silently switch the zone for\n> some of these pages.\n>\n> When freeing such (boottime) hugetlb folios to the buddy, we use\n> free_frozen_pages(). In there we lookup the zone once.\n>\n> Likely also problematic :)\n\nHugeTLB folios weren't supposed to cross zones, but they could do that\nin some cases for bootmem (memblock) allocated pages, causing the\nissue you describe.\n\nI fixed that with 14ed3a595fa4 (\"mm/hugetlb: check bootmem pages for\nzone intersections\"), so they won't cross zones anymore. The other\nallocation methods used for HugeTLB folios, alloc_contig_pages() and\ncma_alloc_folio, won't return anything that crosses a zone boundary by\ntheir nature.\n\nSo I think that's all good.\n\n- Frank\n\n---\n\nOn Mon, Feb 23, 2026 at 11:32AM David Hildenbrand (Arm)\n<david@kernel.org> wrote:\n>\n> On 2/23/26 19:18, Matthew Wilcox wrote:\n> > On Mon, Feb 16, 2026 at 10:06:57AM +0100, David Hildenbrand (Arm) wrote:\n> >> On 2/16/26 00:13, Matthew Wilcox wrote:\n> >>>\n> >>> You're right that different pages in the same folio can have different\n> >>> zone number.  But does it matter ... or to put it another way, why is\n> >>> returning the zone number of the head page the correct way to resolve\n> >>> this?\n> >>\n> >> How can a folio cross zones?\n> >\n> > I thought 1GB pages in hugetlb could cross zones?  Maybe that used to be\n> > true and isn't any more, or maybe it was never true and I was just\n> > confused.\n>\n> I recall that 1G folios could end up in ZONE_MOVABLE (comment in\n> page_is_unmovable()), but my memory is fuzzy when it comes to crossing\n> zones (ZONE_NORMAL -> ZONE_MOVABLE).\n>\n> Freeing+reinitializing the vmemmap for HVO with such folios would\n> already be problematic I suppose: we would silently switch the zone for\n> some of these pages.\n>\n> When freeing such (boottime) hugetlb folios to the buddy, we use\n> free_frozen_pages(). In there we lookup the zone once.\n>\n> Likely also problematic :)\n\nHugeTLB folios weren't supposed to cross zones, but they could do that\nin some cases for bootmem (memblock) allocated pages, causing the\nissue you describe.\n\nI fixed that with 14ed3a595fa4 (\"mm/hugetlb: check bootmem pages for\nzone intersections\"), so they won't cross zones anymore. The other\nallocation methods used for HugeTLB folios, alloc_contig_pages() and\ncma_alloc_folio, won't return anything that crosses a zone boundary by\ntheir nature.\n\nSo I think that's all good.\n\n- Frank\n\n_______________________________________________\nlinux-riscv mailing list\nlinux-riscv@lists.infradead.org\nhttp://lists.infradead.org/mailman/listinfo/linux-riscv",
          "reply_to": "",
          "message_date": "2026-02-23",
          "message_id": ""
        }
      ],
      "analysis_source": "llm",
      "patch_summary": "This patch series removes 'fake head pages' from the HugeTLB vmemmap optimization by changing how tail pages encode their relationship to the head page, simplifying compound_head() and page_ref_add_unless(). The new approach uses a mask-based encoding for architectures where sizeof(struct page) is a power of 2, allowing shared read-only tail pages across huge pages on a NUMA node. This reduces complexity and overhead in the hot path, but testing has shown either no change or only slight performance improvement."
    }
  }
}