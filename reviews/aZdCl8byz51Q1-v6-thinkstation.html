<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Review Comments: Re: [LSF/MM/BPF TOPIC] 64k (or 16k) base page size on x86</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto,
                         "Helvetica Neue", Arial, sans-serif;
            background: #f5f5f5;
            color: #333;
            line-height: 1.6;
            padding: 20px;
            max-width: 900px;
            margin: 0 auto;
        }
        .home-link { margin-bottom: 12px; display: block; }
        .home-link a { color: #0366d6; text-decoration: none; font-size: 0.9em; }
        .home-link a:hover { text-decoration: underline; }

        h1 { font-size: 1.3em; margin-bottom: 2px; color: #1a1a1a; line-height: 1.3; }

        .lore-link { font-size: 0.85em; margin: 4px 0 6px; display: block; }
        .lore-link a { color: #0366d6; text-decoration: none; }
        .lore-link a:hover { text-decoration: underline; }

        .date-range {
            font-size: 0.8em;
            color: #888;
            margin-bottom: 16px;
        }
        .date-range a { color: #0366d6; text-decoration: none; }
        .date-range a:hover { text-decoration: underline; }

        /* thread-node scroll margin so the card isn't clipped at the top */
        .thread-node { scroll-margin-top: 8px; }

        /* ── Patch summary ──────────────────────────────────────────── */
        .patch-summary-block {
            background: #fff;
            border-radius: 8px;
            padding: 12px 16px;
            margin-bottom: 20px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            border-left: 3px solid #4a90d9;
        }
        .patch-summary-label {
            font-size: 0.72em;
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 0.06em;
            color: #4a90d9;
            margin-bottom: 4px;
        }
        .patch-summary-text {
            font-size: 0.88em;
            color: #444;
            line-height: 1.55;
        }

        /* ── Thread tree ────────────────────────────────────────────── */
        .thread-tree {
            display: flex;
            flex-direction: column;
            gap: 6px;
        }

        /* Depth indentation via left border */
        .thread-node { position: relative; }
        .thread-children {
            margin-left: 20px;
            padding-left: 12px;
            border-left: 2px solid #e0e0e0;
            margin-top: 6px;
            display: flex;
            flex-direction: column;
            gap: 6px;
        }

        /* ── Review comment card ────────────────────────────────────── */
        .review-comment {
            background: #fff;
            border-radius: 6px;
            padding: 10px 14px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            font-size: 0.88em;
        }
        .review-comment-header {
            display: flex;
            flex-wrap: wrap;
            align-items: center;
            gap: 6px;
            margin-bottom: 5px;
        }
        .review-author {
            font-weight: 700;
            color: #1a1a1a;
            font-size: 0.95em;
        }

        /* Date chip — links back to the daily report */
        .date-chip {
            font-size: 0.75em;
            color: #777;
            background: #f0f0f0;
            border-radius: 10px;
            padding: 1px 7px;
            text-decoration: none;
            white-space: nowrap;
        }
        a.date-chip:hover { background: #e0e8f5; color: #0366d6; }

        .badge {
            display: inline-block;
            padding: 1px 8px;
            border-radius: 10px;
            font-size: 0.75em;
            font-weight: 600;
        }
        .inline-review-badge {
            display: inline-block;
            padding: 0 6px;
            border-radius: 8px;
            font-size: 0.78em;
            font-weight: 500;
            background: #e3f2fd;
            color: #1565c0;
        }
        .review-tag-badge {
            display: inline-block;
            padding: 0 6px;
            border-radius: 8px;
            font-size: 0.78em;
            font-weight: 500;
            background: #e8f5e9;
            color: #2e7d32;
        }
        .analysis-source-badge {
            display: inline-block;
            padding: 1px 7px;
            border-radius: 10px;
            font-size: 0.72em;
            font-weight: 600;
            border: 1px solid rgba(0,0,0,0.1);
        }

        .review-comment-text {
            color: #444;
            line-height: 1.55;
            margin-bottom: 4px;
        }
        .review-comment-signals {
            margin-top: 3px;
            font-size: 0.85em;
            color: #aaa;
            font-style: italic;
        }

        /* ── Collapsible raw body ───────────────────────────────────── */
        .raw-body-toggle {
            margin-top: 5px;
            font-size: 0.85em;
        }
        .raw-body-toggle summary {
            cursor: pointer;
            color: #888;
            padding: 2px 0;
            font-weight: 500;
            font-size: 0.9em;
            list-style: none;
        }
        .raw-body-toggle summary::-webkit-details-marker { display: none; }
        .raw-body-toggle summary::before { content: "▶ "; font-size: 0.7em; }
        .raw-body-toggle[open] summary::before { content: "▼ "; }
        .raw-body-toggle summary:hover { color: #555; }
        .raw-body-text {
            white-space: pre-wrap;
            font-size: 0.95em;
            background: #f8f8f8;
            padding: 8px 10px;
            border-radius: 4px;
            max-height: 360px;
            overflow-y: auto;
            margin-top: 4px;
            line-height: 1.5;
            color: #444;
            border: 1px solid #e8e8e8;
        }

        .no-reviews {
            color: #aaa;
            font-size: 0.85em;
            font-style: italic;
            padding: 8px 0;
        }

        footer {
            text-align: center;
            color: #bbb;
            font-size: 0.78em;
            margin-top: 36px;
            padding: 16px;
        }
    </style>
</head>
<body>
    <div class="home-link"><a href="../">&larr; Back to reports</a></div>
    <h1>Re: [LSF/MM/BPF TOPIC] 64k (or 16k) base page size on x86</h1>
    <div class="lore-link"><a href="https://lore.kernel.org/all/aZdCl8byz51Q1-v6@thinkstation/" target="_blank">View on lore.kernel.org &rarr;</a></div>
    <div class="date-range">Active on: <a href="#2026-02-19">2026-02-19</a></div>
    
    <div class="thread-tree">
<div class="thread-node depth-0" id="2026-02-19">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Peter Zijlstra</span>
<a class="date-chip" href="../2026-02-19_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-19">2026-02-19</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Peter Zijlstra pointed out that a similar approach was already taken by Andrew Morton, linking to an article from LWN</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Thu, Feb 19, 2026 at 03:08:51PM +0000, Kiryl Shutsemau wrote:
&gt; No, there&#x27;s no new hardware (that I know of). I want to explore what page size
&gt; means.
&gt; 
&gt; The kernel uses the same value - PAGE_SIZE - for two things:
&gt; 
&gt;   - the order-0 buddy allocation size;
&gt; 
&gt;   - the granularity of virtual address space mapping;
&gt; 
&gt; I think we can benefit from separating these two meanings and allowing
&gt; order-0 allocations to be larger than the virtual address space covered by a
&gt; PTE entry.

Didn&#x27;t AA do this a decade ago or somesuch?


---

On Thu, Feb 19, 2026 at 04:17:29PM +0100, Peter Zijlstra wrote:
&gt; On Thu, Feb 19, 2026 at 03:08:51PM +0000, Kiryl Shutsemau wrote:
&gt; &gt; No, there&#x27;s no new hardware (that I know of). I want to explore what page size
&gt; &gt; means.
&gt; &gt; 
&gt; &gt; The kernel uses the same value - PAGE_SIZE - for two things:
&gt; &gt; 
&gt; &gt;   - the order-0 buddy allocation size;
&gt; &gt; 
&gt; &gt;   - the granularity of virtual address space mapping;
&gt; &gt; 
&gt; &gt; I think we can benefit from separating these two meanings and allowing
&gt; &gt; order-0 allocations to be larger than the virtual address space covered by a
&gt; &gt; PTE entry.
&gt; 
&gt; Didn&#x27;t AA do this a decade ago or somesuch?

  https://lwn.net/Articles/240914/
</pre>
</details>
<div class="review-comment-signals">Signals: linking to external resource</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Kiryl Shutsemau (author)</span>
<a class="date-chip" href="../2026-02-19_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-19">2026-02-19</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The reviewer, Kiryl Shutsemau, raised several concerns and questions about the patch to introduce a 64k or 16k base page size on x86. They questioned whether emulation would help zone lock contention and expressed skepticism about serving 1G pages out of the buddy allocator with 4k order-0. They also mentioned that waste of memory for page tables is solvable and suggested switching to slab.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Thu, Feb 19, 2026 at 04:20:45PM +0100, Peter Zijlstra wrote:
&gt; On Thu, Feb 19, 2026 at 04:17:29PM +0100, Peter Zijlstra wrote:
&gt; &gt; On Thu, Feb 19, 2026 at 03:08:51PM +0000, Kiryl Shutsemau wrote:
&gt; &gt; &gt; No, there&#x27;s no new hardware (that I know of). I want to explore what page size
&gt; &gt; &gt; means.
&gt; &gt; &gt; 
&gt; &gt; &gt; The kernel uses the same value - PAGE_SIZE - for two things:
&gt; &gt; &gt; 
&gt; &gt; &gt;   - the order-0 buddy allocation size;
&gt; &gt; &gt; 
&gt; &gt; &gt;   - the granularity of virtual address space mapping;
&gt; &gt; &gt; 
&gt; &gt; &gt; I think we can benefit from separating these two meanings and allowing
&gt; &gt; &gt; order-0 allocations to be larger than the virtual address space covered by a
&gt; &gt; &gt; PTE entry.
&gt; &gt; 
&gt; &gt; Didn&#x27;t AA do this a decade ago or somesuch?
&gt; 
&gt;   https://lwn.net/Articles/240914/

Oh, 2007. It predates me in kernel. Will read up. Thanks!

-- 
  Kiryl Shutsemau / Kirill A. Shutemov


---

On Thu, Feb 19, 2026 at 03:33:47PM +0000, Pedro Falcato wrote:
&gt; On Thu, Feb 19, 2026 at 03:08:51PM +0000, Kiryl Shutsemau wrote:
&gt; &gt; No, there&#x27;s no new hardware (that I know of). I want to explore what page size
&gt; &gt; means.
&gt; &gt; 
&gt; &gt; The kernel uses the same value - PAGE_SIZE - for two things:
&gt; &gt; 
&gt; &gt;   - the order-0 buddy allocation size;
&gt; &gt; 
&gt; &gt;   - the granularity of virtual address space mapping;
&gt; &gt; 
&gt; &gt; I think we can benefit from separating these two meanings and allowing
&gt; &gt; order-0 allocations to be larger than the virtual address space covered by a
&gt; &gt; PTE entry.
&gt; &gt;
&gt; 
&gt; Doesn&#x27;t this idea make less sense these days, with mTHP? Simply by toggling one
&gt; of the entries in /sys/kernel/mm/transparent_hugepage.

mTHP is still best effort. This is way you don&#x27;t need to care about
fragmentation, you will get your 64k page as long as you have free
memory.

&gt; &gt; The main motivation is scalability. Managing memory on multi-terabyte
&gt; &gt; machines in 4k is suboptimal, to say the least.
&gt; &gt; 
&gt; &gt; Potential benefits of the approach (assuming 64k pages):
&gt; &gt; 
&gt; &gt;   - The order-0 page size cuts struct page overhead by a factor of 16. From
&gt; &gt;     ~1.6% of RAM to ~0.1%;
&gt; &gt; 
&gt; &gt;   - TLB wins on machines with TLB coalescing as long as mapping is naturally
&gt; &gt;     aligned;
&gt; &gt; 
&gt; &gt;   - Order-5 allocation is 2M, resulting in less pressure on the zone lock;
&gt; &gt; 
&gt; &gt;   - 1G pages are within possibility for the buddy allocator - order-14
&gt; &gt;     allocation. It can open the road to 1G THPs.
&gt; &gt; 
&gt; &gt;   - As with THP, fewer pages - less pressure on the LRU lock;
&gt; 
&gt; We could perhaps add a way to enforce a min_order globally on the page cache,
&gt; as a way to address it.

Raising min_order is not free. I puts more pressure on page allocator.

&gt; There are some points there which aren&#x27;t addressed by mTHP work in any way
&gt; (1G THPs for one), others which are being addressed separately (memdesc work
&gt; trying to cut down on struct page overhead).
&gt; 
&gt; (I also don&#x27;t understand your point about order-5 allocation, AFAIK pcp will
&gt; cache up to COSTLY_ORDER (3) and PMD order, but I&#x27;m probably not seeing the
&gt; full picture)

With higher base page size, page allocator doesn&#x27;t need to do as much
work to merge/split buddy pages. So serving the same 2M as order-5 is
cheaper than order-9.

-- 
  Kiryl Shutsemau / Kirill A. Shutemov


---

On Thu, Feb 19, 2026 at 04:39:34PM +0100, David Hildenbrand (Arm) wrote:
&gt; On 2/19/26 16:08, Kiryl Shutsemau wrote:
&gt; &gt; No, there&#x27;s no new hardware (that I know of). I want to explore what page size
&gt; &gt; means.
&gt; &gt; 
&gt; &gt; The kernel uses the same value - PAGE_SIZE - for two things:
&gt; &gt; 
&gt; &gt;    - the order-0 buddy allocation size;
&gt; &gt; 
&gt; &gt;    - the granularity of virtual address space mapping;
&gt; &gt; 
&gt; &gt; I think we can benefit from separating these two meanings and allowing
&gt; &gt; order-0 allocations to be larger than the virtual address space covered by a
&gt; &gt; PTE entry.
&gt; &gt; 
&gt; &gt; The main motivation is scalability. Managing memory on multi-terabyte
&gt; &gt; machines in 4k is suboptimal, to say the least.
&gt; &gt; 
&gt; &gt; Potential benefits of the approach (assuming 64k pages):
&gt; &gt; 
&gt; &gt;    - The order-0 page size cuts struct page overhead by a factor of 16. From
&gt; &gt;      ~1.6% of RAM to ~0.1%;
&gt; &gt; 
&gt; &gt;    - TLB wins on machines with TLB coalescing as long as mapping is naturally
&gt; &gt;      aligned;
&gt; &gt; 
&gt; &gt;    - Order-5 allocation is 2M, resulting in less pressure on the zone lock;
&gt; &gt; 
&gt; &gt;    - 1G pages are within possibility for the buddy allocator - order-14
&gt; &gt;      allocation. It can open the road to 1G THPs.
&gt; &gt; 
&gt; &gt;    - As with THP, fewer pages - less pressure on the LRU lock;
&gt; &gt; 
&gt; &gt;    - ...
&gt; &gt; 
&gt; &gt; The trade-off is memory waste (similar to what we have on architectures with
&gt; &gt; native 64k pages today) and complexity, mostly in the core-MM code.
&gt; &gt; 
&gt; &gt; == Design considerations ==
&gt; &gt; 
&gt; &gt; I want to split PAGE_SIZE into two distinct values:
&gt; &gt; 
&gt; &gt;    - PTE_SIZE defines the virtual address space granularity;
&gt; &gt; 
&gt; &gt;    - PG_SIZE defines the size of the order-0 buddy allocation;
&gt; &gt; 
&gt; &gt; PAGE_SIZE is only defined if PTE_SIZE == PG_SIZE. It will flag which code
&gt; &gt; requires conversion, and keep existing code working while conversion is in
&gt; &gt; progress.
&gt; &gt; 
&gt; &gt; The same split happens for other page-related macros: mask, shift,
&gt; &gt; alignment helpers, etc.
&gt; &gt; 
&gt; &gt; PFNs are in PTE_SIZE units.
&gt; &gt; 
&gt; &gt; The buddy allocator and page cache (as well as all I/O) operate in PG_SIZE
&gt; &gt; units.
&gt; &gt; 
&gt; &gt; Userspace mappings are maintained with PTE_SIZE granularity. No ABI changes
&gt; &gt; for userspace. But we might want to communicate PG_SIZE to userspace to
&gt; &gt; get the optimal results for userspace that cares.
&gt; &gt; 
&gt; &gt; PTE_SIZE granularity requires a substantial rework of page fault and VMA
&gt; &gt; handling:
&gt; &gt; 
&gt; &gt;    - A struct page pointer and pgprot_t are not enough to create a PTE entry.
&gt; &gt;      We also need the offset within the page we are creating the PTE for.
&gt; &gt; 
&gt; &gt;    - Since the VMA start can be aligned arbitrarily with respect to the
&gt; &gt;      underlying page, vma-&gt;vm_pgoff has to be changed to vma-&gt;vm_pteoff,
&gt; &gt;      which is in PTE_SIZE units.
&gt; &gt; 
&gt; &gt;    - The page fault handler needs to handle PTE_SIZE &lt; PG_SIZE, including
&gt; &gt;      misaligned cases;
&gt; &gt; 
&gt; &gt; Page faults into file mappings are relatively simple to handle as we
&gt; &gt; always have the page cache to refer to. So you can map only the part of the
&gt; &gt; page that fits in the page table, similarly to fault-around.
&gt; &gt; 
&gt; &gt; Anonymous and file-CoW faults should also be simple as long as the VMA is
&gt; &gt; aligned to PG_SIZE in both the virtual address space and with respect to
&gt; &gt; vm_pgoff. We might waste some memory on the ends of the VMA, but it is
&gt; &gt; tolerable.
&gt; &gt; 
&gt; &gt; Misaligned anonymous and file-CoW faults are a pain. Specifically, mapping
&gt; &gt; pages across a page table boundary. In the worst case, a page is mapped across
&gt; &gt; a PGD entry boundary and PTEs for the page have to be put in two separate
&gt; &gt; subtrees of page tables.
&gt; &gt; 
&gt; &gt; A naive implementation would map different pages on different sides of a
&gt; &gt; page table boundary and accept the waste of one page per page table crossing.
&gt; &gt; The hope is that misaligned mappings are rare, but this is suboptimal.
&gt; &gt; 
&gt; &gt; mremap(2) is the ultimate stress test for the design.
&gt; &gt; 
&gt; &gt; On x86, page tables are allocated from the buddy allocator and if PG_SIZE
&gt; &gt; is greater than 4 KB, we need a way to pack multiple page tables into a
&gt; &gt; single page. We could use the slab allocator for this, but it would
&gt; &gt; require relocating the page-table metadata out of struct page.
&gt; 
&gt; When discussing per-process page sizes with Ryan and Dev, I mentioned that
&gt; having a larger emulated page size could be interesting for other
&gt; architectures as well.
&gt; 
&gt; That is, we would emulate a 64K page size on Intel for user space as well,
&gt; but let the OS work with 4K pages.
&gt; 
&gt; We&#x27;d only allocate+map large folios into user space + pagecache, but still
&gt; allow for page tables etc. to not waste memory.
&gt; 
&gt; So &quot;most&quot; of your allocations in the system would actually be at least 64k,
&gt; reducing zone lock contention etc.

I am not convinced emulation would help zone lock contention. I expect
contention to be higher if page allocator would see a mix of 4k and 64k
requests. It sounds like constant split/merge under the lock.

&gt; It doesn&#x27;t solve all the problems you wanted to tackle on your list (e.g.,
&gt; &quot;struct page&quot; overhead, which will be sorted out by memdescs).

I don&#x27;t think we can serve 1G pages out of buddy allocator with 4k
order-0. And without it, I don&#x27;t see how to get to a viable 1G THPs.

-- 
  Kiryl Shutsemau / Kirill A. Shutemov


---

On Thu, Feb 19, 2026 at 04:39:34PM +0100, David Hildenbrand (Arm) wrote:
&gt; On 2/19/26 16:08, Kiryl Shutsemau wrote:
&gt; &gt; No, there&#x27;s no new hardware (that I know of). I want to explore what page size
&gt; &gt; means.
&gt; &gt; 
&gt; &gt; The kernel uses the same value - PAGE_SIZE - for two things:
&gt; &gt; 
&gt; &gt;    - the order-0 buddy allocation size;
&gt; &gt; 
&gt; &gt;    - the granularity of virtual address space mapping;
&gt; &gt; 
&gt; &gt; I think we can benefit from separating these two meanings and allowing
&gt; &gt; order-0 allocations to be larger than the virtual address space covered by a
&gt; &gt; PTE entry.
&gt; &gt; 
&gt; &gt; The main motivation is scalability. Managing memory on multi-terabyte
&gt; &gt; machines in 4k is suboptimal, to say the least.
&gt; &gt; 
&gt; &gt; Potential benefits of the approach (assuming 64k pages):
&gt; &gt; 
&gt; &gt;    - The order-0 page size cuts struct page overhead by a factor of 16. From
&gt; &gt;      ~1.6% of RAM to ~0.1%;
&gt; &gt; 
&gt; &gt;    - TLB wins on machines with TLB coalescing as long as mapping is naturally
&gt; &gt;      aligned;
&gt; &gt; 
&gt; &gt;    - Order-5 allocation is 2M, resulting in less pressure on the zone lock;
&gt; &gt; 
&gt; &gt;    - 1G pages are within possibility for the buddy allocator - order-14
&gt; &gt;      allocation. It can open the road to 1G THPs.
&gt; &gt; 
&gt; &gt;    - As with THP, fewer pages - less pressure on the LRU lock;
&gt; &gt; 
&gt; &gt;    - ...
&gt; &gt; 
&gt; &gt; The trade-off is memory waste (similar to what we have on architectures with
&gt; &gt; native 64k pages today) and complexity, mostly in the core-MM code.
&gt; &gt; 
&gt; &gt; == Design considerations ==
&gt; &gt; 
&gt; &gt; I want to split PAGE_SIZE into two distinct values:
&gt; &gt; 
&gt; &gt;    - PTE_SIZE defines the virtual address space granularity;
&gt; &gt; 
&gt; &gt;    - PG_SIZE defines the size of the order-0 buddy allocation;
&gt; &gt; 
&gt; &gt; PAGE_SIZE is only defined if PTE_SIZE == PG_SIZE. It will flag which code
&gt; &gt; requires conversion, and keep existing code working while conversion is in
&gt; &gt; progress.
&gt; &gt; 
&gt; &gt; The same split happens for other page-related macros: mask, shift,
&gt; &gt; alignment helpers, etc.
&gt; &gt; 
&gt; &gt; PFNs are in PTE_SIZE units.
&gt; &gt; 
&gt; &gt; The buddy allocator and page cache (as well as all I/O) operate in PG_SIZE
&gt; &gt; units.
&gt; &gt; 
&gt; &gt; Userspace mappings are maintained with PTE_SIZE granularity. No ABI changes
&gt; &gt; for userspace. But we might want to communicate PG_SIZE to userspace to
&gt; &gt; get the optimal results for userspace that cares.
&gt; &gt; 
&gt; &gt; PTE_SIZE granularity requires a substantial rework of page fault and VMA
&gt; &gt; handling:
&gt; &gt; 
&gt; &gt;    - A struct page pointer and pgprot_t are not enough to create a PTE entry.
&gt; &gt;      We also need the offset within the page we are creating the PTE for.
&gt; &gt; 
&gt; &gt;    - Since the VMA start can be aligned arbitrarily with respect to the
&gt; &gt;      underlying page, vma-&gt;vm_pgoff has to be changed to vma-&gt;vm_pteoff,
&gt; &gt;      which is in PTE_SIZE units.
&gt; &gt; 
&gt; &gt;    - The page fault handler needs to handle PTE_SIZE &lt; PG_SIZE, including
&gt; &gt;      misaligned cases;
&gt; &gt; 
&gt; &gt; Page faults into file mappings are relatively simple to handle as we
&gt; &gt; always have the page cache to refer to. So you can map only the part of the
&gt; &gt; page that fits in the page table, similarly to fault-around.
&gt; &gt; 
&gt; &gt; Anonymous and file-CoW faults should also be simple as long as the VMA is
&gt; &gt; aligned to PG_SIZE in both the virtual address space and with respect to
&gt; &gt; vm_pgoff. We might waste some memory on the ends of the VMA, but it is
&gt; &gt; tolerable.
&gt; &gt; 
&gt; &gt; Misaligned anonymous and file-CoW faults are a pain. Specifically, mapping
&gt; &gt; pages across a page table boundary. In the worst case, a page is mapped across
&gt; &gt; a PGD entry boundary and PTEs for the page have to be put in two separate
&gt; &gt; subtrees of page tables.
&gt; &gt; 
&gt; &gt; A naive implementation would map different pages on different sides of a
&gt; &gt; page table boundary and accept the waste of one page per page table crossing.
&gt; &gt; The hope is that misaligned mappings are rare, but this is suboptimal.
&gt; &gt; 
&gt; &gt; mremap(2) is the ultimate stress test for the design.
&gt; &gt; 
&gt; &gt; On x86, page tables are allocated from the buddy allocator and if PG_SIZE
&gt; &gt; is greater than 4 KB, we need a way to pack multiple page tables into a
&gt; &gt; single page. We could use the slab allocator for this, but it would
&gt; &gt; require relocating the page-table metadata out of struct page.
&gt; 
&gt; When discussing per-process page sizes with Ryan and Dev, I mentioned that
&gt; having a larger emulated page size could be interesting for other
&gt; architectures as well.
&gt;
&gt; That is, we would emulate a 64K page size on Intel for user space as well,
&gt; but let the OS work with 4K pages.

Just to clarify, do you want it to be enforced on userspace ABI.
Like, all mappings are 64k aligned?

&gt; We&#x27;d only allocate+map large folios into user space + pagecache, but still
&gt; allow for page tables etc. to not waste memory.

Waste of memory for page table is solvable and pretty straight forward.
Most of such cases can be solve mechanically by switching to slab.

-- 
  Kiryl Shutsemau / Kirill A. Shutemov


---

On Thu, Feb 19, 2026 at 09:08:57AM -0800, Dave Hansen wrote:
&gt; On 2/19/26 07:08, Kiryl Shutsemau wrote:
&gt; &gt;   - The order-0 page size cuts struct page overhead by a factor of 16. From
&gt; &gt;     ~1.6% of RAM to ~0.1%;
&gt; ...
&gt; But, it will mostly be getting better performance at the _cost_ of
&gt; consuming more RAM, not saving RAM.

That&#x27;s fair.

The problem with struct page memory consumption is that it is static and
cannot be reclaimed. You pay the struct page tax no matter what.

Page cache rounding overhead can be large, but a motivated userspace can
keep it under control by avoiding splitting a dataset into many small
files. And this memory is reclaimable.

-- 
  Kiryl Shutsemau / Kirill A. Shutemov


---

On Thu, Feb 19, 2026 at 09:30:36AM -0800, Dave Hansen wrote:
&gt; On 2/19/26 07:08, Kiryl Shutsemau wrote:
&gt; ...
&gt; &gt; The patchset is large:
&gt; &gt; 
&gt; &gt;  378 files changed, 3348 insertions(+), 3102 deletions(-)
&gt; 
&gt; A few notes about the diffstats:
&gt; 
&gt; $ git diff v6.17..HEAD arch/x86 | diffstat | tail -1
&gt;  105 files changed, 874 insertions(+), 843 deletions(-)
&gt; $ git diff v6.17..HEAD mm | diffstat | tail -1
&gt;  53 files changed, 1136 insertions(+), 1069 deletions(-)
&gt; 
&gt; The vast, vast majority of this seems to be the renames. Stuff like:
&gt; 
&gt; &gt; -               new = round_down(new, PAGE_SIZE);
&gt; &gt; +               new = round_down(new, PTE_SIZE);
&gt; 
&gt; or even less worrying:
&gt; 
&gt; &gt; -int set_direct_map_valid_noflush(struct page *page, unsigned nr, bool valid);
&gt; &gt; +int set_direct_map_valid_noflush(struct page *page, unsigned numpages, bool valid);
&gt; 
&gt; That stuff obviously needs to be audited but it&#x27;s far less concerning
&gt; than the logic changes.
&gt; 
&gt; So just for review sanity, if you go forward with this, I&#x27;d very much
&gt; appreciate a strong separation of the purely mechanical bits from any
&gt; logic changes.

That&#x27;s the plan. That&#x27;s the only way I can keep myself sane :P

&gt; &gt; On x86, page tables are allocated from the buddy allocator and if PG_SIZE
&gt; &gt; is greater than 4 KB, we need a way to pack multiple page tables into a
&gt; &gt; single page. We could use the slab allocator for this, but it would
&gt; &gt; require relocating the page-table metadata out of struct page.
&gt; 
&gt; Others mentioned this, but I think this essentially gates what you are
&gt; doing behind a full tree conversion over to ptdescs.

I have not followed ptdescs closely. Need to catch up.

For PoC, I will just waste full order-0 page for page table. Packing is
not required for correctness.

&gt; The most useful thing we can do with this series is look at it and
&gt; decide what _other_ things need to get done before the tree could
&gt; possibly go in that direction, like ptdesc or a the disambiguation
&gt; between PTE_SIZE and PG_SIZE that you&#x27;ve kicked off here.

Right.

-- 
  Kiryl Shutsemau / Kirill A. Shutemov


---

On Thu, Feb 19, 2026 at 05:47:22PM +0000, Matthew Wilcox wrote:
&gt; On Thu, Feb 19, 2026 at 03:08:51PM +0000, Kiryl Shutsemau wrote:
&gt; &gt; On x86, page tables are allocated from the buddy allocator and if PG_SIZE
&gt; &gt; is greater than 4 KB, we need a way to pack multiple page tables into a
&gt; &gt; single page. We could use the slab allocator for this, but it would
&gt; &gt; require relocating the page-table metadata out of struct page.
&gt; 
&gt; Have you looked at the s390/ppc implementations (yes, they&#x27;re different,
&gt; no, that sucks)? 

No, will check it out tomorrow.

&gt; slab seems like the wrong approach to me.

I was the first thing that came to mind. I have not put much time into
it

&gt; There&#x27;s a third approach that I&#x27;ve never looked at which is to allocate
&gt; the larger size, then just use it for N consecutive entries.

Yeah, that&#x27;s a possible way. We would need to populate 16 page table
entries of the parent page table. But you don&#x27;t need to care about
fragmentation within the page.

-- 
  Kiryl Shutsemau / Kirill A. Shutemov
</pre>
</details>
<div class="review-comment-signals">Signals: skepticism, requested changes</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Pedro Falcato</span>
<a class="date-chip" href="../2026-02-19_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-19">2026-02-19</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The reviewer, Pedro Falcato, questioned the relevance of the proposed patch in light of modern memory management techniques such as mTHP and suggested alternative solutions to address the issues raised.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Thu, Feb 19, 2026 at 03:08:51PM +0000, Kiryl Shutsemau wrote:
&gt; No, there&#x27;s no new hardware (that I know of). I want to explore what page size
&gt; means.
&gt; 
&gt; The kernel uses the same value - PAGE_SIZE - for two things:
&gt; 
&gt;   - the order-0 buddy allocation size;
&gt; 
&gt;   - the granularity of virtual address space mapping;
&gt; 
&gt; I think we can benefit from separating these two meanings and allowing
&gt; order-0 allocations to be larger than the virtual address space covered by a
&gt; PTE entry.
&gt;

Doesn&#x27;t this idea make less sense these days, with mTHP? Simply by toggling one
of the entries in /sys/kernel/mm/transparent_hugepage.
 
&gt; The main motivation is scalability. Managing memory on multi-terabyte
&gt; machines in 4k is suboptimal, to say the least.
&gt; 
&gt; Potential benefits of the approach (assuming 64k pages):
&gt; 
&gt;   - The order-0 page size cuts struct page overhead by a factor of 16. From
&gt;     ~1.6% of RAM to ~0.1%;
&gt; 
&gt;   - TLB wins on machines with TLB coalescing as long as mapping is naturally
&gt;     aligned;
&gt; 
&gt;   - Order-5 allocation is 2M, resulting in less pressure on the zone lock;
&gt; 
&gt;   - 1G pages are within possibility for the buddy allocator - order-14
&gt;     allocation. It can open the road to 1G THPs.
&gt; 
&gt;   - As with THP, fewer pages - less pressure on the LRU lock;

We could perhaps add a way to enforce a min_order globally on the page cache,
as a way to address it.

There are some points there which aren&#x27;t addressed by mTHP work in any way
(1G THPs for one), others which are being addressed separately (memdesc work
trying to cut down on struct page overhead).

(I also don&#x27;t understand your point about order-5 allocation, AFAIK pcp will
cache up to COSTLY_ORDER (3) and PMD order, but I&#x27;m probably not seeing the
full picture)


-- 
Pedro


---

On Thu, Feb 19, 2026 at 04:53:10PM +0100, David Hildenbrand (Arm) wrote:
&gt; On 2/19/26 16:50, Kiryl Shutsemau wrote:
&gt; &gt; On Thu, Feb 19, 2026 at 03:33:47PM +0000, Pedro Falcato wrote:
&gt; &gt; &gt; On Thu, Feb 19, 2026 at 03:08:51PM +0000, Kiryl Shutsemau wrote:
&gt; &gt; &gt; &gt; No, there&#x27;s no new hardware (that I know of). I want to explore what page size
&gt; &gt; &gt; &gt; means.
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; The kernel uses the same value - PAGE_SIZE - for two things:
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt;    - the order-0 buddy allocation size;
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt;    - the granularity of virtual address space mapping;
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; I think we can benefit from separating these two meanings and allowing
&gt; &gt; &gt; &gt; order-0 allocations to be larger than the virtual address space covered by a
&gt; &gt; &gt; &gt; PTE entry.
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; 
&gt; &gt; &gt; Doesn&#x27;t this idea make less sense these days, with mTHP? Simply by toggling one
&gt; &gt; &gt; of the entries in /sys/kernel/mm/transparent_hugepage.
&gt; &gt; 
&gt; &gt; mTHP is still best effort. This is way you don&#x27;t need to care about
&gt; &gt; fragmentation, you will get your 64k page as long as you have free
&gt; &gt; memory.
&gt; &gt; 
&gt; &gt; &gt; &gt; The main motivation is scalability. Managing memory on multi-terabyte
&gt; &gt; &gt; &gt; machines in 4k is suboptimal, to say the least.
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; Potential benefits of the approach (assuming 64k pages):
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt;    - The order-0 page size cuts struct page overhead by a factor of 16. From
&gt; &gt; &gt; &gt;      ~1.6% of RAM to ~0.1%;
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt;    - TLB wins on machines with TLB coalescing as long as mapping is naturally
&gt; &gt; &gt; &gt;      aligned;
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt;    - Order-5 allocation is 2M, resulting in less pressure on the zone lock;
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt;    - 1G pages are within possibility for the buddy allocator - order-14
&gt; &gt; &gt; &gt;      allocation. It can open the road to 1G THPs.
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt;    - As with THP, fewer pages - less pressure on the LRU lock;
&gt; &gt; &gt; 
&gt; &gt; &gt; We could perhaps add a way to enforce a min_order globally on the page cache,
&gt; &gt; &gt; as a way to address it.
&gt; &gt; 
&gt; &gt; Raising min_order is not free. I puts more pressure on page allocator.
&gt; &gt; 
&gt; &gt; &gt; There are some points there which aren&#x27;t addressed by mTHP work in any way
&gt; &gt; &gt; (1G THPs for one), others which are being addressed separately (memdesc work
&gt; &gt; &gt; trying to cut down on struct page overhead).
&gt; &gt; &gt; 
&gt; &gt; &gt; (I also don&#x27;t understand your point about order-5 allocation, AFAIK pcp will
&gt; &gt; &gt; cache up to COSTLY_ORDER (3) and PMD order, but I&#x27;m probably not seeing the
&gt; &gt; &gt; full picture)
&gt; &gt; 
&gt; &gt; With higher base page size, page allocator doesn&#x27;t need to do as much
&gt; &gt; work to merge/split buddy pages. So serving the same 2M as order-5 is
&gt; &gt; cheaper than order-9.
&gt; 
&gt; I think the idea is that if most of your allocations (anon + pagecache) are
&gt; 64k instead of 4k, on average, you&#x27;ll just naturally do less merging
&gt; splitting.

Yep. That plus slab_min_order would hopefully yield a system where 90%+
(depending on how your filesystem&#x27;s buffer cache works) allocations are 64K.

-- 
Pedro
</pre>
</details>
<div class="review-comment-signals">Signals: questioned relevance, suggested alternatives</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">David (Arm)</span>
<a class="date-chip" href="../2026-02-19_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-19">2026-02-19</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer David suggested emulating a larger page size for user space while still using smaller pages internally, which could reduce zone lock contention and other issues.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On 2/19/26 16:08, Kiryl Shutsemau wrote:
&gt; No, there&#x27;s no new hardware (that I know of). I want to explore what page size
&gt; means.
&gt; 
&gt; The kernel uses the same value - PAGE_SIZE - for two things:
&gt; 
&gt;    - the order-0 buddy allocation size;
&gt; 
&gt;    - the granularity of virtual address space mapping;
&gt; 
&gt; I think we can benefit from separating these two meanings and allowing
&gt; order-0 allocations to be larger than the virtual address space covered by a
&gt; PTE entry.
&gt; 
&gt; The main motivation is scalability. Managing memory on multi-terabyte
&gt; machines in 4k is suboptimal, to say the least.
&gt; 
&gt; Potential benefits of the approach (assuming 64k pages):
&gt; 
&gt;    - The order-0 page size cuts struct page overhead by a factor of 16. From
&gt;      ~1.6% of RAM to ~0.1%;
&gt; 
&gt;    - TLB wins on machines with TLB coalescing as long as mapping is naturally
&gt;      aligned;
&gt; 
&gt;    - Order-5 allocation is 2M, resulting in less pressure on the zone lock;
&gt; 
&gt;    - 1G pages are within possibility for the buddy allocator - order-14
&gt;      allocation. It can open the road to 1G THPs.
&gt; 
&gt;    - As with THP, fewer pages - less pressure on the LRU lock;
&gt; 
&gt;    - ...
&gt; 
&gt; The trade-off is memory waste (similar to what we have on architectures with
&gt; native 64k pages today) and complexity, mostly in the core-MM code.
&gt; 
&gt; == Design considerations ==
&gt; 
&gt; I want to split PAGE_SIZE into two distinct values:
&gt; 
&gt;    - PTE_SIZE defines the virtual address space granularity;
&gt; 
&gt;    - PG_SIZE defines the size of the order-0 buddy allocation;
&gt; 
&gt; PAGE_SIZE is only defined if PTE_SIZE == PG_SIZE. It will flag which code
&gt; requires conversion, and keep existing code working while conversion is in
&gt; progress.
&gt; 
&gt; The same split happens for other page-related macros: mask, shift,
&gt; alignment helpers, etc.
&gt; 
&gt; PFNs are in PTE_SIZE units.
&gt; 
&gt; The buddy allocator and page cache (as well as all I/O) operate in PG_SIZE
&gt; units.
&gt; 
&gt; Userspace mappings are maintained with PTE_SIZE granularity. No ABI changes
&gt; for userspace. But we might want to communicate PG_SIZE to userspace to
&gt; get the optimal results for userspace that cares.
&gt; 
&gt; PTE_SIZE granularity requires a substantial rework of page fault and VMA
&gt; handling:
&gt; 
&gt;    - A struct page pointer and pgprot_t are not enough to create a PTE entry.
&gt;      We also need the offset within the page we are creating the PTE for.
&gt; 
&gt;    - Since the VMA start can be aligned arbitrarily with respect to the
&gt;      underlying page, vma-&gt;vm_pgoff has to be changed to vma-&gt;vm_pteoff,
&gt;      which is in PTE_SIZE units.
&gt; 
&gt;    - The page fault handler needs to handle PTE_SIZE &lt; PG_SIZE, including
&gt;      misaligned cases;
&gt; 
&gt; Page faults into file mappings are relatively simple to handle as we
&gt; always have the page cache to refer to. So you can map only the part of the
&gt; page that fits in the page table, similarly to fault-around.
&gt; 
&gt; Anonymous and file-CoW faults should also be simple as long as the VMA is
&gt; aligned to PG_SIZE in both the virtual address space and with respect to
&gt; vm_pgoff. We might waste some memory on the ends of the VMA, but it is
&gt; tolerable.
&gt; 
&gt; Misaligned anonymous and file-CoW faults are a pain. Specifically, mapping
&gt; pages across a page table boundary. In the worst case, a page is mapped across
&gt; a PGD entry boundary and PTEs for the page have to be put in two separate
&gt; subtrees of page tables.
&gt; 
&gt; A naive implementation would map different pages on different sides of a
&gt; page table boundary and accept the waste of one page per page table crossing.
&gt; The hope is that misaligned mappings are rare, but this is suboptimal.
&gt; 
&gt; mremap(2) is the ultimate stress test for the design.
&gt; 
&gt; On x86, page tables are allocated from the buddy allocator and if PG_SIZE
&gt; is greater than 4 KB, we need a way to pack multiple page tables into a
&gt; single page. We could use the slab allocator for this, but it would
&gt; require relocating the page-table metadata out of struct page.

When discussing per-process page sizes with Ryan and Dev, I mentioned 
that having a larger emulated page size could be interesting for other 
architectures as well.

That is, we would emulate a 64K page size on Intel for user space as 
well, but let the OS work with 4K pages.

We&#x27;d only allocate+map large folios into user space + pagecache, but 
still allow for page tables etc. to not waste memory.

So &quot;most&quot; of your allocations in the system would actually be at least 
64k, reducing zone lock contention etc.


It doesn&#x27;t solve all the problems you wanted to tackle on your list 
(e.g., &quot;struct page&quot; overhead, which will be sorted out by memdescs).

-- 
Cheers,

David


---

On 2/19/26 16:50, Kiryl Shutsemau wrote:
&gt; On Thu, Feb 19, 2026 at 03:33:47PM +0000, Pedro Falcato wrote:
&gt;&gt; On Thu, Feb 19, 2026 at 03:08:51PM +0000, Kiryl Shutsemau wrote:
&gt;&gt;&gt; No, there&#x27;s no new hardware (that I know of). I want to explore what page size
&gt;&gt;&gt; means.
&gt;&gt;&gt;
&gt;&gt;&gt; The kernel uses the same value - PAGE_SIZE - for two things:
&gt;&gt;&gt;
&gt;&gt;&gt;    - the order-0 buddy allocation size;
&gt;&gt;&gt;
&gt;&gt;&gt;    - the granularity of virtual address space mapping;
&gt;&gt;&gt;
&gt;&gt;&gt; I think we can benefit from separating these two meanings and allowing
&gt;&gt;&gt; order-0 allocations to be larger than the virtual address space covered by a
&gt;&gt;&gt; PTE entry.
&gt;&gt;&gt;
&gt;&gt;
&gt;&gt; Doesn&#x27;t this idea make less sense these days, with mTHP? Simply by toggling one
&gt;&gt; of the entries in /sys/kernel/mm/transparent_hugepage.
&gt; 
&gt; mTHP is still best effort. This is way you don&#x27;t need to care about
&gt; fragmentation, you will get your 64k page as long as you have free
&gt; memory.
&gt; 
&gt;&gt;&gt; The main motivation is scalability. Managing memory on multi-terabyte
&gt;&gt;&gt; machines in 4k is suboptimal, to say the least.
&gt;&gt;&gt;
&gt;&gt;&gt; Potential benefits of the approach (assuming 64k pages):
&gt;&gt;&gt;
&gt;&gt;&gt;    - The order-0 page size cuts struct page overhead by a factor of 16. From
&gt;&gt;&gt;      ~1.6% of RAM to ~0.1%;
&gt;&gt;&gt;
&gt;&gt;&gt;    - TLB wins on machines with TLB coalescing as long as mapping is naturally
&gt;&gt;&gt;      aligned;
&gt;&gt;&gt;
&gt;&gt;&gt;    - Order-5 allocation is 2M, resulting in less pressure on the zone lock;
&gt;&gt;&gt;
&gt;&gt;&gt;    - 1G pages are within possibility for the buddy allocator - order-14
&gt;&gt;&gt;      allocation. It can open the road to 1G THPs.
&gt;&gt;&gt;
&gt;&gt;&gt;    - As with THP, fewer pages - less pressure on the LRU lock;
&gt;&gt;
&gt;&gt; We could perhaps add a way to enforce a min_order globally on the page cache,
&gt;&gt; as a way to address it.
&gt; 
&gt; Raising min_order is not free. I puts more pressure on page allocator.
&gt; 
&gt;&gt; There are some points there which aren&#x27;t addressed by mTHP work in any way
&gt;&gt; (1G THPs for one), others which are being addressed separately (memdesc work
&gt;&gt; trying to cut down on struct page overhead).
&gt;&gt;
&gt;&gt; (I also don&#x27;t understand your point about order-5 allocation, AFAIK pcp will
&gt;&gt; cache up to COSTLY_ORDER (3) and PMD order, but I&#x27;m probably not seeing the
&gt;&gt; full picture)
&gt; 
&gt; With higher base page size, page allocator doesn&#x27;t need to do as much
&gt; work to merge/split buddy pages. So serving the same 2M as order-5 is
&gt; cheaper than order-9.

I think the idea is that if most of your allocations (anon + pagecache) 
are 64k instead of 4k, on average, you&#x27;ll just naturally do less merging 
splitting.

-- 
Cheers,

David


---

On 2/19/26 16:54, Kiryl Shutsemau wrote:
&gt; On Thu, Feb 19, 2026 at 04:39:34PM +0100, David Hildenbrand (Arm) wrote:
&gt;&gt; On 2/19/26 16:08, Kiryl Shutsemau wrote:
&gt;&gt;&gt; No, there&#x27;s no new hardware (that I know of). I want to explore what page size
&gt;&gt;&gt; means.
&gt;&gt;&gt;
&gt;&gt;&gt; The kernel uses the same value - PAGE_SIZE - for two things:
&gt;&gt;&gt;
&gt;&gt;&gt;     - the order-0 buddy allocation size;
&gt;&gt;&gt;
&gt;&gt;&gt;     - the granularity of virtual address space mapping;
&gt;&gt;&gt;
&gt;&gt;&gt; I think we can benefit from separating these two meanings and allowing
&gt;&gt;&gt; order-0 allocations to be larger than the virtual address space covered by a
&gt;&gt;&gt; PTE entry.
&gt;&gt;&gt;
&gt;&gt;&gt; The main motivation is scalability. Managing memory on multi-terabyte
&gt;&gt;&gt; machines in 4k is suboptimal, to say the least.
&gt;&gt;&gt;
&gt;&gt;&gt; Potential benefits of the approach (assuming 64k pages):
&gt;&gt;&gt;
&gt;&gt;&gt;     - The order-0 page size cuts struct page overhead by a factor of 16. From
&gt;&gt;&gt;       ~1.6% of RAM to ~0.1%;
&gt;&gt;&gt;
&gt;&gt;&gt;     - TLB wins on machines with TLB coalescing as long as mapping is naturally
&gt;&gt;&gt;       aligned;
&gt;&gt;&gt;
&gt;&gt;&gt;     - Order-5 allocation is 2M, resulting in less pressure on the zone lock;
&gt;&gt;&gt;
&gt;&gt;&gt;     - 1G pages are within possibility for the buddy allocator - order-14
&gt;&gt;&gt;       allocation. It can open the road to 1G THPs.
&gt;&gt;&gt;
&gt;&gt;&gt;     - As with THP, fewer pages - less pressure on the LRU lock;
&gt;&gt;&gt;
&gt;&gt;&gt;     - ...
&gt;&gt;&gt;
&gt;&gt;&gt; The trade-off is memory waste (similar to what we have on architectures with
&gt;&gt;&gt; native 64k pages today) and complexity, mostly in the core-MM code.
&gt;&gt;&gt;
&gt;&gt;&gt; == Design considerations ==
&gt;&gt;&gt;
&gt;&gt;&gt; I want to split PAGE_SIZE into two distinct values:
&gt;&gt;&gt;
&gt;&gt;&gt;     - PTE_SIZE defines the virtual address space granularity;
&gt;&gt;&gt;
&gt;&gt;&gt;     - PG_SIZE defines the size of the order-0 buddy allocation;
&gt;&gt;&gt;
&gt;&gt;&gt; PAGE_SIZE is only defined if PTE_SIZE == PG_SIZE. It will flag which code
&gt;&gt;&gt; requires conversion, and keep existing code working while conversion is in
&gt;&gt;&gt; progress.
&gt;&gt;&gt;
&gt;&gt;&gt; The same split happens for other page-related macros: mask, shift,
&gt;&gt;&gt; alignment helpers, etc.
&gt;&gt;&gt;
&gt;&gt;&gt; PFNs are in PTE_SIZE units.
&gt;&gt;&gt;
&gt;&gt;&gt; The buddy allocator and page cache (as well as all I/O) operate in PG_SIZE
&gt;&gt;&gt; units.
&gt;&gt;&gt;
&gt;&gt;&gt; Userspace mappings are maintained with PTE_SIZE granularity. No ABI changes
&gt;&gt;&gt; for userspace. But we might want to communicate PG_SIZE to userspace to
&gt;&gt;&gt; get the optimal results for userspace that cares.
&gt;&gt;&gt;
&gt;&gt;&gt; PTE_SIZE granularity requires a substantial rework of page fault and VMA
&gt;&gt;&gt; handling:
&gt;&gt;&gt;
&gt;&gt;&gt;     - A struct page pointer and pgprot_t are not enough to create a PTE entry.
&gt;&gt;&gt;       We also need the offset within the page we are creating the PTE for.
&gt;&gt;&gt;
&gt;&gt;&gt;     - Since the VMA start can be aligned arbitrarily with respect to the
&gt;&gt;&gt;       underlying page, vma-&gt;vm_pgoff has to be changed to vma-&gt;vm_pteoff,
&gt;&gt;&gt;       which is in PTE_SIZE units.
&gt;&gt;&gt;
&gt;&gt;&gt;     - The page fault handler needs to handle PTE_SIZE &lt; PG_SIZE, including
&gt;&gt;&gt;       misaligned cases;
&gt;&gt;&gt;
&gt;&gt;&gt; Page faults into file mappings are relatively simple to handle as we
&gt;&gt;&gt; always have the page cache to refer to. So you can map only the part of the
&gt;&gt;&gt; page that fits in the page table, similarly to fault-around.
&gt;&gt;&gt;
&gt;&gt;&gt; Anonymous and file-CoW faults should also be simple as long as the VMA is
&gt;&gt;&gt; aligned to PG_SIZE in both the virtual address space and with respect to
&gt;&gt;&gt; vm_pgoff. We might waste some memory on the ends of the VMA, but it is
&gt;&gt;&gt; tolerable.
&gt;&gt;&gt;
&gt;&gt;&gt; Misaligned anonymous and file-CoW faults are a pain. Specifically, mapping
&gt;&gt;&gt; pages across a page table boundary. In the worst case, a page is mapped across
&gt;&gt;&gt; a PGD entry boundary and PTEs for the page have to be put in two separate
&gt;&gt;&gt; subtrees of page tables.
&gt;&gt;&gt;
&gt;&gt;&gt; A naive implementation would map different pages on different sides of a
&gt;&gt;&gt; page table boundary and accept the waste of one page per page table crossing.
&gt;&gt;&gt; The hope is that misaligned mappings are rare, but this is suboptimal.
&gt;&gt;&gt;
&gt;&gt;&gt; mremap(2) is the ultimate stress test for the design.
&gt;&gt;&gt;
&gt;&gt;&gt; On x86, page tables are allocated from the buddy allocator and if PG_SIZE
&gt;&gt;&gt; is greater than 4 KB, we need a way to pack multiple page tables into a
&gt;&gt;&gt; single page. We could use the slab allocator for this, but it would
&gt;&gt;&gt; require relocating the page-table metadata out of struct page.
&gt;&gt;
&gt;&gt; When discussing per-process page sizes with Ryan and Dev, I mentioned that
&gt;&gt; having a larger emulated page size could be interesting for other
&gt;&gt; architectures as well.
&gt;&gt;
&gt;&gt; That is, we would emulate a 64K page size on Intel for user space as well,
&gt;&gt; but let the OS work with 4K pages.
&gt;&gt;
&gt;&gt; We&#x27;d only allocate+map large folios into user space + pagecache, but still
&gt;&gt; allow for page tables etc. to not waste memory.
&gt;&gt;
&gt;&gt; So &quot;most&quot; of your allocations in the system would actually be at least 64k,
&gt;&gt; reducing zone lock contention etc.
&gt; 
&gt; I am not convinced emulation would help zone lock contention. I expect
&gt; contention to be higher if page allocator would see a mix of 4k and 64k
&gt; requests. It sounds like constant split/merge under the lock.

If most your allocations are larger, then there isn&#x27;t that much 
splitting/merging.

There will be some for the &lt; 64k allocations of course, but when all 
user space+page cache is &gt;= 64 then the split/merge + zone lock should 
be heavily reduced.

&gt; 
&gt;&gt; It doesn&#x27;t solve all the problems you wanted to tackle on your list (e.g.,
&gt;&gt; &quot;struct page&quot; overhead, which will be sorted out by memdescs).
&gt; 
&gt; I don&#x27;t think we can serve 1G pages out of buddy allocator with 4k
&gt; order-0. And without it, I don&#x27;t see how to get to a viable 1G THPs.

Zi Yan was one working on this, and I think we had ideas on how to make 
that work in the long run.

-- 
Cheers,

David
</pre>
</details>
<div class="review-comment-signals">Signals: NEUTRAL, NO_CONCERN</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Dave Hansen</span>
<a class="date-chip" href="../2026-02-19_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-19">2026-02-19</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Dave Hansen raised concerns about the potential memory waste and complexity introduced by splitting PAGE_SIZE into PTE_SIZE and PG_SIZE. He also pointed out that the proposed changes would likely consume more RAM, not save it, and suggested a strong separation of mechanical bits from logic changes.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On 2/19/26 07:08, Kiryl Shutsemau wrote:
&gt;   - The order-0 page size cuts struct page overhead by a factor of 16. From
&gt;     ~1.6% of RAM to ~0.1%;

First of all, this looks like fun. Nice work! I&#x27;m not opposed at all in
concept to cleaning up things and doing the logical separation you
described to split buddy granularity and mapping granularity. That seems
like a worthy endeavor and some of the union/#define tricks look like a
likely viable way to do it incrementally.

But I don&#x27;t think there&#x27;s going to be a lot of memory savings in the
end. Maybe this would bring the mem= hyperscalers back into the fold and
have them actually start using &#x27;struct page&#x27; again for their VM memory.
Dunno.

But, let&#x27;s look at my kernel directory and round the file sizes up to
4k, 16k and 64k:

find .  -printf &#x27;%s\n&#x27; | while read size; do echo	\
		$(((size + 0x0fff) &amp; 0xfffff000))	\
		$(((size + 0x3fff) &amp; 0xffffc000))	\
		$(((size + 0xffff) &amp; 0xffff0000));
done

... and add them all up:

11,297,648 KB - on disk
11,297,712 KB - in a 4k page cache
12,223,488 KB - in a 16k page cache
16,623,296 KB - in a 64k page cache

So a 64k page cache eats ~5GB of extra memory for a kernel tree (well,
_my_ kernel tree). In other words, if you are looking for memory savings
on my laptop, you&#x27;ll need ~300GB of RAM before &#x27;struct page&#x27; overhead
overwhelms the page cache bloat from a single kernel tree.

The whole kernel obviously isn&#x27;t in the page cache all at the same time.
The page cache across the system is also obviously different than a
kernel tree, but you get the point.

That&#x27;s not to diminish how useful something like this might be,
especially for folks that are sensitive to &#x27;struct page&#x27; overhead or
allocator performance.

But, it will mostly be getting better performance at the _cost_ of
consuming more RAM, not saving RAM.


---

On 2/19/26 07:08, Kiryl Shutsemau wrote:
...
&gt; The patchset is large:
&gt; 
&gt;  378 files changed, 3348 insertions(+), 3102 deletions(-)

A few notes about the diffstats:

$ git diff v6.17..HEAD arch/x86 | diffstat | tail -1
 105 files changed, 874 insertions(+), 843 deletions(-)
$ git diff v6.17..HEAD mm | diffstat | tail -1
 53 files changed, 1136 insertions(+), 1069 deletions(-)

The vast, vast majority of this seems to be the renames. Stuff like:

&gt; -               new = round_down(new, PAGE_SIZE);
&gt; +               new = round_down(new, PTE_SIZE);

or even less worrying:

&gt; -int set_direct_map_valid_noflush(struct page *page, unsigned nr, bool valid);
&gt; +int set_direct_map_valid_noflush(struct page *page, unsigned numpages, bool valid);

That stuff obviously needs to be audited but it&#x27;s far less concerning
than the logic changes.

So just for review sanity, if you go forward with this, I&#x27;d very much
appreciate a strong separation of the purely mechanical bits from any
logic changes.

&gt; On x86, page tables are allocated from the buddy allocator and if PG_SIZE
&gt; is greater than 4 KB, we need a way to pack multiple page tables into a
&gt; single page. We could use the slab allocator for this, but it would
&gt; require relocating the page-table metadata out of struct page.

Others mentioned this, but I think this essentially gates what you are
doing behind a full tree conversion over to ptdescs.

The most useful thing we can do with this series is look at it and
decide what _other_ things need to get done before the tree could
possibly go in that direction, like ptdesc or a the disambiguation
between PTE_SIZE and PG_SIZE that you&#x27;ve kicked off here.


---

On 2/19/26 14:14, Kiryl Shutsemau wrote:
&gt;&gt; Others mentioned this, but I think this essentially gates what you are
&gt;&gt; doing behind a full tree conversion over to ptdescs.
&gt; I have not followed ptdescs closely. Need to catch up.
&gt; 
&gt; For PoC, I will just waste full order-0 page for page table. Packing is
&gt; not required for correctness.

Yeah, I guess padding it out is ugly but effective.

I was trying to figure out how it would apply to the KPTI pgd because we
just flip bit 12 to switch between user and kernel PGDs. But I guess the
8k of PGDs in the current allocation will fit fine in 128k, so it&#x27;s
weird but functional.
</pre>
</details>
<div class="review-comment-signals">Signals: requested changes, potential memory waste</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Matthew Wilcox</span>
<a class="date-chip" href="../2026-02-19_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-19">2026-02-19</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Matthew Wilcox raised concerns about the proposed patch, suggesting alternative approaches and questioning the use of slab allocation.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Thu, Feb 19, 2026 at 03:08:51PM +0000, Kiryl Shutsemau wrote:
&gt; On x86, page tables are allocated from the buddy allocator and if PG_SIZE
&gt; is greater than 4 KB, we need a way to pack multiple page tables into a
&gt; single page. We could use the slab allocator for this, but it would
&gt; require relocating the page-table metadata out of struct page.

Have you looked at the s390/ppc implementations (yes, they&#x27;re different,
no, that sucks)?  slab seems like the wrong approach to me.

There&#x27;s a third approach that I&#x27;ve never looked at which is to allocate
the larger size, then just use it for N consecutive entries.
</pre>
</details>
<div class="review-comment-signals">Signals: requested changes, alternative approach</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Kalesh Singh</span>
<a class="date-chip" href="../2026-02-19_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-19">2026-02-19</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Kalesh Singh expressed interest in discussing the patch at LSFMM and mentioned a related use case on Android, where they emulate a larger userspace page size to enable compatibility testing.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Thu, Feb 19, 2026 at 7:39 AM David Hildenbrand (Arm)
&lt;david@kernel.org&gt; wrote:
&gt;
&gt; On 2/19/26 16:08, Kiryl Shutsemau wrote:
&gt; &gt; No, there&#x27;s no new hardware (that I know of). I want to explore what page size
&gt; &gt; means.
&gt; &gt;
&gt; &gt; The kernel uses the same value - PAGE_SIZE - for two things:
&gt; &gt;
&gt; &gt;    - the order-0 buddy allocation size;
&gt; &gt;
&gt; &gt;    - the granularity of virtual address space mapping;
&gt; &gt;
&gt; &gt; I think we can benefit from separating these two meanings and allowing
&gt; &gt; order-0 allocations to be larger than the virtual address space covered by a
&gt; &gt; PTE entry.
&gt; &gt;
&gt; &gt; The main motivation is scalability. Managing memory on multi-terabyte
&gt; &gt; machines in 4k is suboptimal, to say the least.
&gt; &gt;
&gt; &gt; Potential benefits of the approach (assuming 64k pages):
&gt; &gt;
&gt; &gt;    - The order-0 page size cuts struct page overhead by a factor of 16. From
&gt; &gt;      ~1.6% of RAM to ~0.1%;
&gt; &gt;
&gt; &gt;    - TLB wins on machines with TLB coalescing as long as mapping is naturally
&gt; &gt;      aligned;
&gt; &gt;
&gt; &gt;    - Order-5 allocation is 2M, resulting in less pressure on the zone lock;
&gt; &gt;
&gt; &gt;    - 1G pages are within possibility for the buddy allocator - order-14
&gt; &gt;      allocation. It can open the road to 1G THPs.
&gt; &gt;
&gt; &gt;    - As with THP, fewer pages - less pressure on the LRU lock;
&gt; &gt;
&gt; &gt;    - ...
&gt; &gt;
&gt; &gt; The trade-off is memory waste (similar to what we have on architectures with
&gt; &gt; native 64k pages today) and complexity, mostly in the core-MM code.
&gt; &gt;
&gt; &gt; == Design considerations ==
&gt; &gt;
&gt; &gt; I want to split PAGE_SIZE into two distinct values:
&gt; &gt;
&gt; &gt;    - PTE_SIZE defines the virtual address space granularity;
&gt; &gt;
&gt; &gt;    - PG_SIZE defines the size of the order-0 buddy allocation;
&gt; &gt;
&gt; &gt; PAGE_SIZE is only defined if PTE_SIZE == PG_SIZE. It will flag which code
&gt; &gt; requires conversion, and keep existing code working while conversion is in
&gt; &gt; progress.
&gt; &gt;
&gt; &gt; The same split happens for other page-related macros: mask, shift,
&gt; &gt; alignment helpers, etc.
&gt; &gt;
&gt; &gt; PFNs are in PTE_SIZE units.
&gt; &gt;
&gt; &gt; The buddy allocator and page cache (as well as all I/O) operate in PG_SIZE
&gt; &gt; units.
&gt; &gt;
&gt; &gt; Userspace mappings are maintained with PTE_SIZE granularity. No ABI changes
&gt; &gt; for userspace. But we might want to communicate PG_SIZE to userspace to
&gt; &gt; get the optimal results for userspace that cares.
&gt; &gt;
&gt; &gt; PTE_SIZE granularity requires a substantial rework of page fault and VMA
&gt; &gt; handling:
&gt; &gt;
&gt; &gt;    - A struct page pointer and pgprot_t are not enough to create a PTE entry.
&gt; &gt;      We also need the offset within the page we are creating the PTE for.
&gt; &gt;
&gt; &gt;    - Since the VMA start can be aligned arbitrarily with respect to the
&gt; &gt;      underlying page, vma-&gt;vm_pgoff has to be changed to vma-&gt;vm_pteoff,
&gt; &gt;      which is in PTE_SIZE units.
&gt; &gt;
&gt; &gt;    - The page fault handler needs to handle PTE_SIZE &lt; PG_SIZE, including
&gt; &gt;      misaligned cases;
&gt; &gt;
&gt; &gt; Page faults into file mappings are relatively simple to handle as we
&gt; &gt; always have the page cache to refer to. So you can map only the part of the
&gt; &gt; page that fits in the page table, similarly to fault-around.
&gt; &gt;
&gt; &gt; Anonymous and file-CoW faults should also be simple as long as the VMA is
&gt; &gt; aligned to PG_SIZE in both the virtual address space and with respect to
&gt; &gt; vm_pgoff. We might waste some memory on the ends of the VMA, but it is
&gt; &gt; tolerable.
&gt; &gt;
&gt; &gt; Misaligned anonymous and file-CoW faults are a pain. Specifically, mapping
&gt; &gt; pages across a page table boundary. In the worst case, a page is mapped across
&gt; &gt; a PGD entry boundary and PTEs for the page have to be put in two separate
&gt; &gt; subtrees of page tables.
&gt; &gt;
&gt; &gt; A naive implementation would map different pages on different sides of a
&gt; &gt; page table boundary and accept the waste of one page per page table crossing.
&gt; &gt; The hope is that misaligned mappings are rare, but this is suboptimal.
&gt; &gt;
&gt; &gt; mremap(2) is the ultimate stress test for the design.
&gt; &gt;
&gt; &gt; On x86, page tables are allocated from the buddy allocator and if PG_SIZE
&gt; &gt; is greater than 4 KB, we need a way to pack multiple page tables into a
&gt; &gt; single page. We could use the slab allocator for this, but it would
&gt; &gt; require relocating the page-table metadata out of struct page.
&gt;
&gt; When discussing per-process page sizes with Ryan and Dev, I mentioned
&gt; that having a larger emulated page size could be interesting for other
&gt; architectures as well.
&gt;
&gt; That is, we would emulate a 64K page size on Intel for user space as
&gt; well, but let the OS work with 4K pages.
&gt;
&gt; We&#x27;d only allocate+map large folios into user space + pagecache, but
&gt; still allow for page tables etc. to not waste memory.
&gt;
&gt; So &quot;most&quot; of your allocations in the system would actually be at least
&gt; 64k, reducing zone lock contention etc.
&gt;
&gt;
&gt; It doesn&#x27;t solve all the problems you wanted to tackle on your list
&gt; (e.g., &quot;struct page&quot; overhead, which will be sorted out by memdescs).

Hi Kiryl,

I&#x27;d be interested to discuss this at LSFMM.

On Android, we have a separate but related use case: we emulate the
userspace page size on x86, primarily to enable app developers to
conduct compatibility testing of their apps for 16KB Android devices.
[1]

It mainly works by enforcing a larger granularity on the VMAs to
emulate a userspace page size, somewhat similar to what David
mentioned, while the underlying kernel still operates on a 4KB
granularity. [2]

IIUC the current design would not enfore the larger granularity /
alignment for VMAs to avoid breaking ABI. However, I&#x27;d be interest to
discuss whether it can be extended to cover this usecase as well.

[1]  https://developer.android.com/guide/practices/page-sizes#16kb-emulator
[2] https://source.android.com/docs/core/architecture/16kb-page-size/getting-started-cf-x86-64-pgagnostic

Thanks,
Kalesh




&gt;
&gt; --
&gt; Cheers,
&gt;
&gt; David
&gt;
</pre>
</details>
<div class="review-comment-signals">Signals: NEEDS_WORK</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Zi Yan</span>
<a class="date-chip" href="../2026-02-19_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-19">2026-02-19</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The reviewer questions the necessity of introducing a new page size and suggests that anti-fragmentation can work at larger granularity, such as 1GB, without needing to modify the buddy allocator.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On 19 Feb 2026, at 11:09, David Hildenbrand (Arm) wrote:

&gt; On 2/19/26 16:54, Kiryl Shutsemau wrote:
&gt;&gt; On Thu, Feb 19, 2026 at 04:39:34PM +0100, David Hildenbrand (Arm) wrote:
&gt;&gt;&gt; On 2/19/26 16:08, Kiryl Shutsemau wrote:
&gt;&gt;&gt;&gt; No, there&#x27;s no new hardware (that I know of). I want to explore what page size
&gt;&gt;&gt;&gt; means.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; The kernel uses the same value - PAGE_SIZE - for two things:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;     - the order-0 buddy allocation size;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;     - the granularity of virtual address space mapping;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; I think we can benefit from separating these two meanings and allowing
&gt;&gt;&gt;&gt; order-0 allocations to be larger than the virtual address space covered by a
&gt;&gt;&gt;&gt; PTE entry.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; The main motivation is scalability. Managing memory on multi-terabyte
&gt;&gt;&gt;&gt; machines in 4k is suboptimal, to say the least.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Potential benefits of the approach (assuming 64k pages):
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;     - The order-0 page size cuts struct page overhead by a factor of 16. From
&gt;&gt;&gt;&gt;       ~1.6% of RAM to ~0.1%;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;     - TLB wins on machines with TLB coalescing as long as mapping is naturally
&gt;&gt;&gt;&gt;       aligned;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;     - Order-5 allocation is 2M, resulting in less pressure on the zone lock;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;     - 1G pages are within possibility for the buddy allocator - order-14
&gt;&gt;&gt;&gt;       allocation. It can open the road to 1G THPs.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;     - As with THP, fewer pages - less pressure on the LRU lock;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;     - ...
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; The trade-off is memory waste (similar to what we have on architectures with
&gt;&gt;&gt;&gt; native 64k pages today) and complexity, mostly in the core-MM code.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; == Design considerations ==
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; I want to split PAGE_SIZE into two distinct values:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;     - PTE_SIZE defines the virtual address space granularity;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;     - PG_SIZE defines the size of the order-0 buddy allocation;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; PAGE_SIZE is only defined if PTE_SIZE == PG_SIZE. It will flag which code
&gt;&gt;&gt;&gt; requires conversion, and keep existing code working while conversion is in
&gt;&gt;&gt;&gt; progress.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; The same split happens for other page-related macros: mask, shift,
&gt;&gt;&gt;&gt; alignment helpers, etc.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; PFNs are in PTE_SIZE units.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; The buddy allocator and page cache (as well as all I/O) operate in PG_SIZE
&gt;&gt;&gt;&gt; units.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Userspace mappings are maintained with PTE_SIZE granularity. No ABI changes
&gt;&gt;&gt;&gt; for userspace. But we might want to communicate PG_SIZE to userspace to
&gt;&gt;&gt;&gt; get the optimal results for userspace that cares.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; PTE_SIZE granularity requires a substantial rework of page fault and VMA
&gt;&gt;&gt;&gt; handling:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;     - A struct page pointer and pgprot_t are not enough to create a PTE entry.
&gt;&gt;&gt;&gt;       We also need the offset within the page we are creating the PTE for.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;     - Since the VMA start can be aligned arbitrarily with respect to the
&gt;&gt;&gt;&gt;       underlying page, vma-&gt;vm_pgoff has to be changed to vma-&gt;vm_pteoff,
&gt;&gt;&gt;&gt;       which is in PTE_SIZE units.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;     - The page fault handler needs to handle PTE_SIZE &lt; PG_SIZE, including
&gt;&gt;&gt;&gt;       misaligned cases;
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Page faults into file mappings are relatively simple to handle as we
&gt;&gt;&gt;&gt; always have the page cache to refer to. So you can map only the part of the
&gt;&gt;&gt;&gt; page that fits in the page table, similarly to fault-around.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Anonymous and file-CoW faults should also be simple as long as the VMA is
&gt;&gt;&gt;&gt; aligned to PG_SIZE in both the virtual address space and with respect to
&gt;&gt;&gt;&gt; vm_pgoff. We might waste some memory on the ends of the VMA, but it is
&gt;&gt;&gt;&gt; tolerable.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Misaligned anonymous and file-CoW faults are a pain. Specifically, mapping
&gt;&gt;&gt;&gt; pages across a page table boundary. In the worst case, a page is mapped across
&gt;&gt;&gt;&gt; a PGD entry boundary and PTEs for the page have to be put in two separate
&gt;&gt;&gt;&gt; subtrees of page tables.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; A naive implementation would map different pages on different sides of a
&gt;&gt;&gt;&gt; page table boundary and accept the waste of one page per page table crossing.
&gt;&gt;&gt;&gt; The hope is that misaligned mappings are rare, but this is suboptimal.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; mremap(2) is the ultimate stress test for the design.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; On x86, page tables are allocated from the buddy allocator and if PG_SIZE
&gt;&gt;&gt;&gt; is greater than 4 KB, we need a way to pack multiple page tables into a
&gt;&gt;&gt;&gt; single page. We could use the slab allocator for this, but it would
&gt;&gt;&gt;&gt; require relocating the page-table metadata out of struct page.
&gt;&gt;&gt;
&gt;&gt;&gt; When discussing per-process page sizes with Ryan and Dev, I mentioned that
&gt;&gt;&gt; having a larger emulated page size could be interesting for other
&gt;&gt;&gt; architectures as well.
&gt;&gt;&gt;
&gt;&gt;&gt; That is, we would emulate a 64K page size on Intel for user space as well,
&gt;&gt;&gt; but let the OS work with 4K pages.
&gt;&gt;&gt;
&gt;&gt;&gt; We&#x27;d only allocate+map large folios into user space + pagecache, but still
&gt;&gt;&gt; allow for page tables etc. to not waste memory.
&gt;&gt;&gt;
&gt;&gt;&gt; So &quot;most&quot; of your allocations in the system would actually be at least 64k,
&gt;&gt;&gt; reducing zone lock contention etc.
&gt;&gt;
&gt;&gt; I am not convinced emulation would help zone lock contention. I expect
&gt;&gt; contention to be higher if page allocator would see a mix of 4k and 64k
&gt;&gt; requests. It sounds like constant split/merge under the lock.
&gt;
&gt; If most your allocations are larger, then there isn&#x27;t that much splitting/merging.
&gt;
&gt; There will be some for the &lt; 64k allocations of course, but when all user space+page cache is &gt;= 64 then the split/merge + zone lock should be heavily reduced.
&gt;
&gt;&gt;
&gt;&gt;&gt; It doesn&#x27;t solve all the problems you wanted to tackle on your list (e.g.,
&gt;&gt;&gt; &quot;struct page&quot; overhead, which will be sorted out by memdescs).
&gt;&gt;
&gt;&gt; I don&#x27;t think we can serve 1G pages out of buddy allocator with 4k
&gt;&gt; order-0. And without it, I don&#x27;t see how to get to a viable 1G THPs.
&gt;
&gt; Zi Yan was one working on this, and I think we had ideas on how to make that work in the long run.

Right. The idea is to add super pageblock (or whatever name), which consists of N consecutive
pageblocks, so that anti fragmentation can work at larger granularity, e.g., 1GB, to create
free pages. Whether 1GB free pages from memory compaction need to go into buddy allocator
or not is debatable.

--
Best Regards,
Yan, Zi
</pre>
</details>
<div class="review-comment-signals">Signals: requested changes, debate</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Liam Howlett</span>
<a class="date-chip" href="../2026-02-19_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-19">2026-02-19</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Liam Howlett questioned the benefits of increasing page size to reduce struct page overhead, arguing it would increase memory pressure and degrade primary workloads on multi-workload machines.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">* Kiryl Shutsemau &lt;kas@kernel.org&gt; [260219 17:05]:
&gt; On Thu, Feb 19, 2026 at 09:08:57AM -0800, Dave Hansen wrote:
&gt; &gt; On 2/19/26 07:08, Kiryl Shutsemau wrote:
&gt; &gt; &gt;   - The order-0 page size cuts struct page overhead by a factor of 16. From
&gt; &gt; &gt;     ~1.6% of RAM to ~0.1%;
&gt; &gt; ...
&gt; &gt; But, it will mostly be getting better performance at the _cost_ of
&gt; &gt; consuming more RAM, not saving RAM.
&gt; 
&gt; That&#x27;s fair.
&gt; 
&gt; The problem with struct page memory consumption is that it is static and
&gt; cannot be reclaimed. You pay the struct page tax no matter what.
&gt; 
&gt; Page cache rounding overhead can be large, but a motivated userspace can
&gt; keep it under control by avoiding splitting a dataset into many small
&gt; files. And this memory is reclaimable.
&gt; 

But we are in reclaim a lot more these days.  As I&#x27;m sure you are aware,
we are trying to maximize the resources (both cpu and ram) of any
machine powered on.  Entering reclaim will consume the cpu time and will
affect other tasks.

Especially with multiple workload machines, the tendency is to have a
primary focus with the lower desired work being killed, if necessary.
Reducing the overhead just means more secondary tasks, or a bigger
footprint of the ones already executing.

Increasing the memory pressure will degrade the primary workload more
frequently, even if we recover enough to avoid OOMing the secondary.

While in the struct page tax world, the secondary task would be killed
after a shorter (and less frequently executed) reclaim comes up short.
So, I would think that we would be degrading the primary workload in an
attempt to keep the secondary alive?  Maybe I&#x27;m over-simplifying here?

Near the other end of the spectrum, we have chromebooks that are
constantly in reclaim, even with 4k pages.  I guess these machines would
be destine to maintain the same page size they use today.  That is, this
solution for the struct page tax is only useful if you have a lot of
memory.  But then again, that&#x27;s where the bookkeeping costs become hard
to take.

Thanks,
Liam


</pre>
</details>
<div class="review-comment-signals">Signals: requested changes, technical concerns</div>
</div>
</div>
</div>

    <footer>LKML Daily Activity Tracker</footer>
    <script>
    // When arriving via a date anchor (e.g. #2026-02-15 from a daily report),
    // scroll the anchor into view after a brief delay so layout is complete.
    (function () {
        var hash = window.location.hash;
        if (!hash) return;
        var target = document.getElementById(hash.slice(1));
        if (!target) return;
        setTimeout(function () {
            target.scrollIntoView({behavior: 'smooth', block: 'start'});
        }, 80);
    })();
    </script>
</body>
</html>