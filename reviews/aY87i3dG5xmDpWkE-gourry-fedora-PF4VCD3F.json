{
  "thread_id": "aY87i3dG5xmDpWkE@gourry-fedora-PF4VCD3F",
  "subject": "Re: [RFC PATCH v5 00/10] mm: Hot page tracking and promotion infrastructure",
  "url": "https://lore.kernel.org/all/aY87i3dG5xmDpWkE@gourry-fedora-PF4VCD3F/",
  "dates": {
    "2026-02-13": {
      "report_file": "2026-02-13_ollama_llama3.1-8b.html",
      "developer": "Gregory Price",
      "reviews": [
        {
          "author": "Bharata Rao (author)",
          "summary": "Reviewer Bharata Rao raised concerns about the performance of the pghot subsystem, specifically that it causes a spike in page table updates and hint faults when promotion and demotion are enabled. He also suggested applying the patch series on top of a specific commit (f0b9d8eb98df) and provided a link to his GitHub branch.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "performance concerns",
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "We want isolation of misplaced folios to work in contexts\nwhere VMA isn't available, typically when performing migrations\nfrom a kernel thread context. In order to prepare for that,\nallow migrate_misplaced_folio_prepare() to be called with\na NULL VMA.\n\nWhen migrate_misplaced_folio_prepare() is called with non-NULL\nVMA, it will check if the folio is mapped shared and that requires\nholding PTL lock. This path isn't taken when the function is\ninvoked with NULL VMA (migration outside of process context).\nTherefore, when VMA == NULL, migrate_misplaced_folio_prepare()\ndoes not require the caller to hold the PTL.\n\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n mm/migrate.c | 5 +++--\n 1 file changed, 3 insertions(+), 2 deletions(-)\n\ndiff --git a/mm/migrate.c b/mm/migrate.c\nindex 5169f9717f60..70f8f3ad4fd8 100644\n--- a/mm/migrate.c\n+++ b/mm/migrate.c\n@@ -2652,7 +2652,8 @@ static struct folio *alloc_misplaced_dst_folio(struct folio *src,\n \n /*\n  * Prepare for calling migrate_misplaced_folio() by isolating the folio if\n- * permitted. Must be called with the PTL still held.\n+ * permitted. Must be called with the PTL still held if called with a non-NULL\n+ * vma.\n  */\n int migrate_misplaced_folio_prepare(struct folio *folio,\n \t\tstruct vm_area_struct *vma, int node)\n@@ -2669,7 +2670,7 @@ int migrate_misplaced_folio_prepare(struct folio *folio,\n \t\t * See folio_maybe_mapped_shared() on possible imprecision\n \t\t * when we cannot easily detect if a folio is shared.\n \t\t */\n-\t\tif ((vma->vm_flags & VM_EXEC) && folio_maybe_mapped_shared(folio))\n+\t\tif (vma && (vma->vm_flags & VM_EXEC) && folio_maybe_mapped_shared(folio))\n \t\t\treturn -EACCES;\n \n \t\t/*\n-- \n2.34.1\n\n\n\n---\n\nFrom: Gregory Price <gourry@gourry.net>\n\nTiered memory systems often require migrating multiple folios at once.\nCurrently, migrate_misplaced_folio() handles only one folio per call,\nwhich is inefficient for batch operations. This patch introduces\nmigrate_misplaced_folios_batch(), a batch variant that leverages\nmigrate_pages() internally for improved performance.\n\nThe caller must isolate folios beforehand using\nmigrate_misplaced_folio_prepare(). On return, the folio list will be\nempty regardless of success or failure.\n\nThis function will be used by pghot kmigrated thread.\n\nSigned-off-by: Gregory Price <gourry@gourry.net>\n[Rewrote commit description]\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n include/linux/migrate.h |  6 ++++++\n mm/migrate.c            | 36 ++++++++++++++++++++++++++++++++++++\n 2 files changed, 42 insertions(+)\n\ndiff --git a/include/linux/migrate.h b/include/linux/migrate.h\nindex 26ca00c325d9..f28326b88592 100644\n--- a/include/linux/migrate.h\n+++ b/include/linux/migrate.h\n@@ -103,6 +103,7 @@ static inline int set_movable_ops(const struct movable_operations *ops, enum pag\n int migrate_misplaced_folio_prepare(struct folio *folio,\n \t\tstruct vm_area_struct *vma, int node);\n int migrate_misplaced_folio(struct folio *folio, int node);\n+int migrate_misplaced_folios_batch(struct list_head *folio_list, int node);\n #else\n static inline int migrate_misplaced_folio_prepare(struct folio *folio,\n \t\tstruct vm_area_struct *vma, int node)\n@@ -113,6 +114,11 @@ static inline int migrate_misplaced_folio(struct folio *folio, int node)\n {\n \treturn -EAGAIN; /* can't migrate now */\n }\n+static inline int migrate_misplaced_folios_batch(struct list_head *folio_list,\n+\t\t\t\t\t\t int node)\n+{\n+\treturn -EAGAIN; /* can't migrate now */\n+}\n #endif /* CONFIG_NUMA_BALANCING */\n \n #ifdef CONFIG_MIGRATION\ndiff --git a/mm/migrate.c b/mm/migrate.c\nindex 70f8f3ad4fd8..4a3a9a4ff435 100644\n--- a/mm/migrate.c\n+++ b/mm/migrate.c\n@@ -2747,5 +2747,41 @@ int migrate_misplaced_folio(struct folio *folio, int node)\n \tBUG_ON(!list_empty(&migratepages));\n \treturn nr_remaining ? -EAGAIN : 0;\n }\n+\n+/**\n+ * migrate_misplaced_folios_batch() - Batch variant of migrate_misplaced_folio.\n+ * Attempts to migrate a folio list to the specified destination.\n+ * @folio_list: Isolated list of folios to be batch-migrated.\n+ * @node: The NUMA node ID to where the folios should be migrated.\n+ *\n+ * Caller is expected to have isolated the folios by calling\n+ * migrate_misplaced_folio_prepare(), which will result in an\n+ * elevated reference count on the folio.\n+ *\n+ * This function will un-isolate the folios, drop the elevated reference\n+ * and remove them from the list before returning.\n+ *\n+ * Return: 0 on success and -EAGAIN on failure or partial migration.\n+ *         On return, @folio_list will be empty regardless of success/failure.\n+ */\n+int migrate_misplaced_folios_batch(struct list_head *folio_list, int node)\n+{\n+\tpg_data_t *pgdat = NODE_DATA(node);\n+\tunsigned int nr_succeeded = 0;\n+\tint nr_remaining;\n+\n+\tnr_remaining = migrate_pages(folio_list, alloc_misplaced_dst_folio,\n+\t\t\t\t     NULL, node, MIGRATE_ASYNC,\n+\t\t\t\t     MR_NUMA_MISPLACED, &nr_succeeded);\n+\tif (nr_remaining)\n+\t\tputback_movable_pages(folio_list);\n+\n+\tif (nr_succeeded) {\n+\t\tcount_vm_numa_events(NUMA_PAGE_MIGRATE, nr_succeeded);\n+\t\tmod_node_page_state(pgdat, PGPROMOTE_SUCCESS, nr_succeeded);\n+\t}\n+\tWARN_ON(!list_empty(folio_list));\n+\treturn nr_remaining ? -EAGAIN : 0;\n+}\n #endif /* CONFIG_NUMA_BALANCING */\n #endif /* CONFIG_NUMA */\n-- \n2.34.1\n\n\n\n---\n\nThis introduces a subsystem for collecting memory access\ninformation from different sources. It maintains the hotness\ninformation based on the access history and time of access.\n\nAdditionally, it provides per-lower-tier-node kernel threads\n(named kmigrated) that periodically promote the pages that\nare eligible for promotion.\n\nSub-systems that generate hot page access info can report that\nusing this API:\n\nint pghot_record_access(unsigned long pfn, int nid, int src,\n                        unsigned long time)\n\n@pfn: The PFN of the memory accessed\n@nid: The accessing NUMA node ID\n@src: The temperature source (subsystem) that generated the\n      access info\n@time: The access time in jiffies\n\nSome temperature sources may not provide the nid from which\nthe page was accessed. This is true for sources that use\npage table scanning for PTE Accessed bit. For such sources,\na configurable/default toptier node is used as promotion\ntarget.\n\nThe hotness information is stored for every page of lower\ntier memory in a u8 variable (1 byte) that is part of\nmem_section data structure.\n\nkmigrated is a per-lower-tier-node kernel thread that migrates\nthe folios marked for migration in batches. Each kmigrated\nthread walks the PFN range spanning its node and checks\nfor potential migration candidates.\n\nA bunch of tunables for enabling different hotness sources,\nsetting target_nid, frequency threshold are provided in debugfs.\n\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n Documentation/admin-guide/mm/pghot.txt |  84 ++++++\n include/linux/mmzone.h                 |  21 ++\n include/linux/pghot.h                  |  94 +++++++\n include/linux/vm_event_item.h          |   6 +\n mm/Kconfig                             |  14 +\n mm/Makefile                            |   1 +\n mm/mm_init.c                           |  10 +\n mm/pghot-default.c                     |  73 +++++\n mm/pghot-tunables.c                    | 189 +++++++++++++\n mm/pghot.c                             | 370 +++++++++++++++++++++++++\n mm/vmstat.c                            |   6 +\n 11 files changed, 868 insertions(+)\n create mode 100644 Documentation/admin-guide/mm/pghot.txt\n create mode 100644 include/linux/pghot.h\n create mode 100644 mm/pghot-default.c\n create mode 100644 mm/pghot-tunables.c\n create mode 100644 mm/pghot.c\n\ndiff --git a/Documentation/admin-guide/mm/pghot.txt b/Documentation/admin-guide/mm/pghot.txt\nnew file mode 100644\nindex 000000000000..01291b72e7ab\n--- /dev/null\n+++ b/Documentation/admin-guide/mm/pghot.txt\n@@ -0,0 +1,84 @@\n+.. SPDX-License-Identifier: GPL-2.0\n+\n+=================================\n+PGHOT: Hot Page Tracking Tunables\n+=================================\n+\n+Overview\n+========\n+The PGHOT subsystem tracks frequently accessed pages in lower-tier memory and\n+promotes them to faster tiers. It uses per-PFN hotness metadata and asynchronous\n+migration via per-node kernel threads (kmigrated).\n+\n+This document describes tunables available via **debugfs** and **sysctl** for\n+PGHOT.\n+\n+Debugfs Interface\n+=================\n+Path: /sys/kernel/debug/pghot/\n+\n+1. **enabled_sources**\n+   - Bitmask to enable/disable hotness sources.\n+   - Bits:\n+     - 0: Hardware hints (value 0x1)\n+     - 1: Page table scan (value 0x2)\n+     - 2: Hint faults (value 0x4)\n+   - Default: 0 (disabled)\n+   - Example:\n+     # echo 0x7 > /sys/kernel/debug/pghot/enabled_sources\n+     Enables all sources.\n+\n+2. **target_nid**\n+   - Toptier NUMA node ID to which hot pages should be promoted when source\n+     does not provide nid. Used when hotness source can't provide accessing\n+     NID or when the tracking mode is default.\n+   - Default: 0\n+   - Example:\n+     # echo 1 > /sys/kernel/debug/pghot/target_nid\n+\n+3. **freq_threshold**\n+   - Minimum access frequency before a page is marked ready for promotion.\n+   - Range: 1 to 3\n+   - Default: 2\n+   - Example:\n+     # echo 3 > /sys/kernel/debug/pghot/freq_threshold\n+\n+4. **kmigrated_sleep_ms**\n+   - Sleep interval (ms) for kmigrated thread between scans.\n+   - Default: 100\n+\n+5. **kmigrated_batch_nr**\n+   - Maximum number of folios migrated in one batch.\n+   - Default: 512\n+\n+Sysctl Interface\n+================\n+1. pghot_promote_freq_window_ms\n+\n+Path: /proc/sys/vm/pghot_promote_freq_window_ms\n+\n+- Controls the time window (in ms) for counting access frequency. A page is\n+  considered hot only when **freq_threshold** number of accesses occur with\n+  this time period.\n+- Default: 4000 (4 seconds)\n+- Example:\n+  # sysctl vm.pghot_promote_freq_window_ms=3000\n+\n+Vmstat Counters\n+===============\n+Following vmstat counters provide some stats about pghot subsystem.\n+\n+Path: /proc/vmstat\n+\n+1. **pghot_recorded_accesses**\n+   - Number of total hot page accesses recorded by pghot.\n+\n+2. **pghot_recorded_hwhints**\n+   - Number of recorded accesses reported by hwhints source.\n+\n+3. **pghot_recorded_pgtscans**\n+   - Number of recorded accesses reported by PTE A-bit based source.\n+\n+4. **pghot_recorded_hintfaults**\n+   - Number of recorded accesses reported by NUMA Balancing based\n+     hotness source.\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 75ef7c9f9307..22e08befb096 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -1064,6 +1064,7 @@ enum pgdat_flags {\n \t\t\t\t\t * many pages under writeback\n \t\t\t\t\t */\n \tPGDAT_RECLAIM_LOCKED,\t\t/* prevents concurrent reclaim */\n+\tPGDAT_KMIGRATED_ACTIVATE,\t/* activates kmigrated */\n };\n \n enum zone_flags {\n@@ -1518,6 +1519,10 @@ typedef struct pglist_data {\n #ifdef CONFIG_MEMORY_FAILURE\n \tstruct memory_failure_stats mf_stats;\n #endif\n+#ifdef CONFIG_PGHOT\n+\tstruct task_struct *kmigrated;\n+\twait_queue_head_t kmigrated_wait;\n+#endif\n } pg_data_t;\n \n #define node_present_pages(nid)\t(NODE_DATA(nid)->node_present_pages)\n@@ -1916,12 +1921,28 @@ struct mem_section {\n \tunsigned long section_mem_map;\n \n \tstruct mem_section_usage *usage;\n+#ifdef CONFIG_PGHOT\n+\t/*\n+\t * Per-PFN hotness data for this section.\n+\t * Array of phi_t (u8 in default mode).\n+\t * LSB is used as PGHOT_SECTION_HOT_BIT flag.\n+\t */\n+\tvoid *hot_map;\n+#endif\n #ifdef CONFIG_PAGE_EXTENSION\n \t/*\n \t * If SPARSEMEM, pgdat doesn't have page_ext pointer. We use\n \t * section. (see page_ext.h about this.)\n \t */\n \tstruct page_ext *page_ext;\n+#endif\n+\t/*\n+\t * Padding to maintain consistent mem_section size when exactly\n+\t * one of PGHOT or PAGE_EXTENSION is enabled. This ensures\n+\t * optimal alignment regardless of configuration.\n+\t */\n+#if (defined(CONFIG_PGHOT) && !defined(CONFIG_PAGE_EXTENSION)) || \\\n+\t\t(!defined(CONFIG_PGHOT) && defined(CONFIG_PAGE_EXTENSION))\n \tunsigned long pad;\n #endif\n \t/*\ndiff --git a/include/linux/pghot.h b/include/linux/pghot.h\nnew file mode 100644\nindex 000000000000..88e57aab697b\n--- /dev/null\n+++ b/include/linux/pghot.h\n@@ -0,0 +1,94 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+#ifndef _LINUX_PGHOT_H\n+#define _LINUX_PGHOT_H\n+\n+/* Page hotness temperature sources */\n+enum pghot_src {\n+\tPGHOT_HW_HINTS,\n+\tPGHOT_PGTABLE_SCAN,\n+\tPGHOT_HINT_FAULT,\n+};\n+\n+#ifdef CONFIG_PGHOT\n+#include <linux/static_key.h>\n+\n+extern unsigned int pghot_target_nid;\n+extern unsigned int pghot_src_enabled;\n+extern unsigned int pghot_freq_threshold;\n+extern unsigned int kmigrated_sleep_ms;\n+extern unsigned int kmigrated_batch_nr;\n+extern unsigned int sysctl_pghot_freq_window;\n+\n+void pghot_debug_init(void);\n+\n+DECLARE_STATIC_KEY_FALSE(pghot_src_hwhints);\n+DECLARE_STATIC_KEY_FALSE(pghot_src_pgtscans);\n+DECLARE_STATIC_KEY_FALSE(pghot_src_hintfaults);\n+\n+/*\n+ * Bit positions to enable individual sources in pghot/records_enabled\n+ * of debugfs.\n+ */\n+enum pghot_src_enabled {\n+\tPGHOT_HWHINTS_BIT = 0,\n+\tPGHOT_PGTSCAN_BIT,\n+\tPGHOT_HINTFAULT_BIT,\n+\tPGHOT_MAX_BIT\n+};\n+\n+#define PGHOT_HWHINTS_ENABLED\t\tBIT(PGHOT_HWHINTS_BIT)\n+#define PGHOT_PGTSCAN_ENABLED\t\tBIT(PGHOT_PGTSCAN_BIT)\n+#define PGHOT_HINTFAULT_ENABLED\t\tBIT(PGHOT_HINTFAULT_BIT)\n+#define PGHOT_SRC_ENABLED_MASK\t\tGENMASK(PGHOT_MAX_BIT - 1, 0)\n+\n+#define PGHOT_DEFAULT_FREQ_THRESHOLD\t2\n+\n+#define KMIGRATED_DEFAULT_SLEEP_MS\t100\n+#define KMIGRATED_DEFAULT_BATCH_NR\t512\n+\n+#define PGHOT_DEFAULT_NODE\t\t0\n+\n+#define PGHOT_DEFAULT_FREQ_WINDOW\t(4 * MSEC_PER_SEC)\n+\n+/*\n+ * Bits 0-6 are used to store frequency and time.\n+ * Bit 7 is used to indicate the page is ready for migration.\n+ */\n+#define PGHOT_MIGRATE_READY\t\t7\n+\n+#define PGHOT_FREQ_WIDTH\t\t2\n+/* Bucketed time is stored in 5 bits which can represent up to 4s with HZ=1000 */\n+#define PGHOT_TIME_BUCKETS_WIDTH\t7\n+#define PGHOT_TIME_WIDTH\t\t5\n+#define PGHOT_NID_WIDTH\t\t\t10\n+\n+#define PGHOT_FREQ_SHIFT\t\t0\n+#define PGHOT_TIME_SHIFT\t\t(PGHOT_FREQ_SHIFT + PGHOT_FREQ_WIDTH)\n+\n+#define PGHOT_FREQ_MASK\t\t\tGENMASK(PGHOT_FREQ_WIDTH - 1, 0)\n+#define PGHOT_TIME_MASK\t\t\tGENMASK(PGHOT_TIME_WIDTH - 1, 0)\n+#define PGHOT_TIME_BUCKETS_MASK\t\t(PGHOT_TIME_MASK << PGHOT_TIME_BUCKETS_WIDTH)\n+\n+#define PGHOT_NID_MAX\t\t\t((1 << PGHOT_NID_WIDTH) - 1)\n+#define PGHOT_FREQ_MAX\t\t\t((1 << PGHOT_FREQ_WIDTH) - 1)\n+#define PGHOT_TIME_MAX\t\t\t((1 << PGHOT_TIME_WIDTH) - 1)\n+\n+typedef u8 phi_t;\n+\n+#define PGHOT_RECORD_SIZE\t\tsizeof(phi_t)\n+\n+#define PGHOT_SECTION_HOT_BIT\t\t0\n+#define PGHOT_SECTION_HOT_MASK\t\tBIT(PGHOT_SECTION_HOT_BIT)\n+\n+unsigned long pghot_access_latency(unsigned long old_time, unsigned long time);\n+bool pghot_update_record(phi_t *phi, int nid, unsigned long now);\n+int pghot_get_record(phi_t *phi, int *nid, int *freq, unsigned long *time);\n+\n+int pghot_record_access(unsigned long pfn, int nid, int src, unsigned long now);\n+#else\n+static inline int pghot_record_access(unsigned long pfn, int nid, int src, unsigned long now)\n+{\n+\treturn 0;\n+}\n+#endif /* CONFIG_PGHOT */\n+#endif /* _LINUX_PGHOT_H */\ndiff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h\nindex 92f80b4d69a6..5b8fd93b55fd 100644\n--- a/include/linux/vm_event_item.h\n+++ b/include/linux/vm_event_item.h\n@@ -188,6 +188,12 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,\n \t\tKSTACK_REST,\n #endif\n #endif /* CONFIG_DEBUG_STACK_USAGE */\n+#ifdef CONFIG_PGHOT\n+\t\tPGHOT_RECORDED_ACCESSES,\n+\t\tPGHOT_RECORD_HWHINTS,\n+\t\tPGHOT_RECORD_PGTSCANS,\n+\t\tPGHOT_RECORD_HINTFAULTS,\n+#endif /* CONFIG_PGHOT */\n \t\tNR_VM_EVENT_ITEMS\n };\n \ndiff --git a/mm/Kconfig b/mm/Kconfig\nindex bd0ea5454af8..f4f0147faac5 100644\n--- a/mm/Kconfig\n+++ b/mm/Kconfig\n@@ -1464,6 +1464,20 @@ config PT_RECLAIM\n config FIND_NORMAL_PAGE\n \tdef_bool n\n \n+config PGHOT\n+\tbool \"Hot page tracking and promotion\"\n+\tdef_bool n\n+\tdepends on NUMA && MIGRATION && SPARSEMEM && MMU\n+\thelp\n+\t  A sub-system to track page accesses in lower tier memory and\n+\t  maintain hot page information. Promotes hot pages from lower\n+\t  tiers to top tier by using the memory access information provided\n+\t  by various sources. Asynchronous promotion is done by per-node\n+\t  kernel threads.\n+\n+\t  This adds 1 byte of metadata overhead per page in lower-tier\n+\t  memory nodes.\n+\n source \"mm/damon/Kconfig\"\n \n endmenu\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 2d0570a16e5b..655a27f3a215 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -147,3 +147,4 @@ obj-$(CONFIG_SHRINKER_DEBUG) += shrinker_debug.o\n obj-$(CONFIG_EXECMEM) += execmem.o\n obj-$(CONFIG_TMPFS_QUOTA) += shmem_quota.o\n obj-$(CONFIG_PT_RECLAIM) += pt_reclaim.o\n+obj-$(CONFIG_PGHOT) += pghot.o pghot-tunables.o pghot-default.o\ndiff --git a/mm/mm_init.c b/mm/mm_init.c\nindex fc2a6f1e518f..64109feaa1c3 100644\n--- a/mm/mm_init.c\n+++ b/mm/mm_init.c\n@@ -1401,6 +1401,15 @@ static void pgdat_init_kcompactd(struct pglist_data *pgdat)\n static void pgdat_init_kcompactd(struct pglist_data *pgdat) {}\n #endif\n \n+#ifdef CONFIG_PGHOT\n+static void pgdat_init_kmigrated(struct pglist_data *pgdat)\n+{\n+\tinit_waitqueue_head(&pgdat->kmigrated_wait);\n+}\n+#else\n+static inline void pgdat_init_kmigrated(struct pglist_data *pgdat) {}\n+#endif\n+\n static void __meminit pgdat_init_internals(struct pglist_data *pgdat)\n {\n \tint i;\n@@ -1410,6 +1419,7 @@ static void __meminit pgdat_init_internals(struct pglist_data *pgdat)\n \n \tpgdat_init_split_queue(pgdat);\n \tpgdat_init_kcompactd(pgdat);\n+\tpgdat_init_kmigrated(pgdat);\n \n \tinit_waitqueue_head(&pgdat->kswapd_wait);\n \tinit_waitqueue_head(&pgdat->pfmemalloc_wait);\ndiff --git a/mm/pghot-default.c b/mm/pghot-default.c\nnew file mode 100644\nindex 000000000000..e0a3b2ed2592\n--- /dev/null\n+++ b/mm/pghot-default.c\n@@ -0,0 +1,73 @@\n+// SPDX-License-Identifier: GPL-2.0\n+/*\n+ * pghot: Default mode\n+ *\n+ * 1 byte hotness record per PFN.\n+ * Bucketed time and frequency tracked as part of the record.\n+ * Promotion to @pghot_target_nid by default.\n+ */\n+\n+#include <linux/pghot.h>\n+#include <linux/jiffies.h>\n+\n+/*\n+ * @time is regular time, @old_time is bucketed time.\n+ */\n+unsigned long pghot_access_latency(unsigned long old_time, unsigned long time)\n+{\n+\ttime &= PGHOT_TIME_BUCKETS_MASK;\n+\told_time <<= PGHOT_TIME_BUCKETS_WIDTH;\n+\n+\treturn jiffies_to_msecs((time - old_time) & PGHOT_TIME_BUCKETS_MASK);\n+}\n+\n+bool pghot_update_record(phi_t *phi, int nid, unsigned long now)\n+{\n+\tphi_t freq, old_freq, hotness, old_hotness, old_time;\n+\tphi_t time = now >> PGHOT_TIME_BUCKETS_WIDTH;\n+\n+\told_hotness = READ_ONCE(*phi);\n+\tdo {\n+\t\tbool new_window = false;\n+\n+\t\thotness = old_hotness;\n+\t\told_freq = (hotness >> PGHOT_FREQ_SHIFT) & PGHOT_FREQ_MASK;\n+\t\told_time = (hotness >> PGHOT_TIME_SHIFT) & PGHOT_TIME_MASK;\n+\n+\t\tif (pghot_access_latency(old_time, now) > sysctl_pghot_freq_window)\n+\t\t\tnew_window = true;\n+\n+\t\tif (new_window)\n+\t\t\tfreq = 1;\n+\t\telse if (old_freq < PGHOT_FREQ_MAX)\n+\t\t\tfreq = old_freq + 1;\n+\t\telse\n+\t\t\tfreq = old_freq;\n+\n+\t\thotness &= ~(PGHOT_FREQ_MASK << PGHOT_FREQ_SHIFT);\n+\t\thotness &= ~(PGHOT_TIME_MASK << PGHOT_TIME_SHIFT);\n+\n+\t\thotness |= (freq & PGHOT_FREQ_MASK) << PGHOT_FREQ_SHIFT;\n+\t\thotness |= (time & PGHOT_TIME_MASK) << PGHOT_TIME_SHIFT;\n+\n+\t\tif (freq >= pghot_freq_threshold)\n+\t\t\thotness |= BIT(PGHOT_MIGRATE_READY);\n+\t} while (unlikely(!try_cmpxchg(phi, &old_hotness, hotness)));\n+\treturn !!(hotness & BIT(PGHOT_MIGRATE_READY));\n+}\n+\n+int pghot_get_record(phi_t *phi, int *nid, int *freq, unsigned long *time)\n+{\n+\tphi_t old_hotness, hotness = 0;\n+\n+\told_hotness = READ_ONCE(*phi);\n+\tdo {\n+\t\tif (!(old_hotness & BIT(PGHOT_MIGRATE_READY)))\n+\t\t\treturn -EINVAL;\n+\t} while (unlikely(!try_cmpxchg(phi, &old_hotness, hotness)));\n+\n+\t*nid = pghot_target_nid;\n+\t*freq = (old_hotness >> PGHOT_FREQ_SHIFT) & PGHOT_FREQ_MASK;\n+\t*time = (old_hotness >> PGHOT_TIME_SHIFT) & PGHOT_TIME_MASK;\n+\treturn 0;\n+}\ndiff --git a/mm/pghot-tunables.c b/mm/pghot-tunables.c\nnew file mode 100644\nindex 000000000000..79afbcb1e4f0\n--- /dev/null\n+++ b/mm/pghot-tunables.c\n@@ -0,0 +1,189 @@\n+// SPDX-License-Identifier: GPL-2.0\n+/*\n+ * pghot tunables in debugfs\n+ */\n+#include <linux/pghot.h>\n+#include <linux/memory-tiers.h>\n+#include <linux/debugfs.h>\n+\n+static struct dentry *debugfs_pghot;\n+static DEFINE_MUTEX(pghot_tunables_lock);\n+\n+static ssize_t pghot_freq_th_write(struct file *filp, const char __user *ubuf,\n+\t\t\t\t   size_t cnt, loff_t *ppos)\n+{\n+\tchar buf[16];\n+\tunsigned int freq;\n+\n+\tif (cnt > 15)\n+\t\tcnt = 15;\n+\n+\tif (copy_from_user(&buf, ubuf, cnt))\n+\t\treturn -EFAULT;\n+\tbuf[cnt] = '\\0';\n+\n+\tif (kstrtouint(buf, 10, &freq))\n+\t\treturn -EINVAL;\n+\n+\tif (!freq || freq > PGHOT_FREQ_MAX)\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&pghot_tunables_lock);\n+\tpghot_freq_threshold = freq;\n+\tmutex_unlock(&pghot_tunables_lock);\n+\n+\t*ppos += cnt;\n+\treturn cnt;\n+}\n+\n+static int pghot_freq_th_show(struct seq_file *m, void *v)\n+{\n+\tseq_printf(m, \"%d\\n\", pghot_freq_threshold);\n+\treturn 0;\n+}\n+\n+static int pghot_freq_th_open(struct inode *inode, struct file *filp)\n+{\n+\treturn single_open(filp, pghot_freq_th_show, NULL);\n+}\n+\n+static const struct file_operations pghot_freq_th_fops = {\n+\t.open\t\t= pghot_freq_th_open,\n+\t.write\t\t= pghot_freq_th_write,\n+\t.read\t\t= seq_read,\n+\t.llseek\t\t= seq_lseek,\n+\t.release\t= seq_release,\n+};\n+\n+static ssize_t pghot_target_nid_write(struct file *filp, const char __user *ubuf,\n+\t\t\t\t      size_t cnt, loff_t *ppos)\n+{\n+\tchar buf[16];\n+\tunsigned int nid;\n+\n+\tif (cnt > 15)\n+\t\tcnt = 15;\n+\n+\tif (copy_from_user(&buf, ubuf, cnt))\n+\t\treturn -EFAULT;\n+\tbuf[cnt] = '\\0';\n+\n+\tif (kstrtouint(buf, 10, &nid))\n+\t\treturn -EINVAL;\n+\n+\tif (nid > PGHOT_NID_MAX || !node_online(nid) || !node_is_toptier(nid))\n+\t\treturn -EINVAL;\n+\tmutex_lock(&pghot_tunables_lock);\n+\tpghot_target_nid = nid;\n+\tmutex_unlock(&pghot_tunables_lock);\n+\n+\t*ppos += cnt;\n+\treturn cnt;\n+}\n+\n+static int pghot_target_nid_show(struct seq_file *m, void *v)\n+{\n+\tseq_printf(m, \"%d\\n\", pghot_target_nid);\n+\treturn 0;\n+}\n+\n+static int pghot_target_nid_open(struct inode *inode, struct file *filp)\n+{\n+\treturn single_open(filp, pghot_target_nid_show, NULL);\n+}\n+\n+static const struct file_operations pghot_target_nid_fops = {\n+\t.open\t\t= pghot_target_nid_open,\n+\t.write\t\t= pghot_target_nid_write,\n+\t.read\t\t= seq_read,\n+\t.llseek\t\t= seq_lseek,\n+\t.release\t= seq_release,\n+};\n+\n+static void pghot_src_enabled_update(unsigned int enabled)\n+{\n+\tunsigned int changed = pghot_src_enabled ^ enabled;\n+\n+\tif (changed & PGHOT_HWHINTS_ENABLED) {\n+\t\tif (enabled & PGHOT_HWHINTS_ENABLED)\n+\t\t\tstatic_branch_enable(&pghot_src_hwhints);\n+\t\telse\n+\t\t\tstatic_branch_disable(&pghot_src_hwhints);\n+\t}\n+\n+\tif (changed & PGHOT_PGTSCAN_ENABLED) {\n+\t\tif (enabled & PGHOT_PGTSCAN_ENABLED)\n+\t\t\tstatic_branch_enable(&pghot_src_pgtscans);\n+\t\telse\n+\t\t\tstatic_branch_disable(&pghot_src_pgtscans);\n+\t}\n+\n+\tif (changed & PGHOT_HINTFAULT_ENABLED) {\n+\t\tif (enabled & PGHOT_HINTFAULT_ENABLED)\n+\t\t\tstatic_branch_enable(&pghot_src_hintfaults);\n+\t\telse\n+\t\t\tstatic_branch_disable(&pghot_src_hintfaults);\n+\t}\n+}\n+\n+static ssize_t pghot_src_enabled_write(struct file *filp, const char __user *ubuf,\n+\t\t\t\t\t   size_t cnt, loff_t *ppos)\n+{\n+\tchar buf[16];\n+\tunsigned int enabled;\n+\n+\tif (cnt > 15)\n+\t\tcnt = 15;\n+\n+\tif (copy_from_user(&buf, ubuf, cnt))\n+\t\treturn -EFAULT;\n+\tbuf[cnt] = '\\0';\n+\n+\tif (kstrtouint(buf, 0, &enabled))\n+\t\treturn -EINVAL;\n+\n+\tif (enabled & ~PGHOT_SRC_ENABLED_MASK)\n+\t\treturn -EINVAL;\n+\n+\tmutex_lock(&pghot_tunables_lock);\n+\tpghot_src_enabled_update(enabled);\n+\tpghot_src_enabled = enabled;\n+\tmutex_unlock(&pghot_tunables_lock);\n+\n+\t*ppos += cnt;\n+\treturn cnt;\n+}\n+\n+static int pghot_src_enabled_show(struct seq_file *m, void *v)\n+{\n+\tseq_printf(m, \"%d\\n\", pghot_src_enabled);\n+\treturn 0;\n+}\n+\n+static int pghot_src_enabled_open(struct inode *inode, struct file *filp)\n+{\n+\treturn single_open(filp, pghot_src_enabled_show, NULL);\n+}\n+\n+static const struct file_operations pghot_src_enabled_fops = {\n+\t.open\t\t= pghot_src_enabled_open,\n+\t.write\t\t= pghot_src_enabled_write,\n+\t.read\t\t= seq_read,\n+\t.llseek\t\t= seq_lseek,\n+\t.release\t= seq_release,\n+};\n+\n+void pghot_debug_init(void)\n+{\n+\tdebugfs_pghot = debugfs_create_dir(\"pghot\", NULL);\n+\tdebugfs_create_file(\"enabled_sources\", 0644, debugfs_pghot, NULL,\n+\t\t\t    &pghot_src_enabled_fops);\n+\tdebugfs_create_file(\"target_nid\", 0644, debugfs_pghot, NULL,\n+\t\t\t    &pghot_target_nid_fops);\n+\tdebugfs_create_file(\"freq_threshold\", 0644, debugfs_pghot, NULL,\n+\t\t\t    &pghot_freq_th_fops);\n+\tdebugfs_create_u32(\"kmigrated_sleep_ms\", 0644, debugfs_pghot,\n+\t\t\t    &kmigrated_sleep_ms);\n+\tdebugfs_create_u32(\"kmigrated_batch_nr\", 0644, debugfs_pghot,\n+\t\t\t    &kmigrated_batch_nr);\n+}\ndiff --git a/mm/pghot.c b/mm/pghot.c\nnew file mode 100644\nindex 000000000000..95b5012d5b99\n--- /dev/null\n+++ b/mm/pghot.c\n@@ -0,0 +1,370 @@\n+// SPDX-License-Identifier: GPL-2.0\n+/*\n+ * Maintains information about hot pages from slower tier nodes and\n+ * promotes them.\n+ *\n+ * Per-PFN hotness information is stored for lower tier nodes in\n+ * mem_section.\n+ *\n+ * In the default mode, a single byte (u8) is used to store\n+ * the frequency of access and last access time. Promotions are done\n+ * to a default toptier NID.\n+ *\n+ * A kernel thread named kmigrated is provided to migrate or promote\n+ * the hot pages. kmigrated runs for each lower tier node. It iterates\n+ * over the node's PFNs and  migrates pages marked for migration into\n+ * their targeted nodes.\n+ */\n+#include <linux/mm.h>\n+#include <linux/migrate.h>\n+#include <linux/memory-tiers.h>\n+#include <linux/pghot.h>\n+\n+unsigned int pghot_target_nid = PGHOT_DEFAULT_NODE;\n+unsigned int pghot_src_enabled;\n+unsigned int pghot_freq_threshold = PGHOT_DEFAULT_FREQ_THRESHOLD;\n+unsigned int kmigrated_sleep_ms = KMIGRATED_DEFAULT_SLEEP_MS;\n+unsigned int kmigrated_batch_nr = KMIGRATED_DEFAULT_BATCH_NR;\n+\n+unsigned int sysctl_pghot_freq_window = PGHOT_DEFAULT_FREQ_WINDOW;\n+\n+DEFINE_STATIC_KEY_FALSE(pghot_src_hwhints);\n+DEFINE_STATIC_KEY_FALSE(pghot_src_pgtscans);\n+DEFINE_STATIC_KEY_FALSE(pghot_src_hintfaults);\n+\n+#ifdef CONFIG_SYSCTL\n+static const struct ctl_table pghot_sysctls[] = {\n+\t{\n+\t\t.procname       = \"pghot_promote_freq_window_ms\",\n+\t\t.data           = &sysctl_pghot_freq_window,\n+\t\t.maxlen         = sizeof(unsigned int),\n+\t\t.mode           = 0644,\n+\t\t.proc_handler   = proc_dointvec_minmax,\n+\t\t.extra1         = SYSCTL_ZERO,\n+\t},\n+};\n+#endif\n+\n+static bool kmigrated_started __ro_after_init;\n+\n+/**\n+ * pghot_record_access() - Record page accesses from lower tier memory\n+ * for the purpose of tracking page hotness and subsequent promotion.\n+ *\n+ * @pfn: PFN of the page\n+ * @nid: Unused\n+ * @src: The identifier of the sub-system that reports the access\n+ * @now: Access time in jiffies\n+ *\n+ * Updates the frequency and time of access and marks the page as\n+ * ready for migration if the frequency crosses a threshold. The pages\n+ * marked for migration are migrated by kmigrated kernel thread.\n+ *\n+ * Return: 0 on success and -EINVAL on failure to record the access.\n+ */\n+int pghot_record_access(unsigned long pfn, int nid, int src, unsigned long now)\n+{\n+\tstruct mem_section *ms;\n+\tstruct folio *folio;\n+\tphi_t *phi, *hot_map;\n+\tstruct page *page;\n+\n+\tif (!kmigrated_started)\n+\t\treturn -EINVAL;\n+\n+\tif (nid >= PGHOT_NID_MAX)\n+\t\treturn -EINVAL;\n+\n+\tswitch (src) {\n+\tcase PGHOT_HW_HINTS:\n+\t\tif (!static_branch_likely(&pghot_src_hwhints))\n+\t\t\treturn -EINVAL;\n+\t\tcount_vm_event(PGHOT_RECORD_HWHINTS);\n+\t\tbreak;\n+\tcase PGHOT_PGTABLE_SCAN:\n+\t\tif (!static_branch_likely(&pghot_src_pgtscans))\n+\t\t\treturn -EINVAL;\n+\t\tcount_vm_event(PGHOT_RECORD_PGTSCANS);\n+\t\tbreak;\n+\tcase PGHOT_HINT_FAULT:\n+\t\tif (!static_branch_likely(&pghot_src_hintfaults))\n+\t\t\treturn -EINVAL;\n+\t\tcount_vm_event(PGHOT_RECORD_HINTFAULTS);\n+\t\tbreak;\n+\tdefault:\n+\t\treturn -EINVAL;\n+\t}\n+\n+\t/*\n+\t * Record only accesses from lower tiers.\n+\t */\n+\tif (node_is_toptier(pfn_to_nid(pfn)))\n+\t\treturn 0;\n+\n+\t/*\n+\t * Reject the non-migratable pages right away.\n+\t */\n+\tpage = pfn_to_online_page(pfn);\n+\tif (!page || is_zone_device_page(page))\n+\t\treturn 0;\n+\n+\tfolio = page_folio(page);\n+\tif (!folio_test_lru(folio))\n+\t\treturn 0;\n+\n+\t/* Get the hotness slot corresponding to the 1st PFN of the folio */\n+\tpfn = folio_pfn(folio);\n+\tms = __pfn_to_section(pfn);\n+\tif (!ms || !ms->hot_map)\n+\t\treturn -EINVAL;\n+\n+\thot_map = (phi_t *)(((unsigned long)(ms->hot_map)) & ~PGHOT_SECTION_HOT_MASK);\n+\tphi = &hot_map[pfn % PAGES_PER_SECTION];\n+\n+\tcount_vm_event(PGHOT_RECORDED_ACCESSES);\n+\n+\t/*\n+\t * Update the hotness parameters.\n+\t */\n+\tif (pghot_update_record(phi, nid, now)) {\n+\t\tset_bit(PGHOT_SECTION_HOT_BIT, (unsigned long *)&ms->hot_map);\n+\t\tset_bit(PGDAT_KMIGRATED_ACTIVATE, &page_pgdat(page)->flags);\n+\t}\n+\treturn 0;\n+}\n+\n+static int pghot_get_hotness(unsigned long pfn, int *nid, int *freq,\n+\t\t\t     unsigned long *time)\n+{\n+\tphi_t *phi, *hot_map;\n+\tstruct mem_section *ms;\n+\n+\tms = __pfn_to_section(pfn);\n+\tif (!ms || !ms->hot_map)\n+\t\treturn -EINVAL;\n+\n+\thot_map = (phi_t *)(((unsigned long)(ms->hot_map)) & ~PGHOT_SECTION_HOT_MASK);\n+\tphi = &hot_map[pfn % PAGES_PER_SECTION];\n+\n+\treturn pghot_get_record(phi, nid, freq, time);\n+}\n+\n+/*\n+ * Walks the PFNs of the zone, isolates and migrates them in batches.\n+ */\n+static void kmigrated_walk_zone(unsigned long start_pfn, unsigned long end_pfn,\n+\t\t\t\tint src_nid)\n+{\n+\tint cur_nid = NUMA_NO_NODE;\n+\tLIST_HEAD(migrate_list);\n+\tint batch_count = 0;\n+\tstruct folio *folio;\n+\tstruct page *page;\n+\tunsigned long pfn;\n+\n+\tpfn = start_pfn;\n+\tdo {\n+\t\tint nid = NUMA_NO_NODE, nr = 1;\n+\t\tint freq = 0;\n+\t\tunsigned long time = 0;\n+\n+\t\tif (!pfn_valid(pfn))\n+\t\t\tgoto out_next;\n+\n+\t\tpage = pfn_to_online_page(pfn);\n+\t\tif (!page)\n+\t\t\tgoto out_next;\n+\n+\t\tfolio = page_folio(page);\n+\t\tnr = folio_nr_pages(folio);\n+\t\tif (folio_nid(folio) != src_nid)\n+\t\t\tgoto out_next;\n+\n+\t\tif (!folio_test_lru(folio))\n+\t\t\tgoto out_next;\n+\n+\t\tif (pghot_get_hotness(pfn, &nid, &freq, &time))\n+\t\t\tgoto out_next;\n+\n+\t\tif (nid == NUMA_NO_NODE)\n+\t\t\tnid = pghot_target_nid;\n+\n+\t\tif (folio_nid(folio) == nid)\n+\t\t\tgoto out_next;\n+\n+\t\tif (migrate_misplaced_folio_prepare(folio, NULL, nid))\n+\t\t\tgoto out_next;\n+\n+\t\tif (cur_nid == NUMA_NO_NODE)\n+\t\t\tcur_nid = nid;\n+\n+\t\t/* If NID changed, flush the previous batch first */\n+\t\tif (cur_nid != nid) {\n+\t\t\tif (!list_empty(&migrate_list))\n+\t\t\t\tmigrate_misplaced_folios_batch(&migrate_list, cur_nid);\n+\t\t\tcur_nid = nid;\n+\t\t\tbatch_count = 0;\n+\t\t\tcond_resched();\n+\t\t}\n+\n+\t\tlist_add(&folio->lru, &migrate_list);\n+\n+\t\tif (++batch_count > kmigrated_batch_nr) {\n+\t\t\tmigrate_misplaced_folios_batch(&migrate_list, cur_nid);\n+\t\t\tbatch_count = 0;\n+\t\t\tcond_resched();\n+\t\t}\n+out_next:\n+\t\tpfn += nr;\n+\t} while (pfn < end_pfn);\n+\tif (!list_empty(&migrate_list))\n+\t\tmigrate_misplaced_folios_batch(&migrate_list, cur_nid);\n+}\n+\n+static void kmigrated_do_work(pg_data_t *pgdat)\n+{\n+\tunsigned long section_nr, s_begin, start_pfn;\n+\tstruct mem_section *ms;\n+\tint nid;\n+\n+\tclear_bit(PGDAT_KMIGRATED_ACTIVATE, &pgdat->flags);\n+\t/* s_begin = first_present_section_nr(); */\n+\ts_begin = next_present_section_nr(-1);\n+\tfor_each_present_section_nr(s_begin, section_nr) {\n+\t\tstart_pfn = section_nr_to_pfn(section_nr);\n+\t\tms = __nr_to_section(section_nr);\n+\n+\t\tif (!pfn_valid(start_pfn))\n+\t\t\tcontinue;\n+\n+\t\tnid = pfn_to_nid(start_pfn);\n+\t\tif (node_is_toptier(nid) || nid != pgdat->node_id)\n+\t\t\tcontinue;\n+\n+\t\tif (!test_and_clear_bit(PGHOT_SECTION_HOT_BIT, (unsigned long *)&ms->hot_map))\n+\t\t\tcontinue;\n+\n+\t\tkmigrated_walk_zone(start_pfn, start_pfn + PAGES_PER_SECTION,\n+\t\t\t\t    pgdat->node_id);\n+\t}\n+}\n+\n+static inline bool kmigrated_work_requested(pg_data_t *pgdat)\n+{\n+\treturn test_bit(PGDAT_KMIGRATED_ACTIVATE, &pgdat->flags);\n+}\n+\n+/*\n+ * Per-node kthread that iterates over its PFNs and migrates the\n+ * pages that have been marked for migration.\n+ */\n+static int kmigrated(void *p)\n+{\n+\tlong timeout = msecs_to_jiffies(kmigrated_sleep_ms);\n+\tpg_data_t *pgdat = p;\n+\n+\twhile (!kthread_should_stop()) {\n+\t\tif (wait_event_timeout(pgdat->kmigrated_wait, kmigrated_work_requested(pgdat),\n+\t\t\t\t       timeout))\n+\t\t\tkmigrated_do_work(pgdat);\n+\t}\n+\treturn 0;\n+}\n+\n+static int kmigrated_run(int nid)\n+{\n+\tpg_data_t *pgdat = NODE_DATA(nid);\n+\tint ret;\n+\n+\tif (node_is_toptier(nid))\n+\t\treturn 0;\n+\n+\tif (!pgdat->kmigrated) {\n+\t\tpgdat->kmigrated = kthread_create_on_node(kmigrated, pgdat, nid,\n+\t\t\t\t\t\t\t  \"kmigrated%d\", nid);\n+\t\tif (IS_ERR(pgdat->kmigrated)) {\n+\t\t\tret = PTR_ERR(pgdat->kmigrated);\n+\t\t\tpgdat->kmigrated = NULL;\n+\t\t\tpr_err(\"Failed to start kmigrated%d, ret %d\\n\", nid, ret);\n+\t\t\treturn ret;\n+\t\t}\n+\t\tpr_info(\"pghot: Started kmigrated thread for node %d\\n\", nid);\n+\t}\n+\twake_up_process(pgdat->kmigrated);\n+\treturn 0;\n+}\n+\n+static void pghot_free_hot_map(void)\n+{\n+\tunsigned long section_nr, s_begin;\n+\tstruct mem_section *ms;\n+\n+\t/* s_begin = first_present_section_nr(); */\n+\ts_begin = next_present_section_nr(-1);\n+\tfor_each_present_section_nr(s_begin, section_nr) {\n+\t\tms = __nr_to_section(section_nr);\n+\t\tkfree(ms->hot_map);\n+\t}\n+}\n+\n+static int pghot_alloc_hot_map(void)\n+{\n+\tunsigned long section_nr, s_begin, start_pfn;\n+\tstruct mem_section *ms;\n+\tint nid;\n+\n+\t/* s_begin = first_present_section_nr(); */\n+\ts_begin = next_present_section_nr(-1);\n+\tfor_each_present_section_nr(s_begin, section_nr) {\n+\t\tms = __nr_to_section(section_nr);\n+\t\tstart_pfn = section_nr_to_pfn(section_nr);\n+\t\tnid = pfn_to_nid(start_pfn);\n+\n+\t\tif (node_is_toptier(nid) || !pfn_valid(start_pfn))\n+\t\t\tcontinue;\n+\n+\t\tms->hot_map = kcalloc_node(PAGES_PER_SECTION, PGHOT_RECORD_SIZE, GFP_KERNEL,\n+\t\t\t\t\t   nid);\n+\t\tif (!ms->hot_map)\n+\t\t\tgoto out_free_hot_map;\n+\t}\n+\treturn 0;\n+\n+out_free_hot_map:\n+\tpghot_free_hot_map();\n+\treturn -ENOMEM;\n+}\n+\n+static int __init pghot_init(void)\n+{\n+\tpg_data_t *pgdat;\n+\tint nid, ret;\n+\n+\tret = pghot_alloc_hot_map();\n+\tif (ret)\n+\t\treturn ret;\n+\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tret = kmigrated_run(nid);\n+\t\tif (ret)\n+\t\t\tgoto out_stop_kthread;\n+\t}\n+\tregister_sysctl_init(\"vm\", pghot_sysctls);\n+\tpghot_debug_init();\n+\n+\tkmigrated_started = true;\n+\treturn 0;\n+\n+out_stop_kthread:\n+\tfor_each_node_state(nid, N_MEMORY) {\n+\t\tpgdat = NODE_DATA(nid);\n+\t\tif (pgdat->kmigrated) {\n+\t\t\tkthread_stop(pgdat->kmigrated);\n+\t\t\tpgdat->kmigrated = NULL;\n+\t\t}\n+\t}\n+\tpghot_free_hot_map();\n+\treturn ret;\n+}\n+\n+late_initcall_sync(pghot_init)\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 65de88cdf40e..f6f91b9dd887 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -1501,6 +1501,12 @@ const char * const vmstat_text[] = {\n \t[I(KSTACK_REST)]\t\t\t= \"kstack_rest\",\n #endif\n #endif\n+#ifdef CONFIG_PGHOT\n+\t[I(PGHOT_RECORDED_ACCESSES)]\t\t= \"pghot_recorded_accesses\",\n+\t[I(PGHOT_RECORD_HWHINTS)]\t\t= \"pghot_recorded_hwhints\",\n+\t[I(PGHOT_RECORD_PGTSCANS)]\t\t= \"pghot_recorded_pgtscans\",\n+\t[I(PGHOT_RECORD_HINTFAULTS)]\t\t= \"pghot_recorded_hintfaults\",\n+#endif /* CONFIG_PGHOT */\n #undef I\n #endif /* CONFIG_VM_EVENT_COUNTERS */\n };\n-- \n2.34.1\n\n\n\n---\n\nBy default, one byte per PFN is used to store hotness information.\nLimited number of bits are used to store the access time leading\nto coarse-grained time tracking. Also there aren't enough bits\nto track the toptier NID explicitly and hence the default target_nid\nis used for promotion.\n\nThis precise mode relaxes the above situation by storing the\nhotness information in 4 bytes per PFN. More fine-grained\naccess time tracking and toptier NID tracking becomes possible\nin this mode.\n\nTypically useful when toptier consists of more than one node.\n\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n Documentation/admin-guide/mm/pghot.txt |  4 +-\n include/linux/mmzone.h                 |  2 +-\n include/linux/pghot.h                  | 31 ++++++++++++\n mm/Kconfig                             | 11 ++++\n mm/Makefile                            |  7 ++-\n mm/pghot-precise.c                     | 70 ++++++++++++++++++++++++++\n mm/pghot.c                             | 13 +++--\n 7 files changed, 130 insertions(+), 8 deletions(-)\n create mode 100644 mm/pghot-precise.c\n\ndiff --git a/Documentation/admin-guide/mm/pghot.txt b/Documentation/admin-guide/mm/pghot.txt\nindex 01291b72e7ab..b329e692ef89 100644\n--- a/Documentation/admin-guide/mm/pghot.txt\n+++ b/Documentation/admin-guide/mm/pghot.txt\n@@ -38,7 +38,7 @@ Path: /sys/kernel/debug/pghot/\n \n 3. **freq_threshold**\n    - Minimum access frequency before a page is marked ready for promotion.\n-   - Range: 1 to 3\n+   - Range: 1 to 3 in default mode, 1 to 7 in precision mode.\n    - Default: 2\n    - Example:\n      # echo 3 > /sys/kernel/debug/pghot/freq_threshold\n@@ -60,7 +60,7 @@ Path: /proc/sys/vm/pghot_promote_freq_window_ms\n - Controls the time window (in ms) for counting access frequency. A page is\n   considered hot only when **freq_threshold** number of accesses occur with\n   this time period.\n-- Default: 4000 (4 seconds)\n+- Default: 4000 (4 seconds) in default mode and 5000 (5s) in precision mode.\n - Example:\n   # sysctl vm.pghot_promote_freq_window_ms=3000\n \ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 22e08befb096..49c374064fc2 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -1924,7 +1924,7 @@ struct mem_section {\n #ifdef CONFIG_PGHOT\n \t/*\n \t * Per-PFN hotness data for this section.\n-\t * Array of phi_t (u8 in default mode).\n+\t * Array of phi_t (u8 in default mode, u32 in precision mode).\n \t * LSB is used as PGHOT_SECTION_HOT_BIT flag.\n \t */\n \tvoid *hot_map;\ndiff --git a/include/linux/pghot.h b/include/linux/pghot.h\nindex 88e57aab697b..d3d59b0c0cf6 100644\n--- a/include/linux/pghot.h\n+++ b/include/linux/pghot.h\n@@ -48,6 +48,36 @@ enum pghot_src_enabled {\n \n #define PGHOT_DEFAULT_NODE\t\t0\n \n+#if defined(CONFIG_PGHOT_PRECISE)\n+#define PGHOT_DEFAULT_FREQ_WINDOW\t(5 * MSEC_PER_SEC)\n+\n+/*\n+ * Bits 0-26 are used to store nid, frequency and time.\n+ * Bits 27-30 are unused now.\n+ * Bit 31 is used to indicate the page is ready for migration.\n+ */\n+#define PGHOT_MIGRATE_READY\t\t31\n+\n+#define PGHOT_NID_WIDTH\t\t\t10\n+#define PGHOT_FREQ_WIDTH\t\t3\n+/* time is stored in 14 bits which can represent up to 16s with HZ=1000 */\n+#define PGHOT_TIME_WIDTH\t\t14\n+\n+#define PGHOT_NID_SHIFT\t\t\t0\n+#define PGHOT_FREQ_SHIFT\t\t(PGHOT_NID_SHIFT + PGHOT_NID_WIDTH)\n+#define PGHOT_TIME_SHIFT\t\t(PGHOT_FREQ_SHIFT + PGHOT_FREQ_WIDTH)\n+\n+#define PGHOT_NID_MASK\t\t\tGENMASK(PGHOT_NID_WIDTH - 1, 0)\n+#define PGHOT_FREQ_MASK\t\t\tGENMASK(PGHOT_FREQ_WIDTH - 1, 0)\n+#define PGHOT_TIME_MASK\t\t\tGENMASK(PGHOT_TIME_WIDTH - 1, 0)\n+\n+#define PGHOT_NID_MAX\t\t\t((1 << PGHOT_NID_WIDTH) - 1)\n+#define PGHOT_FREQ_MAX\t\t\t((1 << PGHOT_FREQ_WIDTH) - 1)\n+#define PGHOT_TIME_MAX\t\t\t((1 << PGHOT_TIME_WIDTH) - 1)\n+\n+typedef u32 phi_t;\n+\n+#else\t/* !CONFIG_PGHOT_PRECISE */\n #define PGHOT_DEFAULT_FREQ_WINDOW\t(4 * MSEC_PER_SEC)\n \n /*\n@@ -74,6 +104,7 @@ enum pghot_src_enabled {\n #define PGHOT_TIME_MAX\t\t\t((1 << PGHOT_TIME_WIDTH) - 1)\n \n typedef u8 phi_t;\n+#endif /* CONFIG_PGHOT_PRECISE */\n \n #define PGHOT_RECORD_SIZE\t\tsizeof(phi_t)\n \ndiff --git a/mm/Kconfig b/mm/Kconfig\nindex f4f0147faac5..fde5aee3e16f 100644\n--- a/mm/Kconfig\n+++ b/mm/Kconfig\n@@ -1478,6 +1478,17 @@ config PGHOT\n \t  This adds 1 byte of metadata overhead per page in lower-tier\n \t  memory nodes.\n \n+config PGHOT_PRECISE\n+\tbool \"Hot page tracking precision mode\"\n+\tdef_bool n\n+\tdepends on PGHOT\n+\thelp\n+\t  Enables precision mode for tracking hot pages with pghot sub-system.\n+\t  Adds fine-grained access time tracking and explicit toptier target\n+\t  NID tracking. Precise hot page tracking comes at the cost of using\n+\t  4 bytes per page against the default one byte per page. Preferable\n+\t  to enable this on systems with multiple nodes in toptier.\n+\n source \"mm/damon/Kconfig\"\n \n endmenu\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 655a27f3a215..89f999647752 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -147,4 +147,9 @@ obj-$(CONFIG_SHRINKER_DEBUG) += shrinker_debug.o\n obj-$(CONFIG_EXECMEM) += execmem.o\n obj-$(CONFIG_TMPFS_QUOTA) += shmem_quota.o\n obj-$(CONFIG_PT_RECLAIM) += pt_reclaim.o\n-obj-$(CONFIG_PGHOT) += pghot.o pghot-tunables.o pghot-default.o\n+obj-$(CONFIG_PGHOT) += pghot.o pghot-tunables.o\n+ifdef CONFIG_PGHOT_PRECISE\n+obj-$(CONFIG_PGHOT) += pghot-precise.o\n+else\n+obj-$(CONFIG_PGHOT) += pghot-default.o\n+endif\ndiff --git a/mm/pghot-precise.c b/mm/pghot-precise.c\nnew file mode 100644\nindex 000000000000..d8d4f15b3f9f\n--- /dev/null\n+++ b/mm/pghot-precise.c\n@@ -0,0 +1,70 @@\n+// SPDX-License-Identifier: GPL-2.0\n+/*\n+ * pghot: Precision mode\n+ *\n+ * 4 byte hotness record per PFN (u32)\n+ * NID, time and frequency tracked as part of the record.\n+ */\n+\n+#include <linux/pghot.h>\n+#include <linux/jiffies.h>\n+\n+unsigned long pghot_access_latency(unsigned long old_time, unsigned long time)\n+{\n+\treturn jiffies_to_msecs((time - old_time) & PGHOT_TIME_MASK);\n+}\n+\n+bool pghot_update_record(phi_t *phi, int nid, unsigned long now)\n+{\n+\tphi_t freq, old_freq, hotness, old_hotness, old_time, old_nid;\n+\tphi_t time = now & PGHOT_TIME_MASK;\n+\n+\told_hotness = READ_ONCE(*phi);\n+\tdo {\n+\t\tbool new_window = false;\n+\n+\t\thotness = old_hotness;\n+\t\told_nid = (hotness >> PGHOT_NID_SHIFT) & PGHOT_NID_MASK;\n+\t\told_freq = (hotness >> PGHOT_FREQ_SHIFT) & PGHOT_FREQ_MASK;\n+\t\told_time = (hotness >> PGHOT_TIME_SHIFT) & PGHOT_TIME_MASK;\n+\n+\t\tif (pghot_access_latency(old_time, time) > sysctl_pghot_freq_window)\n+\t\t\tnew_window = true;\n+\n+\t\tif (new_window)\n+\t\t\tfreq = 1;\n+\t\telse if (old_freq < PGHOT_FREQ_MAX)\n+\t\t\tfreq = old_freq + 1;\n+\t\telse\n+\t\t\tfreq = old_freq;\n+\t\tnid = (nid == NUMA_NO_NODE) ? pghot_target_nid : nid;\n+\n+\t\thotness &= ~(PGHOT_NID_MASK << PGHOT_NID_SHIFT);\n+\t\thotness &= ~(PGHOT_FREQ_MASK << PGHOT_FREQ_SHIFT);\n+\t\thotness &= ~(PGHOT_TIME_MASK << PGHOT_TIME_SHIFT);\n+\n+\t\thotness |= (nid & PGHOT_NID_MASK) << PGHOT_NID_SHIFT;\n+\t\thotness |= (freq & PGHOT_FREQ_MASK) << PGHOT_FREQ_SHIFT;\n+\t\thotness |= (time & PGHOT_TIME_MASK) << PGHOT_TIME_SHIFT;\n+\n+\t\tif (freq >= pghot_freq_threshold)\n+\t\t\thotness |= BIT(PGHOT_MIGRATE_READY);\n+\t} while (unlikely(!try_cmpxchg(phi, &old_hotness, hotness)));\n+\treturn !!(hotness & BIT(PGHOT_MIGRATE_READY));\n+}\n+\n+int pghot_get_record(phi_t *phi, int *nid, int *freq, unsigned long *time)\n+{\n+\tphi_t old_hotness, hotness = 0;\n+\n+\told_hotness = READ_ONCE(*phi);\n+\tdo {\n+\t\tif (!(old_hotness & BIT(PGHOT_MIGRATE_READY)))\n+\t\t\treturn -EINVAL;\n+\t} while (unlikely(!try_cmpxchg(phi, &old_hotness, hotness)));\n+\n+\t*nid = (old_hotness >> PGHOT_NID_SHIFT) & PGHOT_NID_MASK;\n+\t*freq = (old_hotness >> PGHOT_FREQ_SHIFT) & PGHOT_FREQ_MASK;\n+\t*time = (old_hotness >> PGHOT_TIME_SHIFT) & PGHOT_TIME_MASK;\n+\treturn 0;\n+}\ndiff --git a/mm/pghot.c b/mm/pghot.c\nindex 95b5012d5b99..bf1d9029cbaa 100644\n--- a/mm/pghot.c\n+++ b/mm/pghot.c\n@@ -10,6 +10,9 @@\n  * the frequency of access and last access time. Promotions are done\n  * to a default toptier NID.\n  *\n+ * In the precision mode, 4 bytes are used to store the frequency\n+ * of access, last access time and the accessing NID.\n+ *\n  * A kernel thread named kmigrated is provided to migrate or promote\n  * the hot pages. kmigrated runs for each lower tier node. It iterates\n  * over the node's PFNs and  migrates pages marked for migration into\n@@ -52,13 +55,15 @@ static bool kmigrated_started __ro_after_init;\n  * for the purpose of tracking page hotness and subsequent promotion.\n  *\n  * @pfn: PFN of the page\n- * @nid: Unused\n+ * @nid: Target NID to where the page needs to be migrated in precision\n+ *       mode but unused in default mode\n  * @src: The identifier of the sub-system that reports the access\n  * @now: Access time in jiffies\n  *\n- * Updates the frequency and time of access and marks the page as\n- * ready for migration if the frequency crosses a threshold. The pages\n- * marked for migration are migrated by kmigrated kernel thread.\n+ * Updates the NID (in precision mode only), frequency and time of access\n+ * and marks the page as ready for migration if the frequency crosses a\n+ * threshold. The pages marked for migration are migrated by kmigrated\n+ * kernel thread.\n  *\n  * Return: 0 on success and -EINVAL on failure to record the access.\n  */\n-- \n2.34.1\n\n\n\n---\n\nCurrently hot page promotion (NUMA_BALANCING_MEMORY_TIERING\nmode of NUMA Balancing) does hot page detection (via hint faults),\nhot page classification and eventual promotion, all by itself and\nsits within the scheduler.\n\nWith pghot, the new hot page tracking and promotion mechanism\nbeing available, NUMA Balancing can limit itself to detection\nof hot pages (via hint faults) and off-load rest of the\nfunctionality to the common hot page tracking system.\n\npghot_record_access(PGHOT_HINT_FAULT) API is used to feed the\nhot page info to pghot. In addition, the migration rate limiting\nand dynamic threshold logic are moved to kmigrated so that the\nsame can be used for hot pages reported by other sources too.\n\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n kernel/sched/debug.c |   1 -\n kernel/sched/fair.c  | 152 ++-----------------------------------------\n mm/huge_memory.c     |  26 ++------\n mm/memory.c          |  31 ++-------\n mm/pghot.c           | 124 +++++++++++++++++++++++++++++++++++\n 5 files changed, 141 insertions(+), 193 deletions(-)\n\ndiff --git a/kernel/sched/debug.c b/kernel/sched/debug.c\nindex 41caa22e0680..02931902a9c6 100644\n--- a/kernel/sched/debug.c\n+++ b/kernel/sched/debug.c\n@@ -520,7 +520,6 @@ static __init int sched_init_debug(void)\n \tdebugfs_create_u32(\"scan_period_min_ms\", 0644, numa, &sysctl_numa_balancing_scan_period_min);\n \tdebugfs_create_u32(\"scan_period_max_ms\", 0644, numa, &sysctl_numa_balancing_scan_period_max);\n \tdebugfs_create_u32(\"scan_size_mb\", 0644, numa, &sysctl_numa_balancing_scan_size);\n-\tdebugfs_create_u32(\"hot_threshold_ms\", 0644, numa, &sysctl_numa_balancing_hot_threshold);\n #endif /* CONFIG_NUMA_BALANCING */\n \n \tdebugfs_create_file(\"debug\", 0444, debugfs_sched, NULL, &sched_debug_fops);\ndiff --git a/kernel/sched/fair.c b/kernel/sched/fair.c\nindex da46c3164537..4e70f58fbbfa 100644\n--- a/kernel/sched/fair.c\n+++ b/kernel/sched/fair.c\n@@ -125,11 +125,6 @@ int __weak arch_asym_cpu_priority(int cpu)\n static unsigned int sysctl_sched_cfs_bandwidth_slice\t\t= 5000UL;\n #endif\n \n-#ifdef CONFIG_NUMA_BALANCING\n-/* Restrict the NUMA promotion throughput (MB/s) for each target node. */\n-static unsigned int sysctl_numa_balancing_promote_rate_limit = 65536;\n-#endif\n-\n #ifdef CONFIG_SYSCTL\n static const struct ctl_table sched_fair_sysctls[] = {\n #ifdef CONFIG_CFS_BANDWIDTH\n@@ -142,16 +137,6 @@ static const struct ctl_table sched_fair_sysctls[] = {\n \t\t.extra1         = SYSCTL_ONE,\n \t},\n #endif\n-#ifdef CONFIG_NUMA_BALANCING\n-\t{\n-\t\t.procname\t= \"numa_balancing_promote_rate_limit_MBps\",\n-\t\t.data\t\t= &sysctl_numa_balancing_promote_rate_limit,\n-\t\t.maxlen\t\t= sizeof(unsigned int),\n-\t\t.mode\t\t= 0644,\n-\t\t.proc_handler\t= proc_dointvec_minmax,\n-\t\t.extra1\t\t= SYSCTL_ZERO,\n-\t},\n-#endif /* CONFIG_NUMA_BALANCING */\n };\n \n static int __init sched_fair_sysctl_init(void)\n@@ -1427,9 +1412,6 @@ unsigned int sysctl_numa_balancing_scan_size = 256;\n /* Scan @scan_size MB every @scan_period after an initial @scan_delay in ms */\n unsigned int sysctl_numa_balancing_scan_delay = 1000;\n \n-/* The page with hint page fault latency < threshold in ms is considered hot */\n-unsigned int sysctl_numa_balancing_hot_threshold = MSEC_PER_SEC;\n-\n struct numa_group {\n \trefcount_t refcount;\n \n@@ -1784,108 +1766,6 @@ static inline bool cpupid_valid(int cpupid)\n \treturn cpupid_to_cpu(cpupid) < nr_cpu_ids;\n }\n \n-/*\n- * For memory tiering mode, if there are enough free pages (more than\n- * enough watermark defined here) in fast memory node, to take full\n- * advantage of fast memory capacity, all recently accessed slow\n- * memory pages will be migrated to fast memory node without\n- * considering hot threshold.\n- */\n-static bool pgdat_free_space_enough(struct pglist_data *pgdat)\n-{\n-\tint z;\n-\tunsigned long enough_wmark;\n-\n-\tenough_wmark = max(1UL * 1024 * 1024 * 1024 >> PAGE_SHIFT,\n-\t\t\t   pgdat->node_present_pages >> 4);\n-\tfor (z = pgdat->nr_zones - 1; z >= 0; z--) {\n-\t\tstruct zone *zone = pgdat->node_zones + z;\n-\n-\t\tif (!populated_zone(zone))\n-\t\t\tcontinue;\n-\n-\t\tif (zone_watermark_ok(zone, 0,\n-\t\t\t\t      promo_wmark_pages(zone) + enough_wmark,\n-\t\t\t\t      ZONE_MOVABLE, 0))\n-\t\t\treturn true;\n-\t}\n-\treturn false;\n-}\n-\n-/*\n- * For memory tiering mode, when page tables are scanned, the scan\n- * time will be recorded in struct page in addition to make page\n- * PROT_NONE for slow memory page.  So when the page is accessed, in\n- * hint page fault handler, the hint page fault latency is calculated\n- * via,\n- *\n- *\thint page fault latency = hint page fault time - scan time\n- *\n- * The smaller the hint page fault latency, the higher the possibility\n- * for the page to be hot.\n- */\n-static int numa_hint_fault_latency(struct folio *folio)\n-{\n-\tint last_time, time;\n-\n-\ttime = jiffies_to_msecs(jiffies);\n-\tlast_time = folio_xchg_access_time(folio, time);\n-\n-\treturn (time - last_time) & PAGE_ACCESS_TIME_MASK;\n-}\n-\n-/*\n- * For memory tiering mode, too high promotion/demotion throughput may\n- * hurt application latency.  So we provide a mechanism to rate limit\n- * the number of pages that are tried to be promoted.\n- */\n-static bool numa_promotion_rate_limit(struct pglist_data *pgdat,\n-\t\t\t\t      unsigned long rate_limit, int nr)\n-{\n-\tunsigned long nr_cand;\n-\tunsigned int now, start;\n-\n-\tnow = jiffies_to_msecs(jiffies);\n-\tmod_node_page_state(pgdat, PGPROMOTE_CANDIDATE, nr);\n-\tnr_cand = node_page_state(pgdat, PGPROMOTE_CANDIDATE);\n-\tstart = pgdat->nbp_rl_start;\n-\tif (now - start > MSEC_PER_SEC &&\n-\t    cmpxchg(&pgdat->nbp_rl_start, start, now) == start)\n-\t\tpgdat->nbp_rl_nr_cand = nr_cand;\n-\tif (nr_cand - pgdat->nbp_rl_nr_cand >= rate_limit)\n-\t\treturn true;\n-\treturn false;\n-}\n-\n-#define NUMA_MIGRATION_ADJUST_STEPS\t16\n-\n-static void numa_promotion_adjust_threshold(struct pglist_data *pgdat,\n-\t\t\t\t\t    unsigned long rate_limit,\n-\t\t\t\t\t    unsigned int ref_th)\n-{\n-\tunsigned int now, start, th_period, unit_th, th;\n-\tunsigned long nr_cand, ref_cand, diff_cand;\n-\n-\tnow = jiffies_to_msecs(jiffies);\n-\tth_period = sysctl_numa_balancing_scan_period_max;\n-\tstart = pgdat->nbp_th_start;\n-\tif (now - start > th_period &&\n-\t    cmpxchg(&pgdat->nbp_th_start, start, now) == start) {\n-\t\tref_cand = rate_limit *\n-\t\t\tsysctl_numa_balancing_scan_period_max / MSEC_PER_SEC;\n-\t\tnr_cand = node_page_state(pgdat, PGPROMOTE_CANDIDATE);\n-\t\tdiff_cand = nr_cand - pgdat->nbp_th_nr_cand;\n-\t\tunit_th = ref_th * 2 / NUMA_MIGRATION_ADJUST_STEPS;\n-\t\tth = pgdat->nbp_threshold ? : ref_th;\n-\t\tif (diff_cand > ref_cand * 11 / 10)\n-\t\t\tth = max(th - unit_th, unit_th);\n-\t\telse if (diff_cand < ref_cand * 9 / 10)\n-\t\t\tth = min(th + unit_th, ref_th * 2);\n-\t\tpgdat->nbp_th_nr_cand = nr_cand;\n-\t\tpgdat->nbp_threshold = th;\n-\t}\n-}\n-\n bool should_numa_migrate_memory(struct task_struct *p, struct folio *folio,\n \t\t\t\tint src_nid, int dst_cpu)\n {\n@@ -1901,33 +1781,11 @@ bool should_numa_migrate_memory(struct task_struct *p, struct folio *folio,\n \n \t/*\n \t * The pages in slow memory node should be migrated according\n-\t * to hot/cold instead of private/shared.\n-\t */\n-\tif (folio_use_access_time(folio)) {\n-\t\tstruct pglist_data *pgdat;\n-\t\tunsigned long rate_limit;\n-\t\tunsigned int latency, th, def_th;\n-\t\tlong nr = folio_nr_pages(folio);\n-\n-\t\tpgdat = NODE_DATA(dst_nid);\n-\t\tif (pgdat_free_space_enough(pgdat)) {\n-\t\t\t/* workload changed, reset hot threshold */\n-\t\t\tpgdat->nbp_threshold = 0;\n-\t\t\tmod_node_page_state(pgdat, PGPROMOTE_CANDIDATE_NRL, nr);\n-\t\t\treturn true;\n-\t\t}\n-\n-\t\tdef_th = sysctl_numa_balancing_hot_threshold;\n-\t\trate_limit = MB_TO_PAGES(sysctl_numa_balancing_promote_rate_limit);\n-\t\tnuma_promotion_adjust_threshold(pgdat, rate_limit, def_th);\n-\n-\t\tth = pgdat->nbp_threshold ? : def_th;\n-\t\tlatency = numa_hint_fault_latency(folio);\n-\t\tif (latency >= th)\n-\t\t\treturn false;\n-\n-\t\treturn !numa_promotion_rate_limit(pgdat, rate_limit, nr);\n-\t}\n+\t * to hot/cold instead of private/shared. Also the migration\n+\t * of such pages are handled by kmigrated.\n+\t */\n+\tif (folio_use_access_time(folio))\n+\t\treturn true;\n \n \tthis_cpupid = cpu_pid_to_cpupid(dst_cpu, current->pid);\n \tlast_cpupid = folio_xchg_last_cpupid(folio, this_cpupid);\ndiff --git a/mm/huge_memory.c b/mm/huge_memory.c\nindex 40cf59301c21..f52587e70b3c 100644\n--- a/mm/huge_memory.c\n+++ b/mm/huge_memory.c\n@@ -40,6 +40,7 @@\n #include <linux/pgalloc.h>\n #include <linux/pgalloc_tag.h>\n #include <linux/pagewalk.h>\n+#include <linux/pghot.h>\n \n #include <asm/tlb.h>\n #include \"internal.h\"\n@@ -2217,29 +2218,12 @@ vm_fault_t do_huge_pmd_numa_page(struct vm_fault *vmf)\n \n \ttarget_nid = numa_migrate_check(folio, vmf, haddr, &flags, writable,\n \t\t\t\t\t&last_cpupid);\n+\tnid = target_nid;\n \tif (target_nid == NUMA_NO_NODE)\n \t\tgoto out_map;\n-\tif (migrate_misplaced_folio_prepare(folio, vma, target_nid)) {\n-\t\tflags |= TNF_MIGRATE_FAIL;\n-\t\tgoto out_map;\n-\t}\n-\t/* The folio is isolated and isolation code holds a folio reference. */\n-\tspin_unlock(vmf->ptl);\n-\twritable = false;\n \n-\tif (!migrate_misplaced_folio(folio, target_nid)) {\n-\t\tflags |= TNF_MIGRATED;\n-\t\tnid = target_nid;\n-\t\ttask_numa_fault(last_cpupid, nid, HPAGE_PMD_NR, flags);\n-\t\treturn 0;\n-\t}\n+\twritable = false;\n \n-\tflags |= TNF_MIGRATE_FAIL;\n-\tvmf->ptl = pmd_lock(vma->vm_mm, vmf->pmd);\n-\tif (unlikely(!pmd_same(pmdp_get(vmf->pmd), vmf->orig_pmd))) {\n-\t\tspin_unlock(vmf->ptl);\n-\t\treturn 0;\n-\t}\n out_map:\n \t/* Restore the PMD */\n \tpmd = pmd_modify(pmdp_get(vmf->pmd), vma->vm_page_prot);\n@@ -2250,8 +2234,10 @@ vm_fault_t do_huge_pmd_numa_page(struct vm_fault *vmf)\n \tupdate_mmu_cache_pmd(vma, vmf->address, vmf->pmd);\n \tspin_unlock(vmf->ptl);\n \n-\tif (nid != NUMA_NO_NODE)\n+\tif (nid != NUMA_NO_NODE) {\n+\t\tpghot_record_access(folio_pfn(folio), nid, PGHOT_HINT_FAULT, jiffies);\n \t\ttask_numa_fault(last_cpupid, nid, HPAGE_PMD_NR, flags);\n+\t}\n \treturn 0;\n }\n \ndiff --git a/mm/memory.c b/mm/memory.c\nindex 2a55edc48a65..98a9a3b675a0 100644\n--- a/mm/memory.c\n+++ b/mm/memory.c\n@@ -75,6 +75,7 @@\n #include <linux/perf_event.h>\n #include <linux/ptrace.h>\n #include <linux/vmalloc.h>\n+#include <linux/pghot.h>\n #include <linux/sched/sysctl.h>\n #include <linux/pgalloc.h>\n #include <linux/uaccess.h>\n@@ -6046,34 +6047,12 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \n \ttarget_nid = numa_migrate_check(folio, vmf, vmf->address, &flags,\n \t\t\t\t\twritable, &last_cpupid);\n+\tnid = target_nid;\n \tif (target_nid == NUMA_NO_NODE)\n \t\tgoto out_map;\n-\tif (migrate_misplaced_folio_prepare(folio, vma, target_nid)) {\n-\t\tflags |= TNF_MIGRATE_FAIL;\n-\t\tgoto out_map;\n-\t}\n-\t/* The folio is isolated and isolation code holds a folio reference. */\n-\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n+\n \twritable = false;\n \tignore_writable = true;\n-\n-\t/* Migrate to the requested node */\n-\tif (!migrate_misplaced_folio(folio, target_nid)) {\n-\t\tnid = target_nid;\n-\t\tflags |= TNF_MIGRATED;\n-\t\ttask_numa_fault(last_cpupid, nid, nr_pages, flags);\n-\t\treturn 0;\n-\t}\n-\n-\tflags |= TNF_MIGRATE_FAIL;\n-\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,\n-\t\t\t\t       vmf->address, &vmf->ptl);\n-\tif (unlikely(!vmf->pte))\n-\t\treturn 0;\n-\tif (unlikely(!pte_same(ptep_get(vmf->pte), vmf->orig_pte))) {\n-\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);\n-\t\treturn 0;\n-\t}\n out_map:\n \t/*\n \t * Make it present again, depending on how arch implements\n@@ -6087,8 +6066,10 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)\n \t\t\t\t\t    writable);\n \tpte_unmap_unlock(vmf->pte, vmf->ptl);\n \n-\tif (nid != NUMA_NO_NODE)\n+\tif (nid != NUMA_NO_NODE) {\n+\t\tpghot_record_access(folio_pfn(folio), nid, PGHOT_HINT_FAULT, jiffies);\n \t\ttask_numa_fault(last_cpupid, nid, nr_pages, flags);\n+\t}\n \treturn 0;\n }\n \ndiff --git a/mm/pghot.c b/mm/pghot.c\nindex bf1d9029cbaa..6fc76c1eaff8 100644\n--- a/mm/pghot.c\n+++ b/mm/pghot.c\n@@ -17,6 +17,9 @@\n  * the hot pages. kmigrated runs for each lower tier node. It iterates\n  * over the node's PFNs and  migrates pages marked for migration into\n  * their targeted nodes.\n+ *\n+ * Migration rate-limiting and dynamic threshold logic implementations\n+ * were moved from NUMA Balancing mode 2.\n  */\n #include <linux/mm.h>\n #include <linux/migrate.h>\n@@ -31,6 +34,12 @@ unsigned int kmigrated_batch_nr = KMIGRATED_DEFAULT_BATCH_NR;\n \n unsigned int sysctl_pghot_freq_window = PGHOT_DEFAULT_FREQ_WINDOW;\n \n+/* Restrict the NUMA promotion throughput (MB/s) for each target node. */\n+static unsigned int sysctl_pghot_promote_rate_limit = 65536;\n+\n+#define KMIGRATED_MIGRATION_ADJUST_STEPS\t16\n+#define KMIGRATED_PROMOTION_THRESHOLD_WINDOW\t60000\n+\n DEFINE_STATIC_KEY_FALSE(pghot_src_hwhints);\n DEFINE_STATIC_KEY_FALSE(pghot_src_pgtscans);\n DEFINE_STATIC_KEY_FALSE(pghot_src_hintfaults);\n@@ -45,6 +54,14 @@ static const struct ctl_table pghot_sysctls[] = {\n \t\t.proc_handler   = proc_dointvec_minmax,\n \t\t.extra1         = SYSCTL_ZERO,\n \t},\n+\t{\n+\t\t.procname\t= \"pghot_promote_rate_limit_MBps\",\n+\t\t.data\t\t= &sysctl_pghot_promote_rate_limit,\n+\t\t.maxlen\t\t= sizeof(unsigned int),\n+\t\t.mode\t\t= 0644,\n+\t\t.proc_handler\t= proc_dointvec_minmax,\n+\t\t.extra1\t\t= SYSCTL_ZERO,\n+\t},\n };\n #endif\n \n@@ -138,6 +155,110 @@ int pghot_record_access(unsigned long pfn, int nid, int src, unsigned long now)\n \treturn 0;\n }\n \n+/*\n+ * For memory tiering mode, if there are enough free pages (more than\n+ * enough watermark defined here) in fast memory node, to take full\n+ * advantage of fast memory capacity, all recently accessed slow\n+ * memory pages will be migrated to fast memory node without\n+ * considering hot threshold.\n+ */\n+static bool pgdat_free_space_enough(struct pglist_data *pgdat)\n+{\n+\tint z;\n+\tunsigned long enough_wmark;\n+\n+\tenough_wmark = max(1UL * 1024 * 1024 * 1024 >> PAGE_SHIFT,\n+\t\t\t   pgdat->node_present_pages >> 4);\n+\tfor (z = pgdat->nr_zones - 1; z >= 0; z--) {\n+\t\tstruct zone *zone = pgdat->node_zones + z;\n+\n+\t\tif (!populated_zone(zone))\n+\t\t\tcontinue;\n+\n+\t\tif (zone_watermark_ok(zone, 0,\n+\t\t\t\t      promo_wmark_pages(zone) + enough_wmark,\n+\t\t\t\t      ZONE_MOVABLE, 0))\n+\t\t\treturn true;\n+\t}\n+\treturn false;\n+}\n+\n+/*\n+ * For memory tiering mode, too high promotion/demotion throughput may\n+ * hurt application latency.  So we provide a mechanism to rate limit\n+ * the number of pages that are tried to be promoted.\n+ */\n+static bool kmigrated_promotion_rate_limit(struct pglist_data *pgdat, unsigned long rate_limit,\n+\t\t\t\t\t   int nr, unsigned long now_ms)\n+{\n+\tunsigned long nr_cand;\n+\tunsigned int start;\n+\n+\tmod_node_page_state(pgdat, PGPROMOTE_CANDIDATE, nr);\n+\tnr_cand = node_page_state(pgdat, PGPROMOTE_CANDIDATE);\n+\tstart = pgdat->nbp_rl_start;\n+\tif (now_ms - start > MSEC_PER_SEC &&\n+\t    cmpxchg(&pgdat->nbp_rl_start, start, now_ms) == start)\n+\t\tpgdat->nbp_rl_nr_cand = nr_cand;\n+\tif (nr_cand - pgdat->nbp_rl_nr_cand >= rate_limit)\n+\t\treturn true;\n+\treturn false;\n+}\n+\n+static void kmigrated_promotion_adjust_threshold(struct pglist_data *pgdat,\n+\t\t\t\t\t\t unsigned long rate_limit, unsigned int ref_th,\n+\t\t\t\t\t\t unsigned long now_ms)\n+{\n+\tunsigned int start, th_period, unit_th, th;\n+\tunsigned long nr_cand, ref_cand, diff_cand;\n+\n+\tth_period = KMIGRATED_PROMOTION_THRESHOLD_WINDOW;\n+\tstart = pgdat->nbp_th_start;\n+\tif (now_ms - start > th_period &&\n+\t    cmpxchg(&pgdat->nbp_th_start, start, now_ms) == start) {\n+\t\tref_cand = rate_limit *\n+\t\t\tKMIGRATED_PROMOTION_THRESHOLD_WINDOW / MSEC_PER_SEC;\n+\t\tnr_cand = node_page_state(pgdat, PGPROMOTE_CANDIDATE);\n+\t\tdiff_cand = nr_cand - pgdat->nbp_th_nr_cand;\n+\t\tunit_th = ref_th * 2 / KMIGRATED_MIGRATION_ADJUST_STEPS;\n+\t\tth = pgdat->nbp_threshold ? : ref_th;\n+\t\tif (diff_cand > ref_cand * 11 / 10)\n+\t\t\tth = max(th - unit_th, unit_th);\n+\t\telse if (diff_cand < ref_cand * 9 / 10)\n+\t\t\tth = min(th + unit_th, ref_th * 2);\n+\t\tpgdat->nbp_th_nr_cand = nr_cand;\n+\t\tpgdat->nbp_threshold = th;\n+\t}\n+}\n+\n+static bool kmigrated_should_migrate_memory(unsigned long nr_pages, int nid,\n+\t\t\t\t\t    unsigned long time)\n+{\n+\tstruct pglist_data *pgdat;\n+\tunsigned long rate_limit;\n+\tunsigned int th, def_th;\n+\tunsigned long now_ms = jiffies_to_msecs(jiffies); /* Based on full-width jiffies */\n+\tunsigned long now = jiffies;\n+\n+\tpgdat = NODE_DATA(nid);\n+\tif (pgdat_free_space_enough(pgdat)) {\n+\t\t/* workload changed, reset hot threshold */\n+\t\tpgdat->nbp_threshold = 0;\n+\t\tmod_node_page_state(pgdat, PGPROMOTE_CANDIDATE_NRL, nr_pages);\n+\t\treturn true;\n+\t}\n+\n+\tdef_th = sysctl_pghot_freq_window;\n+\trate_limit = MB_TO_PAGES(sysctl_pghot_promote_rate_limit);\n+\tkmigrated_promotion_adjust_threshold(pgdat, rate_limit, def_th, now_ms);\n+\n+\tth = pgdat->nbp_threshold ? : def_th;\n+\tif (pghot_access_latency(time, now) >= th)\n+\t\treturn false;\n+\n+\treturn !kmigrated_promotion_rate_limit(pgdat, rate_limit, nr_pages, now_ms);\n+}\n+\n static int pghot_get_hotness(unsigned long pfn, int *nid, int *freq,\n \t\t\t     unsigned long *time)\n {\n@@ -197,6 +318,9 @@ static void kmigrated_walk_zone(unsigned long start_pfn, unsigned long end_pfn,\n \t\tif (folio_nid(folio) == nid)\n \t\t\tgoto out_next;\n \n+\t\tif (!kmigrated_should_migrate_memory(nr, nid, time))\n+\t\t\tgoto out_next;\n+\n \t\tif (migrate_misplaced_folio_prepare(folio, NULL, nid))\n \t\t\tgoto out_next;\n \n-- \n2.34.1\n\n\n\n---\n\nUse IBS (Instruction Based Sampling) feature present\nin AMD processors for memory access tracking. The access\ninformation obtained from IBS via NMI is fed to pghot\nsub-system for futher action.\n\nIn addition to many other information related to the memory\naccess, IBS provides physical (and virtual) address of the access\nand indicates if the access came from slower tier. Only memory\naccesses originating from slower tiers are further acted upon\nby this driver.\n\nThe samples are initially accumulated in percpu buffers which\nare flushed to pghot hot page tracking mechanism using irq_work.\n\nTODO: Many counters are added to vmstat just as debugging aid\nfor now.\n\nAbout IBS\n---------\nIBS can be programmed to provide data about instruction\nexecution periodically. This is done by programming a desired\nsample count (number of ops) in a control register. When the\nprogrammed number of ops are dispatched, a micro-op gets tagged,\nvarious information about the tagged micro-op's execution is\npopulated in IBS execution MSRs and an interrupt is raised.\nWhile IBS provides a lot of data for each sample, for the\npurpose of  memory access profiling, we are interested in\nlinear and physical address of the memory access that reached\nDRAM. Recent AMD processors provide further filtering where\nit is possible to limit the sampling to those ops that had\nan L3 miss which greately reduces the non-useful samples.\n\nWhile IBS provides capability to sample instruction fetch\nand execution, only IBS execution sampling is used here\nto collect data about memory accesses that occur during\nthe instruction execution.\n\nMore information about IBS is available in Sec 13.3 of\nAMD64 Architecture Programmer's Manual, Volume 2:System\nProgramming which is present at:\nhttps://bugzilla.kernel.org/attachment.cgi?id=288923\n\nInformation about MSRs used for programming IBS can be\nfound in Sec 2.1.14.4 of PPR Vol 1 for AMD Family 19h\nModel 11h B1 which is currently present at:\nhttps://www.amd.com/system/files/TechDocs/55901_0.25.zip\n\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n arch/x86/events/amd/ibs.c        |  10 +\n arch/x86/include/asm/msr-index.h |  16 ++\n arch/x86/mm/Makefile             |   1 +\n arch/x86/mm/ibs.c                | 317 +++++++++++++++++++++++++++++++\n include/linux/pghot.h            |   8 +\n include/linux/vm_event_item.h    |  19 ++\n mm/Kconfig                       |  13 ++\n mm/vmstat.c                      |  19 ++\n 8 files changed, 403 insertions(+)\n create mode 100644 arch/x86/mm/ibs.c\n\ndiff --git a/arch/x86/events/amd/ibs.c b/arch/x86/events/amd/ibs.c\nindex aca89f23d2e0..dc544d084c17 100644\n--- a/arch/x86/events/amd/ibs.c\n+++ b/arch/x86/events/amd/ibs.c\n@@ -13,6 +13,7 @@\n #include <linux/ptrace.h>\n #include <linux/syscore_ops.h>\n #include <linux/sched/clock.h>\n+#include <linux/pghot.h>\n \n #include <asm/apic.h>\n #include <asm/msr.h>\n@@ -1760,6 +1761,15 @@ static __init int amd_ibs_init(void)\n {\n \tu32 caps;\n \n+\t/*\n+\t * TODO: Find a clean way to disable perf IBS so that IBS\n+\t * can be used for memory access profiling.\n+\t */\n+\tif (hwmem_access_profiler_inuse()) {\n+\t\tpr_info(\"IBS isn't available for perf use\\n\");\n+\t\treturn 0;\n+\t}\n+\n \tcaps = __get_ibs_caps();\n \tif (!caps)\n \t\treturn -ENODEV;\t/* ibs not supported by the cpu */\ndiff --git a/arch/x86/include/asm/msr-index.h b/arch/x86/include/asm/msr-index.h\nindex 3d0a0950d20a..3c5d69ec83a2 100644\n--- a/arch/x86/include/asm/msr-index.h\n+++ b/arch/x86/include/asm/msr-index.h\n@@ -784,6 +784,22 @@\n /* AMD Last Branch Record MSRs */\n #define MSR_AMD64_LBR_SELECT\t\t\t0xc000010e\n \n+/* AMD IBS MSR bits */\n+#define MSR_AMD64_IBSOPDATA2_DATASRC\t\t\t0x7\n+#define MSR_AMD64_IBSOPDATA2_DATASRC_LCL_CACHE\t\t0x1\n+#define MSR_AMD64_IBSOPDATA2_DATASRC_PEER_CACHE_NEAR\t0x2\n+#define MSR_AMD64_IBSOPDATA2_DATASRC_DRAM\t\t0x3\n+#define MSR_AMD64_IBSOPDATA2_DATASRC_FAR_CCX_CACHE\t0x5\n+#define MSR_AMD64_IBSOPDATA2_DATASRC_EXT_MEM\t\t0x8\n+#define\tMSR_AMD64_IBSOPDATA2_RMTNODE\t\t\t0x10\n+\n+#define MSR_AMD64_IBSOPDATA3_LDOP\t\tBIT_ULL(0)\n+#define MSR_AMD64_IBSOPDATA3_STOP\t\tBIT_ULL(1)\n+#define MSR_AMD64_IBSOPDATA3_DCMISS\t\tBIT_ULL(7)\n+#define MSR_AMD64_IBSOPDATA3_LADDR_VALID\tBIT_ULL(17)\n+#define MSR_AMD64_IBSOPDATA3_PADDR_VALID\tBIT_ULL(18)\n+#define MSR_AMD64_IBSOPDATA3_L2MISS\t\tBIT_ULL(20)\n+\n /* Zen4 */\n #define MSR_ZEN4_BP_CFG                 0xc001102e\n #define MSR_ZEN4_BP_CFG_BP_SPEC_REDUCE_BIT 4\ndiff --git a/arch/x86/mm/Makefile b/arch/x86/mm/Makefile\nindex 5b9908f13dcf..361a456582e9 100644\n--- a/arch/x86/mm/Makefile\n+++ b/arch/x86/mm/Makefile\n@@ -57,3 +57,4 @@ obj-$(CONFIG_X86_MEM_ENCRYPT)\t+= mem_encrypt.o\n obj-$(CONFIG_AMD_MEM_ENCRYPT)\t+= mem_encrypt_amd.o\n \n obj-$(CONFIG_AMD_MEM_ENCRYPT)\t+= mem_encrypt_boot.o\n+obj-$(CONFIG_HWMEM_PROFILER)\t+= ibs.o\ndiff --git a/arch/x86/mm/ibs.c b/arch/x86/mm/ibs.c\nnew file mode 100644\nindex 000000000000..752f688375f9\n--- /dev/null\n+++ b/arch/x86/mm/ibs.c\n@@ -0,0 +1,317 @@\n+// SPDX-License-Identifier: GPL-2.0\n+\n+#include <linux/init.h>\n+#include <linux/pghot.h>\n+#include <linux/percpu.h>\n+#include <linux/workqueue.h>\n+#include <linux/irq_work.h>\n+\n+#include <asm/nmi.h>\n+#include <asm/perf_event.h> /* TODO: Move defns like IBS_OP_ENABLE into non-perf header */\n+#include <asm/apic.h>\n+\n+bool hwmem_access_profiling;\n+\n+static u64 ibs_config __read_mostly;\n+static u32 ibs_caps;\n+\n+#define IBS_NR_SAMPLES\t150\n+\n+/*\n+ * Basic access info captured for each memory access.\n+ */\n+struct ibs_sample {\n+\tunsigned long pfn;\n+\tunsigned long time;\t/* jiffies when accessed */\n+\tint nid;\t\t/* Accessing node ID, if known */\n+};\n+\n+/*\n+ * Percpu buffer of access samples. Samples are accumulated here\n+ * before pushing them to pghot sub-system for further action.\n+ */\n+struct ibs_sample_pcpu {\n+\tstruct ibs_sample samples[IBS_NR_SAMPLES];\n+\tint head, tail;\n+};\n+\n+struct ibs_sample_pcpu __percpu *ibs_s;\n+\n+/*\n+ * The workqueue for pushing the percpu access samples to pghot sub-system.\n+ */\n+static struct work_struct ibs_work;\n+static struct irq_work ibs_irq_work;\n+\n+bool hwmem_access_profiler_inuse(void)\n+{\n+\treturn hwmem_access_profiling;\n+}\n+\n+/*\n+ * Record the IBS-reported access sample in percpu buffer.\n+ * Called from IBS NMI handler.\n+ */\n+static int ibs_push_sample(unsigned long pfn, int nid, unsigned long time)\n+{\n+\tstruct ibs_sample_pcpu *ibs_pcpu = raw_cpu_ptr(ibs_s);\n+\tint next = ibs_pcpu->head + 1;\n+\n+\tif (next >= IBS_NR_SAMPLES)\n+\t\tnext = 0;\n+\n+\tif (next == ibs_pcpu->tail)\n+\t\treturn 0;\n+\n+\tibs_pcpu->samples[ibs_pcpu->head].pfn = pfn;\n+\tibs_pcpu->samples[ibs_pcpu->head].time = time;\n+\tibs_pcpu->samples[ibs_pcpu->head].nid = nid;\n+\tibs_pcpu->head = next;\n+\treturn 1;\n+}\n+\n+static int ibs_pop_sample(struct ibs_sample *s)\n+{\n+\tstruct ibs_sample_pcpu *ibs_pcpu = raw_cpu_ptr(ibs_s);\n+\n+\tint next = ibs_pcpu->tail + 1;\n+\n+\tif (ibs_pcpu->head == ibs_pcpu->tail)\n+\t\treturn 0;\n+\n+\tif (next >= IBS_NR_SAMPLES)\n+\t\tnext = 0;\n+\n+\t*s = ibs_pcpu->samples[ibs_pcpu->tail];\n+\tibs_pcpu->tail = next;\n+\treturn 1;\n+}\n+\n+/*\n+ * Remove access samples from percpu buffer and send them\n+ * to pghot sub-system for further action.\n+ */\n+static void ibs_work_handler(struct work_struct *work)\n+{\n+\tstruct ibs_sample s;\n+\n+\twhile (ibs_pop_sample(&s))\n+\t\tpghot_record_access(s.pfn, s.nid, PGHOT_HW_HINTS, s.time);\n+}\n+\n+static void ibs_irq_handler(struct irq_work *i)\n+{\n+\tschedule_work_on(smp_processor_id(), &ibs_work);\n+}\n+\n+/*\n+ * IBS NMI handler: Process the memory access info reported by IBS.\n+ *\n+ * Reads the MSRs to collect all the information about the reported\n+ * memory access, validates the access, stores the valid sample and\n+ * schedules the work on this CPU to further process the sample.\n+ */\n+static int ibs_overflow_handler(unsigned int cmd, struct pt_regs *regs)\n+{\n+\tstruct mm_struct *mm = current->mm;\n+\tu64 ops_ctl, ops_data3, ops_data2;\n+\tu64 laddr = -1, paddr = -1;\n+\tu64 data_src, rmt_node;\n+\tstruct page *page;\n+\tunsigned long pfn;\n+\n+\trdmsrl(MSR_AMD64_IBSOPCTL, ops_ctl);\n+\n+\t/*\n+\t * When IBS sampling period is reprogrammed via read-modify-update\n+\t * of MSR_AMD64_IBSOPCTL, overflow NMIs could be generated with\n+\t * IBS_OP_ENABLE not set. For such cases, return as HANDLED.\n+\t *\n+\t * With this, the handler will say \"handled\" for all NMIs that\n+\t * aren't related to this NMI.  This stems from the limitation of\n+\t * having both status and control bits in one MSR.\n+\t */\n+\tif (!(ops_ctl & IBS_OP_VAL))\n+\t\tgoto handled;\n+\n+\twrmsrl(MSR_AMD64_IBSOPCTL, ops_ctl & ~IBS_OP_VAL);\n+\n+\tcount_vm_event(HWHINT_NR_EVENTS);\n+\n+\tif (!user_mode(regs)) {\n+\t\tcount_vm_event(HWHINT_KERNEL);\n+\t\tgoto handled;\n+\t}\n+\n+\tif (!mm) {\n+\t\tcount_vm_event(HWHINT_KTHREAD);\n+\t\tgoto handled;\n+\t}\n+\n+\trdmsrl(MSR_AMD64_IBSOPDATA3, ops_data3);\n+\n+\t/* Load/Store ops only */\n+\t/* TODO: DataSrc isn't valid for stores, so filter out stores? */\n+\tif (!(ops_data3 & (MSR_AMD64_IBSOPDATA3_LDOP |\n+\t\t\t   MSR_AMD64_IBSOPDATA3_STOP))) {\n+\t\tcount_vm_event(HWHINT_NON_LOAD_STORES);\n+\t\tgoto handled;\n+\t}\n+\n+\t/* Discard the sample if it was L1 or L2 hit */\n+\tif (!(ops_data3 & (MSR_AMD64_IBSOPDATA3_DCMISS |\n+\t\t\t   MSR_AMD64_IBSOPDATA3_L2MISS))) {\n+\t\tcount_vm_event(HWHINT_DC_L2_HITS);\n+\t\tgoto handled;\n+\t}\n+\n+\trdmsrl(MSR_AMD64_IBSOPDATA2, ops_data2);\n+\tdata_src = ops_data2 & MSR_AMD64_IBSOPDATA2_DATASRC;\n+\tif (ibs_caps & IBS_CAPS_ZEN4)\n+\t\tdata_src |= ((ops_data2 & 0xC0) >> 3);\n+\n+\tswitch (data_src) {\n+\tcase MSR_AMD64_IBSOPDATA2_DATASRC_LCL_CACHE:\n+\t\tcount_vm_event(HWHINT_LOCAL_L3L1L2);\n+\t\tbreak;\n+\tcase MSR_AMD64_IBSOPDATA2_DATASRC_PEER_CACHE_NEAR:\n+\t\tcount_vm_event(HWHINT_LOCAL_PEER_CACHE_NEAR);\n+\t\tbreak;\n+\tcase MSR_AMD64_IBSOPDATA2_DATASRC_DRAM:\n+\t\tcount_vm_event(HWHINT_DRAM_ACCESSES);\n+\t\tbreak;\n+\tcase MSR_AMD64_IBSOPDATA2_DATASRC_EXT_MEM:\n+\t\tcount_vm_event(HWHINT_CXL_ACCESSES);\n+\t\tbreak;\n+\tcase MSR_AMD64_IBSOPDATA2_DATASRC_FAR_CCX_CACHE:\n+\t\tcount_vm_event(HWHINT_FAR_CACHE_HITS);\n+\t\tbreak;\n+\t}\n+\n+\trmt_node = ops_data2 & MSR_AMD64_IBSOPDATA2_RMTNODE;\n+\tif (rmt_node)\n+\t\tcount_vm_event(HWHINT_REMOTE_NODE);\n+\n+\t/* Is linear addr valid? */\n+\tif (ops_data3 & MSR_AMD64_IBSOPDATA3_LADDR_VALID)\n+\t\trdmsrl(MSR_AMD64_IBSDCLINAD, laddr);\n+\telse {\n+\t\tcount_vm_event(HWHINT_LADDR_INVALID);\n+\t\tgoto handled;\n+\t}\n+\n+\t/* Discard kernel address accesses */\n+\tif (laddr & (1UL << 63)) {\n+\t\tcount_vm_event(HWHINT_KERNEL_ADDR);\n+\t\tgoto handled;\n+\t}\n+\n+\t/* Is phys addr valid? */\n+\tif (ops_data3 & MSR_AMD64_IBSOPDATA3_PADDR_VALID)\n+\t\trdmsrl(MSR_AMD64_IBSDCPHYSAD, paddr);\n+\telse {\n+\t\tcount_vm_event(HWHINT_PADDR_INVALID);\n+\t\tgoto handled;\n+\t}\n+\n+\tpfn = PHYS_PFN(paddr);\n+\tpage = pfn_to_online_page(pfn);\n+\tif (!page)\n+\t\tgoto handled;\n+\n+\tif (!PageLRU(page)) {\n+\t\tcount_vm_event(HWHINT_NON_LRU);\n+\t\tgoto handled;\n+\t}\n+\n+\tif (!ibs_push_sample(pfn, numa_node_id(), jiffies)) {\n+\t\tcount_vm_event(HWHINT_BUFFER_FULL);\n+\t\tgoto handled;\n+\t}\n+\n+\tirq_work_queue(&ibs_irq_work);\n+\tcount_vm_event(HWHINT_USEFUL_SAMPLES);\n+\n+handled:\n+\treturn NMI_HANDLED;\n+}\n+\n+static inline int get_ibs_lvt_offset(void)\n+{\n+\tu64 val;\n+\n+\trdmsrl(MSR_AMD64_IBSCTL, val);\n+\tif (!(val & IBSCTL_LVT_OFFSET_VALID))\n+\t\treturn -EINVAL;\n+\n+\treturn val & IBSCTL_LVT_OFFSET_MASK;\n+}\n+\n+static void setup_APIC_ibs(void)\n+{\n+\tint offset;\n+\n+\toffset = get_ibs_lvt_offset();\n+\tif (offset < 0)\n+\t\tgoto failed;\n+\n+\tif (!setup_APIC_eilvt(offset, 0, APIC_EILVT_MSG_NMI, 0))\n+\t\treturn;\n+failed:\n+\tpr_warn(\"IBS APIC setup failed on cpu #%d\\n\",\n+\t\tsmp_processor_id());\n+}\n+\n+static void clear_APIC_ibs(void)\n+{\n+\tint offset;\n+\n+\toffset = get_ibs_lvt_offset();\n+\tif (offset >= 0)\n+\t\tsetup_APIC_eilvt(offset, 0, APIC_EILVT_MSG_FIX, 1);\n+}\n+\n+static int x86_amd_ibs_access_profile_startup(unsigned int cpu)\n+{\n+\tsetup_APIC_ibs();\n+\treturn 0;\n+}\n+\n+static int x86_amd_ibs_access_profile_teardown(unsigned int cpu)\n+{\n+\tclear_APIC_ibs();\n+\treturn 0;\n+}\n+\n+static int __init ibs_access_profiling_init(void)\n+{\n+\tif (!boot_cpu_has(X86_FEATURE_IBS)) {\n+\t\tpr_info(\"IBS capability is unavailable for access profiling\\n\");\n+\t\treturn 0;\n+\t}\n+\n+\tibs_s = alloc_percpu_gfp(struct ibs_sample_pcpu, GFP_KERNEL | __GFP_ZERO);\n+\tif (!ibs_s)\n+\t\treturn 0;\n+\n+\tINIT_WORK(&ibs_work, ibs_work_handler);\n+\tinit_irq_work(&ibs_irq_work, ibs_irq_handler);\n+\n+\t/* Uses IBS Op sampling */\n+\tibs_config = IBS_OP_CNT_CTL | IBS_OP_ENABLE;\n+\tibs_caps = cpuid_eax(IBS_CPUID_FEATURES);\n+\tif (ibs_caps & IBS_CAPS_ZEN4)\n+\t\tibs_config |= IBS_OP_L3MISSONLY;\n+\n+\tregister_nmi_handler(NMI_LOCAL, ibs_overflow_handler, 0, \"ibs\");\n+\n+\tcpuhp_setup_state(CPUHP_AP_PERF_X86_AMD_IBS_STARTING,\n+\t\t\t  \"x86/amd/ibs_access_profile:starting\",\n+\t\t\t  x86_amd_ibs_access_profile_startup,\n+\t\t\t  x86_amd_ibs_access_profile_teardown);\n+\n+\tpr_info(\"IBS setup for memory access profiling\\n\");\n+\treturn 0;\n+}\n+\n+arch_initcall(ibs_access_profiling_init);\ndiff --git a/include/linux/pghot.h b/include/linux/pghot.h\nindex d3d59b0c0cf6..20ea9767dbdd 100644\n--- a/include/linux/pghot.h\n+++ b/include/linux/pghot.h\n@@ -2,6 +2,14 @@\n #ifndef _LINUX_PGHOT_H\n #define _LINUX_PGHOT_H\n \n+#include <linux/types.h>\n+\n+#ifdef CONFIG_HWMEM_PROFILER\n+bool hwmem_access_profiler_inuse(void);\n+#else\n+static inline bool hwmem_access_profiler_inuse(void) { return false; }\n+#endif\n+\n /* Page hotness temperature sources */\n enum pghot_src {\n \tPGHOT_HW_HINTS,\ndiff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h\nindex 5b8fd93b55fd..67efbca9051c 100644\n--- a/include/linux/vm_event_item.h\n+++ b/include/linux/vm_event_item.h\n@@ -193,6 +193,25 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,\n \t\tPGHOT_RECORD_HWHINTS,\n \t\tPGHOT_RECORD_PGTSCANS,\n \t\tPGHOT_RECORD_HINTFAULTS,\n+#ifdef CONFIG_HWMEM_PROFILER\n+\t\tHWHINT_NR_EVENTS,\n+\t\tHWHINT_KERNEL,\n+\t\tHWHINT_KTHREAD,\n+\t\tHWHINT_NON_LOAD_STORES,\n+\t\tHWHINT_DC_L2_HITS,\n+\t\tHWHINT_LOCAL_L3L1L2,\n+\t\tHWHINT_LOCAL_PEER_CACHE_NEAR,\n+\t\tHWHINT_FAR_CACHE_HITS,\n+\t\tHWHINT_DRAM_ACCESSES,\n+\t\tHWHINT_CXL_ACCESSES,\n+\t\tHWHINT_REMOTE_NODE,\n+\t\tHWHINT_LADDR_INVALID,\n+\t\tHWHINT_KERNEL_ADDR,\n+\t\tHWHINT_PADDR_INVALID,\n+\t\tHWHINT_NON_LRU,\n+\t\tHWHINT_BUFFER_FULL,\n+\t\tHWHINT_USEFUL_SAMPLES,\n+#endif /* CONFIG_HWMEM_PROFILER */\n #endif /* CONFIG_PGHOT */\n \t\tNR_VM_EVENT_ITEMS\n };\ndiff --git a/mm/Kconfig b/mm/Kconfig\nindex fde5aee3e16f..07b16aece877 100644\n--- a/mm/Kconfig\n+++ b/mm/Kconfig\n@@ -1489,6 +1489,19 @@ config PGHOT_PRECISE\n \t  4 bytes per page against the default one byte per page. Preferable\n \t  to enable this on systems with multiple nodes in toptier.\n \n+config HWMEM_PROFILER\n+\tbool \"HW based memory access profiling\"\n+\tdef_bool n\n+\tdepends on PGHOT\n+\tdepends on X86_64\n+\thelp\n+\t  Some hardware platforms are capable of providing memory access\n+\t  information in direct and actionable manner. Instruction Based\n+\t  Sampling (IBS) present on AMD Zen CPUs in one such example.\n+\t  Memory accesses obtained via such HW based mechanisms are\n+\t  rolled up to PGHOT sub-system for further action like hot page\n+\t  promotion or NUMA Balancing\n+\n source \"mm/damon/Kconfig\"\n \n endmenu\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex f6f91b9dd887..62c47f44edf0 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -1506,6 +1506,25 @@ const char * const vmstat_text[] = {\n \t[I(PGHOT_RECORD_HWHINTS)]\t\t= \"pghot_recorded_hwhints\",\n \t[I(PGHOT_RECORD_PGTSCANS)]\t\t= \"pghot_recorded_pgtscans\",\n \t[I(PGHOT_RECORD_HINTFAULTS)]\t\t= \"pghot_recorded_hintfaults\",\n+#ifdef CONFIG_HWMEM_PROFILER\n+\t[I(HWHINT_NR_EVENTS)]\t\t\t= \"hwhint_nr_events\",\n+\t[I(HWHINT_KERNEL)]\t\t\t= \"hwhint_kernel\",\n+\t[I(HWHINT_KTHREAD)]\t\t\t= \"hwhint_kthread\",\n+\t[I(HWHINT_NON_LOAD_STORES)]\t\t= \"hwhint_non_load_stores\",\n+\t[I(HWHINT_DC_L2_HITS)]\t\t\t= \"hwhint_dc_l2_hits\",\n+\t[I(HWHINT_LOCAL_L3L1L2)]\t\t= \"hwhint_local_l3l1l2\",\n+\t[I(HWHINT_LOCAL_PEER_CACHE_NEAR)]\t= \"hwhint_local_peer_cache_near\",\n+\t[I(HWHINT_FAR_CACHE_HITS)]\t\t= \"hwhint_far_cache_hits\",\n+\t[I(HWHINT_DRAM_ACCESSES)]\t\t= \"hwhint_dram_accesses\",\n+\t[I(HWHINT_CXL_ACCESSES)]\t\t= \"hwhint_cxl_accesses\",\n+\t[I(HWHINT_REMOTE_NODE)]\t\t\t= \"hwhint_remote_node\",\n+\t[I(HWHINT_LADDR_INVALID)]\t\t= \"hwhint_invalid_laddr\",\n+\t[I(HWHINT_KERNEL_ADDR)]\t\t\t= \"hwhint_kernel_addr\",\n+\t[I(HWHINT_PADDR_INVALID)]\t\t= \"hwhint_invalid_paddr\",\n+\t[I(HWHINT_NON_LRU)]\t\t\t= \"hwhint_non_lru\",\n+\t[I(HWHINT_BUFFER_FULL)]\t\t\t= \"hwhint_buffer_full\",\n+\t[I(HWHINT_USEFUL_SAMPLES)]\t\t= \"hwhint_useful_samples\",\n+#endif /* CONFIG_HWMEM_PROFILER */\n #endif /* CONFIG_PGHOT */\n #undef I\n #endif /* CONFIG_VM_EVENT_COUNTERS */\n-- \n2.34.1\n\n\n\n---\n\nEnable IBS memory access data collection for user memory\naccesses by programming the required MSRs. The profiling\nis turned ON only for user mode execution and turned OFF\nfor kernel mode execution. Profiling is explicitly disabled\nfor NMI handler too.\n\nTODOs:\n\n- IBS sampling rate is kept fixed for now.\n- Arch/vendor separation/isolation of the code needs relook.\n\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n arch/x86/include/asm/entry-common.h |  3 +++\n arch/x86/include/asm/hardirq.h      |  2 ++\n arch/x86/mm/ibs.c                   | 32 +++++++++++++++++++++++++++++\n include/linux/pghot.h               |  4 ++++\n 4 files changed, 41 insertions(+)\n\ndiff --git a/arch/x86/include/asm/entry-common.h b/arch/x86/include/asm/entry-common.h\nindex ce3eb6d5fdf9..0f381a63669e 100644\n--- a/arch/x86/include/asm/entry-common.h\n+++ b/arch/x86/include/asm/entry-common.h\n@@ -4,6 +4,7 @@\n \n #include <linux/randomize_kstack.h>\n #include <linux/user-return-notifier.h>\n+#include <linux/pghot.h>\n \n #include <asm/nospec-branch.h>\n #include <asm/io_bitmap.h>\n@@ -13,6 +14,7 @@\n /* Check that the stack and regs on entry from user mode are sane. */\n static __always_inline void arch_enter_from_user_mode(struct pt_regs *regs)\n {\n+\thwmem_access_profiling_stop();\n \tif (IS_ENABLED(CONFIG_DEBUG_ENTRY)) {\n \t\t/*\n \t\t * Make sure that the entry code gave us a sensible EFLAGS\n@@ -106,6 +108,7 @@ static inline void arch_exit_to_user_mode_prepare(struct pt_regs *regs,\n static __always_inline void arch_exit_to_user_mode(void)\n {\n \tamd_clear_divider();\n+\thwmem_access_profiling_start();\n }\n #define arch_exit_to_user_mode arch_exit_to_user_mode\n \ndiff --git a/arch/x86/include/asm/hardirq.h b/arch/x86/include/asm/hardirq.h\nindex 6b6d472baa0b..e80c305c17d1 100644\n--- a/arch/x86/include/asm/hardirq.h\n+++ b/arch/x86/include/asm/hardirq.h\n@@ -91,4 +91,6 @@ static __always_inline bool kvm_get_cpu_l1tf_flush_l1d(void)\n static __always_inline void kvm_set_cpu_l1tf_flush_l1d(void) { }\n #endif /* IS_ENABLED(CONFIG_KVM_INTEL) */\n \n+#define arch_nmi_enter()\thwmem_access_profiling_stop()\n+#define arch_nmi_exit()\t\thwmem_access_profiling_start()\n #endif /* _ASM_X86_HARDIRQ_H */\ndiff --git a/arch/x86/mm/ibs.c b/arch/x86/mm/ibs.c\nindex 752f688375f9..d0d93f09432d 100644\n--- a/arch/x86/mm/ibs.c\n+++ b/arch/x86/mm/ibs.c\n@@ -16,6 +16,7 @@ static u64 ibs_config __read_mostly;\n static u32 ibs_caps;\n \n #define IBS_NR_SAMPLES\t150\n+#define IBS_SAMPLE_PERIOD      10000\n \n /*\n  * Basic access info captured for each memory access.\n@@ -43,6 +44,36 @@ struct ibs_sample_pcpu __percpu *ibs_s;\n static struct work_struct ibs_work;\n static struct irq_work ibs_irq_work;\n \n+void hwmem_access_profiling_stop(void)\n+{\n+\tu64 ops_ctl;\n+\n+\tif (!hwmem_access_profiling)\n+\t\treturn;\n+\n+\trdmsrl(MSR_AMD64_IBSOPCTL, ops_ctl);\n+\twrmsrl(MSR_AMD64_IBSOPCTL, ops_ctl & ~IBS_OP_ENABLE);\n+}\n+\n+void hwmem_access_profiling_start(void)\n+{\n+\tu64 config = 0;\n+\tunsigned int period = IBS_SAMPLE_PERIOD;\n+\n+\tif (!hwmem_access_profiling)\n+\t\treturn;\n+\n+\t/* Disable IBS for kernel thread */\n+\tif (!current->mm)\n+\t\tgoto out;\n+\n+\tconfig = (period >> 4) & IBS_OP_MAX_CNT;\n+\tconfig |= (period & IBS_OP_MAX_CNT_EXT_MASK);\n+\tconfig |= ibs_config;\n+out:\n+\twrmsrl(MSR_AMD64_IBSOPCTL, config);\n+}\n+\n bool hwmem_access_profiler_inuse(void)\n {\n \treturn hwmem_access_profiling;\n@@ -310,6 +341,7 @@ static int __init ibs_access_profiling_init(void)\n \t\t\t  x86_amd_ibs_access_profile_startup,\n \t\t\t  x86_amd_ibs_access_profile_teardown);\n \n+\thwmem_access_profiling = true;\n \tpr_info(\"IBS setup for memory access profiling\\n\");\n \treturn 0;\n }\ndiff --git a/include/linux/pghot.h b/include/linux/pghot.h\nindex 20ea9767dbdd..603791183102 100644\n--- a/include/linux/pghot.h\n+++ b/include/linux/pghot.h\n@@ -6,8 +6,12 @@\n \n #ifdef CONFIG_HWMEM_PROFILER\n bool hwmem_access_profiler_inuse(void);\n+void hwmem_access_profiling_start(void);\n+void hwmem_access_profiling_stop(void);\n #else\n static inline bool hwmem_access_profiler_inuse(void) { return false; }\n+static inline void hwmem_access_profiling_start(void) {}\n+static inline void hwmem_access_profiling_stop(void) {}\n #endif\n \n /* Page hotness temperature sources */\n-- \n2.34.1\n\n\n\n---\n\nFrom: Kinsey Ho <kinseyho@google.com>\n\nRefactor the existing MGLRU page table walking logic to make it\nresumable.\n\nAdditionally, introduce two hooks into the MGLRU page table walk:\naccessed callback and flush callback. The accessed callback is called\nfor each accessed page detected via the scanned accessed bit. The flush\ncallback is called when the accessed callback reports that a flush is\nrequired. This allows for processing pages in batches for efficiency.\n\nWith a generalised page table walk, introduce a new scan function which\nrepeatedly scans on the same young generation and does not add a new\nyoung generation.\n\nSigned-off-by: Kinsey Ho <kinseyho@google.com>\nSigned-off-by: Yuanchu Xie <yuanchu@google.com>\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n include/linux/mmzone.h |   5 ++\n mm/internal.h          |   4 +\n mm/vmscan.c            | 181 +++++++++++++++++++++++++++++++----------\n 3 files changed, 145 insertions(+), 45 deletions(-)\n\ndiff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\nindex 49c374064fc2..26350a4951ff 100644\n--- a/include/linux/mmzone.h\n+++ b/include/linux/mmzone.h\n@@ -548,6 +548,8 @@ struct lru_gen_mm_walk {\n \tunsigned long seq;\n \t/* the next address within an mm to scan */\n \tunsigned long next_addr;\n+\t/* called for each accessed pte/pmd */\n+\tbool (*accessed_cb)(unsigned long pfn);\n \t/* to batch promoted pages */\n \tint nr_pages[MAX_NR_GENS][ANON_AND_FILE][MAX_NR_ZONES];\n \t/* to batch the mm stats */\n@@ -555,6 +557,9 @@ struct lru_gen_mm_walk {\n \t/* total batched items */\n \tint batched;\n \tint swappiness;\n+\t/* for the pmd under scanning */\n+\tint nr_young_pte;\n+\tint nr_total_pte;\n \tbool force_scan;\n };\n \ndiff --git a/mm/internal.h b/mm/internal.h\nindex e430da900430..426db1ae286f 100644\n--- a/mm/internal.h\n+++ b/mm/internal.h\n@@ -538,6 +538,10 @@ extern unsigned long highest_memmap_pfn;\n bool folio_isolate_lru(struct folio *folio);\n void folio_putback_lru(struct folio *folio);\n extern void reclaim_throttle(pg_data_t *pgdat, enum vmscan_throttle_state reason);\n+void set_task_reclaim_state(struct task_struct *task,\n+\t\t\t\t   struct reclaim_state *rs);\n+void lru_gen_scan_lruvec(struct lruvec *lruvec, unsigned long seq,\n+\t\t\t bool (*accessed_cb)(unsigned long), void (*flush_cb)(void));\n #ifdef CONFIG_NUMA\n int user_proactive_reclaim(char *buf,\n \t\t\t   struct mem_cgroup *memcg, pg_data_t *pgdat);\ndiff --git a/mm/vmscan.c b/mm/vmscan.c\nindex 670fe9fae5ba..02f3dd128638 100644\n--- a/mm/vmscan.c\n+++ b/mm/vmscan.c\n@@ -289,7 +289,7 @@ static int sc_swappiness(struct scan_control *sc, struct mem_cgroup *memcg)\n \t\t\tcontinue;\t\t\t\t\\\n \t\telse\n \n-static void set_task_reclaim_state(struct task_struct *task,\n+void set_task_reclaim_state(struct task_struct *task,\n \t\t\t\t   struct reclaim_state *rs)\n {\n \t/* Check for an overwrite */\n@@ -3058,7 +3058,7 @@ static bool iterate_mm_list(struct lru_gen_mm_walk *walk, struct mm_struct **ite\n \n \tVM_WARN_ON_ONCE(mm_state->seq + 1 < walk->seq);\n \n-\tif (walk->seq <= mm_state->seq)\n+\tif (!walk->accessed_cb && walk->seq <= mm_state->seq)\n \t\tgoto done;\n \n \tif (!mm_state->head)\n@@ -3484,16 +3484,14 @@ static void walk_update_folio(struct lru_gen_mm_walk *walk, struct folio *folio,\n \t}\n }\n \n-static bool walk_pte_range(pmd_t *pmd, unsigned long start, unsigned long end,\n-\t\t\t   struct mm_walk *args)\n+static int walk_pte_range(pmd_t *pmd, unsigned long start, unsigned long end,\n+\t\t\t   struct mm_walk *args, bool *suitable)\n {\n \tint i;\n \tbool dirty;\n \tpte_t *pte;\n \tspinlock_t *ptl;\n \tunsigned long addr;\n-\tint total = 0;\n-\tint young = 0;\n \tstruct folio *last = NULL;\n \tstruct lru_gen_mm_walk *walk = args->private;\n \tstruct mem_cgroup *memcg = lruvec_memcg(walk->lruvec);\n@@ -3501,19 +3499,24 @@ static bool walk_pte_range(pmd_t *pmd, unsigned long start, unsigned long end,\n \tDEFINE_MAX_SEQ(walk->lruvec);\n \tint gen = lru_gen_from_seq(max_seq);\n \tpmd_t pmdval;\n+\tint err = 0;\n \n \tpte = pte_offset_map_rw_nolock(args->mm, pmd, start & PMD_MASK, &pmdval, &ptl);\n-\tif (!pte)\n-\t\treturn false;\n+\tif (!pte) {\n+\t\t*suitable = false;\n+\t\treturn err;\n+\t}\n \n \tif (!spin_trylock(ptl)) {\n \t\tpte_unmap(pte);\n-\t\treturn true;\n+\t\t*suitable = true;\n+\t\treturn err;\n \t}\n \n \tif (unlikely(!pmd_same(pmdval, pmdp_get_lockless(pmd)))) {\n \t\tpte_unmap_unlock(pte, ptl);\n-\t\treturn false;\n+\t\t*suitable = false;\n+\t\treturn err;\n \t}\n \n \tarch_enter_lazy_mmu_mode();\n@@ -3522,8 +3525,9 @@ static bool walk_pte_range(pmd_t *pmd, unsigned long start, unsigned long end,\n \t\tunsigned long pfn;\n \t\tstruct folio *folio;\n \t\tpte_t ptent = ptep_get(pte + i);\n+\t\tbool do_flush;\n \n-\t\ttotal++;\n+\t\twalk->nr_total_pte++;\n \t\twalk->mm_stats[MM_LEAF_TOTAL]++;\n \n \t\tpfn = get_pte_pfn(ptent, args->vma, addr, pgdat);\n@@ -3547,23 +3551,36 @@ static bool walk_pte_range(pmd_t *pmd, unsigned long start, unsigned long end,\n \t\tif (pte_dirty(ptent))\n \t\t\tdirty = true;\n \n-\t\tyoung++;\n+\t\twalk->nr_young_pte++;\n \t\twalk->mm_stats[MM_LEAF_YOUNG]++;\n+\n+\t\tif (!walk->accessed_cb)\n+\t\t\tcontinue;\n+\n+\t\tdo_flush = walk->accessed_cb(pfn);\n+\t\tif (do_flush) {\n+\t\t\twalk->next_addr = addr + PAGE_SIZE;\n+\n+\t\t\terr = -EAGAIN;\n+\t\t\tbreak;\n+\t\t}\n \t}\n \n \twalk_update_folio(walk, last, gen, dirty);\n \tlast = NULL;\n \n-\tif (i < PTRS_PER_PTE && get_next_vma(PMD_MASK, PAGE_SIZE, args, &start, &end))\n+\tif (!err && i < PTRS_PER_PTE &&\n+\t    get_next_vma(PMD_MASK, PAGE_SIZE, args, &start, &end))\n \t\tgoto restart;\n \n \tarch_leave_lazy_mmu_mode();\n \tpte_unmap_unlock(pte, ptl);\n \n-\treturn suitable_to_scan(total, young);\n+\t*suitable = suitable_to_scan(walk->nr_total_pte, walk->nr_young_pte);\n+\treturn err;\n }\n \n-static void walk_pmd_range_locked(pud_t *pud, unsigned long addr, struct vm_area_struct *vma,\n+static int walk_pmd_range_locked(pud_t *pud, unsigned long addr, struct vm_area_struct *vma,\n \t\t\t\t  struct mm_walk *args, unsigned long *bitmap, unsigned long *first)\n {\n \tint i;\n@@ -3576,6 +3593,7 @@ static void walk_pmd_range_locked(pud_t *pud, unsigned long addr, struct vm_area\n \tstruct pglist_data *pgdat = lruvec_pgdat(walk->lruvec);\n \tDEFINE_MAX_SEQ(walk->lruvec);\n \tint gen = lru_gen_from_seq(max_seq);\n+\tint err = 0;\n \n \tVM_WARN_ON_ONCE(pud_leaf(*pud));\n \n@@ -3583,13 +3601,13 @@ static void walk_pmd_range_locked(pud_t *pud, unsigned long addr, struct vm_area\n \tif (*first == -1) {\n \t\t*first = addr;\n \t\tbitmap_zero(bitmap, MIN_LRU_BATCH);\n-\t\treturn;\n+\t\treturn err;\n \t}\n \n \ti = addr == -1 ? 0 : pmd_index(addr) - pmd_index(*first);\n \tif (i && i <= MIN_LRU_BATCH) {\n \t\t__set_bit(i - 1, bitmap);\n-\t\treturn;\n+\t\treturn err;\n \t}\n \n \tpmd = pmd_offset(pud, *first);\n@@ -3603,6 +3621,7 @@ static void walk_pmd_range_locked(pud_t *pud, unsigned long addr, struct vm_area\n \tdo {\n \t\tunsigned long pfn;\n \t\tstruct folio *folio;\n+\t\tbool do_flush;\n \n \t\t/* don't round down the first address */\n \t\taddr = i ? (*first & PMD_MASK) + i * PMD_SIZE : *first;\n@@ -3639,6 +3658,17 @@ static void walk_pmd_range_locked(pud_t *pud, unsigned long addr, struct vm_area\n \t\t\tdirty = true;\n \n \t\twalk->mm_stats[MM_LEAF_YOUNG]++;\n+\t\tif (!walk->accessed_cb)\n+\t\t\tgoto next;\n+\n+\t\tdo_flush = walk->accessed_cb(pfn);\n+\t\tif (do_flush) {\n+\t\t\ti = find_next_bit(bitmap, MIN_LRU_BATCH, i) + 1;\n+\n+\t\t\twalk->next_addr = (*first & PMD_MASK) + i * PMD_SIZE;\n+\t\t\terr = -EAGAIN;\n+\t\t\tbreak;\n+\t\t}\n next:\n \t\ti = i > MIN_LRU_BATCH ? 0 : find_next_bit(bitmap, MIN_LRU_BATCH, i) + 1;\n \t} while (i <= MIN_LRU_BATCH);\n@@ -3649,9 +3679,10 @@ static void walk_pmd_range_locked(pud_t *pud, unsigned long addr, struct vm_area\n \tspin_unlock(ptl);\n done:\n \t*first = -1;\n+\treturn err;\n }\n \n-static void walk_pmd_range(pud_t *pud, unsigned long start, unsigned long end,\n+static int walk_pmd_range(pud_t *pud, unsigned long start, unsigned long end,\n \t\t\t   struct mm_walk *args)\n {\n \tint i;\n@@ -3663,6 +3694,7 @@ static void walk_pmd_range(pud_t *pud, unsigned long start, unsigned long end,\n \tunsigned long first = -1;\n \tstruct lru_gen_mm_walk *walk = args->private;\n \tstruct lru_gen_mm_state *mm_state = get_mm_state(walk->lruvec);\n+\tint err = 0;\n \n \tVM_WARN_ON_ONCE(pud_leaf(*pud));\n \n@@ -3676,6 +3708,7 @@ static void walk_pmd_range(pud_t *pud, unsigned long start, unsigned long end,\n \t/* walk_pte_range() may call get_next_vma() */\n \tvma = args->vma;\n \tfor (i = pmd_index(start), addr = start; addr != end; i++, addr = next) {\n+\t\tbool suitable;\n \t\tpmd_t val = pmdp_get_lockless(pmd + i);\n \n \t\tnext = pmd_addr_end(addr, end);\n@@ -3692,7 +3725,10 @@ static void walk_pmd_range(pud_t *pud, unsigned long start, unsigned long end,\n \t\t\twalk->mm_stats[MM_LEAF_TOTAL]++;\n \n \t\t\tif (pfn != -1)\n-\t\t\t\twalk_pmd_range_locked(pud, addr, vma, args, bitmap, &first);\n+\t\t\t\terr = walk_pmd_range_locked(pud, addr, vma, args,\n+\t\t\t\t\t\tbitmap, &first);\n+\t\t\tif (err)\n+\t\t\t\treturn err;\n \t\t\tcontinue;\n \t\t}\n \n@@ -3701,33 +3737,51 @@ static void walk_pmd_range(pud_t *pud, unsigned long start, unsigned long end,\n \t\t\tif (!pmd_young(val))\n \t\t\t\tcontinue;\n \n-\t\t\twalk_pmd_range_locked(pud, addr, vma, args, bitmap, &first);\n+\t\t\terr = walk_pmd_range_locked(pud, addr, vma, args,\n+\t\t\t\t\t\tbitmap, &first);\n+\t\t\tif (err)\n+\t\t\t\treturn err;\n \t\t}\n \n \t\tif (!walk->force_scan && !test_bloom_filter(mm_state, walk->seq, pmd + i))\n \t\t\tcontinue;\n \n+\t\terr = walk_pte_range(&val, addr, next, args, &suitable);\n+\t\tif (err && walk->next_addr < next && first == -1)\n+\t\t\treturn err;\n+\n+\t\twalk->nr_total_pte = 0;\n+\t\twalk->nr_young_pte = 0;\n+\n \t\twalk->mm_stats[MM_NONLEAF_FOUND]++;\n \n-\t\tif (!walk_pte_range(&val, addr, next, args))\n-\t\t\tcontinue;\n+\t\tif (!suitable)\n+\t\t\tgoto next;\n \n \t\twalk->mm_stats[MM_NONLEAF_ADDED]++;\n \n \t\t/* carry over to the next generation */\n \t\tupdate_bloom_filter(mm_state, walk->seq + 1, pmd + i);\n+next:\n+\t\tif (err) {\n+\t\t\twalk->next_addr = first;\n+\t\t\treturn err;\n+\t\t}\n \t}\n \n-\twalk_pmd_range_locked(pud, -1, vma, args, bitmap, &first);\n+\terr = walk_pmd_range_locked(pud, -1, vma, args, bitmap, &first);\n \n-\tif (i < PTRS_PER_PMD && get_next_vma(PUD_MASK, PMD_SIZE, args, &start, &end))\n+\tif (!err && i < PTRS_PER_PMD &&\n+\t    get_next_vma(PUD_MASK, PMD_SIZE, args, &start, &end))\n \t\tgoto restart;\n+\n+\treturn err;\n }\n \n static int walk_pud_range(p4d_t *p4d, unsigned long start, unsigned long end,\n \t\t\t  struct mm_walk *args)\n {\n-\tint i;\n+\tint i, err;\n \tpud_t *pud;\n \tunsigned long addr;\n \tunsigned long next;\n@@ -3745,7 +3799,9 @@ static int walk_pud_range(p4d_t *p4d, unsigned long start, unsigned long end,\n \t\tif (!pud_present(val) || WARN_ON_ONCE(pud_leaf(val)))\n \t\t\tcontinue;\n \n-\t\twalk_pmd_range(&val, addr, next, args);\n+\t\terr = walk_pmd_range(&val, addr, next, args);\n+\t\tif (err)\n+\t\t\treturn err;\n \n \t\tif (need_resched() || walk->batched >= MAX_LRU_BATCH) {\n \t\t\tend = (addr | ~PUD_MASK) + 1;\n@@ -3766,40 +3822,48 @@ static int walk_pud_range(p4d_t *p4d, unsigned long start, unsigned long end,\n \treturn -EAGAIN;\n }\n \n-static void walk_mm(struct mm_struct *mm, struct lru_gen_mm_walk *walk)\n+static int try_walk_mm(struct mm_struct *mm, struct lru_gen_mm_walk *walk)\n {\n+\tint err;\n \tstatic const struct mm_walk_ops mm_walk_ops = {\n \t\t.test_walk = should_skip_vma,\n \t\t.p4d_entry = walk_pud_range,\n \t\t.walk_lock = PGWALK_RDLOCK,\n \t};\n-\tint err;\n \tstruct lruvec *lruvec = walk->lruvec;\n \n-\twalk->next_addr = FIRST_USER_ADDRESS;\n+\tDEFINE_MAX_SEQ(lruvec);\n \n-\tdo {\n-\t\tDEFINE_MAX_SEQ(lruvec);\n+\terr = -EBUSY;\n \n-\t\terr = -EBUSY;\n+\t/* another thread might have called inc_max_seq() */\n+\tif (walk->seq != max_seq)\n+\t\treturn err;\n \n-\t\t/* another thread might have called inc_max_seq() */\n-\t\tif (walk->seq != max_seq)\n-\t\t\tbreak;\n+\t/* the caller might be holding the lock for write */\n+\tif (mmap_read_trylock(mm)) {\n+\t\terr = walk_page_range(mm, walk->next_addr, ULONG_MAX,\n+\t\t\t\t      &mm_walk_ops, walk);\n \n-\t\t/* the caller might be holding the lock for write */\n-\t\tif (mmap_read_trylock(mm)) {\n-\t\t\terr = walk_page_range(mm, walk->next_addr, ULONG_MAX, &mm_walk_ops, walk);\n+\t\tmmap_read_unlock(mm);\n+\t}\n \n-\t\t\tmmap_read_unlock(mm);\n-\t\t}\n+\tif (walk->batched) {\n+\t\tspin_lock_irq(&lruvec->lru_lock);\n+\t\treset_batch_size(walk);\n+\t\tspin_unlock_irq(&lruvec->lru_lock);\n+\t}\n \n-\t\tif (walk->batched) {\n-\t\t\tspin_lock_irq(&lruvec->lru_lock);\n-\t\t\treset_batch_size(walk);\n-\t\t\tspin_unlock_irq(&lruvec->lru_lock);\n-\t\t}\n+\treturn err;\n+}\n+\n+static void walk_mm(struct mm_struct *mm, struct lru_gen_mm_walk *walk)\n+{\n+\tint err;\n \n+\twalk->next_addr = FIRST_USER_ADDRESS;\n+\tdo {\n+\t\terr = try_walk_mm(mm, walk);\n \t\tcond_resched();\n \t} while (err == -EAGAIN);\n }\n@@ -4011,6 +4075,33 @@ static bool inc_max_seq(struct lruvec *lruvec, unsigned long seq, int swappiness\n \treturn success;\n }\n \n+void lru_gen_scan_lruvec(struct lruvec *lruvec, unsigned long seq,\n+\t\t\t bool (*accessed_cb)(unsigned long), void (*flush_cb)(void))\n+{\n+\tstruct lru_gen_mm_walk *walk = current->reclaim_state->mm_walk;\n+\tstruct mm_struct *mm = NULL;\n+\n+\twalk->lruvec = lruvec;\n+\twalk->seq = seq;\n+\twalk->accessed_cb = accessed_cb;\n+\twalk->swappiness = MAX_SWAPPINESS;\n+\n+\tdo {\n+\t\tint err = -EBUSY;\n+\n+\t\titerate_mm_list(walk, &mm);\n+\t\tif (!mm)\n+\t\t\tbreak;\n+\n+\t\twalk->next_addr = FIRST_USER_ADDRESS;\n+\t\tdo {\n+\t\t\terr = try_walk_mm(mm, walk);\n+\t\t\tcond_resched();\n+\t\t\tflush_cb();\n+\t\t} while (err == -EAGAIN);\n+\t} while (mm);\n+}\n+\n static bool try_to_inc_max_seq(struct lruvec *lruvec, unsigned long seq,\n \t\t\t       int swappiness, bool force_scan)\n {\n-- \n2.34.1\n\n\n\n---\n\nFrom: Kinsey Ho <kinseyho@google.com>\n\nIntroduce a new kernel daemon, klruscand, that periodically invokes the\nMGLRU page table walk. It leverages the new callbacks to gather access\ninformation and forwards it to pghot sub-system for promotion decisions.\n\nThis benefits from reusing the existing MGLRU page table walk\ninfrastructure, which is optimized with features such as hierarchical\nscanning and bloom filters to reduce CPU overhead.\n\nAs an additional optimization to be added in the future, we can tune\nthe scan intervals for each memcg.\n\nSigned-off-by: Kinsey Ho <kinseyho@google.com>\nSigned-off-by: Yuanchu Xie <yuanchu@google.com>\n[Reduced the scan interval to 500ms, KLRUSCAND to default n in config]\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n mm/Kconfig     |   8 ++++\n mm/Makefile    |   1 +\n mm/klruscand.c | 110 +++++++++++++++++++++++++++++++++++++++++++++++++\n 3 files changed, 119 insertions(+)\n create mode 100644 mm/klruscand.c\n\ndiff --git a/mm/Kconfig b/mm/Kconfig\nindex 07b16aece877..9e9eca8db8bf 100644\n--- a/mm/Kconfig\n+++ b/mm/Kconfig\n@@ -1502,6 +1502,14 @@ config HWMEM_PROFILER\n \t  rolled up to PGHOT sub-system for further action like hot page\n \t  promotion or NUMA Balancing\n \n+config KLRUSCAND\n+\tbool \"Kernel lower tier access scan daemon\"\n+\tdefault n\n+\tdepends on PGHOT && LRU_GEN_WALKS_MMU\n+\thelp\n+\t  Scan for accesses from lower tiers by invoking MGLRU to perform\n+\t  page table walks.\n+\n source \"mm/damon/Kconfig\"\n \n endmenu\ndiff --git a/mm/Makefile b/mm/Makefile\nindex 89f999647752..c68df497a063 100644\n--- a/mm/Makefile\n+++ b/mm/Makefile\n@@ -153,3 +153,4 @@ obj-$(CONFIG_PGHOT) += pghot-precise.o\n else\n obj-$(CONFIG_PGHOT) += pghot-default.o\n endif\n+obj-$(CONFIG_KLRUSCAND) += klruscand.o\ndiff --git a/mm/klruscand.c b/mm/klruscand.c\nnew file mode 100644\nindex 000000000000..13a41b38d67d\n--- /dev/null\n+++ b/mm/klruscand.c\n@@ -0,0 +1,110 @@\n+// SPDX-License-Identifier: GPL-2.0-only\n+#include <linux/memcontrol.h>\n+#include <linux/kthread.h>\n+#include <linux/module.h>\n+#include <linux/vmalloc.h>\n+#include <linux/memory-tiers.h>\n+#include <linux/pghot.h>\n+\n+#include \"internal.h\"\n+\n+#define KLRUSCAND_INTERVAL 500\n+#define BATCH_SIZE (2 << 16)\n+\n+static struct task_struct *scan_thread;\n+static unsigned long pfn_batch[BATCH_SIZE];\n+static int batch_index;\n+\n+static void flush_cb(void)\n+{\n+\tint i;\n+\n+\tfor (i = 0; i < batch_index; i++) {\n+\t\tunsigned long pfn = pfn_batch[i];\n+\n+\t\tpghot_record_access(pfn, NUMA_NO_NODE, PGHOT_PGTABLE_SCAN, jiffies);\n+\n+\t\tif (i % 16 == 0)\n+\t\t\tcond_resched();\n+\t}\n+\tbatch_index = 0;\n+}\n+\n+static bool accessed_cb(unsigned long pfn)\n+{\n+\tWARN_ON_ONCE(batch_index == BATCH_SIZE);\n+\n+\tif (batch_index < BATCH_SIZE)\n+\t\tpfn_batch[batch_index++] = pfn;\n+\n+\treturn batch_index == BATCH_SIZE;\n+}\n+\n+static int klruscand_run(void *unused)\n+{\n+\tstruct lru_gen_mm_walk *walk;\n+\n+\twalk = kzalloc(sizeof(*walk),\n+\t\t       __GFP_HIGH | __GFP_NOMEMALLOC | __GFP_NOWARN);\n+\tif (!walk)\n+\t\treturn -ENOMEM;\n+\n+\twhile (!kthread_should_stop()) {\n+\t\tunsigned long next_wake_time;\n+\t\tlong sleep_time;\n+\t\tstruct mem_cgroup *memcg;\n+\t\tint flags;\n+\t\tint nid;\n+\n+\t\tnext_wake_time = jiffies + msecs_to_jiffies(KLRUSCAND_INTERVAL);\n+\n+\t\tfor_each_node_state(nid, N_MEMORY) {\n+\t\t\tpg_data_t *pgdat = NODE_DATA(nid);\n+\t\t\tstruct reclaim_state rs = { 0 };\n+\n+\t\t\tif (node_is_toptier(nid))\n+\t\t\t\tcontinue;\n+\n+\t\t\trs.mm_walk = walk;\n+\t\t\tset_task_reclaim_state(current, &rs);\n+\t\t\tflags = memalloc_noreclaim_save();\n+\n+\t\t\tmemcg = mem_cgroup_iter(NULL, NULL, NULL);\n+\t\t\tdo {\n+\t\t\t\tstruct lruvec *lruvec =\n+\t\t\t\t\tmem_cgroup_lruvec(memcg, pgdat);\n+\t\t\t\tunsigned long max_seq =\n+\t\t\t\t\tREAD_ONCE((lruvec)->lrugen.max_seq);\n+\n+\t\t\t\tlru_gen_scan_lruvec(lruvec, max_seq, accessed_cb, flush_cb);\n+\t\t\t\tcond_resched();\n+\t\t\t} while ((memcg = mem_cgroup_iter(NULL, memcg, NULL)));\n+\n+\t\t\tmemalloc_noreclaim_restore(flags);\n+\t\t\tset_task_reclaim_state(current, NULL);\n+\t\t\tmemset(walk, 0, sizeof(*walk));\n+\t\t}\n+\n+\t\tsleep_time = next_wake_time - jiffies;\n+\t\tif (sleep_time > 0 && sleep_time != MAX_SCHEDULE_TIMEOUT)\n+\t\t\tschedule_timeout_idle(sleep_time);\n+\t}\n+\tkfree(walk);\n+\treturn 0;\n+}\n+\n+static int __init klruscand_init(void)\n+{\n+\tstruct task_struct *task;\n+\n+\ttask = kthread_run(klruscand_run, NULL, \"klruscand\");\n+\n+\tif (IS_ERR(task)) {\n+\t\tpr_err(\"Failed to create klruscand kthread\\n\");\n+\t\treturn PTR_ERR(task);\n+\t}\n+\n+\tscan_thread = task;\n+\treturn 0;\n+}\n+module_init(klruscand_init);\n-- \n2.34.1\n\n\n\n---\n\nUnmapped page cache pages that end up in lower tiers don't get\npromoted easily. There were attempts to identify such pages and\nget them promoted as part of NUMA Balancing earlier [1]. The\nsame idea is taken forward here by using folio_mark_accessed()\nas a source of hotness.\n\nLower tier accesses from folio_mark_accessed() are reported to\npghot sub-system for hotness tracking and subsequent promotion.\n\nTODO: Need a better naming for this hotness source. Need to\nbetter understand/evaluate the overhead of hotness info\ncollection from this path.\n\n[1] https://lore.kernel.org/linux-mm/20250411221111.493193-1-gourry@gourry.net/\n\nSigned-off-by: Bharata B Rao <bharata@amd.com>\n---\n Documentation/admin-guide/mm/pghot.txt | 7 ++++++-\n include/linux/pghot.h                  | 5 +++++\n include/linux/vm_event_item.h          | 1 +\n mm/pghot-tunables.c                    | 7 +++++++\n mm/pghot.c                             | 6 ++++++\n mm/swap.c                              | 8 ++++++++\n mm/vmstat.c                            | 1 +\n 7 files changed, 34 insertions(+), 1 deletion(-)\n\ndiff --git a/Documentation/admin-guide/mm/pghot.txt b/Documentation/admin-guide/mm/pghot.txt\nindex b329e692ef89..c8eb61064247 100644\n--- a/Documentation/admin-guide/mm/pghot.txt\n+++ b/Documentation/admin-guide/mm/pghot.txt\n@@ -23,9 +23,10 @@ Path: /sys/kernel/debug/pghot/\n      - 0: Hardware hints (value 0x1)\n      - 1: Page table scan (value 0x2)\n      - 2: Hint faults (value 0x4)\n+     - 3: folio_mark_accessed (value 0x8)\n    - Default: 0 (disabled)\n    - Example:\n-     # echo 0x7 > /sys/kernel/debug/pghot/enabled_sources\n+     # echo 0xf > /sys/kernel/debug/pghot/enabled_sources\n      Enables all sources.\n \n 2. **target_nid**\n@@ -82,3 +83,7 @@ Path: /proc/vmstat\n 4. **pghot_recorded_hintfaults**\n    - Number of recorded accesses reported by NUMA Balancing based\n      hotness source.\n+\n+5. **pghot_recorded_fma**\n+   - Number of recorded accesses reported by folio_mark_accessed()\n+     hotness source.\ndiff --git a/include/linux/pghot.h b/include/linux/pghot.h\nindex 603791183102..8cf9dfb5365a 100644\n--- a/include/linux/pghot.h\n+++ b/include/linux/pghot.h\n@@ -19,6 +19,7 @@ enum pghot_src {\n \tPGHOT_HW_HINTS,\n \tPGHOT_PGTABLE_SCAN,\n \tPGHOT_HINT_FAULT,\n+\tPGHOT_FMA,\n };\n \n #ifdef CONFIG_PGHOT\n@@ -36,6 +37,7 @@ void pghot_debug_init(void);\n DECLARE_STATIC_KEY_FALSE(pghot_src_hwhints);\n DECLARE_STATIC_KEY_FALSE(pghot_src_pgtscans);\n DECLARE_STATIC_KEY_FALSE(pghot_src_hintfaults);\n+DECLARE_STATIC_KEY_FALSE(pghot_src_fma);\n \n /*\n  * Bit positions to enable individual sources in pghot/records_enabled\n@@ -45,6 +47,7 @@ enum pghot_src_enabled {\n \tPGHOT_HWHINTS_BIT = 0,\n \tPGHOT_PGTSCAN_BIT,\n \tPGHOT_HINTFAULT_BIT,\n+\tPGHOT_FMA_BIT,\n \tPGHOT_MAX_BIT\n };\n \n@@ -52,6 +55,8 @@ enum pghot_src_enabled {\n #define PGHOT_PGTSCAN_ENABLED\t\tBIT(PGHOT_PGTSCAN_BIT)\n #define PGHOT_HINTFAULT_ENABLED\t\tBIT(PGHOT_HINTFAULT_BIT)\n #define PGHOT_SRC_ENABLED_MASK\t\tGENMASK(PGHOT_MAX_BIT - 1, 0)\n+#define PGHOT_FMA_ENABLED\t\tBIT(PGHOT_FMA_BIT)\n+#define PGHOT_SRC_ENABLED_MASK\t\tGENMASK(PGHOT_MAX_BIT - 1, 0)\n \n #define PGHOT_DEFAULT_FREQ_THRESHOLD\t2\n \ndiff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h\nindex 67efbca9051c..ac1f28646b9c 100644\n--- a/include/linux/vm_event_item.h\n+++ b/include/linux/vm_event_item.h\n@@ -193,6 +193,7 @@ enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,\n \t\tPGHOT_RECORD_HWHINTS,\n \t\tPGHOT_RECORD_PGTSCANS,\n \t\tPGHOT_RECORD_HINTFAULTS,\n+\t\tPGHOT_RECORD_FMA,\n #ifdef CONFIG_HWMEM_PROFILER\n \t\tHWHINT_NR_EVENTS,\n \t\tHWHINT_KERNEL,\ndiff --git a/mm/pghot-tunables.c b/mm/pghot-tunables.c\nindex 79afbcb1e4f0..11c7f742a1be 100644\n--- a/mm/pghot-tunables.c\n+++ b/mm/pghot-tunables.c\n@@ -124,6 +124,13 @@ static void pghot_src_enabled_update(unsigned int enabled)\n \t\telse\n \t\t\tstatic_branch_disable(&pghot_src_hintfaults);\n \t}\n+\n+\tif (changed & PGHOT_FMA_ENABLED) {\n+\t\tif (enabled & PGHOT_FMA_ENABLED)\n+\t\t\tstatic_branch_enable(&pghot_src_fma);\n+\t\telse\n+\t\t\tstatic_branch_disable(&pghot_src_fma);\n+\t}\n }\n \n static ssize_t pghot_src_enabled_write(struct file *filp, const char __user *ubuf,\ndiff --git a/mm/pghot.c b/mm/pghot.c\nindex 6fc76c1eaff8..537f4af816ff 100644\n--- a/mm/pghot.c\n+++ b/mm/pghot.c\n@@ -43,6 +43,7 @@ static unsigned int sysctl_pghot_promote_rate_limit = 65536;\n DEFINE_STATIC_KEY_FALSE(pghot_src_hwhints);\n DEFINE_STATIC_KEY_FALSE(pghot_src_pgtscans);\n DEFINE_STATIC_KEY_FALSE(pghot_src_hintfaults);\n+DEFINE_STATIC_KEY_FALSE(pghot_src_fma);\n \n #ifdef CONFIG_SYSCTL\n static const struct ctl_table pghot_sysctls[] = {\n@@ -113,6 +114,11 @@ int pghot_record_access(unsigned long pfn, int nid, int src, unsigned long now)\n \t\t\treturn -EINVAL;\n \t\tcount_vm_event(PGHOT_RECORD_HINTFAULTS);\n \t\tbreak;\n+\tcase PGHOT_FMA:\n+\t\tif (!static_branch_likely(&pghot_src_fma))\n+\t\t\treturn -EINVAL;\n+\t\tcount_vm_event(PGHOT_RECORD_FMA);\n+\t\tbreak;\n \tdefault:\n \t\treturn -EINVAL;\n \t}\ndiff --git a/mm/swap.c b/mm/swap.c\nindex 2260dcd2775e..31a654b19844 100644\n--- a/mm/swap.c\n+++ b/mm/swap.c\n@@ -37,6 +37,8 @@\n #include <linux/page_idle.h>\n #include <linux/local_lock.h>\n #include <linux/buffer_head.h>\n+#include <linux/pghot.h>\n+#include <linux/memory-tiers.h>\n \n #include \"internal.h\"\n \n@@ -454,8 +456,14 @@ static bool lru_gen_clear_refs(struct folio *folio)\n  */\n void folio_mark_accessed(struct folio *folio)\n {\n+\tunsigned long pfn = folio_pfn(folio);\n+\n \tif (folio_test_dropbehind(folio))\n \t\treturn;\n+\n+\tif (!node_is_toptier(pfn_to_nid(pfn)))\n+\t\tpghot_record_access(pfn, NUMA_NO_NODE, PGHOT_FMA, jiffies);\n+\n \tif (lru_gen_enabled()) {\n \t\tlru_gen_inc_refs(folio);\n \t\treturn;\ndiff --git a/mm/vmstat.c b/mm/vmstat.c\nindex 62c47f44edf0..c4d90baf440b 100644\n--- a/mm/vmstat.c\n+++ b/mm/vmstat.c\n@@ -1506,6 +1506,7 @@ const char * const vmstat_text[] = {\n \t[I(PGHOT_RECORD_HWHINTS)]\t\t= \"pghot_recorded_hwhints\",\n \t[I(PGHOT_RECORD_PGTSCANS)]\t\t= \"pghot_recorded_pgtscans\",\n \t[I(PGHOT_RECORD_HINTFAULTS)]\t\t= \"pghot_recorded_hintfaults\",\n+\t[I(PGHOT_RECORD_FMA)]\t\t\t= \"pghot_recorded_fma\",\n #ifdef CONFIG_HWMEM_PROFILER\n \t[I(HWHINT_NR_EVENTS)]\t\t\t= \"hwhint_nr_events\",\n \t[I(HWHINT_KERNEL)]\t\t\t= \"hwhint_kernel\",\n-- \n2.34.1\n\n\n\n---\n\nOn 29-Jan-26 8:10 PM, Bharata B Rao wrote:\n> \n> Results\n> =======\n> TODO: Will post benchmark nubmers as reply to this patchset soon.\n\nHere is the first set of results from a microbenchmark:\n\nTest system details\n-------------------\n3 node AMD Zen5 system with 2 regular NUMA nodes (0, 1) and a CXL node (2)\n\n$ numactl -H\navailable: 3 nodes (0-2)\nnode 0 cpus: 0-95,192-287\nnode 0 size: 128460 MB\nnode 1 cpus: 96-191,288-383\nnode 1 size: 128893 MB\nnode 2 cpus:\nnode 2 size: 257993 MB\nnode distances:\nnode   0   1   2\n  0:  10  32  50\n  1:  32  10  60\n  2:  255  255  10\n\nHotness sources\n---------------\nNUMAB0 - Without NUMA Balancing in base case and with no source enabled\n         in the patched case. No migrations occur.\nNUMAB2 - Existing hot page promotion for the base case and\n         use of hint faults as source in the patched case.\npgtscan - Klruscand (MGLRU based PTE A bit scanning) source\nhwhints - IBS as source\n\nPghot by default promotes after two accesses but for NUMAB2 source,\npromotion is done after one access to match the base behaviour.\n(/sys/kernel/debug/pghot/freq_threshold=1)\n\n==============================================================\nScenario 1 - Enough memory in toptier and hence only promotion\n==============================================================\nMulti-threaded application with 64 threads that access memory at 4K granularity\nrepetitively and randomly. The number of accesses per thread and the randomness\npattern for each thread are fixed beforehand. The accesses are divided into\nstores and loads in the ratio of 50:50.\n\nBenchmark threads run on Node 0, while memory is initially provisioned on\nCXL node 2 before the accesses start.\n\nRepetitive accesses results in lowertier pages becoming hot and kmigrated\ndetecting and migrating them. The benchmark score is the time taken to finish\nthe accesses in microseconds. The sooner it finishes the better it is. All the\nnumbers shown below are average of 3 runs.\n\nDefault mode - Time taken (microseconds, lower is better)\n---------------------------------------------------------\nSource          Base            Pghot\n---------------------------------------------------------\nNUMAB0          117,069,417     115,802,776\nNUMAB2          102,918,471     103,378,828\npgtscan         NA              110,203,286\nhwhints         NA              92,880,388\n---------------------------------------------------------\n\nDefault mode - Pages migrated (pgpromote_success)\n---------------------------------------------------------\nSource          Base            Pghot\n---------------------------------------------------------\nNUMAB0          0               0\nNUMAB2          2097147         2097131\npgtscan         NA              2097130\nhwhints         NA              1706556\n---------------------------------------------------------\n\nPrecision mode - Time taken (microseconds, lower is better)\n-----------------------------------------------------------\nSource          Base            Pghot\n-----------------------------------------------------------\nNUMAB0          117,069,417     115,078,527\nNUMAB2          102,918,471     101,742,985\npgtscan         NA              110,024,513     NA\nhwhints         NA              101,163,603     NA\n-----------------------------------------------------------\n\nPrecision mode - Pages migrated (pgpromote_success)\n---------------------------------------------------\nSource          Base            Pghot\n---------------------------------------------------\nNUMAB0          0               0\nNUMAB2          2097147         2097144\npgtscan         NA              2097129\nhwhints         NA              1144304\n---------------------------------------------------\n\n- The NUMAB2 benchmark numbers and pgpromote_success numbers more\n  or less match in base and patched case.\n- Though the pgtscan case promotes all possible pages, the\n  benchmark number suffers. This source needs tuning.\n- Hwhints case is able to provide benchmark numbers similar to\n  base NUMAB2 even with less number of migrations.\n- With both default and precision modes of pghot the benchmark\n  behaves more or less similarly.\n\n==============================================================\nScenario 2 - Toptier memory overcommited, promotion + demotion\n==============================================================\nSingle threaded application that allocates memory on both DRAM and CXL nodes\nusing mmap(MAP_POPULATE). Every 1G region of allocated memory on CXL node is\naccessed at 4K granularity randomly and repetitively to build up the notion\nof hotness in the 1GB region that is under access. This should drive promotion.\nFor promotion to work successfully, the DRAM memory that has been provisioned\n(and not being accessed) should be demoted first. There is enough free memory\nin the CXL node to for demotions.\n\nIn summary, this benchmark creates a memory pressure on DRAM node and does\nCXL memory accesses to drive both demotion and promotion.\n\nThe number of accesses are fixed and hence, the quicker the accessed pages\nget promoted to DRAM, the sooner the benchmark is expected to finish.\nAll the numbers shown below are average of 3 runs.\n\nDRAM-node                       = 1\nCXL-node                        = 2\nInitial DRAM alloc ratio        = 75%\nAllocation-size                 = 171798691840\nInitial DRAM Alloc-size         = 128849018880\nInitial CXL Alloc-size          = 42949672960\nHot-region-size                 = 1073741824\nNr-regions                      = 160\nNr-regions DRAM                 = 120 (provisioned but not accessed)\nNr-hot-regions CXL              = 40\nAccess pattern                  = random\nAccess granularity              = 4096\nDelay b/n accesses              = 0\nLoad/store ratio                = 50l50s\nTHP used                        = no\nNr accesses                     = 42949672960\nNr repetitions                  = 1024\n\nDefault mode - Time taken (microseconds, lower is better)\n------------------------------------------------------\nSource          Base            Pghot\n------------------------------------------------------\nNUMAB0          63,809,267      60,794,786\nNUMAB2          67,541,601      62,376,991\npgtscan         NA              67,902,126\nhwhints         NA              59,872,525\n------------------------------------------------------\n\nDefault mode - Pages migrated (pgpromote_success)\n-------------------------------------------------\nSource          Base            Pghot\n-------------------------------------------------\nNUMAB0          0               0\nNUMAB2          179635          932693  (High R2R variation in base)\npgtscan         NA              27487\nhwhints         NA              274\n---------------------------------------\n\nPrecision mode - Time taken (microseconds, lower is better)\n------------------------------------------------------\nSource          Base            Pghot\n------------------------------------------------------\nNUMAB0          63,809,267      64,553,914\nNUMAB2          67,541,601      62,148,082\npgtscan         NA              65,073,396\nhwhints         NA              59,958,655\n------------------------------------------------------\n\nPrecision mode - Pages migrated (pgpromote_success)\n---------------------------------------------------\nSource          Base            Pghot\n---------------------------------------------------\nNUMAB0          0               0\nNUMAB2          179635          988360  (High R2R variaion in base)\npgtscan         NA              21418   (High R2R variation in patched)\nhwhints         NA              174     (High R2R variation in patched)\n---------------------------------------------------\n\n- The base case itself doesn't show any improvement in benchmark numbers due\n  to hot page promotion. The same pattern is seen in pghot case with all\n  the sources except hwhints. The benchmark itself may need tuning so that\n  promotion helps.\n- There is a high run to run variation in the number of pages promoted in\n  base case.\n- Most promotion attempts in base case fail because the NUMA hint fault\n  latency is found to exceed the threshold value (default threshold\n  is 1000ms) in majority of the promotion attempts.\n- Unlike base NUMAB2 where the hint fault latency is the difference between the\n  PTE update time (during scanning) and the access time (hint fault), pghot uses\n  a single latency threshold (4000ms in pghot-default and 5000ms in\n  pghot-precise) for two purposes.\n        1. If the time difference between successive accesses are within the\n           threshold, the page is marked as hot.\n        2. Later when kmigrated picks up the page for migration, it will migrate\n           only if the difference between the current time and the time when the\n          page was marked hot is with the threshold.\n  Because of the above difference in behaviour, more number of pages get\n  qualified for promotion compared to base NUMAB2.\n\n\n\n---\n\nOn 29-Jan-26 8:10 PM, Bharata B Rao wrote:\n> Results\n> =======\n> TODO: Will post benchmark nubmers as reply to this patchset soon.\n\nNumbers from redis-memtier benchmark:\n\nTest system details\n-------------------\n3 node AMD Zen5 system with 2 regular NUMA nodes (0, 1) and a CXL node (2)\n\n$ numactl -H\navailable: 3 nodes (0-2)\nnode 0 cpus: 0-95,192-287\nnode 0 size: 128460 MB\nnode 1 cpus: 96-191,288-383\nnode 1 size: 128893 MB\nnode 2 cpus:\nnode 2 size: 257993 MB\nnode distances:\nnode   0   1   2\n  0:  10  32  50\n  1:  32  10  60\n  2:  255  255  10\n\nHotness sources\n---------------\nNUMAB0 - Without NUMA Balancing in base case and with no source enabled\n         in the patched case. No migrations occur.\nNUMAB2 - Existing hot page promotion for the base case and\n         use of hint faults as source in the patched case.\n\nPghot by default promotes after two accesses but for NUMAB2 source,\npromotion is done after one access to match the base behaviour.\n(/sys/kernel/debug/pghot/freq_threshold=1)\n\n==============================================================\nScenario 1 - Enough memory in toptier and hence only promotion\n==============================================================\nIn the setup phase, 64GB database is provisioned and explicitly moved\nto Node 2 by migrating redis-server's memory to Node 2.\nMemtier is run on Node 1.\n\nParallel distribution, 50% of the keys accessed, each 4 times.\n16        Threads\n100       Connections per thread\n77808     Requests per client\n\n==================================================================================================\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9\nLatency       KB/sec\n--------------------------------------------------------------------------------------------------\nBase, NUMAB0\nTotals     225827.75       226.49746       225.27900       425.98300\n454.65500    513106.09\n--------------------------------------------------------------------------------------------------\nBase, NUMAB2\nTotals     254869.29       205.61759       216.06300       399.35900\n454.65500    579091.74\n--------------------------------------------------------------------------------------------------\npghot-default, NUMAB2\nTotals     264229.35       202.81411       215.03900       393.21500\n446.46300    600358.86\n--------------------------------------------------------------------------------------------------\npghot-precise, NUMAB2\nTotals     261136.17       203.32692       215.03900       391.16700\n446.46300    593330.81\n==================================================================================================\n\npgpromote_success\n==================================\nBase, NUMAB0            0\nBase, NUMAB2            10,435,178\npghot-default, NUMAB2   10,435,031\npghot-precise, NUMAB2   10,435,245\n==================================\n\n- There is a clear benefit of hot page promotion seen. Both\n  base and pghot show similar benefits.\n- The number of pages promoted in both cases are more or less\n  same.\n\n==============================================================\nScenario 2 - Toptier memory overcommited, promotion + demotion\n==============================================================\nIn the setup phase, 192GB database is provisioned. The database occupies\nNode 1 entirely(~128GB) and spills over to Node 2 (~64GB).\nMemtier is run on Node 1.\n\nParallel distribution, 50% of the keys accessed, each 4 times.\n16        Threads\n100       Connections per thread\n233424    Requests per client\n\n==================================================================================================\nType         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9\nLatency       KB/sec\n--------------------------------------------------------------------------------------------------\nBase, NUMAB0\nTotals     246474.55       211.90623       192.51100       370.68700\n448.51100    560235.63\n--------------------------------------------------------------------------------------------------\nBase, NUMAB2\nTotals     232790.88       221.18604       214.01500       419.83900\n509.95100    529132.72\n--------------------------------------------------------------------------------------------------\npghot-default, NUMAB2\nTotals     241615.60       216.12761       210.94300       391.16700\n475.13500    549191.27\n--------------------------------------------------------------------------------------------------\npghot-precise, NUMAB2\nTotals     238557.37       217.57630       207.87100       395.26300\n471.03900    542239.92\n==================================================================================================\n                        pgpromote_success       pgdemote_kswapd\n===============================================================\nBase, NUMAB0            0                       832,494\nBase, NUMAB2            352,075                 720,409\npghot-default, NUMAB2   25,865,321              26,154,984\npghot-precise, NUMAB2   25,525,429              25,838,095\n===============================================================\n\n- No clear benefit is seen with hot page promotion both in base and pghot case.\n- Most promotion attempts in base case fail because the NUMA hint fault latency\n  is found to exceed the threshold value (default threshold of 1000ms) in\n  majority of the promotion attempts.\n- Unlike base NUMAB2 where the hint fault latency is the difference between the\n  PTE update time (during scanning) and the access time (hint fault), pghot uses\n  a single latency threshold (4000ms in pghot-default and 5000ms in\n  pghot-precise) for two purposes.\n        1. If the time difference between successive accesses are within the\n           threshold, the page is marked as hot.\n        2. Later when kmigrated picks up the page for migration, it will migrate\n           only if the difference between the current time and the time when the\n          page was marked hot is with the threshold.\n  Because of the above difference in behaviour, more number of pages get\n  qualified for promotion compared to base NUMAB2.\n\n\n---\n\nOn 29-Jan-26 8:10 PM, Bharata B Rao wrote:\n> \n> Results\n> =======\n> TODO: Will post benchmark nubmers as reply to this patchset soon.\n\nHere are Graph500 numbers for the hint fault source:\n\nTest system details\n-------------------\n3 node AMD Zen5 system with 2 regular NUMA nodes (0, 1) and a CXL node (2)\n\n$ numactl -H\navailable: 3 nodes (0-2)\nnode 0 cpus: 0-95,192-287\nnode 0 size: 128460 MB\nnode 1 cpus: 96-191,288-383\nnode 1 size: 128893 MB\nnode 2 cpus:\nnode 2 size: 257993 MB\nnode distances:\nnode   0   1   2\n  0:  10  32  50\n  1:  32  10  60\n  2:  255  255  10\n\nHotness sources\n---------------\nNUMAB0 - Without NUMA Balancing in base case and with no source enabled\n         in the pghot case. No migrations occur.\nNUMAB2 - Existing hot page promotion for the base case and\n         use of hint faults as source in the pghot case.\n\nPghot by default promotes after two accesses but for NUMAB2 source,\npromotion is done after one access to match the base behaviour.\n(/sys/kernel/debug/pghot/freq_threshold=1)\n\nGraph500 details\n----------------\nCommand: mpirun -n 128 --bind-to core --map-by core\ngraph500/src/graph500_reference_bfs 28 16\n\nAfter the graph creation, the processes are stopped and data is migrated\nto CXL node 2 before continuing so that BFS phase starts accessing lower\ntier memory.\n\nTotal memory usage is slightly over 100GB and will fit within Node 0 and 1.\nHence there is no memory pressure to induce demotions.\n\n=====================================================================================\n                        Base            Base            pghot-default\npghot-precise\n                        NUMAB0          NUMAB2          NUMAB2          NUMAB2\n=====================================================================================\nharmonic_mean_TEPS      5.10676e+08     7.56804e+08     5.92473e+08     7.47091e+08\nmean_time               8.41027         5.67508         7.24915         5.74886\nmedian_TEPS             5.11535e+08     7.24252e+08     5.63155e+08     7.71638e+08\nmax_TEPS                5.1785e+08      1.06051e+09     7.88018e+08     1.0504e+09\n\npgpromote_success       0               13557718        13737730        13734469\nnuma_pte_updates        0               26491591        26848847        26726856\nnuma_hint_faults        0               13558077        13882743        13798024\n=====================================================================================\n\n\n- The base case shows a good improvement with NUMAB2(48%) in harmonic_mean_TEPS.\n- The same improvement gets maintained with pghot-precise too (46%).\n- pghot-default mode doesn't show benefit even when achieving similar page promotion\n  numbers. This mode doesn't track accessing NID and by default promotes to NID=0\n  which probably isn't all that beneficial as processes are running on both Node 0\n  and Node 1.\n\n\n\n---\n\nOn 29-Jan-26 8:10 PM, Bharata B Rao wrote:\n> +\n> +/*\n> + * Walks the PFNs of the zone, isolates and migrates them in batches.\n> + */\n> +static void kmigrated_walk_zone(unsigned long start_pfn, unsigned long end_pfn,\n> +\t\t\t\tint src_nid)\n> +{\n> +\tint cur_nid = NUMA_NO_NODE;\n> +\tLIST_HEAD(migrate_list);\n> +\tint batch_count = 0;\n> +\tstruct folio *folio;\n> +\tstruct page *page;\n> +\tunsigned long pfn;\n> +\n> +\tpfn = start_pfn;\n> +\tdo {\n> +\t\tint nid = NUMA_NO_NODE, nr = 1;\n> +\t\tint freq = 0;\n> +\t\tunsigned long time = 0;\n> +\n> +\t\tif (!pfn_valid(pfn))\n> +\t\t\tgoto out_next;\n> +\n> +\t\tpage = pfn_to_online_page(pfn);\n> +\t\tif (!page)\n> +\t\t\tgoto out_next;\n> +\n> +\t\tfolio = page_folio(page);\n> +\t\tnr = folio_nr_pages(folio);\n> +\t\tif (folio_nid(folio) != src_nid)\n> +\t\t\tgoto out_next;\n> +\n> +\t\tif (!folio_test_lru(folio))\n> +\t\t\tgoto out_next;\n> +\n> +\t\tif (pghot_get_hotness(pfn, &nid, &freq, &time))\n> +\t\t\tgoto out_next;\n> +\n> +\t\tif (nid == NUMA_NO_NODE)\n> +\t\t\tnid = pghot_target_nid;\n> +\n> +\t\tif (folio_nid(folio) == nid)\n> +\t\t\tgoto out_next;\n> +\n> +\t\tif (migrate_misplaced_folio_prepare(folio, NULL, nid))\n> +\t\t\tgoto out_next;\n\nWe should hold a folio reference before the above call which will isolate the\nfolio from LRU. Otherwise we may hit\n\nVM_BUG_ON_FOLIO(!folio_ref_count(folio), folio)\n\nin folio_isolate_lru().\n\nI hit this only when running Graph500 benchmark and have fixed it in\nthe github at: https://github.com/AMDESE/linux-mm/tree/bharata/pghot-rfcv6-pre\n\nThe numbers that I have posted for micro-benchmarks and redis-memtier are\nwithout this fix while Graph500 numbers are with this fix.\n\nRegards,\nBharata.\n\n\n---\n\n\nOn 11-Feb-26 9:38 PM, Gregory Price wrote:\n> On Wed, Feb 11, 2026 at 09:10:23PM +0530, Bharata B Rao wrote:\n>> On 29-Jan-26 8:10 PM, Bharata B Rao wrote:\n>>> +\n>>> +/*\n>>> + * Walks the PFNs of the zone, isolates and migrates them in batches.\n>>> + */\n>>> +static void kmigrated_walk_zone(unsigned long start_pfn, unsigned long end_pfn,\n>>> +\t\t\t\tint src_nid)\n>>> +{\n>>> +\tint cur_nid = NUMA_NO_NODE;\n>>> +\tLIST_HEAD(migrate_list);\n>>> +\tint batch_count = 0;\n>>> +\tstruct folio *folio;\n>>> +\tstruct page *page;\n>>> +\tunsigned long pfn;\n>>> +\n>>> +\tpfn = start_pfn;\n>>> +\tdo {\n>>> +\t\tint nid = NUMA_NO_NODE, nr = 1;\n>>> +\t\tint freq = 0;\n>>> +\t\tunsigned long time = 0;\n>>> +\n>>> +\t\tif (!pfn_valid(pfn))\n>>> +\t\t\tgoto out_next;\n>>> +\n>>> +\t\tpage = pfn_to_online_page(pfn);\n>>> +\t\tif (!page)\n>>> +\t\t\tgoto out_next;\n>>> +\n>>> +\t\tfolio = page_folio(page);\n>>> +\t\tnr = folio_nr_pages(folio);\n>>> +\t\tif (folio_nid(folio) != src_nid)\n>>> +\t\t\tgoto out_next;\n>>> +\n>>> +\t\tif (!folio_test_lru(folio))\n>>> +\t\t\tgoto out_next;\n>>> +\n>>> +\t\tif (pghot_get_hotness(pfn, &nid, &freq, &time))\n>>> +\t\t\tgoto out_next;\n>>> +\n>>> +\t\tif (nid == NUMA_NO_NODE)\n>>> +\t\t\tnid = pghot_target_nid;\n>>> +\n>>> +\t\tif (folio_nid(folio) == nid)\n>>> +\t\t\tgoto out_next;\n>>> +\n>>> +\t\tif (migrate_misplaced_folio_prepare(folio, NULL, nid))\n>>> +\t\t\tgoto out_next;\n>>\n>> We should hold a folio reference before the above call which will isolate the\n>> folio from LRU. Otherwise we may hit\n>>\n> \n> Also relevant note from other work I'm doing, we may want a fast-out for\n> zone-device folios here.  We should not bother tracking those at all.\n\nYes, zone device folios aren't not tracked by pghot. They get discarded\nby pghot_record_access() itself.\n\n> \n> (this may also become relevant for private-node memory as well, but I\n> may try to generalize zone_device & private-node checks as the\n> conditions are very similar).\n\nGood.\n\nRegards,\nBharata.\n\n\n---\n\nOn 11-Feb-26 9:34 PM, Gregory Price wrote:\n> On Wed, Feb 11, 2026 at 09:00:26PM +0530, Bharata B Rao wrote:\n>> =====================================================================================\n>>                         Base            Base            pghot-default\n>> pghot-precise\n>>                         NUMAB0          NUMAB2          NUMAB2          NUMAB2\n>> =====================================================================================\n>> harmonic_mean_TEPS      5.10676e+08     7.56804e+08     5.92473e+08     7.47091e+08\n>> mean_time               8.41027         5.67508         7.24915         5.74886\n>> median_TEPS             5.11535e+08     7.24252e+08     5.63155e+08     7.71638e+08\n>> max_TEPS                5.1785e+08      1.06051e+09     7.88018e+08     1.0504e+09\n>>\n>> pgpromote_success       0               13557718        13737730        13734469\n>> numa_pte_updates        0               26491591        26848847        26726856\n>> numa_hint_faults        0               13558077        13882743        13798024\n>> =====================================================================================\n>>\n> \n> Can you contextualize TEPS?  Higher better? Higher worse? etc.\n\nIn the Graph500 benchmark, higher TEPS (Traversed Edges Per Second) values are\nbetter.\n\nRegards,\nBharata.\n\n\n---\n\nOn 11-Feb-26 9:00 PM, Bharata B Rao wrote:\n> On 29-Jan-26 8:10 PM, Bharata B Rao wrote:\n>>\n>> Results\n>> =======\n>> TODO: Will post benchmark nubmers as reply to this patchset soon.\n> \n> Here are Graph500 numbers for the hint fault source:\n> \n> Test system details\n> -------------------\n> 3 node AMD Zen5 system with 2 regular NUMA nodes (0, 1) and a CXL node (2)\n> \n> $ numactl -H\n> available: 3 nodes (0-2)\n> node 0 cpus: 0-95,192-287\n> node 0 size: 128460 MB\n> node 1 cpus: 96-191,288-383\n> node 1 size: 128893 MB\n> node 2 cpus:\n> node 2 size: 257993 MB\n> node distances:\n> node   0   1   2\n>   0:  10  32  50\n>   1:  32  10  60\n>   2:  255  255  10\n> \n> Hotness sources\n> ---------------\n> NUMAB0 - Without NUMA Balancing in base case and with no source enabled\n>          in the pghot case. No migrations occur.\n> NUMAB2 - Existing hot page promotion for the base case and\n>          use of hint faults as source in the pghot case.\n> \n> Pghot by default promotes after two accesses but for NUMAB2 source,\n> promotion is done after one access to match the base behaviour.\n> (/sys/kernel/debug/pghot/freq_threshold=1)\n> \n\nThese numbers are from scenario where demotion is present:\n\n=============================================\nOver-committed scenario, promotion + demotion\n=============================================\nCommand: mpirun -n 128 --bind-to core --map-by core\n/home/bharata/benchmarks/graph500/src/graph500_reference_bfs 30 16\n\nThe scale factor of 30 results in around 400GB of memory being\nprovisioned resulting in the data spilling over to CXL node.\nNo explicit migration of data is done in this case unlike the\nprevious case.\n\n=====================================================================================\n                        Base            Base            pghot-default\npghot-precise\n                        NUMAB0          NUMAB2          NUMAB2          NUMAB2\n=====================================================================================\nharmonic_mean_TEPS      9.28713e+08     7.90431e+08     7.32193e+08     7.81051e+08\nmean_time               18.4984         21.7346         23.4634         21.9956\nmedian_TEPS             9.25707e+08     7.86684e+08     7.27053e+08     7.82823e+08\nmax_TEPS                9.57632e+08     8.4758e+08      8.22172e+08     7.9889e+08\n\npgpromote_success       0               22846743        22807167        25994988\npgpromote_candidate     0               24628924        29436044        27029173\npgpromote_candidate_nrl 0               140921          220             38387\npgdemote_kswapd         0               41523110        45121134        50042594\nnuma_pte_updates        0               121904763       71503891        68779424\nnuma_hint_faults        0               81708126        29583391        27176332\n=====================================================================================\n\n- In the base case, the benchmark suffers when promotion and demotion are\n  enabled (NUMAB2 case).\n- Same behaviour is seen with both modes of pghot.\n- Though the overall benchmark numbers remain more or less same with base and\n  pghot NUMAB2 cases, the number of pte updates and hint faults are seen\n  to spike up during some runs. Yet to understand the exact reason for this.\n\n\n---\n\n\n\nOn 13-Feb-26 8:26 PM, Gregory Price wrote:\n> On Thu, Jan 29, 2026 at 08:10:33PM +0530, Bharata B Rao wrote:\n>> Hi,\n>>\n>> This is v5 of pghot, a hot-page tracking and promotion subsystem.\n>> The major change in v5 is reducing the default hotness record size\n>> to 1 byte per PFN and adding an optional precision mode\n>> (CONFIG_PGHOT_PRECISE) that uses 4 bytes per PFN.\n>>\n> \n> In the future can you add a \n> \n> base-commit:\n> \n> for the series?  Make's it easier to automate pulling it in for testing\n> and backports etc.\n\nGood suggestion, will do thanks.\n\nBTW this series applies on f0b9d8eb98df.\nLatest github branch:\nhttps://github.com/AMDESE/linux-mm/tree/bharata/pghot-rfcv6-pre\n\nRegards,\nBharata.\n\n",
          "reply_to": ""
        },
        {
          "author": "Gregory Price",
          "summary": "Reviewer Gregory Price raised technical concerns about the patch, specifically asking for clarification on TEPS benchmark and suggesting alternative promotion strategies to improve accuracy.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "requested changes",
            "suggested improvements"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Wed, Feb 11, 2026 at 09:00:26PM +0530, Bharata B Rao wrote:\n> =====================================================================================\n>                         Base            Base            pghot-default\n> pghot-precise\n>                         NUMAB0          NUMAB2          NUMAB2          NUMAB2\n> =====================================================================================\n> harmonic_mean_TEPS      5.10676e+08     7.56804e+08     5.92473e+08     7.47091e+08\n> mean_time               8.41027         5.67508         7.24915         5.74886\n> median_TEPS             5.11535e+08     7.24252e+08     5.63155e+08     7.71638e+08\n> max_TEPS                5.1785e+08      1.06051e+09     7.88018e+08     1.0504e+09\n> \n> pgpromote_success       0               13557718        13737730        13734469\n> numa_pte_updates        0               26491591        26848847        26726856\n> numa_hint_faults        0               13558077        13882743        13798024\n> =====================================================================================\n> \n\nCan you contextualize TEPS?  Higher better? Higher worse? etc.\nUnfamiliar with this benchmark.\n\n~Gregory\n\n\n---\n\nOn Wed, Feb 11, 2026 at 09:00:26PM +0530, Bharata B Rao wrote:\n> - pghot-default mode doesn't show benefit even when achieving similar page promotion\n>   numbers. This mode doesn't track accessing NID and by default promotes to NID=0\n>   which probably isn't all that beneficial as processes are running on both Node 0\n>   and Node 1.\n>\n\nLacking access-nid data, maybe it's better to select a random (or\nround-robin) node in the upper tier?  That would at least approach 1/N\naccuracy in promotion for most access patterns.\n\n~Gregory\n\n\n---\n\nOn Wed, Feb 11, 2026 at 09:10:23PM +0530, Bharata B Rao wrote:\n> On 29-Jan-26 8:10 PM, Bharata B Rao wrote:\n> > +\n> > +/*\n> > + * Walks the PFNs of the zone, isolates and migrates them in batches.\n> > + */\n> > +static void kmigrated_walk_zone(unsigned long start_pfn, unsigned long end_pfn,\n> > +\t\t\t\tint src_nid)\n> > +{\n> > +\tint cur_nid = NUMA_NO_NODE;\n> > +\tLIST_HEAD(migrate_list);\n> > +\tint batch_count = 0;\n> > +\tstruct folio *folio;\n> > +\tstruct page *page;\n> > +\tunsigned long pfn;\n> > +\n> > +\tpfn = start_pfn;\n> > +\tdo {\n> > +\t\tint nid = NUMA_NO_NODE, nr = 1;\n> > +\t\tint freq = 0;\n> > +\t\tunsigned long time = 0;\n> > +\n> > +\t\tif (!pfn_valid(pfn))\n> > +\t\t\tgoto out_next;\n> > +\n> > +\t\tpage = pfn_to_online_page(pfn);\n> > +\t\tif (!page)\n> > +\t\t\tgoto out_next;\n> > +\n> > +\t\tfolio = page_folio(page);\n> > +\t\tnr = folio_nr_pages(folio);\n> > +\t\tif (folio_nid(folio) != src_nid)\n> > +\t\t\tgoto out_next;\n> > +\n> > +\t\tif (!folio_test_lru(folio))\n> > +\t\t\tgoto out_next;\n> > +\n> > +\t\tif (pghot_get_hotness(pfn, &nid, &freq, &time))\n> > +\t\t\tgoto out_next;\n> > +\n> > +\t\tif (nid == NUMA_NO_NODE)\n> > +\t\t\tnid = pghot_target_nid;\n> > +\n> > +\t\tif (folio_nid(folio) == nid)\n> > +\t\t\tgoto out_next;\n> > +\n> > +\t\tif (migrate_misplaced_folio_prepare(folio, NULL, nid))\n> > +\t\t\tgoto out_next;\n> \n> We should hold a folio reference before the above call which will isolate the\n> folio from LRU. Otherwise we may hit\n> \n\nAlso relevant note from other work I'm doing, we may want a fast-out for\nzone-device folios here.  We should not bother tracking those at all.\n\n(this may also become relevant for private-node memory as well, but I\nmay try to generalize zone_device & private-node checks as the\nconditions are very similar).\n\n~Gregory\n\n\n---\n\nOn Thu, Jan 29, 2026 at 08:10:33PM +0530, Bharata B Rao wrote:\n> Hi,\n> \n> This is v5 of pghot, a hot-page tracking and promotion subsystem.\n> The major change in v5 is reducing the default hotness record size\n> to 1 byte per PFN and adding an optional precision mode\n> (CONFIG_PGHOT_PRECISE) that uses 4 bytes per PFN.\n> \n\nIn the future can you add a \n\nbase-commit:\n\nfor the series?  Make's it easier to automate pulling it in for testing\nand backports etc.\n\n~Gregory\n",
          "reply_to": ""
        }
      ],
      "analysis_source": "llm"
    }
  }
}