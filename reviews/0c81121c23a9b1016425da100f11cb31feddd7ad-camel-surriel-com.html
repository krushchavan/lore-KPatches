<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Review Comments: Re: [LSF/MM/BPF TOPIC] Beyond 2MB: Why Terabyte-Scale Machines Need 1GB Transparent Huge Pages</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto,
                         "Helvetica Neue", Arial, sans-serif;
            background: #f5f5f5;
            color: #333;
            line-height: 1.6;
            padding: 20px;
            max-width: 900px;
            margin: 0 auto;
        }
        .home-link { margin-bottom: 12px; display: block; }
        .home-link a { color: #0366d6; text-decoration: none; font-size: 0.9em; }
        .home-link a:hover { text-decoration: underline; }

        h1 { font-size: 1.3em; margin-bottom: 2px; color: #1a1a1a; line-height: 1.3; }

        .lore-link { font-size: 0.85em; margin: 4px 0 6px; display: block; }
        .lore-link a { color: #0366d6; text-decoration: none; }
        .lore-link a:hover { text-decoration: underline; }

        .date-range {
            font-size: 0.8em;
            color: #888;
            margin-bottom: 16px;
        }
        .date-range a { color: #0366d6; text-decoration: none; }
        .date-range a:hover { text-decoration: underline; }

        /* thread-node scroll margin so the card isn't clipped at the top */
        .thread-node { scroll-margin-top: 8px; }

        /* ── Patch summary ──────────────────────────────────────────── */
        .patch-summary-block {
            background: #fff;
            border-radius: 8px;
            padding: 12px 16px;
            margin-bottom: 20px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            border-left: 3px solid #4a90d9;
        }
        .patch-summary-label {
            font-size: 0.72em;
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 0.06em;
            color: #4a90d9;
            margin-bottom: 4px;
        }
        .patch-summary-text {
            font-size: 0.88em;
            color: #444;
            line-height: 1.55;
        }

        /* ── Thread tree ────────────────────────────────────────────── */
        .thread-tree {
            display: flex;
            flex-direction: column;
            gap: 6px;
        }

        /* Depth indentation via left border */
        .thread-node { position: relative; }
        .thread-children {
            margin-left: 20px;
            padding-left: 12px;
            border-left: 2px solid #e0e0e0;
            margin-top: 6px;
            display: flex;
            flex-direction: column;
            gap: 6px;
        }

        /* ── Review comment card ────────────────────────────────────── */
        .review-comment {
            background: #fff;
            border-radius: 6px;
            padding: 10px 14px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            font-size: 0.88em;
        }
        .review-comment-header {
            display: flex;
            flex-wrap: wrap;
            align-items: center;
            gap: 6px;
            margin-bottom: 5px;
        }
        .review-author {
            font-weight: 700;
            color: #1a1a1a;
            font-size: 0.95em;
        }

        /* Date chip — links back to the daily report */
        .date-chip {
            font-size: 0.75em;
            color: #777;
            background: #f0f0f0;
            border-radius: 10px;
            padding: 1px 7px;
            text-decoration: none;
            white-space: nowrap;
        }
        a.date-chip:hover { background: #e0e8f5; color: #0366d6; }

        .badge {
            display: inline-block;
            padding: 1px 8px;
            border-radius: 10px;
            font-size: 0.75em;
            font-weight: 600;
        }
        .inline-review-badge {
            display: inline-block;
            padding: 0 6px;
            border-radius: 8px;
            font-size: 0.78em;
            font-weight: 500;
            background: #e3f2fd;
            color: #1565c0;
        }
        .review-tag-badge {
            display: inline-block;
            padding: 0 6px;
            border-radius: 8px;
            font-size: 0.78em;
            font-weight: 500;
            background: #e8f5e9;
            color: #2e7d32;
        }
        .analysis-source-badge {
            display: inline-block;
            padding: 1px 7px;
            border-radius: 10px;
            font-size: 0.72em;
            font-weight: 600;
            border: 1px solid rgba(0,0,0,0.1);
        }

        .review-comment-text {
            color: #444;
            line-height: 1.55;
            margin-bottom: 4px;
        }
        .review-comment-signals {
            margin-top: 3px;
            font-size: 0.85em;
            color: #aaa;
            font-style: italic;
        }

        /* ── Collapsible raw body ───────────────────────────────────── */
        .raw-body-toggle {
            margin-top: 5px;
            font-size: 0.85em;
        }
        .raw-body-toggle summary {
            cursor: pointer;
            color: #888;
            padding: 2px 0;
            font-weight: 500;
            font-size: 0.9em;
            list-style: none;
        }
        .raw-body-toggle summary::-webkit-details-marker { display: none; }
        .raw-body-toggle summary::before { content: "▶ "; font-size: 0.7em; }
        .raw-body-toggle[open] summary::before { content: "▼ "; }
        .raw-body-toggle summary:hover { color: #555; }
        .raw-body-text {
            white-space: pre-wrap;
            font-size: 0.95em;
            background: #f8f8f8;
            padding: 8px 10px;
            border-radius: 4px;
            max-height: 360px;
            overflow-y: auto;
            margin-top: 4px;
            line-height: 1.5;
            color: #444;
            border: 1px solid #e8e8e8;
        }

        .no-reviews {
            color: #aaa;
            font-size: 0.85em;
            font-style: italic;
            padding: 8px 0;
        }

        footer {
            text-align: center;
            color: #bbb;
            font-size: 0.78em;
            margin-top: 36px;
            padding: 16px;
        }
    </style>
</head>
<body>
    <div class="home-link"><a href="../">&larr; Back to reports</a></div>
    <h1>Re: [LSF/MM/BPF TOPIC] Beyond 2MB: Why Terabyte-Scale Machines Need 1GB Transparent Huge Pages</h1>
    <div class="lore-link"><a href="https://lore.kernel.org/all/0c81121c23a9b1016425da100f11cb31feddd7ad.camel@surriel.com/" target="_blank">View on lore.kernel.org &rarr;</a></div>
    <div class="date-range">Active on: <a href="#2026-02-19">2026-02-19</a></div>
    
    <div class="thread-tree">
<div class="thread-node depth-0" id="2026-02-19">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">David (Arm)</span>
<a class="date-chip" href="../2026-02-19_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-19">2026-02-19</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer David raised concerns about the proposed solution to migrate all memory away instead of splitting a THP, but noted that this approach has limitations when remapping a 1 GiB THP for use by PMDs.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">
&gt; 
&gt; I see 1G THPs being opportunistically used ideally at the start of the application
&gt; or by the allocator (jemalloc/tcmalloc) when there is plenty of free memory
&gt; available and a greater chance of getting 1G THPs.
&gt; 
&gt; Splitting strategy
&gt; ==================
&gt; 
&gt; When PUD THP must be break -- for COW after fork, partial munmap, mprotect on
&gt; a subregion, or reclaim -- it splits directly from PUD to PTE level, converting
&gt; 1 PUD entry into 262,144 PTE entries. The ideal solution would be to split to
&gt; PMDs and only the necessary PMDs to PTEs. This is something that would hopefully
&gt; be possible with Davids proposal [3].

There once was this proposal where we would, instead of splitting a THP, 
migrate all memory away instead. That means, instead of splitting the 1 
GiB THP, you would instead return it to the page allocator where 
somebody else could use it.

However, we cannot easily do the same when remapping a 1 GiB THP to be 
mapped by PMDs etc. I think there are examples where that just doesn&#x27;t 
work or is not desired.

But I considered that in general (avoid folio_split()) an interesting 
approach. The remapping part is a bit different though.

-- 
Cheers,

David

</pre>
</details>
<div class="review-comment-signals">Signals: requested changes</div>
</div>
<div class="thread-children">
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Johannes Weiner</span>
<a class="date-chip" href="../2026-02-19_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-19">2026-02-19</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Johannes Weiner raised concerns about losing contiguity in TLB coalescing if huge pages are split, suggesting a lazy approach to migration when larger pages are not available.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Thu, Feb 19, 2026 at 05:00:19PM +0100, David Hildenbrand (Arm) wrote:
&gt; 
&gt; &gt; 
&gt; &gt; I see 1G THPs being opportunistically used ideally at the start of the application
&gt; &gt; or by the allocator (jemalloc/tcmalloc) when there is plenty of free memory
&gt; &gt; available and a greater chance of getting 1G THPs.
&gt; &gt; 
&gt; &gt; Splitting strategy
&gt; &gt; ==================
&gt; &gt; 
&gt; &gt; When PUD THP must be break -- for COW after fork, partial munmap, mprotect on
&gt; &gt; a subregion, or reclaim -- it splits directly from PUD to PTE level, converting
&gt; &gt; 1 PUD entry into 262,144 PTE entries. The ideal solution would be to split to
&gt; &gt; PMDs and only the necessary PMDs to PTEs. This is something that would hopefully
&gt; &gt; be possible with Davids proposal [3].
&gt; 
&gt; There once was this proposal where we would, instead of splitting a THP, 
&gt; migrate all memory away instead. That means, instead of splitting the 1 
&gt; GiB THP, you would instead return it to the page allocator where 
&gt; somebody else could use it.

With TLB coalescing, there is benefit in preserving contiguity. If you
lop off the last 4k of a 2M-backed range, a split still gives you 511
contiguously mapped pfns that can be coalesced.

It would be unfortunate to lose that for pure virtual memory splits,
while there is no demand or no shortage of huge pages. But it might be
possible to do this lazily, e.g. when somebody has trouble getting a
larger page, scan the deferred split lists for candidates to migrate.

</pre>
</details>
<div class="review-comment-signals">Signals: requested changes</div>
</div>
<div class="thread-children">
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Zi Yan</span>
<a class="date-chip" href="../2026-02-19_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-19">2026-02-19</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Zi Yan questioned the CPU-specific assumptions in the patch, asking about AMD and ARM&#x27;s PTE coalescing capabilities.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On 19 Feb 2026, at 11:48, Johannes Weiner wrote:

&gt; On Thu, Feb 19, 2026 at 05:00:19PM +0100, David Hildenbrand (Arm) wrote:
&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; I see 1G THPs being opportunistically used ideally at the start of the application
&gt;&gt;&gt; or by the allocator (jemalloc/tcmalloc) when there is plenty of free memory
&gt;&gt;&gt; available and a greater chance of getting 1G THPs.
&gt;&gt;&gt;
&gt;&gt;&gt; Splitting strategy
&gt;&gt;&gt; ==================
&gt;&gt;&gt;
&gt;&gt;&gt; When PUD THP must be break -- for COW after fork, partial munmap, mprotect on
&gt;&gt;&gt; a subregion, or reclaim -- it splits directly from PUD to PTE level, converting
&gt;&gt;&gt; 1 PUD entry into 262,144 PTE entries. The ideal solution would be to split to
&gt;&gt;&gt; PMDs and only the necessary PMDs to PTEs. This is something that would hopefully
&gt;&gt;&gt; be possible with Davids proposal [3].
&gt;&gt;
&gt;&gt; There once was this proposal where we would, instead of splitting a THP,
&gt;&gt; migrate all memory away instead. That means, instead of splitting the 1
&gt;&gt; GiB THP, you would instead return it to the page allocator where
&gt;&gt; somebody else could use it.
&gt;
&gt; With TLB coalescing, there is benefit in preserving contiguity. If you
&gt; lop off the last 4k of a 2M-backed range, a split still gives you 511
&gt; contiguously mapped pfns that can be coalesced.

Which CPU are you referring to? AMD\u2019s PTE coalescing works up to 32KB
and ARM\u2019s contig PTE supports larger sizes. BTW, do we have PMD level
ARM contiguous bit support?

&gt;
&gt; It would be unfortunate to lose that for pure virtual memory splits,
&gt; while there is no demand or no shortage of huge pages. But it might be
&gt; possible to do this lazily, e.g. when somebody has trouble getting a
&gt; larger page, scan the deferred split lists for candidates to migrate.


Best Regards,
Yan, Zi

</pre>
</details>
<div class="review-comment-signals">Signals: question, request for clarification</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">David (Arm)</span>
<a class="date-chip" href="../2026-02-19_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-19">2026-02-19</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer David (Arm) suggested an alternative solution for large memory machines, proposing migration to larger folios instead of implementing 1GB transparent huge pages.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On 2/19/26 18:08, Johannes Weiner wrote:
&gt; On Thu, Feb 19, 2026 at 11:52:57AM -0500, Zi Yan wrote:
&gt;&gt; On 19 Feb 2026, at 11:48, Johannes Weiner wrote:
&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; With TLB coalescing, there is benefit in preserving contiguity. If you
&gt;&gt;&gt; lop off the last 4k of a 2M-backed range, a split still gives you 511
&gt;&gt;&gt; contiguously mapped pfns that can be coalesced.
&gt;&gt;
&gt;&gt; Which CPU are you referring to? AMD\u2019s PTE coalescing works up to 32KB
&gt;&gt; and ARM\u2019s contig PTE supports larger sizes. BTW, do we have PMD level
&gt;&gt; ARM contiguous bit support?
&gt; 
&gt; I&#x27;m not aware of a CPU that will coalesce the 511 entries into a
&gt; single one. But *any* coalescing effects will be lost when the range
&gt; is scattered into discontiguous 4k pagelets.

You could of course migrate to larger folios, not necessarily 4k.

-- 
Cheers,

David

</pre>
</details>
<div class="review-comment-signals">Signals: alternative solution, no strong objection</div>
</div>
</div>
</div>
</div>
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Zi Yan</span>
<a class="date-chip" href="../2026-02-19_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-19">2026-02-19</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The reviewer raised concerns about splitting 1GB huge pages, suggesting a non-uniform split approach to keep after-split folios as large as possible. They also proposed a hardware-based solution to enable remapping of TLB entries.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On 19 Feb 2026, at 11:00, David Hildenbrand (Arm) wrote:

&gt;&gt;
&gt;&gt; I see 1G THPs being opportunistically used ideally at the start of the application
&gt;&gt; or by the allocator (jemalloc/tcmalloc) when there is plenty of free memory
&gt;&gt; available and a greater chance of getting 1G THPs.
&gt;&gt;
&gt;&gt; Splitting strategy
&gt;&gt; ==================
&gt;&gt;
&gt;&gt; When PUD THP must be break -- for COW after fork, partial munmap, mprotect on
&gt;&gt; a subregion, or reclaim -- it splits directly from PUD to PTE level, converting
&gt;&gt; 1 PUD entry into 262,144 PTE entries. The ideal solution would be to split to
&gt;&gt; PMDs and only the necessary PMDs to PTEs. This is something that would hopefully
&gt;&gt; be possible with Davids proposal [3].

With mapping of folios &gt; PMD with PMDs, you can use non uniform split to keep
after-split folios as large as possible.

&gt;
&gt; There once was this proposal where we would, instead of splitting a THP, migrate all memory away instead. That means, instead of splitting the 1 GiB THP, you would instead return it to the page allocator where somebody else could use it.

This sounds more reasonable than splitting 1GB itself.

&gt;
&gt; However, we cannot easily do the same when remapping a 1 GiB THP to be mapped by PMDs etc. I think there are examples where that just doesn&#x27;t work or is not desired.
&gt;
&gt; But I considered that in general (avoid folio_split()) an interesting approach. The remapping part is a bit different though.

If HW can support multiple TLB entries translating to the same physical frame
and allow translation priority of TLB entries, this remapping would be easy
and we can still keep the 1GB PUD mapping. Basically, we can have 1GB TLB entry
pointing to the 1GB folio and another 4KB TLB entry pointing to the remapped
region and overriding the part in the original 1GB vaddr region.

Without that, SW will need to split the PUD into PMDs and PTEs.


Best Regards,
Yan, Zi

</pre>
</details>
<div class="review-comment-signals">Signals: requested changes, alternative solutions</div>
</div>
<div class="thread-children">
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Johannes Weiner</span>
<a class="date-chip" href="../2026-02-19_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-19">2026-02-19</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Johannes Weiner expressed concerns that the benefits of 1GB Transparent Huge Pages would be lost due to coalescing effects when the range is scattered into discontiguous 4k pagelets.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Thu, Feb 19, 2026 at 11:52:57AM -0500, Zi Yan wrote:
&gt; On 19 Feb 2026, at 11:48, Johannes Weiner wrote:
&gt; 
&gt; &gt; On Thu, Feb 19, 2026 at 05:00:19PM +0100, David Hildenbrand (Arm) wrote:
&gt; &gt;&gt;
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt; I see 1G THPs being opportunistically used ideally at the start of the application
&gt; &gt;&gt;&gt; or by the allocator (jemalloc/tcmalloc) when there is plenty of free memory
&gt; &gt;&gt;&gt; available and a greater chance of getting 1G THPs.
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt; Splitting strategy
&gt; &gt;&gt;&gt; ==================
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt; When PUD THP must be break -- for COW after fork, partial munmap, mprotect on
&gt; &gt;&gt;&gt; a subregion, or reclaim -- it splits directly from PUD to PTE level, converting
&gt; &gt;&gt;&gt; 1 PUD entry into 262,144 PTE entries. The ideal solution would be to split to
&gt; &gt;&gt;&gt; PMDs and only the necessary PMDs to PTEs. This is something that would hopefully
&gt; &gt;&gt;&gt; be possible with Davids proposal [3].
&gt; &gt;&gt;
&gt; &gt;&gt; There once was this proposal where we would, instead of splitting a THP,
&gt; &gt;&gt; migrate all memory away instead. That means, instead of splitting the 1
&gt; &gt;&gt; GiB THP, you would instead return it to the page allocator where
&gt; &gt;&gt; somebody else could use it.
&gt; &gt;
&gt; &gt; With TLB coalescing, there is benefit in preserving contiguity. If you
&gt; &gt; lop off the last 4k of a 2M-backed range, a split still gives you 511
&gt; &gt; contiguously mapped pfns that can be coalesced.
&gt; 
&gt; Which CPU are you referring to? AMD\u2019s PTE coalescing works up to 32KB
&gt; and ARM\u2019s contig PTE supports larger sizes. BTW, do we have PMD level
&gt; ARM contiguous bit support?

I&#x27;m not aware of a CPU that will coalesce the 511 entries into a
single one. But *any* coalescing effects will be lost when the range
is scattered into discontiguous 4k pagelets.

</pre>
</details>
<div class="review-comment-signals">Signals: coalescing effects, concerns about patch</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">David (Arm)</span>
<a class="date-chip" href="../2026-02-19_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-19">2026-02-19</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer David raised concerns about the integration of 1GB Transparent Huge Pages (THPs) into the existing THP infrastructure, specifically questioning whether it&#x27;s used for hugetlb only and implying potential issues with its usage.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On 2/19/26 17:52, Zi Yan wrote:
&gt; On 19 Feb 2026, at 11:48, Johannes Weiner wrote:
&gt; 
&gt;&gt; On Thu, Feb 19, 2026 at 05:00:19PM +0100, David Hildenbrand (Arm) wrote:
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; There once was this proposal where we would, instead of splitting a THP,
&gt;&gt;&gt; migrate all memory away instead. That means, instead of splitting the 1
&gt;&gt;&gt; GiB THP, you would instead return it to the page allocator where
&gt;&gt;&gt; somebody else could use it.
&gt;&gt;
&gt;&gt; With TLB coalescing, there is benefit in preserving contiguity. If you
&gt;&gt; lop off the last 4k of a 2M-backed range, a split still gives you 511
&gt;&gt; contiguously mapped pfns that can be coalesced.
&gt; 
&gt; Which CPU are you referring to? AMD\u2019s PTE coalescing works up to 32KB
&gt; and ARM\u2019s contig PTE supports larger sizes. BTW, do we have PMD level
&gt; ARM contiguous bit support?

Yes. It&#x27;s used for hugetlb only so far, obviously (np THP &gt; PMD).

-- 
Cheers,

David

</pre>
</details>
<div class="review-comment-signals">Signals: requested changes</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Matthew Wilcox</span>
<a class="date-chip" href="../2026-02-19_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-19">2026-02-19</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Matthew Wilcox expressed skepticism about the feasibility of implementing 1GB transparent huge pages, citing potential hardware compatibility issues.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Thu, Feb 19, 2026 at 11:49:27AM -0500, Zi Yan wrote:
&gt; If HW can support multiple TLB entries translating to the same physical frame
&gt; and allow translation priority of TLB entries, this remapping would be easy
&gt; and we can still keep the 1GB PUD mapping. Basically, we can have 1GB TLB entry
&gt; pointing to the 1GB folio and another 4KB TLB entry pointing to the remapped
&gt; region and overriding the part in the original 1GB vaddr region.

Uh, do you know any hardware that supports that?  Every CPU I&#x27;m familiar
with has notes suggesting that trying to do this will cause you to Have
A Very Bad Day.

</pre>
</details>
<div class="review-comment-signals">Signals: requested changes, hardware concerns</div>
</div>
<div class="thread-children">
<div class="thread-node depth-3">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Zi Yan</span>
<a class="date-chip" href="../2026-02-19_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-19">2026-02-19</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The reviewer initially responded with a joke but then raised a technical concern about the performance implications of 1GB Transparent Huge Pages on TLB hits and potential additional translations.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On 19 Feb 2026, at 12:13, Matthew Wilcox wrote:

&gt; On Thu, Feb 19, 2026 at 11:49:27AM -0500, Zi Yan wrote:
&gt;&gt; If HW can support multiple TLB entries translating to the same physical frame
&gt;&gt; and allow translation priority of TLB entries, this remapping would be easy
&gt;&gt; and we can still keep the 1GB PUD mapping. Basically, we can have 1GB TLB entry
&gt;&gt; pointing to the 1GB folio and another 4KB TLB entry pointing to the remapped
&gt;&gt; region and overriding the part in the original 1GB vaddr region.
&gt;
&gt; Uh, do you know any hardware that supports that?  Every CPU I&#x27;m familiar
&gt; with has notes suggesting that trying to do this will cause you to Have
&gt; A Very Bad Day.

No. I was imagining it. :)

But thinking about it more, that means for every &gt;PTE TLB hit, HW needs to know
whether any sub-range has an additional translation. It is easy if all sub-range
translations are present in the TLB. Otherwise, a per sub range bitmap or rewalks
of each sub range is needed. Never mind, thank you for waking me up in my
daydream.

Best Regards,
Yan, Zi

</pre>
</details>
<div class="review-comment-signals">Signals: requested changes</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Rik Riel</span>
<a class="date-chip" href="../2026-02-19_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-19">2026-02-19</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Rik Riel raised concerns about the long-term implications of 1TB pages and suggested that physical memory handling needs to be reevaluated for such large page sizes, proposing a separation between movable and non-movable allocations.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Thu, 2026-02-19 at 15:53 +0000, Usama Arif wrote:
&gt; 
&gt; Is CMA needed to make this work?
&gt; ================================
&gt; 
&gt; The short answer is no. 1G THPs can be gotten without it. CMA can
&gt; help a lot
&gt; ofcourse, but we dont *need* it. For e.g. I can run the very simple
&gt; case of
&gt; trying to get 1G pages in the upstream kernel without CMA on my
&gt; server via
&gt; hugetlb and it works. The server has been up for more than 2 weeks
&gt; (so pretty
&gt; fragmented), is running a bunch of stuff in the background, uses 0
&gt; CMA memory,
&gt; and I tried to get 100x1G pages on it and it worked.
&gt; It uses folio_alloc_gigantic, which is exactly what this RFC uses:

While I agree with the idea of starting simple, I think
we should ask the question of what we want physical memory
handling to look like if 1TB pages become more common,
and applications start to rely on them to meet their
performance goals.

We have CMA balancing code today. It seems to work, but
it likely is not the long term direction we want to go,
mostly due to the way CMA does allocations.

It seems clear that in order to prevent memory fragmentation,
we need to split up system memory in some way between an area
that is used only for movable allocations, and an area where
any kind of allocation can go.

This would need something similar to CMA balancing to prevent
false OOMs for non-movable allocations.

However, beyond that I really do not have any idea of what
things should look like.

What do we want the kernel to do here?


-- 
All Rights Reversed.

</pre>
</details>
<div class="review-comment-signals">Signals: requested changes, technical concerns</div>
</div>
<div class="thread-children">
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">David (Arm)</span>
<a class="date-chip" href="../2026-02-19_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-19">2026-02-19</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The reviewer, David from Arm, suggests exploring alternative solutions to implement 1GB transparent huge pages, such as modifying the buddy system and compaction algorithms to handle larger page sizes or introducing sub-zones.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On 2/19/26 20:02, Rik van Riel wrote:
&gt; On Thu, 2026-02-19 at 15:53 +0000, Usama Arif wrote:
&gt;&gt;
&gt;&gt; Is CMA needed to make this work?
&gt;&gt; ================================
&gt;&gt;
&gt;&gt; The short answer is no. 1G THPs can be gotten without it. CMA can
&gt;&gt; help a lot
&gt;&gt; ofcourse, but we dont *need* it. For e.g. I can run the very simple
&gt;&gt; case of
&gt;&gt; trying to get 1G pages in the upstream kernel without CMA on my
&gt;&gt; server via
&gt;&gt; hugetlb and it works. The server has been up for more than 2 weeks
&gt;&gt; (so pretty
&gt;&gt; fragmented), is running a bunch of stuff in the background, uses 0
&gt;&gt; CMA memory,
&gt;&gt; and I tried to get 100x1G pages on it and it worked.
&gt;&gt; It uses folio_alloc_gigantic, which is exactly what this RFC uses:
&gt; 
&gt; While I agree with the idea of starting simple, I think
&gt; we should ask the question of what we want physical memory
&gt; handling to look like if 1TB pages become more common,
&gt; and applications start to rely on them to meet their
&gt; performance goals.
&gt; 
&gt; We have CMA balancing code today. It seems to work, but
&gt; it likely is not the long term direction we want to go,
&gt; mostly due to the way CMA does allocations.
&gt; 
&gt; It seems clear that in order to prevent memory fragmentation,
&gt; we need to split up system memory in some way between an area
&gt; that is used only for movable allocations, and an area where
&gt; any kind of allocation can go.
&gt; 
&gt; This would need something similar to CMA balancing to prevent
&gt; false OOMs for non-movable allocations.
&gt; 
&gt; However, beyond that I really do not have any idea of what
&gt; things should look like.
&gt; 
&gt; What do we want the kernel to do here?

This subtopic is certainly worth a separate session as it&#x27;s quite 
involved, but I assume the right (tm) thing to do will be

(a) Teaching the buddy to manage pages larger than the current maximum
     buddy order. There will certainly be some work required to get to
     that point (and Zi Yan already did some work). It might also be
     fair to say that order &gt; current  buddy order might behave different
     at least to some degree (thinking about relation to zone alignment,
     section sizes etc).

     If we require vmemmap for these larger orders, maybe the buddy order
     could more easily exceed the section size; I don&#x27;t remember all of
     the details why that limitation was in place (but one of them was
     memmap continuity within a high-order buddy page, which is only
     guaranteed within a memory section with CONFIG_SPARSEMEM).

(b) Teaching compaction etc. to *also* compact/group on a larger
     granularity (in addition to current sized pageblocks). When we
     discussed that in the past we used the term superblock, that
     Zi Yan just brought up again in another thread [1].



There was a proposal a while ago to internally separate zones into 
chunks of memory (I think the proposal used DRAM banks, such that you 
could more easily power down unused DRAM banks). I&#x27;m not saying we 
should do that, but maybe something like sub-zones could be something to 
explore. Maybe not.

Big, more complex topic :)


[1] 
https://lore.kernel.org/r/34730030-48F6-4D0C-91EA-998A5AF93F5F@nvidia.com

-- 
Cheers,

David


</pre>
</details>
<div class="review-comment-signals">Signals: requested changes, alternative solutions</div>
</div>
</div>
</div>
</div>
</div>

    <footer>LKML Daily Activity Tracker</footer>
    <script>
    // When arriving via a date anchor (e.g. #2026-02-15 from a daily report),
    // scroll the anchor into view after a brief delay so layout is complete.
    (function () {
        var hash = window.location.hash;
        if (!hash) return;
        var target = document.getElementById(hash.slice(1));
        if (!target) return;
        setTimeout(function () {
            target.scrollIntoView({behavior: 'smooth', block: 'start'});
        }, 80);
    })();
    </script>
</body>
</html>