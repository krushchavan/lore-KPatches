{
  "thread_id": "aZcxWsWO7AxQW6JC@thinkstation",
  "subject": "Re: [LSF/MM/BPF TOPIC] 64k (or 16k) base page size on x86",
  "url": "https://lore.kernel.org/all/aZcxWsWO7AxQW6JC@thinkstation/",
  "dates": {
    "2026-02-19": {
      "report_file": "2026-02-19_ollama_llama3.1-8b.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "Peter Zijlstra",
          "summary": "Peter Zijlstra questioned the novelty of the proposed change, suggesting that it was already done by another architecture in the past.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "questioning the originality of the idea"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Thu, Feb 19, 2026 at 03:08:51PM +0000, Kiryl Shutsemau wrote:\n> No, there's no new hardware (that I know of). I want to explore what page size\n> means.\n> \n> The kernel uses the same value - PAGE_SIZE - for two things:\n> \n>   - the order-0 buddy allocation size;\n> \n>   - the granularity of virtual address space mapping;\n> \n> I think we can benefit from separating these two meanings and allowing\n> order-0 allocations to be larger than the virtual address space covered by a\n> PTE entry.\n\nDidn't AA do this a decade ago or somesuch?\n",
          "reply_to": "Kiryl Shutsemau"
        },
        {
          "author": "Peter Zijlstra",
          "summary": "Peter Zijlstra raised concerns about the potential performance impact of using a larger page size, specifically citing the need for careful consideration of TLB behavior and cache alignment.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "performance",
            "TLB"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Thu, Feb 19, 2026 at 04:17:29PM +0100, Peter Zijlstra wrote:\n> On Thu, Feb 19, 2026 at 03:08:51PM +0000, Kiryl Shutsemau wrote:\n> > No, there's no new hardware (that I know of). I want to explore what page size\n> > means.\n> > \n> > The kernel uses the same value - PAGE_SIZE - for two things:\n> > \n> >   - the order-0 buddy allocation size;\n> > \n> >   - the granularity of virtual address space mapping;\n> > \n> > I think we can benefit from separating these two meanings and allowing\n> > order-0 allocations to be larger than the virtual address space covered by a\n> > PTE entry.\n> \n> Didn't AA do this a decade ago or somesuch?\n\n  https://lwn.net/Articles/240914/\n",
          "reply_to": ""
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The reviewer acknowledged the patch and expressed willingness to learn more about it, but did not provide any specific feedback or concerns.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "acknowledgment",
            "request for information"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Thu, Feb 19, 2026 at 04:20:45PM +0100, Peter Zijlstra wrote:\n> On Thu, Feb 19, 2026 at 04:17:29PM +0100, Peter Zijlstra wrote:\n> > On Thu, Feb 19, 2026 at 03:08:51PM +0000, Kiryl Shutsemau wrote:\n> > > No, there's no new hardware (that I know of). I want to explore what page size\n> > > means.\n> > > \n> > > The kernel uses the same value - PAGE_SIZE - for two things:\n> > > \n> > >   - the order-0 buddy allocation size;\n> > > \n> > >   - the granularity of virtual address space mapping;\n> > > \n> > > I think we can benefit from separating these two meanings and allowing\n> > > order-0 allocations to be larger than the virtual address space covered by a\n> > > PTE entry.\n> > \n> > Didn't AA do this a decade ago or somesuch?\n> \n>   https://lwn.net/Articles/240914/\n\nOh, 2007. It predates me in kernel. Will read up. Thanks!\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n",
          "reply_to": "Peter Zijlstra"
        },
        {
          "author": "Pedro Falcato",
          "summary": "Reviewer Pedro Falcato questions the relevance of the patch in light of mTHP (memory Transparent Huge Pages) and suggests alternative solutions, such as enforcing a minimum allocation order globally on the page cache.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "alternative solutions"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Thu, Feb 19, 2026 at 03:08:51PM +0000, Kiryl Shutsemau wrote:\n> No, there's no new hardware (that I know of). I want to explore what page size\n> means.\n> \n> The kernel uses the same value - PAGE_SIZE - for two things:\n> \n>   - the order-0 buddy allocation size;\n> \n>   - the granularity of virtual address space mapping;\n> \n> I think we can benefit from separating these two meanings and allowing\n> order-0 allocations to be larger than the virtual address space covered by a\n> PTE entry.\n>\n\nDoesn't this idea make less sense these days, with mTHP? Simply by toggling one\nof the entries in /sys/kernel/mm/transparent_hugepage.\n \n> The main motivation is scalability. Managing memory on multi-terabyte\n> machines in 4k is suboptimal, to say the least.\n> \n> Potential benefits of the approach (assuming 64k pages):\n> \n>   - The order-0 page size cuts struct page overhead by a factor of 16. From\n>     ~1.6% of RAM to ~0.1%;\n> \n>   - TLB wins on machines with TLB coalescing as long as mapping is naturally\n>     aligned;\n> \n>   - Order-5 allocation is 2M, resulting in less pressure on the zone lock;\n> \n>   - 1G pages are within possibility for the buddy allocator - order-14\n>     allocation. It can open the road to 1G THPs.\n> \n>   - As with THP, fewer pages - less pressure on the LRU lock;\n\nWe could perhaps add a way to enforce a min_order globally on the page cache,\nas a way to address it.\n\nThere are some points there which aren't addressed by mTHP work in any way\n(1G THPs for one), others which are being addressed separately (memdesc work\ntrying to cut down on struct page overhead).\n\n(I also don't understand your point about order-5 allocation, AFAIK pcp will\ncache up to COSTLY_ORDER (3) and PMD order, but I'm probably not seeing the\nfull picture)\n\n\n-- \nPedro\n",
          "reply_to": "Kiryl Shutsemau"
        },
        {
          "author": "David (Arm)",
          "summary": "Reviewer David from Arm discussed the patch proposing a larger page size on x86, suggesting an alternative approach where user space emulates a 64k page size but the OS works with 4k pages. He pointed out that this would reduce zone lock contention and memory waste, similar to what is already done on some other architectures like Arm.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "alternative approach"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On 2/19/26 16:08, Kiryl Shutsemau wrote:\n> No, there's no new hardware (that I know of). I want to explore what page size\n> means.\n> \n> The kernel uses the same value - PAGE_SIZE - for two things:\n> \n>    - the order-0 buddy allocation size;\n> \n>    - the granularity of virtual address space mapping;\n> \n> I think we can benefit from separating these two meanings and allowing\n> order-0 allocations to be larger than the virtual address space covered by a\n> PTE entry.\n> \n> The main motivation is scalability. Managing memory on multi-terabyte\n> machines in 4k is suboptimal, to say the least.\n> \n> Potential benefits of the approach (assuming 64k pages):\n> \n>    - The order-0 page size cuts struct page overhead by a factor of 16. From\n>      ~1.6% of RAM to ~0.1%;\n> \n>    - TLB wins on machines with TLB coalescing as long as mapping is naturally\n>      aligned;\n> \n>    - Order-5 allocation is 2M, resulting in less pressure on the zone lock;\n> \n>    - 1G pages are within possibility for the buddy allocator - order-14\n>      allocation. It can open the road to 1G THPs.\n> \n>    - As with THP, fewer pages - less pressure on the LRU lock;\n> \n>    - ...\n> \n> The trade-off is memory waste (similar to what we have on architectures with\n> native 64k pages today) and complexity, mostly in the core-MM code.\n> \n> == Design considerations ==\n> \n> I want to split PAGE_SIZE into two distinct values:\n> \n>    - PTE_SIZE defines the virtual address space granularity;\n> \n>    - PG_SIZE defines the size of the order-0 buddy allocation;\n> \n> PAGE_SIZE is only defined if PTE_SIZE == PG_SIZE. It will flag which code\n> requires conversion, and keep existing code working while conversion is in\n> progress.\n> \n> The same split happens for other page-related macros: mask, shift,\n> alignment helpers, etc.\n> \n> PFNs are in PTE_SIZE units.\n> \n> The buddy allocator and page cache (as well as all I/O) operate in PG_SIZE\n> units.\n> \n> Userspace mappings are maintained with PTE_SIZE granularity. No ABI changes\n> for userspace. But we might want to communicate PG_SIZE to userspace to\n> get the optimal results for userspace that cares.\n> \n> PTE_SIZE granularity requires a substantial rework of page fault and VMA\n> handling:\n> \n>    - A struct page pointer and pgprot_t are not enough to create a PTE entry.\n>      We also need the offset within the page we are creating the PTE for.\n> \n>    - Since the VMA start can be aligned arbitrarily with respect to the\n>      underlying page, vma->vm_pgoff has to be changed to vma->vm_pteoff,\n>      which is in PTE_SIZE units.\n> \n>    - The page fault handler needs to handle PTE_SIZE < PG_SIZE, including\n>      misaligned cases;\n> \n> Page faults into file mappings are relatively simple to handle as we\n> always have the page cache to refer to. So you can map only the part of the\n> page that fits in the page table, similarly to fault-around.\n> \n> Anonymous and file-CoW faults should also be simple as long as the VMA is\n> aligned to PG_SIZE in both the virtual address space and with respect to\n> vm_pgoff. We might waste some memory on the ends of the VMA, but it is\n> tolerable.\n> \n> Misaligned anonymous and file-CoW faults are a pain. Specifically, mapping\n> pages across a page table boundary. In the worst case, a page is mapped across\n> a PGD entry boundary and PTEs for the page have to be put in two separate\n> subtrees of page tables.\n> \n> A naive implementation would map different pages on different sides of a\n> page table boundary and accept the waste of one page per page table crossing.\n> The hope is that misaligned mappings are rare, but this is suboptimal.\n> \n> mremap(2) is the ultimate stress test for the design.\n> \n> On x86, page tables are allocated from the buddy allocator and if PG_SIZE\n> is greater than 4 KB, we need a way to pack multiple page tables into a\n> single page. We could use the slab allocator for this, but it would\n> require relocating the page-table metadata out of struct page.\n\nWhen discussing per-process page sizes with Ryan and Dev, I mentioned \nthat having a larger emulated page size could be interesting for other \narchitectures as well.\n\nThat is, we would emulate a 64K page size on Intel for user space as \nwell, but let the OS work with 4K pages.\n\nWe'd only allocate+map large folios into user space + pagecache, but \nstill allow for page tables etc. to not waste memory.\n\nSo \"most\" of your allocations in the system would actually be at least \n64k, reducing zone lock contention etc.\n\n\nIt doesn't solve all the problems you wanted to tackle on your list \n(e.g., \"struct page\" overhead, which will be sorted out by memdescs).\n\n-- \nCheers,\n\nDavid\n\n\n---\n\nOn 2/19/26 16:50, Kiryl Shutsemau wrote:\n> On Thu, Feb 19, 2026 at 03:33:47PM +0000, Pedro Falcato wrote:\n>> On Thu, Feb 19, 2026 at 03:08:51PM +0000, Kiryl Shutsemau wrote:\n>>> No, there's no new hardware (that I know of). I want to explore what page size\n>>> means.\n>>>\n>>> The kernel uses the same value - PAGE_SIZE - for two things:\n>>>\n>>>    - the order-0 buddy allocation size;\n>>>\n>>>    - the granularity of virtual address space mapping;\n>>>\n>>> I think we can benefit from separating these two meanings and allowing\n>>> order-0 allocations to be larger than the virtual address space covered by a\n>>> PTE entry.\n>>>\n>>\n>> Doesn't this idea make less sense these days, with mTHP? Simply by toggling one\n>> of the entries in /sys/kernel/mm/transparent_hugepage.\n> \n> mTHP is still best effort. This is way you don't need to care about\n> fragmentation, you will get your 64k page as long as you have free\n> memory.\n> \n>>> The main motivation is scalability. Managing memory on multi-terabyte\n>>> machines in 4k is suboptimal, to say the least.\n>>>\n>>> Potential benefits of the approach (assuming 64k pages):\n>>>\n>>>    - The order-0 page size cuts struct page overhead by a factor of 16. From\n>>>      ~1.6% of RAM to ~0.1%;\n>>>\n>>>    - TLB wins on machines with TLB coalescing as long as mapping is naturally\n>>>      aligned;\n>>>\n>>>    - Order-5 allocation is 2M, resulting in less pressure on the zone lock;\n>>>\n>>>    - 1G pages are within possibility for the buddy allocator - order-14\n>>>      allocation. It can open the road to 1G THPs.\n>>>\n>>>    - As with THP, fewer pages - less pressure on the LRU lock;\n>>\n>> We could perhaps add a way to enforce a min_order globally on the page cache,\n>> as a way to address it.\n> \n> Raising min_order is not free. I puts more pressure on page allocator.\n> \n>> There are some points there which aren't addressed by mTHP work in any way\n>> (1G THPs for one), others which are being addressed separately (memdesc work\n>> trying to cut down on struct page overhead).\n>>\n>> (I also don't understand your point about order-5 allocation, AFAIK pcp will\n>> cache up to COSTLY_ORDER (3) and PMD order, but I'm probably not seeing the\n>> full picture)\n> \n> With higher base page size, page allocator doesn't need to do as much\n> work to merge/split buddy pages. So serving the same 2M as order-5 is\n> cheaper than order-9.\n\nI think the idea is that if most of your allocations (anon + pagecache) \nare 64k instead of 4k, on average, you'll just naturally do less merging \nsplitting.\n\n-- \nCheers,\n\nDavid\n\n\n---\n\nOn 2/19/26 16:54, Kiryl Shutsemau wrote:\n> On Thu, Feb 19, 2026 at 04:39:34PM +0100, David Hildenbrand (Arm) wrote:\n>> On 2/19/26 16:08, Kiryl Shutsemau wrote:\n>>> No, there's no new hardware (that I know of). I want to explore what page size\n>>> means.\n>>>\n>>> The kernel uses the same value - PAGE_SIZE - for two things:\n>>>\n>>>     - the order-0 buddy allocation size;\n>>>\n>>>     - the granularity of virtual address space mapping;\n>>>\n>>> I think we can benefit from separating these two meanings and allowing\n>>> order-0 allocations to be larger than the virtual address space covered by a\n>>> PTE entry.\n>>>\n>>> The main motivation is scalability. Managing memory on multi-terabyte\n>>> machines in 4k is suboptimal, to say the least.\n>>>\n>>> Potential benefits of the approach (assuming 64k pages):\n>>>\n>>>     - The order-0 page size cuts struct page overhead by a factor of 16. From\n>>>       ~1.6% of RAM to ~0.1%;\n>>>\n>>>     - TLB wins on machines with TLB coalescing as long as mapping is naturally\n>>>       aligned;\n>>>\n>>>     - Order-5 allocation is 2M, resulting in less pressure on the zone lock;\n>>>\n>>>     - 1G pages are within possibility for the buddy allocator - order-14\n>>>       allocation. It can open the road to 1G THPs.\n>>>\n>>>     - As with THP, fewer pages - less pressure on the LRU lock;\n>>>\n>>>     - ...\n>>>\n>>> The trade-off is memory waste (similar to what we have on architectures with\n>>> native 64k pages today) and complexity, mostly in the core-MM code.\n>>>\n>>> == Design considerations ==\n>>>\n>>> I want to split PAGE_SIZE into two distinct values:\n>>>\n>>>     - PTE_SIZE defines the virtual address space granularity;\n>>>\n>>>     - PG_SIZE defines the size of the order-0 buddy allocation;\n>>>\n>>> PAGE_SIZE is only defined if PTE_SIZE == PG_SIZE. It will flag which code\n>>> requires conversion, and keep existing code working while conversion is in\n>>> progress.\n>>>\n>>> The same split happens for other page-related macros: mask, shift,\n>>> alignment helpers, etc.\n>>>\n>>> PFNs are in PTE_SIZE units.\n>>>\n>>> The buddy allocator and page cache (as well as all I/O) operate in PG_SIZE\n>>> units.\n>>>\n>>> Userspace mappings are maintained with PTE_SIZE granularity. No ABI changes\n>>> for userspace. But we might want to communicate PG_SIZE to userspace to\n>>> get the optimal results for userspace that cares.\n>>>\n>>> PTE_SIZE granularity requires a substantial rework of page fault and VMA\n>>> handling:\n>>>\n>>>     - A struct page pointer and pgprot_t are not enough to create a PTE entry.\n>>>       We also need the offset within the page we are creating the PTE for.\n>>>\n>>>     - Since the VMA start can be aligned arbitrarily with respect to the\n>>>       underlying page, vma->vm_pgoff has to be changed to vma->vm_pteoff,\n>>>       which is in PTE_SIZE units.\n>>>\n>>>     - The page fault handler needs to handle PTE_SIZE < PG_SIZE, including\n>>>       misaligned cases;\n>>>\n>>> Page faults into file mappings are relatively simple to handle as we\n>>> always have the page cache to refer to. So you can map only the part of the\n>>> page that fits in the page table, similarly to fault-around.\n>>>\n>>> Anonymous and file-CoW faults should also be simple as long as the VMA is\n>>> aligned to PG_SIZE in both the virtual address space and with respect to\n>>> vm_pgoff. We might waste some memory on the ends of the VMA, but it is\n>>> tolerable.\n>>>\n>>> Misaligned anonymous and file-CoW faults are a pain. Specifically, mapping\n>>> pages across a page table boundary. In the worst case, a page is mapped across\n>>> a PGD entry boundary and PTEs for the page have to be put in two separate\n>>> subtrees of page tables.\n>>>\n>>> A naive implementation would map different pages on different sides of a\n>>> page table boundary and accept the waste of one page per page table crossing.\n>>> The hope is that misaligned mappings are rare, but this is suboptimal.\n>>>\n>>> mremap(2) is the ultimate stress test for the design.\n>>>\n>>> On x86, page tables are allocated from the buddy allocator and if PG_SIZE\n>>> is greater than 4 KB, we need a way to pack multiple page tables into a\n>>> single page. We could use the slab allocator for this, but it would\n>>> require relocating the page-table metadata out of struct page.\n>>\n>> When discussing per-process page sizes with Ryan and Dev, I mentioned that\n>> having a larger emulated page size could be interesting for other\n>> architectures as well.\n>>\n>> That is, we would emulate a 64K page size on Intel for user space as well,\n>> but let the OS work with 4K pages.\n>>\n>> We'd only allocate+map large folios into user space + pagecache, but still\n>> allow for page tables etc. to not waste memory.\n>>\n>> So \"most\" of your allocations in the system would actually be at least 64k,\n>> reducing zone lock contention etc.\n> \n> I am not convinced emulation would help zone lock contention. I expect\n> contention to be higher if page allocator would see a mix of 4k and 64k\n> requests. It sounds like constant split/merge under the lock.\n\nIf most your allocations are larger, then there isn't that much \nsplitting/merging.\n\nThere will be some for the < 64k allocations of course, but when all \nuser space+page cache is >= 64 then the split/merge + zone lock should \nbe heavily reduced.\n\n> \n>> It doesn't solve all the problems you wanted to tackle on your list (e.g.,\n>> \"struct page\" overhead, which will be sorted out by memdescs).\n> \n> I don't think we can serve 1G pages out of buddy allocator with 4k\n> order-0. And without it, I don't see how to get to a viable 1G THPs.\n\nZi Yan was one working on this, and I think we had ideas on how to make \nthat work in the long run.\n\n-- \nCheers,\n\nDavid\n\n\n---\n\n>> When discussing per-process page sizes with Ryan and Dev, I mentioned that\n>> having a larger emulated page size could be interesting for other\n>> architectures as well.\n>>\n>> That is, we would emulate a 64K page size on Intel for user space as well,\n>> but let the OS work with 4K pages.\n> \n> Just to clarify, do you want it to be enforced on userspace ABI.\n> Like, all mappings are 64k aligned?\n\nRight, see the proposal from Dev on the list.\n\n From user-space POV, the pagesize would be 64K for these emulated \nprocesses. That is, VMAs must be suitable aligned etc.\n\nOne key thing I think is that you could run such emulated-64k process \n(that actually support it!) with 4k processes on the same machine, like \nArm is considering.\n\nYou would have no weird \"vma crosses base pages\" handling, which is just \nrather nasty and makes my head hurt.\n\n> \n>> We'd only allocate+map large folios into user space + pagecache, but still\n>> allow for page tables etc. to not waste memory.\n> \n> Waste of memory for page table is solvable and pretty straight forward.\n> Most of such cases can be solve mechanically by switching to slab.\n\nWell, yes, like Willy says, there are already similar custom solutions \nfor s390x and ppc.\n\nPasha talked recently about the memory waste of 16k kernel stacks and \nhow we would want to reduce that to 4k. In your proposal, it would be \n64k, unless you somehow manage to allocate multiple kernel stacks from \nthe same 64k page. My head hurts thinking about whether that could work, \nmaybe it could (no idea about guard pages in there, though).\n\n\nLet's take a look at the history of page size usage on Arm (people can \nfeel free to correct me):\n\n(1) Most distros were using 64k on Arm.\n\n(2) People realized that 64k was suboptimal many use cases (memory\n     waste for stacks, pagecache, etc) and started to switch to 4k. I\n     remember that mostly HPC-centric users sticked to 64k, but there was\n     also demand from others to be able to stay on 64k.\n\n(3) Arm improved performance on a 4k kernel by adding cont-pte support,\n     trying to get closer to 64k native performance.\n\n(4) Achieving 64k native performance is hard, which is why per-process\n     page sizes are being explored to get the best out of both worlds\n     (use 64k page size only where it really matters for performance).\n\nArm clearly has the added benefit of actually benefiting from hardware \nsupport for 64k.\n\nIIUC, what you are proposing feels a bit like traveling back in time \nwhen it comes to the memory waste problem that Arm users encountered.\n\nWhere do you see the big difference to 64k on Arm in your proposal? \nWould you currently also be running 64k Arm in production and the memory \nwaste etc is acceptable?\n\n-- \nCheers,\n\nDavid\n\n\n---\n\nOn 2/20/26 13:07, Kiryl Shutsemau wrote:\n> On Fri, Feb 20, 2026 at 11:24:37AM +0100, David Hildenbrand (Arm) wrote:\n>>>\n>>> Just to clarify, do you want it to be enforced on userspace ABI.\n>>> Like, all mappings are 64k aligned?\n>>\n>> Right, see the proposal from Dev on the list.\n>>\n>>  From user-space POV, the pagesize would be 64K for these emulated processes.\n>> That is, VMAs must be suitable aligned etc.\n> \n> Well, it will drastically limit the adoption. We have too much legacy\n> stuff on x86.\n\nI'd assume that many applications nowadays can deal with differing page \nsizes (thanks to some other architectures paving the way).\n\nBut yes, some real legacy stuff, or stuff that ever only cared about \nintel still hardcodes pagesize=4k.\n\nIn Meta's fleet, I'd be quite interesting how much conversion there \nwould have to be done.\n\nFor legacy apps, you could still run them as 4k pagesize on the same \nsystem, of course.\n\n> \n>>>\n>>> Waste of memory for page table is solvable and pretty straight forward.\n>>> Most of such cases can be solve mechanically by switching to slab.\n>>\n>> Well, yes, like Willy says, there are already similar custom solutions for\n>> s390x and ppc.\n>>\n>> Pasha talked recently about the memory waste of 16k kernel stacks and how we\n>> would want to reduce that to 4k. In your proposal, it would be 64k, unless\n>> you somehow manage to allocate multiple kernel stacks from the same 64k\n>> page. My head hurts thinking about whether that could work, maybe it could\n>> (no idea about guard pages in there, though).\n> \n> Kernel stack is allocated from vmalloc. I think mapping them with\n> sub-page granularity should be doable.\n\nI still have to wrap my head around the sub-page mapping here as well. \nIt's scary.\n\nRe mapcount: I think if any part of the page is mapped, it would be \nconsidered mapped -> mapcount += 1.\n\n> \n> BTW, do you see any reason why slab-allocated stack wouldn't work for\n> large base page sizes? There's no requirement for it be aligned to page\n> or PTE, right?\n\nI'd assume that would work. Devil is in the detail with these things \nbefore we have memdescs.\n\nE.g., page table have a dedicated type (PGTY_table) and store separate \nmetadata in the ptdesc. For kernel stack there was once a proposal to \nhave a type but it is not upstream.\n\n> \n>> Let's take a look at the history of page size usage on Arm (people can feel\n>> free to correct me):\n>>\n>> (1) Most distros were using 64k on Arm.\n>>\n>> (2) People realized that 64k was suboptimal many use cases (memory\n>>      waste for stacks, pagecache, etc) and started to switch to 4k. I\n>>      remember that mostly HPC-centric users sticked to 64k, but there was\n>>      also demand from others to be able to stay on 64k.\n>>\n>> (3) Arm improved performance on a 4k kernel by adding cont-pte support,\n>>      trying to get closer to 64k native performance.\n>>\n>> (4) Achieving 64k native performance is hard, which is why per-process\n>>      page sizes are being explored to get the best out of both worlds\n>>      (use 64k page size only where it really matters for performance).\n>>\n>> Arm clearly has the added benefit of actually benefiting from hardware\n>> support for 64k.\n>>\n>> IIUC, what you are proposing feels a bit like traveling back in time when it\n>> comes to the memory waste problem that Arm users encountered.\n>>\n>> Where do you see the big difference to 64k on Arm in your proposal? Would\n>> you currently also be running 64k Arm in production and the memory waste etc\n>> is acceptable?\n> \n> That's the point. I don't see a big difference to 64k Arm. I want to\n> bring this option to x86: at some machine size it makes sense trade\n> memory consumption for scalability. I am targeting it to machines with\n> over 2TiB of RAM.\n> \n> BTW, we do run 64k Arm in our fleet. There's some growing pains, but it\n> looks good in general We have no plans to switch to 4k (or 16k) at the\n> moment. 512M THPs also look good on some workloads.\n\nOkay, that's valuable information, thanks!\n\nBeing able to remove the sub-page mapping part (or being able to just \nhide it somewhere deep down in arch code) would make this a lot easier \nto digest.\n\n-- \nCheers,\n\nDavid\n",
          "reply_to": "Kiryl Shutsemau"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The reviewer raised concerns about the proposed changes, specifically mentioning that raising min_order is not free and puts more pressure on the page allocator. They also pointed out that the benefits of higher base page size are related to reduced work for the page allocator.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "technical concerns"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Thu, Feb 19, 2026 at 03:33:47PM +0000, Pedro Falcato wrote:\n> On Thu, Feb 19, 2026 at 03:08:51PM +0000, Kiryl Shutsemau wrote:\n> > No, there's no new hardware (that I know of). I want to explore what page size\n> > means.\n> > \n> > The kernel uses the same value - PAGE_SIZE - for two things:\n> > \n> >   - the order-0 buddy allocation size;\n> > \n> >   - the granularity of virtual address space mapping;\n> > \n> > I think we can benefit from separating these two meanings and allowing\n> > order-0 allocations to be larger than the virtual address space covered by a\n> > PTE entry.\n> >\n> \n> Doesn't this idea make less sense these days, with mTHP? Simply by toggling one\n> of the entries in /sys/kernel/mm/transparent_hugepage.\n\nmTHP is still best effort. This is way you don't need to care about\nfragmentation, you will get your 64k page as long as you have free\nmemory.\n\n> > The main motivation is scalability. Managing memory on multi-terabyte\n> > machines in 4k is suboptimal, to say the least.\n> > \n> > Potential benefits of the approach (assuming 64k pages):\n> > \n> >   - The order-0 page size cuts struct page overhead by a factor of 16. From\n> >     ~1.6% of RAM to ~0.1%;\n> > \n> >   - TLB wins on machines with TLB coalescing as long as mapping is naturally\n> >     aligned;\n> > \n> >   - Order-5 allocation is 2M, resulting in less pressure on the zone lock;\n> > \n> >   - 1G pages are within possibility for the buddy allocator - order-14\n> >     allocation. It can open the road to 1G THPs.\n> > \n> >   - As with THP, fewer pages - less pressure on the LRU lock;\n> \n> We could perhaps add a way to enforce a min_order globally on the page cache,\n> as a way to address it.\n\nRaising min_order is not free. I puts more pressure on page allocator.\n\n> There are some points there which aren't addressed by mTHP work in any way\n> (1G THPs for one), others which are being addressed separately (memdesc work\n> trying to cut down on struct page overhead).\n> \n> (I also don't understand your point about order-5 allocation, AFAIK pcp will\n> cache up to COSTLY_ORDER (3) and PMD order, but I'm probably not seeing the\n> full picture)\n\nWith higher base page size, page allocator doesn't need to do as much\nwork to merge/split buddy pages. So serving the same 2M as order-5 is\ncheaper than order-9.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n",
          "reply_to": "Pedro Falcato"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The reviewer raised concerns about the proposed change to a 64k base page size on x86, specifically questioning its feasibility and potential impact on legacy code, zone lock contention, and memory waste.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "technical concerns"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Thu, Feb 19, 2026 at 04:39:34PM +0100, David Hildenbrand (Arm) wrote:\n> On 2/19/26 16:08, Kiryl Shutsemau wrote:\n> > No, there's no new hardware (that I know of). I want to explore what page size\n> > means.\n> > \n> > The kernel uses the same value - PAGE_SIZE - for two things:\n> > \n> >    - the order-0 buddy allocation size;\n> > \n> >    - the granularity of virtual address space mapping;\n> > \n> > I think we can benefit from separating these two meanings and allowing\n> > order-0 allocations to be larger than the virtual address space covered by a\n> > PTE entry.\n> > \n> > The main motivation is scalability. Managing memory on multi-terabyte\n> > machines in 4k is suboptimal, to say the least.\n> > \n> > Potential benefits of the approach (assuming 64k pages):\n> > \n> >    - The order-0 page size cuts struct page overhead by a factor of 16. From\n> >      ~1.6% of RAM to ~0.1%;\n> > \n> >    - TLB wins on machines with TLB coalescing as long as mapping is naturally\n> >      aligned;\n> > \n> >    - Order-5 allocation is 2M, resulting in less pressure on the zone lock;\n> > \n> >    - 1G pages are within possibility for the buddy allocator - order-14\n> >      allocation. It can open the road to 1G THPs.\n> > \n> >    - As with THP, fewer pages - less pressure on the LRU lock;\n> > \n> >    - ...\n> > \n> > The trade-off is memory waste (similar to what we have on architectures with\n> > native 64k pages today) and complexity, mostly in the core-MM code.\n> > \n> > == Design considerations ==\n> > \n> > I want to split PAGE_SIZE into two distinct values:\n> > \n> >    - PTE_SIZE defines the virtual address space granularity;\n> > \n> >    - PG_SIZE defines the size of the order-0 buddy allocation;\n> > \n> > PAGE_SIZE is only defined if PTE_SIZE == PG_SIZE. It will flag which code\n> > requires conversion, and keep existing code working while conversion is in\n> > progress.\n> > \n> > The same split happens for other page-related macros: mask, shift,\n> > alignment helpers, etc.\n> > \n> > PFNs are in PTE_SIZE units.\n> > \n> > The buddy allocator and page cache (as well as all I/O) operate in PG_SIZE\n> > units.\n> > \n> > Userspace mappings are maintained with PTE_SIZE granularity. No ABI changes\n> > for userspace. But we might want to communicate PG_SIZE to userspace to\n> > get the optimal results for userspace that cares.\n> > \n> > PTE_SIZE granularity requires a substantial rework of page fault and VMA\n> > handling:\n> > \n> >    - A struct page pointer and pgprot_t are not enough to create a PTE entry.\n> >      We also need the offset within the page we are creating the PTE for.\n> > \n> >    - Since the VMA start can be aligned arbitrarily with respect to the\n> >      underlying page, vma->vm_pgoff has to be changed to vma->vm_pteoff,\n> >      which is in PTE_SIZE units.\n> > \n> >    - The page fault handler needs to handle PTE_SIZE < PG_SIZE, including\n> >      misaligned cases;\n> > \n> > Page faults into file mappings are relatively simple to handle as we\n> > always have the page cache to refer to. So you can map only the part of the\n> > page that fits in the page table, similarly to fault-around.\n> > \n> > Anonymous and file-CoW faults should also be simple as long as the VMA is\n> > aligned to PG_SIZE in both the virtual address space and with respect to\n> > vm_pgoff. We might waste some memory on the ends of the VMA, but it is\n> > tolerable.\n> > \n> > Misaligned anonymous and file-CoW faults are a pain. Specifically, mapping\n> > pages across a page table boundary. In the worst case, a page is mapped across\n> > a PGD entry boundary and PTEs for the page have to be put in two separate\n> > subtrees of page tables.\n> > \n> > A naive implementation would map different pages on different sides of a\n> > page table boundary and accept the waste of one page per page table crossing.\n> > The hope is that misaligned mappings are rare, but this is suboptimal.\n> > \n> > mremap(2) is the ultimate stress test for the design.\n> > \n> > On x86, page tables are allocated from the buddy allocator and if PG_SIZE\n> > is greater than 4 KB, we need a way to pack multiple page tables into a\n> > single page. We could use the slab allocator for this, but it would\n> > require relocating the page-table metadata out of struct page.\n> \n> When discussing per-process page sizes with Ryan and Dev, I mentioned that\n> having a larger emulated page size could be interesting for other\n> architectures as well.\n> \n> That is, we would emulate a 64K page size on Intel for user space as well,\n> but let the OS work with 4K pages.\n> \n> We'd only allocate+map large folios into user space + pagecache, but still\n> allow for page tables etc. to not waste memory.\n> \n> So \"most\" of your allocations in the system would actually be at least 64k,\n> reducing zone lock contention etc.\n\nI am not convinced emulation would help zone lock contention. I expect\ncontention to be higher if page allocator would see a mix of 4k and 64k\nrequests. It sounds like constant split/merge under the lock.\n\n> It doesn't solve all the problems you wanted to tackle on your list (e.g.,\n> \"struct page\" overhead, which will be sorted out by memdescs).\n\nI don't think we can serve 1G pages out of buddy allocator with 4k\norder-0. And without it, I don't see how to get to a viable 1G THPs.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n\n---\n\nOn Thu, Feb 19, 2026 at 04:39:34PM +0100, David Hildenbrand (Arm) wrote:\n> On 2/19/26 16:08, Kiryl Shutsemau wrote:\n> > No, there's no new hardware (that I know of). I want to explore what page size\n> > means.\n> > \n> > The kernel uses the same value - PAGE_SIZE - for two things:\n> > \n> >    - the order-0 buddy allocation size;\n> > \n> >    - the granularity of virtual address space mapping;\n> > \n> > I think we can benefit from separating these two meanings and allowing\n> > order-0 allocations to be larger than the virtual address space covered by a\n> > PTE entry.\n> > \n> > The main motivation is scalability. Managing memory on multi-terabyte\n> > machines in 4k is suboptimal, to say the least.\n> > \n> > Potential benefits of the approach (assuming 64k pages):\n> > \n> >    - The order-0 page size cuts struct page overhead by a factor of 16. From\n> >      ~1.6% of RAM to ~0.1%;\n> > \n> >    - TLB wins on machines with TLB coalescing as long as mapping is naturally\n> >      aligned;\n> > \n> >    - Order-5 allocation is 2M, resulting in less pressure on the zone lock;\n> > \n> >    - 1G pages are within possibility for the buddy allocator - order-14\n> >      allocation. It can open the road to 1G THPs.\n> > \n> >    - As with THP, fewer pages - less pressure on the LRU lock;\n> > \n> >    - ...\n> > \n> > The trade-off is memory waste (similar to what we have on architectures with\n> > native 64k pages today) and complexity, mostly in the core-MM code.\n> > \n> > == Design considerations ==\n> > \n> > I want to split PAGE_SIZE into two distinct values:\n> > \n> >    - PTE_SIZE defines the virtual address space granularity;\n> > \n> >    - PG_SIZE defines the size of the order-0 buddy allocation;\n> > \n> > PAGE_SIZE is only defined if PTE_SIZE == PG_SIZE. It will flag which code\n> > requires conversion, and keep existing code working while conversion is in\n> > progress.\n> > \n> > The same split happens for other page-related macros: mask, shift,\n> > alignment helpers, etc.\n> > \n> > PFNs are in PTE_SIZE units.\n> > \n> > The buddy allocator and page cache (as well as all I/O) operate in PG_SIZE\n> > units.\n> > \n> > Userspace mappings are maintained with PTE_SIZE granularity. No ABI changes\n> > for userspace. But we might want to communicate PG_SIZE to userspace to\n> > get the optimal results for userspace that cares.\n> > \n> > PTE_SIZE granularity requires a substantial rework of page fault and VMA\n> > handling:\n> > \n> >    - A struct page pointer and pgprot_t are not enough to create a PTE entry.\n> >      We also need the offset within the page we are creating the PTE for.\n> > \n> >    - Since the VMA start can be aligned arbitrarily with respect to the\n> >      underlying page, vma->vm_pgoff has to be changed to vma->vm_pteoff,\n> >      which is in PTE_SIZE units.\n> > \n> >    - The page fault handler needs to handle PTE_SIZE < PG_SIZE, including\n> >      misaligned cases;\n> > \n> > Page faults into file mappings are relatively simple to handle as we\n> > always have the page cache to refer to. So you can map only the part of the\n> > page that fits in the page table, similarly to fault-around.\n> > \n> > Anonymous and file-CoW faults should also be simple as long as the VMA is\n> > aligned to PG_SIZE in both the virtual address space and with respect to\n> > vm_pgoff. We might waste some memory on the ends of the VMA, but it is\n> > tolerable.\n> > \n> > Misaligned anonymous and file-CoW faults are a pain. Specifically, mapping\n> > pages across a page table boundary. In the worst case, a page is mapped across\n> > a PGD entry boundary and PTEs for the page have to be put in two separate\n> > subtrees of page tables.\n> > \n> > A naive implementation would map different pages on different sides of a\n> > page table boundary and accept the waste of one page per page table crossing.\n> > The hope is that misaligned mappings are rare, but this is suboptimal.\n> > \n> > mremap(2) is the ultimate stress test for the design.\n> > \n> > On x86, page tables are allocated from the buddy allocator and if PG_SIZE\n> > is greater than 4 KB, we need a way to pack multiple page tables into a\n> > single page. We could use the slab allocator for this, but it would\n> > require relocating the page-table metadata out of struct page.\n> \n> When discussing per-process page sizes with Ryan and Dev, I mentioned that\n> having a larger emulated page size could be interesting for other\n> architectures as well.\n>\n> That is, we would emulate a 64K page size on Intel for user space as well,\n> but let the OS work with 4K pages.\n\nJust to clarify, do you want it to be enforced on userspace ABI.\nLike, all mappings are 64k aligned?\n\n> We'd only allocate+map large folios into user space + pagecache, but still\n> allow for page tables etc. to not waste memory.\n\nWaste of memory for page table is solvable and pretty straight forward.\nMost of such cases can be solve mechanically by switching to slab.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n\n---\n\nOn Fri, Feb 20, 2026 at 11:24:37AM +0100, David Hildenbrand (Arm) wrote:\n> > > When discussing per-process page sizes with Ryan and Dev, I mentioned that\n> > > having a larger emulated page size could be interesting for other\n> > > architectures as well.\n> > > \n> > > That is, we would emulate a 64K page size on Intel for user space as well,\n> > > but let the OS work with 4K pages.\n> > \n> > Just to clarify, do you want it to be enforced on userspace ABI.\n> > Like, all mappings are 64k aligned?\n> \n> Right, see the proposal from Dev on the list.\n> \n> From user-space POV, the pagesize would be 64K for these emulated processes.\n> That is, VMAs must be suitable aligned etc.\n\nWell, it will drastically limit the adoption. We have too much legacy\nstuff on x86.\n\n> > > We'd only allocate+map large folios into user space + pagecache, but still\n> > > allow for page tables etc. to not waste memory.\n> > \n> > Waste of memory for page table is solvable and pretty straight forward.\n> > Most of such cases can be solve mechanically by switching to slab.\n> \n> Well, yes, like Willy says, there are already similar custom solutions for\n> s390x and ppc.\n> \n> Pasha talked recently about the memory waste of 16k kernel stacks and how we\n> would want to reduce that to 4k. In your proposal, it would be 64k, unless\n> you somehow manage to allocate multiple kernel stacks from the same 64k\n> page. My head hurts thinking about whether that could work, maybe it could\n> (no idea about guard pages in there, though).\n\nKernel stack is allocated from vmalloc. I think mapping them with\nsub-page granularity should be doable.\n\nBTW, do you see any reason why slab-allocated stack wouldn't work for\nlarge base page sizes? There's no requirement for it be aligned to page\nor PTE, right?\n\n> Let's take a look at the history of page size usage on Arm (people can feel\n> free to correct me):\n> \n> (1) Most distros were using 64k on Arm.\n> \n> (2) People realized that 64k was suboptimal many use cases (memory\n>     waste for stacks, pagecache, etc) and started to switch to 4k. I\n>     remember that mostly HPC-centric users sticked to 64k, but there was\n>     also demand from others to be able to stay on 64k.\n> \n> (3) Arm improved performance on a 4k kernel by adding cont-pte support,\n>     trying to get closer to 64k native performance.\n> \n> (4) Achieving 64k native performance is hard, which is why per-process\n>     page sizes are being explored to get the best out of both worlds\n>     (use 64k page size only where it really matters for performance).\n> \n> Arm clearly has the added benefit of actually benefiting from hardware\n> support for 64k.\n> \n> IIUC, what you are proposing feels a bit like traveling back in time when it\n> comes to the memory waste problem that Arm users encountered.\n> \n> Where do you see the big difference to 64k on Arm in your proposal? Would\n> you currently also be running 64k Arm in production and the memory waste etc\n> is acceptable?\n\nThat's the point. I don't see a big difference to 64k Arm. I want to\nbring this option to x86: at some machine size it makes sense trade\nmemory consumption for scalability. I am targeting it to machines with\nover 2TiB of RAM.\n\nBTW, we do run 64k Arm in our fleet. There's some growing pains, but it\nlooks good in general We have no plans to switch to 4k (or 16k) at the\nmoment. 512M THPs also look good on some workloads.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n",
          "reply_to": "David (Arm)"
        },
        {
          "author": "Dave Hansen",
          "summary": "Dave Hansen raised concerns about the potential memory waste and complexity introduced by splitting PAGE_SIZE into PTE_SIZE and PG_SIZE, citing a significant increase in kernel size due to padding. He suggests separating mechanical changes from logic changes and emphasizes that this patch is not primarily about saving RAM but rather improving performance.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested_changes",
            "concerns_about_complexity"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On 2/19/26 07:08, Kiryl Shutsemau wrote:\n>   - The order-0 page size cuts struct page overhead by a factor of 16. From\n>     ~1.6% of RAM to ~0.1%;\n\nFirst of all, this looks like fun. Nice work! I'm not opposed at all in\nconcept to cleaning up things and doing the logical separation you\ndescribed to split buddy granularity and mapping granularity. That seems\nlike a worthy endeavor and some of the union/#define tricks look like a\nlikely viable way to do it incrementally.\n\nBut I don't think there's going to be a lot of memory savings in the\nend. Maybe this would bring the mem= hyperscalers back into the fold and\nhave them actually start using 'struct page' again for their VM memory.\nDunno.\n\nBut, let's look at my kernel directory and round the file sizes up to\n4k, 16k and 64k:\n\nfind .  -printf '%s\\n' | while read size; do echo\t\\\n\t\t$(((size + 0x0fff) & 0xfffff000))\t\\\n\t\t$(((size + 0x3fff) & 0xffffc000))\t\\\n\t\t$(((size + 0xffff) & 0xffff0000));\ndone\n\n... and add them all up:\n\n11,297,648 KB - on disk\n11,297,712 KB - in a 4k page cache\n12,223,488 KB - in a 16k page cache\n16,623,296 KB - in a 64k page cache\n\nSo a 64k page cache eats ~5GB of extra memory for a kernel tree (well,\n_my_ kernel tree). In other words, if you are looking for memory savings\non my laptop, you'll need ~300GB of RAM before 'struct page' overhead\noverwhelms the page cache bloat from a single kernel tree.\n\nThe whole kernel obviously isn't in the page cache all at the same time.\nThe page cache across the system is also obviously different than a\nkernel tree, but you get the point.\n\nThat's not to diminish how useful something like this might be,\nespecially for folks that are sensitive to 'struct page' overhead or\nallocator performance.\n\nBut, it will mostly be getting better performance at the _cost_ of\nconsuming more RAM, not saving RAM.\n\n\n---\n\nOn 2/19/26 07:08, Kiryl Shutsemau wrote:\n...\n> The patchset is large:\n> \n>  378 files changed, 3348 insertions(+), 3102 deletions(-)\n\nA few notes about the diffstats:\n\n$ git diff v6.17..HEAD arch/x86 | diffstat | tail -1\n 105 files changed, 874 insertions(+), 843 deletions(-)\n$ git diff v6.17..HEAD mm | diffstat | tail -1\n 53 files changed, 1136 insertions(+), 1069 deletions(-)\n\nThe vast, vast majority of this seems to be the renames. Stuff like:\n\n> -               new = round_down(new, PAGE_SIZE);\n> +               new = round_down(new, PTE_SIZE);\n\nor even less worrying:\n\n> -int set_direct_map_valid_noflush(struct page *page, unsigned nr, bool valid);\n> +int set_direct_map_valid_noflush(struct page *page, unsigned numpages, bool valid);\n\nThat stuff obviously needs to be audited but it's far less concerning\nthan the logic changes.\n\nSo just for review sanity, if you go forward with this, I'd very much\nappreciate a strong separation of the purely mechanical bits from any\nlogic changes.\n\n> On x86, page tables are allocated from the buddy allocator and if PG_SIZE\n> is greater than 4 KB, we need a way to pack multiple page tables into a\n> single page. We could use the slab allocator for this, but it would\n> require relocating the page-table metadata out of struct page.\n\nOthers mentioned this, but I think this essentially gates what you are\ndoing behind a full tree conversion over to ptdescs.\n\nThe most useful thing we can do with this series is look at it and\ndecide what _other_ things need to get done before the tree could\npossibly go in that direction, like ptdesc or a the disambiguation\nbetween PTE_SIZE and PG_SIZE that you've kicked off here.\n\n\n---\n\nOn 2/19/26 14:14, Kiryl Shutsemau wrote:\n>> Others mentioned this, but I think this essentially gates what you are\n>> doing behind a full tree conversion over to ptdescs.\n> I have not followed ptdescs closely. Need to catch up.\n> \n> For PoC, I will just waste full order-0 page for page table. Packing is\n> not required for correctness.\n\nYeah, I guess padding it out is ugly but effective.\n\nI was trying to figure out how it would apply to the KPTI pgd because we\njust flip bit 12 to switch between user and kernel PGDs. But I guess the\n8k of PGDs in the current allocation will fit fine in 128k, so it's\nweird but functional.\n",
          "reply_to": "Kiryl Shutsemau"
        },
        {
          "author": "Matthew Wilcox",
          "summary": "Matthew Wilcox raised concerns about the proposed patch to introduce a 64k base page size on x86, suggesting that slab is not the right approach and proposing an alternative method of allocating larger pages for N consecutive entries.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "alternative proposal"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Thu, Feb 19, 2026 at 03:08:51PM +0000, Kiryl Shutsemau wrote:\n> On x86, page tables are allocated from the buddy allocator and if PG_SIZE\n> is greater than 4 KB, we need a way to pack multiple page tables into a\n> single page. We could use the slab allocator for this, but it would\n> require relocating the page-table metadata out of struct page.\n\nHave you looked at the s390/ppc implementations (yes, they're different,\nno, that sucks)?  slab seems like the wrong approach to me.\n\nThere's a third approach that I've never looked at which is to allocate\nthe larger size, then just use it for N consecutive entries.\n",
          "reply_to": "Kiryl Shutsemau"
        },
        {
          "author": "Pedro Falcato",
          "summary": "The reviewer agrees with the patch and suggests that it will result in a system where most allocations are 64k, which is a positive outcome.",
          "sentiment": "positive",
          "sentiment_signals": [
            "agreement",
            "optimism"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Thu, Feb 19, 2026 at 04:53:10PM +0100, David Hildenbrand (Arm) wrote:\n> On 2/19/26 16:50, Kiryl Shutsemau wrote:\n> > On Thu, Feb 19, 2026 at 03:33:47PM +0000, Pedro Falcato wrote:\n> > > On Thu, Feb 19, 2026 at 03:08:51PM +0000, Kiryl Shutsemau wrote:\n> > > > No, there's no new hardware (that I know of). I want to explore what page size\n> > > > means.\n> > > > \n> > > > The kernel uses the same value - PAGE_SIZE - for two things:\n> > > > \n> > > >    - the order-0 buddy allocation size;\n> > > > \n> > > >    - the granularity of virtual address space mapping;\n> > > > \n> > > > I think we can benefit from separating these two meanings and allowing\n> > > > order-0 allocations to be larger than the virtual address space covered by a\n> > > > PTE entry.\n> > > > \n> > > \n> > > Doesn't this idea make less sense these days, with mTHP? Simply by toggling one\n> > > of the entries in /sys/kernel/mm/transparent_hugepage.\n> > \n> > mTHP is still best effort. This is way you don't need to care about\n> > fragmentation, you will get your 64k page as long as you have free\n> > memory.\n> > \n> > > > The main motivation is scalability. Managing memory on multi-terabyte\n> > > > machines in 4k is suboptimal, to say the least.\n> > > > \n> > > > Potential benefits of the approach (assuming 64k pages):\n> > > > \n> > > >    - The order-0 page size cuts struct page overhead by a factor of 16. From\n> > > >      ~1.6% of RAM to ~0.1%;\n> > > > \n> > > >    - TLB wins on machines with TLB coalescing as long as mapping is naturally\n> > > >      aligned;\n> > > > \n> > > >    - Order-5 allocation is 2M, resulting in less pressure on the zone lock;\n> > > > \n> > > >    - 1G pages are within possibility for the buddy allocator - order-14\n> > > >      allocation. It can open the road to 1G THPs.\n> > > > \n> > > >    - As with THP, fewer pages - less pressure on the LRU lock;\n> > > \n> > > We could perhaps add a way to enforce a min_order globally on the page cache,\n> > > as a way to address it.\n> > \n> > Raising min_order is not free. I puts more pressure on page allocator.\n> > \n> > > There are some points there which aren't addressed by mTHP work in any way\n> > > (1G THPs for one), others which are being addressed separately (memdesc work\n> > > trying to cut down on struct page overhead).\n> > > \n> > > (I also don't understand your point about order-5 allocation, AFAIK pcp will\n> > > cache up to COSTLY_ORDER (3) and PMD order, but I'm probably not seeing the\n> > > full picture)\n> > \n> > With higher base page size, page allocator doesn't need to do as much\n> > work to merge/split buddy pages. So serving the same 2M as order-5 is\n> > cheaper than order-9.\n> \n> I think the idea is that if most of your allocations (anon + pagecache) are\n> 64k instead of 4k, on average, you'll just naturally do less merging\n> splitting.\n\nYep. That plus slab_min_order would hopefully yield a system where 90%+\n(depending on how your filesystem's buffer cache works) allocations are 64K.\n\n-- \nPedro\n",
          "reply_to": "David (Arm)"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The reviewer agrees that the problem of struct page memory consumption is a concern, but notes that it's static and cannot be reclaimed, unlike page cache rounding overhead which can be controlled by userspace.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "acknowledged the plan"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Thu, Feb 19, 2026 at 09:08:57AM -0800, Dave Hansen wrote:\n> On 2/19/26 07:08, Kiryl Shutsemau wrote:\n> >   - The order-0 page size cuts struct page overhead by a factor of 16. From\n> >     ~1.6% of RAM to ~0.1%;\n> ...\n> But, it will mostly be getting better performance at the _cost_ of\n> consuming more RAM, not saving RAM.\n\nThat's fair.\n\nThe problem with struct page memory consumption is that it is static and\ncannot be reclaimed. You pay the struct page tax no matter what.\n\nPage cache rounding overhead can be large, but a motivated userspace can\nkeep it under control by avoiding splitting a dataset into many small\nfiles. And this memory is reclaimable.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n\n---\n\nOn Thu, Feb 19, 2026 at 09:30:36AM -0800, Dave Hansen wrote:\n> On 2/19/26 07:08, Kiryl Shutsemau wrote:\n> ...\n> > The patchset is large:\n> > \n> >  378 files changed, 3348 insertions(+), 3102 deletions(-)\n> \n> A few notes about the diffstats:\n> \n> $ git diff v6.17..HEAD arch/x86 | diffstat | tail -1\n>  105 files changed, 874 insertions(+), 843 deletions(-)\n> $ git diff v6.17..HEAD mm | diffstat | tail -1\n>  53 files changed, 1136 insertions(+), 1069 deletions(-)\n> \n> The vast, vast majority of this seems to be the renames. Stuff like:\n> \n> > -               new = round_down(new, PAGE_SIZE);\n> > +               new = round_down(new, PTE_SIZE);\n> \n> or even less worrying:\n> \n> > -int set_direct_map_valid_noflush(struct page *page, unsigned nr, bool valid);\n> > +int set_direct_map_valid_noflush(struct page *page, unsigned numpages, bool valid);\n> \n> That stuff obviously needs to be audited but it's far less concerning\n> than the logic changes.\n> \n> So just for review sanity, if you go forward with this, I'd very much\n> appreciate a strong separation of the purely mechanical bits from any\n> logic changes.\n\nThat's the plan. That's the only way I can keep myself sane :P\n\n> > On x86, page tables are allocated from the buddy allocator and if PG_SIZE\n> > is greater than 4 KB, we need a way to pack multiple page tables into a\n> > single page. We could use the slab allocator for this, but it would\n> > require relocating the page-table metadata out of struct page.\n> \n> Others mentioned this, but I think this essentially gates what you are\n> doing behind a full tree conversion over to ptdescs.\n\nI have not followed ptdescs closely. Need to catch up.\n\nFor PoC, I will just waste full order-0 page for page table. Packing is\nnot required for correctness.\n\n> The most useful thing we can do with this series is look at it and\n> decide what _other_ things need to get done before the tree could\n> possibly go in that direction, like ptdesc or a the disambiguation\n> between PTE_SIZE and PG_SIZE that you've kicked off here.\n\nRight.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n",
          "reply_to": "Dave Hansen"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The reviewer, Kiryl Shutsemau, raised concerns about implementing a 64k or 16k base page size on x86, specifically questioning the need for such a change and suggesting alternative approaches.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "NEEDS_WORK",
            "CONTENTIOUS"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Thu, Feb 19, 2026 at 05:47:22PM +0000, Matthew Wilcox wrote:\n> On Thu, Feb 19, 2026 at 03:08:51PM +0000, Kiryl Shutsemau wrote:\n> > On x86, page tables are allocated from the buddy allocator and if PG_SIZE\n> > is greater than 4 KB, we need a way to pack multiple page tables into a\n> > single page. We could use the slab allocator for this, but it would\n> > require relocating the page-table metadata out of struct page.\n> \n> Have you looked at the s390/ppc implementations (yes, they're different,\n> no, that sucks)? \n\nNo, will check it out tomorrow.\n\n> slab seems like the wrong approach to me.\n\nI was the first thing that came to mind. I have not put much time into\nit\n\n> There's a third approach that I've never looked at which is to allocate\n> the larger size, then just use it for N consecutive entries.\n\nYeah, that's a possible way. We would need to populate 16 page table\nentries of the parent page table. But you don't need to care about\nfragmentation within the page.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n",
          "reply_to": "Matthew Wilcox"
        },
        {
          "author": "Kalesh Singh",
          "summary": "Kalesh Singh raised concerns about extending the patch to cover a related use case, specifically enforcing a larger granularity on VMAs for Android's 16KB page size emulation.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "technical concerns"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Thu, Feb 19, 2026 at 7:39AM David Hildenbrand (Arm)\n<david@kernel.org> wrote:\n>\n> On 2/19/26 16:08, Kiryl Shutsemau wrote:\n> > No, there's no new hardware (that I know of). I want to explore what page size\n> > means.\n> >\n> > The kernel uses the same value - PAGE_SIZE - for two things:\n> >\n> >    - the order-0 buddy allocation size;\n> >\n> >    - the granularity of virtual address space mapping;\n> >\n> > I think we can benefit from separating these two meanings and allowing\n> > order-0 allocations to be larger than the virtual address space covered by a\n> > PTE entry.\n> >\n> > The main motivation is scalability. Managing memory on multi-terabyte\n> > machines in 4k is suboptimal, to say the least.\n> >\n> > Potential benefits of the approach (assuming 64k pages):\n> >\n> >    - The order-0 page size cuts struct page overhead by a factor of 16. From\n> >      ~1.6% of RAM to ~0.1%;\n> >\n> >    - TLB wins on machines with TLB coalescing as long as mapping is naturally\n> >      aligned;\n> >\n> >    - Order-5 allocation is 2M, resulting in less pressure on the zone lock;\n> >\n> >    - 1G pages are within possibility for the buddy allocator - order-14\n> >      allocation. It can open the road to 1G THPs.\n> >\n> >    - As with THP, fewer pages - less pressure on the LRU lock;\n> >\n> >    - ...\n> >\n> > The trade-off is memory waste (similar to what we have on architectures with\n> > native 64k pages today) and complexity, mostly in the core-MM code.\n> >\n> > == Design considerations ==\n> >\n> > I want to split PAGE_SIZE into two distinct values:\n> >\n> >    - PTE_SIZE defines the virtual address space granularity;\n> >\n> >    - PG_SIZE defines the size of the order-0 buddy allocation;\n> >\n> > PAGE_SIZE is only defined if PTE_SIZE == PG_SIZE. It will flag which code\n> > requires conversion, and keep existing code working while conversion is in\n> > progress.\n> >\n> > The same split happens for other page-related macros: mask, shift,\n> > alignment helpers, etc.\n> >\n> > PFNs are in PTE_SIZE units.\n> >\n> > The buddy allocator and page cache (as well as all I/O) operate in PG_SIZE\n> > units.\n> >\n> > Userspace mappings are maintained with PTE_SIZE granularity. No ABI changes\n> > for userspace. But we might want to communicate PG_SIZE to userspace to\n> > get the optimal results for userspace that cares.\n> >\n> > PTE_SIZE granularity requires a substantial rework of page fault and VMA\n> > handling:\n> >\n> >    - A struct page pointer and pgprot_t are not enough to create a PTE entry.\n> >      We also need the offset within the page we are creating the PTE for.\n> >\n> >    - Since the VMA start can be aligned arbitrarily with respect to the\n> >      underlying page, vma->vm_pgoff has to be changed to vma->vm_pteoff,\n> >      which is in PTE_SIZE units.\n> >\n> >    - The page fault handler needs to handle PTE_SIZE < PG_SIZE, including\n> >      misaligned cases;\n> >\n> > Page faults into file mappings are relatively simple to handle as we\n> > always have the page cache to refer to. So you can map only the part of the\n> > page that fits in the page table, similarly to fault-around.\n> >\n> > Anonymous and file-CoW faults should also be simple as long as the VMA is\n> > aligned to PG_SIZE in both the virtual address space and with respect to\n> > vm_pgoff. We might waste some memory on the ends of the VMA, but it is\n> > tolerable.\n> >\n> > Misaligned anonymous and file-CoW faults are a pain. Specifically, mapping\n> > pages across a page table boundary. In the worst case, a page is mapped across\n> > a PGD entry boundary and PTEs for the page have to be put in two separate\n> > subtrees of page tables.\n> >\n> > A naive implementation would map different pages on different sides of a\n> > page table boundary and accept the waste of one page per page table crossing.\n> > The hope is that misaligned mappings are rare, but this is suboptimal.\n> >\n> > mremap(2) is the ultimate stress test for the design.\n> >\n> > On x86, page tables are allocated from the buddy allocator and if PG_SIZE\n> > is greater than 4 KB, we need a way to pack multiple page tables into a\n> > single page. We could use the slab allocator for this, but it would\n> > require relocating the page-table metadata out of struct page.\n>\n> When discussing per-process page sizes with Ryan and Dev, I mentioned\n> that having a larger emulated page size could be interesting for other\n> architectures as well.\n>\n> That is, we would emulate a 64K page size on Intel for user space as\n> well, but let the OS work with 4K pages.\n>\n> We'd only allocate+map large folios into user space + pagecache, but\n> still allow for page tables etc. to not waste memory.\n>\n> So \"most\" of your allocations in the system would actually be at least\n> 64k, reducing zone lock contention etc.\n>\n>\n> It doesn't solve all the problems you wanted to tackle on your list\n> (e.g., \"struct page\" overhead, which will be sorted out by memdescs).\n\nHi Kiryl,\n\nI'd be interested to discuss this at LSFMM.\n\nOn Android, we have a separate but related use case: we emulate the\nuserspace page size on x86, primarily to enable app developers to\nconduct compatibility testing of their apps for 16KB Android devices.\n[1]\n\nIt mainly works by enforcing a larger granularity on the VMAs to\nemulate a userspace page size, somewhat similar to what David\nmentioned, while the underlying kernel still operates on a 4KB\ngranularity. [2]\n\nIIUC the current design would not enfore the larger granularity /\nalignment for VMAs to avoid breaking ABI. However, I'd be interest to\ndiscuss whether it can be extended to cover this usecase as well.\n\n[1]  https://developer.android.com/guide/practices/page-sizes#16kb-emulator\n[2] https://source.android.com/docs/core/architecture/16kb-page-size/getting-started-cf-x86-64-pgagnostic\n\nThanks,\nKalesh\n\n\n\n\n>\n> --\n> Cheers,\n>\n> David\n>\n\n\n---\n\nOn Fri, Feb 20, 2026 at 8:30AM David Hildenbrand (Arm)\n<david@kernel.org> wrote:\n>\n> On 2/20/26 13:07, Kiryl Shutsemau wrote:\n> > On Fri, Feb 20, 2026 at 11:24:37AM +0100, David Hildenbrand (Arm) wrote:\n> >>>\n> >>> Just to clarify, do you want it to be enforced on userspace ABI.\n> >>> Like, all mappings are 64k aligned?\n> >>\n> >> Right, see the proposal from Dev on the list.\n> >>\n> >>  From user-space POV, the pagesize would be 64K for these emulated processes.\n> >> That is, VMAs must be suitable aligned etc.\n> >\n> > Well, it will drastically limit the adoption. We have too much legacy\n> > stuff on x86.\n>\n> I'd assume that many applications nowadays can deal with differing page\n> sizes (thanks to some other architectures paving the way).\n>\n> But yes, some real legacy stuff, or stuff that ever only cared about\n> intel still hardcodes pagesize=4k.\n\nI think most issues will stem from linkers setting the default ELF\nsegment alignment (max-page-size) for x86 to 4096. So those ELFs will\nnot load correctly or at all on the larger emulated granularity.\n\n-- Kalesh\n\n>\n> In Meta's fleet, I'd be quite interesting how much conversion there\n> would have to be done.\n>\n> For legacy apps, you could still run them as 4k pagesize on the same\n> system, of course.\n>\n> >\n> >>>\n> >>> Waste of memory for page table is solvable and pretty straight forward.\n> >>> Most of such cases can be solve mechanically by switching to slab.\n> >>\n> >> Well, yes, like Willy says, there are already similar custom solutions for\n> >> s390x and ppc.\n> >>\n> >> Pasha talked recently about the memory waste of 16k kernel stacks and how we\n> >> would want to reduce that to 4k. In your proposal, it would be 64k, unless\n> >> you somehow manage to allocate multiple kernel stacks from the same 64k\n> >> page. My head hurts thinking about whether that could work, maybe it could\n> >> (no idea about guard pages in there, though).\n> >\n> > Kernel stack is allocated from vmalloc. I think mapping them with\n> > sub-page granularity should be doable.\n>\n> I still have to wrap my head around the sub-page mapping here as well.\n> It's scary.\n>\n> Re mapcount: I think if any part of the page is mapped, it would be\n> considered mapped -> mapcount += 1.\n>\n> >\n> > BTW, do you see any reason why slab-allocated stack wouldn't work for\n> > large base page sizes? There's no requirement for it be aligned to page\n> > or PTE, right?\n>\n> I'd assume that would work. Devil is in the detail with these things\n> before we have memdescs.\n>\n> E.g., page table have a dedicated type (PGTY_table) and store separate\n> metadata in the ptdesc. For kernel stack there was once a proposal to\n> have a type but it is not upstream.\n>\n> >\n> >> Let's take a look at the history of page size usage on Arm (people can feel\n> >> free to correct me):\n> >>\n> >> (1) Most distros were using 64k on Arm.\n> >>\n> >> (2) People realized that 64k was suboptimal many use cases (memory\n> >>      waste for stacks, pagecache, etc) and started to switch to 4k. I\n> >>      remember that mostly HPC-centric users sticked to 64k, but there was\n> >>      also demand from others to be able to stay on 64k.\n> >>\n> >> (3) Arm improved performance on a 4k kernel by adding cont-pte support,\n> >>      trying to get closer to 64k native performance.\n> >>\n> >> (4) Achieving 64k native performance is hard, which is why per-process\n> >>      page sizes are being explored to get the best out of both worlds\n> >>      (use 64k page size only where it really matters for performance).\n> >>\n> >> Arm clearly has the added benefit of actually benefiting from hardware\n> >> support for 64k.\n> >>\n> >> IIUC, what you are proposing feels a bit like traveling back in time when it\n> >> comes to the memory waste problem that Arm users encountered.\n> >>\n> >> Where do you see the big difference to 64k on Arm in your proposal? Would\n> >> you currently also be running 64k Arm in production and the memory waste etc\n> >> is acceptable?\n> >\n> > That's the point. I don't see a big difference to 64k Arm. I want to\n> > bring this option to x86: at some machine size it makes sense trade\n> > memory consumption for scalability. I am targeting it to machines with\n> > over 2TiB of RAM.\n> >\n> > BTW, we do run 64k Arm in our fleet. There's some growing pains, but it\n> > looks good in general We have no plans to switch to 4k (or 16k) at the\n> > moment. 512M THPs also look good on some workloads.\n>\n> Okay, that's valuable information, thanks!\n>\n> Being able to remove the sub-page mapping part (or being able to just\n> hide it somewhere deep down in arch code) would make this a lot easier\n> to digest.\n>\n> --\n> Cheers,\n>\n> David\n>\n\n",
          "reply_to": "David (Arm)"
        },
        {
          "author": "Zi Yan",
          "summary": "The reviewer questions the necessity of introducing a super pageblock for anti-fragmentation at larger granularity, specifically 1GB, and whether these free pages should go into the buddy allocator.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "questioning necessity",
            "debate"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On 19 Feb 2026, at 11:09, David Hildenbrand (Arm) wrote:\n\n> On 2/19/26 16:54, Kiryl Shutsemau wrote:\n>> On Thu, Feb 19, 2026 at 04:39:34PM +0100, David Hildenbrand (Arm) wrote:\n>>> On 2/19/26 16:08, Kiryl Shutsemau wrote:\n>>>> No, there's no new hardware (that I know of). I want to explore what page size\n>>>> means.\n>>>>\n>>>> The kernel uses the same value - PAGE_SIZE - for two things:\n>>>>\n>>>>     - the order-0 buddy allocation size;\n>>>>\n>>>>     - the granularity of virtual address space mapping;\n>>>>\n>>>> I think we can benefit from separating these two meanings and allowing\n>>>> order-0 allocations to be larger than the virtual address space covered by a\n>>>> PTE entry.\n>>>>\n>>>> The main motivation is scalability. Managing memory on multi-terabyte\n>>>> machines in 4k is suboptimal, to say the least.\n>>>>\n>>>> Potential benefits of the approach (assuming 64k pages):\n>>>>\n>>>>     - The order-0 page size cuts struct page overhead by a factor of 16. From\n>>>>       ~1.6% of RAM to ~0.1%;\n>>>>\n>>>>     - TLB wins on machines with TLB coalescing as long as mapping is naturally\n>>>>       aligned;\n>>>>\n>>>>     - Order-5 allocation is 2M, resulting in less pressure on the zone lock;\n>>>>\n>>>>     - 1G pages are within possibility for the buddy allocator - order-14\n>>>>       allocation. It can open the road to 1G THPs.\n>>>>\n>>>>     - As with THP, fewer pages - less pressure on the LRU lock;\n>>>>\n>>>>     - ...\n>>>>\n>>>> The trade-off is memory waste (similar to what we have on architectures with\n>>>> native 64k pages today) and complexity, mostly in the core-MM code.\n>>>>\n>>>> == Design considerations ==\n>>>>\n>>>> I want to split PAGE_SIZE into two distinct values:\n>>>>\n>>>>     - PTE_SIZE defines the virtual address space granularity;\n>>>>\n>>>>     - PG_SIZE defines the size of the order-0 buddy allocation;\n>>>>\n>>>> PAGE_SIZE is only defined if PTE_SIZE == PG_SIZE. It will flag which code\n>>>> requires conversion, and keep existing code working while conversion is in\n>>>> progress.\n>>>>\n>>>> The same split happens for other page-related macros: mask, shift,\n>>>> alignment helpers, etc.\n>>>>\n>>>> PFNs are in PTE_SIZE units.\n>>>>\n>>>> The buddy allocator and page cache (as well as all I/O) operate in PG_SIZE\n>>>> units.\n>>>>\n>>>> Userspace mappings are maintained with PTE_SIZE granularity. No ABI changes\n>>>> for userspace. But we might want to communicate PG_SIZE to userspace to\n>>>> get the optimal results for userspace that cares.\n>>>>\n>>>> PTE_SIZE granularity requires a substantial rework of page fault and VMA\n>>>> handling:\n>>>>\n>>>>     - A struct page pointer and pgprot_t are not enough to create a PTE entry.\n>>>>       We also need the offset within the page we are creating the PTE for.\n>>>>\n>>>>     - Since the VMA start can be aligned arbitrarily with respect to the\n>>>>       underlying page, vma->vm_pgoff has to be changed to vma->vm_pteoff,\n>>>>       which is in PTE_SIZE units.\n>>>>\n>>>>     - The page fault handler needs to handle PTE_SIZE < PG_SIZE, including\n>>>>       misaligned cases;\n>>>>\n>>>> Page faults into file mappings are relatively simple to handle as we\n>>>> always have the page cache to refer to. So you can map only the part of the\n>>>> page that fits in the page table, similarly to fault-around.\n>>>>\n>>>> Anonymous and file-CoW faults should also be simple as long as the VMA is\n>>>> aligned to PG_SIZE in both the virtual address space and with respect to\n>>>> vm_pgoff. We might waste some memory on the ends of the VMA, but it is\n>>>> tolerable.\n>>>>\n>>>> Misaligned anonymous and file-CoW faults are a pain. Specifically, mapping\n>>>> pages across a page table boundary. In the worst case, a page is mapped across\n>>>> a PGD entry boundary and PTEs for the page have to be put in two separate\n>>>> subtrees of page tables.\n>>>>\n>>>> A naive implementation would map different pages on different sides of a\n>>>> page table boundary and accept the waste of one page per page table crossing.\n>>>> The hope is that misaligned mappings are rare, but this is suboptimal.\n>>>>\n>>>> mremap(2) is the ultimate stress test for the design.\n>>>>\n>>>> On x86, page tables are allocated from the buddy allocator and if PG_SIZE\n>>>> is greater than 4 KB, we need a way to pack multiple page tables into a\n>>>> single page. We could use the slab allocator for this, but it would\n>>>> require relocating the page-table metadata out of struct page.\n>>>\n>>> When discussing per-process page sizes with Ryan and Dev, I mentioned that\n>>> having a larger emulated page size could be interesting for other\n>>> architectures as well.\n>>>\n>>> That is, we would emulate a 64K page size on Intel for user space as well,\n>>> but let the OS work with 4K pages.\n>>>\n>>> We'd only allocate+map large folios into user space + pagecache, but still\n>>> allow for page tables etc. to not waste memory.\n>>>\n>>> So \"most\" of your allocations in the system would actually be at least 64k,\n>>> reducing zone lock contention etc.\n>>\n>> I am not convinced emulation would help zone lock contention. I expect\n>> contention to be higher if page allocator would see a mix of 4k and 64k\n>> requests. It sounds like constant split/merge under the lock.\n>\n> If most your allocations are larger, then there isn't that much splitting/merging.\n>\n> There will be some for the < 64k allocations of course, but when all user space+page cache is >= 64 then the split/merge + zone lock should be heavily reduced.\n>\n>>\n>>> It doesn't solve all the problems you wanted to tackle on your list (e.g.,\n>>> \"struct page\" overhead, which will be sorted out by memdescs).\n>>\n>> I don't think we can serve 1G pages out of buddy allocator with 4k\n>> order-0. And without it, I don't see how to get to a viable 1G THPs.\n>\n> Zi Yan was one working on this, and I think we had ideas on how to make that work in the long run.\n\nRight. The idea is to add super pageblock (or whatever name), which consists of N consecutive\npageblocks, so that anti fragmentation can work at larger granularity, e.g., 1GB, to create\nfree pages. Whether 1GB free pages from memory compaction need to go into buddy allocator\nor not is debatable.\n\n--\nBest Regards,\nYan, Zi\n",
          "reply_to": "David (Arm)"
        },
        {
          "author": "Liam Howlett",
          "summary": "The reviewer raised concerns that increasing page size may lead to increased memory pressure and longer time spent in reclaim, potentially degrading primary workloads. He also questioned the usefulness of this solution for systems with limited memory.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "trade-off analysis"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "* Kiryl Shutsemau <kas@kernel.org> [260219 17:05]:\n> On Thu, Feb 19, 2026 at 09:08:57AM -0800, Dave Hansen wrote:\n> > On 2/19/26 07:08, Kiryl Shutsemau wrote:\n> > >   - The order-0 page size cuts struct page overhead by a factor of 16. From\n> > >     ~1.6% of RAM to ~0.1%;\n> > ...\n> > But, it will mostly be getting better performance at the _cost_ of\n> > consuming more RAM, not saving RAM.\n> \n> That's fair.\n> \n> The problem with struct page memory consumption is that it is static and\n> cannot be reclaimed. You pay the struct page tax no matter what.\n> \n> Page cache rounding overhead can be large, but a motivated userspace can\n> keep it under control by avoiding splitting a dataset into many small\n> files. And this memory is reclaimable.\n> \n\nBut we are in reclaim a lot more these days.  As I'm sure you are aware,\nwe are trying to maximize the resources (both cpu and ram) of any\nmachine powered on.  Entering reclaim will consume the cpu time and will\naffect other tasks.\n\nEspecially with multiple workload machines, the tendency is to have a\nprimary focus with the lower desired work being killed, if necessary.\nReducing the overhead just means more secondary tasks, or a bigger\nfootprint of the ones already executing.\n\nIncreasing the memory pressure will degrade the primary workload more\nfrequently, even if we recover enough to avoid OOMing the secondary.\n\nWhile in the struct page tax world, the secondary task would be killed\nafter a shorter (and less frequently executed) reclaim comes up short.\nSo, I would think that we would be degrading the primary workload in an\nattempt to keep the secondary alive?  Maybe I'm over-simplifying here?\n\nNear the other end of the spectrum, we have chromebooks that are\nconstantly in reclaim, even with 4k pages.  I guess these machines would\nbe destine to maintain the same page size they use today.  That is, this\nsolution for the struct page tax is only useful if you have a lot of\nmemory.  But then again, that's where the bookkeeping costs become hard\nto take.\n\nThanks,\nLiam\n\n\n\n---\n\n* Kiryl Shutsemau <kas@kernel.org> [260220 07:33]:\n> On Thu, Feb 19, 2026 at 10:28:20PM -0500, Liam R. Howlett wrote:\n> > * Kiryl Shutsemau <kas@kernel.org> [260219 17:05]:\n> > > On Thu, Feb 19, 2026 at 09:08:57AM -0800, Dave Hansen wrote:\n> > > > On 2/19/26 07:08, Kiryl Shutsemau wrote:\n> > > > >   - The order-0 page size cuts struct page overhead by a factor of 16. From\n> > > > >     ~1.6% of RAM to ~0.1%;\n> > > > ...\n> > > > But, it will mostly be getting better performance at the _cost_ of\n> > > > consuming more RAM, not saving RAM.\n> > > \n> > > That's fair.\n> > > \n> > > The problem with struct page memory consumption is that it is static and\n> > > cannot be reclaimed. You pay the struct page tax no matter what.\n> > > \n> > > Page cache rounding overhead can be large, but a motivated userspace can\n> > > keep it under control by avoiding splitting a dataset into many small\n> > > files. And this memory is reclaimable.\n> > > \n> > \n> > But we are in reclaim a lot more these days.  As I'm sure you are aware,\n> > we are trying to maximize the resources (both cpu and ram) of any\n> > machine powered on.  Entering reclaim will consume the cpu time and will\n> > affect other tasks.\n> > \n> > Especially with multiple workload machines, the tendency is to have a\n> > primary focus with the lower desired work being killed, if necessary.\n> > Reducing the overhead just means more secondary tasks, or a bigger\n> > footprint of the ones already executing.\n> > \n> > Increasing the memory pressure will degrade the primary workload more\n> > frequently, even if we recover enough to avoid OOMing the secondary.\n> > \n> > While in the struct page tax world, the secondary task would be killed\n> > after a shorter (and less frequently executed) reclaim comes up short.\n> > So, I would think that we would be degrading the primary workload in an\n> > attempt to keep the secondary alive?  Maybe I'm over-simplifying here?\n> \n> I am not sure I fully follow your point.\n> \n> Sizing tasks and scheduling tasks between machines is hard in general.\n> I don't think the balance between struct page tax and page cache\n> rounding overhead is going to be the primary factor.\n\nI think there are more trade offs than what you listed.  It's still\nprobably worth doing, but I wanted to know if you though that this would\ncause us to spend more time in reclaim, which seems to be implied above.\nSo, another trade-off might be all the reclaim penalty being paid more\nfrequently?\n\n...\n\nThanks,\nLiam\n",
          "reply_to": "Kiryl Shutsemau"
        },
        {
          "author": "David Laight",
          "summary": "The reviewer raised concerns about potential issues with the proposed patch, specifically mentioning 'random' buffers that are PAGE_SIZE rather than 4k and questioning how it affects mmap of kernel memory and PCIe window alignment.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "uncertainty"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Thu, 19 Feb 2026 15:08:51 +0000\nKiryl Shutsemau <kas@kernel.org> wrote:\n\n> No, there's no new hardware (that I know of). I want to explore what page size\n> means.\n> \n> The kernel uses the same value - PAGE_SIZE - for two things:\n> \n>   - the order-0 buddy allocation size;\n> \n>   - the granularity of virtual address space mapping;\n\nAlso the 'random' buffers that are PAGE_SIZE rather than 4k.\n\nI also wonder how is affects mmap of kernel memory and the alignement\nof PCIe windows (etc).\n\n\tDavid\n",
          "reply_to": "Kiryl Shutsemau"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The reviewer suggests adding a knob to enforce the new page size without breaking ABI, and prefers advertising a new value for preferred virtual address space granularity.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "prefers alternative approach"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Thu, Feb 19, 2026 at 03:24:37PM -0800, Kalesh Singh wrote:\n> On Thu, Feb 19, 2026 at 7:39\\u202fAM David Hildenbrand (Arm)\n> <david@kernel.org> wrote:\n> >\n> > On 2/19/26 16:08, Kiryl Shutsemau wrote:\n> > > No, there's no new hardware (that I know of). I want to explore what page size\n> > > means.\n> > >\n> > > The kernel uses the same value - PAGE_SIZE - for two things:\n> > >\n> > >    - the order-0 buddy allocation size;\n> > >\n> > >    - the granularity of virtual address space mapping;\n> > >\n> > > I think we can benefit from separating these two meanings and allowing\n> > > order-0 allocations to be larger than the virtual address space covered by a\n> > > PTE entry.\n> > >\n> > > The main motivation is scalability. Managing memory on multi-terabyte\n> > > machines in 4k is suboptimal, to say the least.\n> > >\n> > > Potential benefits of the approach (assuming 64k pages):\n> > >\n> > >    - The order-0 page size cuts struct page overhead by a factor of 16. From\n> > >      ~1.6% of RAM to ~0.1%;\n> > >\n> > >    - TLB wins on machines with TLB coalescing as long as mapping is naturally\n> > >      aligned;\n> > >\n> > >    - Order-5 allocation is 2M, resulting in less pressure on the zone lock;\n> > >\n> > >    - 1G pages are within possibility for the buddy allocator - order-14\n> > >      allocation. It can open the road to 1G THPs.\n> > >\n> > >    - As with THP, fewer pages - less pressure on the LRU lock;\n> > >\n> > >    - ...\n> > >\n> > > The trade-off is memory waste (similar to what we have on architectures with\n> > > native 64k pages today) and complexity, mostly in the core-MM code.\n> > >\n> > > == Design considerations ==\n> > >\n> > > I want to split PAGE_SIZE into two distinct values:\n> > >\n> > >    - PTE_SIZE defines the virtual address space granularity;\n> > >\n> > >    - PG_SIZE defines the size of the order-0 buddy allocation;\n> > >\n> > > PAGE_SIZE is only defined if PTE_SIZE == PG_SIZE. It will flag which code\n> > > requires conversion, and keep existing code working while conversion is in\n> > > progress.\n> > >\n> > > The same split happens for other page-related macros: mask, shift,\n> > > alignment helpers, etc.\n> > >\n> > > PFNs are in PTE_SIZE units.\n> > >\n> > > The buddy allocator and page cache (as well as all I/O) operate in PG_SIZE\n> > > units.\n> > >\n> > > Userspace mappings are maintained with PTE_SIZE granularity. No ABI changes\n> > > for userspace. But we might want to communicate PG_SIZE to userspace to\n> > > get the optimal results for userspace that cares.\n> > >\n> > > PTE_SIZE granularity requires a substantial rework of page fault and VMA\n> > > handling:\n> > >\n> > >    - A struct page pointer and pgprot_t are not enough to create a PTE entry.\n> > >      We also need the offset within the page we are creating the PTE for.\n> > >\n> > >    - Since the VMA start can be aligned arbitrarily with respect to the\n> > >      underlying page, vma->vm_pgoff has to be changed to vma->vm_pteoff,\n> > >      which is in PTE_SIZE units.\n> > >\n> > >    - The page fault handler needs to handle PTE_SIZE < PG_SIZE, including\n> > >      misaligned cases;\n> > >\n> > > Page faults into file mappings are relatively simple to handle as we\n> > > always have the page cache to refer to. So you can map only the part of the\n> > > page that fits in the page table, similarly to fault-around.\n> > >\n> > > Anonymous and file-CoW faults should also be simple as long as the VMA is\n> > > aligned to PG_SIZE in both the virtual address space and with respect to\n> > > vm_pgoff. We might waste some memory on the ends of the VMA, but it is\n> > > tolerable.\n> > >\n> > > Misaligned anonymous and file-CoW faults are a pain. Specifically, mapping\n> > > pages across a page table boundary. In the worst case, a page is mapped across\n> > > a PGD entry boundary and PTEs for the page have to be put in two separate\n> > > subtrees of page tables.\n> > >\n> > > A naive implementation would map different pages on different sides of a\n> > > page table boundary and accept the waste of one page per page table crossing.\n> > > The hope is that misaligned mappings are rare, but this is suboptimal.\n> > >\n> > > mremap(2) is the ultimate stress test for the design.\n> > >\n> > > On x86, page tables are allocated from the buddy allocator and if PG_SIZE\n> > > is greater than 4 KB, we need a way to pack multiple page tables into a\n> > > single page. We could use the slab allocator for this, but it would\n> > > require relocating the page-table metadata out of struct page.\n> >\n> > When discussing per-process page sizes with Ryan and Dev, I mentioned\n> > that having a larger emulated page size could be interesting for other\n> > architectures as well.\n> >\n> > That is, we would emulate a 64K page size on Intel for user space as\n> > well, but let the OS work with 4K pages.\n> >\n> > We'd only allocate+map large folios into user space + pagecache, but\n> > still allow for page tables etc. to not waste memory.\n> >\n> > So \"most\" of your allocations in the system would actually be at least\n> > 64k, reducing zone lock contention etc.\n> >\n> >\n> > It doesn't solve all the problems you wanted to tackle on your list\n> > (e.g., \"struct page\" overhead, which will be sorted out by memdescs).\n> \n> Hi Kiryl,\n> \n> I'd be interested to discuss this at LSFMM.\n> \n> On Android, we have a separate but related use case: we emulate the\n> userspace page size on x86, primarily to enable app developers to\n> conduct compatibility testing of their apps for 16KB Android devices.\n> [1]\n> \n> It mainly works by enforcing a larger granularity on the VMAs to\n> emulate a userspace page size, somewhat similar to what David\n> mentioned, while the underlying kernel still operates on a 4KB\n> granularity. [2]\n> \n> IIUC the current design would not enfore the larger granularity /\n> alignment for VMAs to avoid breaking ABI. However, I'd be interest to\n> discuss whether it can be extended to cover this usecase as well.\n\nI don't want to break ABI, but might add a knob (maybe personality(2) ?)\nfor enforcement to see what breaks.\n\nIn general, I would prefer to advertise a new value to userspace that\nwould mean preferred virtual address space granularity.\n\n> \n> [1]  https://developer.android.com/guide/practices/page-sizes#16kb-emulator\n> [2] https://source.android.com/docs/core/architecture/16kb-page-size/getting-started-cf-x86-64-pgagnostic\n> \n> Thanks,\n> Kalesh\n> \n> \n> \n> \n> >\n> > --\n> > Cheers,\n> >\n> > David\n> >\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n",
          "reply_to": "Kalesh Singh"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The reviewer agrees that using PAGE_SIZE without reason is a problem and notes that userspace can map memory at PTE_SIZE granularity, but no specific technical concerns are raised.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "no clear signal"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Fri, Feb 20, 2026 at 09:04:09AM +0000, David Laight wrote:\n> On Thu, 19 Feb 2026 15:08:51 +0000\n> Kiryl Shutsemau <kas@kernel.org> wrote:\n> \n> > No, there's no new hardware (that I know of). I want to explore what page size\n> > means.\n> > \n> > The kernel uses the same value - PAGE_SIZE - for two things:\n> > \n> >   - the order-0 buddy allocation size;\n> > \n> >   - the granularity of virtual address space mapping;\n> \n> Also the 'random' buffers that are PAGE_SIZE rather than 4k.\n\nYeah, in some places we use PAGE_SIZE just because without any reason.\n\n> I also wonder how is affects mmap of kernel memory and the alignement\n> of PCIe windows (etc).\n\nKernel, as userspace, is free to map memory PTE_SIZE granularity.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n",
          "reply_to": "David Laight"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The reviewer raised concerns about the patch, questioning its primary motivation and suggesting that smaller machines won't benefit from 64k pages.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "lacking clear technical concerns"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Thu, Feb 19, 2026 at 10:28:20PM -0500, Liam R. Howlett wrote:\n> * Kiryl Shutsemau <kas@kernel.org> [260219 17:05]:\n> > On Thu, Feb 19, 2026 at 09:08:57AM -0800, Dave Hansen wrote:\n> > > On 2/19/26 07:08, Kiryl Shutsemau wrote:\n> > > >   - The order-0 page size cuts struct page overhead by a factor of 16. From\n> > > >     ~1.6% of RAM to ~0.1%;\n> > > ...\n> > > But, it will mostly be getting better performance at the _cost_ of\n> > > consuming more RAM, not saving RAM.\n> > \n> > That's fair.\n> > \n> > The problem with struct page memory consumption is that it is static and\n> > cannot be reclaimed. You pay the struct page tax no matter what.\n> > \n> > Page cache rounding overhead can be large, but a motivated userspace can\n> > keep it under control by avoiding splitting a dataset into many small\n> > files. And this memory is reclaimable.\n> > \n> \n> But we are in reclaim a lot more these days.  As I'm sure you are aware,\n> we are trying to maximize the resources (both cpu and ram) of any\n> machine powered on.  Entering reclaim will consume the cpu time and will\n> affect other tasks.\n> \n> Especially with multiple workload machines, the tendency is to have a\n> primary focus with the lower desired work being killed, if necessary.\n> Reducing the overhead just means more secondary tasks, or a bigger\n> footprint of the ones already executing.\n> \n> Increasing the memory pressure will degrade the primary workload more\n> frequently, even if we recover enough to avoid OOMing the secondary.\n> \n> While in the struct page tax world, the secondary task would be killed\n> after a shorter (and less frequently executed) reclaim comes up short.\n> So, I would think that we would be degrading the primary workload in an\n> attempt to keep the secondary alive?  Maybe I'm over-simplifying here?\n\nI am not sure I fully follow your point.\n\nSizing tasks and scheduling tasks between machines is hard in general.\nI don't think the balance between struct page tax and page cache\nrounding overhead is going to be the primary factor.\n\n> Near the other end of the spectrum, we have chromebooks that are\n> constantly in reclaim, even with 4k pages.  I guess these machines would\n> be destine to maintain the same page size they use today.  That is, this\n> solution for the struct page tax is only useful if you have a lot of\n> memory.  But then again, that's where the bookkeeping costs become hard\n> to take.\n\nSmaller machines are not target for 64k pages. They will not benefit\nfrom them.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n\n\n---\n\nOn Fri, Feb 20, 2026 at 10:17:45AM -0500, Liam R. Howlett wrote:\n> * Kiryl Shutsemau <kas@kernel.org> [260220 07:33]:\n> > On Thu, Feb 19, 2026 at 10:28:20PM -0500, Liam R. Howlett wrote:\n> > > * Kiryl Shutsemau <kas@kernel.org> [260219 17:05]:\n> > > > On Thu, Feb 19, 2026 at 09:08:57AM -0800, Dave Hansen wrote:\n> > > > > On 2/19/26 07:08, Kiryl Shutsemau wrote:\n> > > > > >   - The order-0 page size cuts struct page overhead by a factor of 16. From\n> > > > > >     ~1.6% of RAM to ~0.1%;\n> > > > > ...\n> > > > > But, it will mostly be getting better performance at the _cost_ of\n> > > > > consuming more RAM, not saving RAM.\n> > > > \n> > > > That's fair.\n> > > > \n> > > > The problem with struct page memory consumption is that it is static and\n> > > > cannot be reclaimed. You pay the struct page tax no matter what.\n> > > > \n> > > > Page cache rounding overhead can be large, but a motivated userspace can\n> > > > keep it under control by avoiding splitting a dataset into many small\n> > > > files. And this memory is reclaimable.\n> > > > \n> > > \n> > > But we are in reclaim a lot more these days.  As I'm sure you are aware,\n> > > we are trying to maximize the resources (both cpu and ram) of any\n> > > machine powered on.  Entering reclaim will consume the cpu time and will\n> > > affect other tasks.\n> > > \n> > > Especially with multiple workload machines, the tendency is to have a\n> > > primary focus with the lower desired work being killed, if necessary.\n> > > Reducing the overhead just means more secondary tasks, or a bigger\n> > > footprint of the ones already executing.\n> > > \n> > > Increasing the memory pressure will degrade the primary workload more\n> > > frequently, even if we recover enough to avoid OOMing the secondary.\n> > > \n> > > While in the struct page tax world, the secondary task would be killed\n> > > after a shorter (and less frequently executed) reclaim comes up short.\n> > > So, I would think that we would be degrading the primary workload in an\n> > > attempt to keep the secondary alive?  Maybe I'm over-simplifying here?\n> > \n> > I am not sure I fully follow your point.\n> > \n> > Sizing tasks and scheduling tasks between machines is hard in general.\n> > I don't think the balance between struct page tax and page cache\n> > rounding overhead is going to be the primary factor.\n> \n> I think there are more trade offs than what you listed.  It's still\n> probably worth doing, but I wanted to know if you though that this would\n> cause us to spend more time in reclaim, which seems to be implied above.\n> So, another trade-off might be all the reclaim penalty being paid more\n> frequently?\n\nI am not sure.\n\nKernel would need to do less work in reclaim per unit of memory.\nDepending on workloads you might see less allocation events and\ntherefore less frequent reclaim.\n\nIt's all too hand-wavy at the stage.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov\n",
          "reply_to": "Liam Howlett"
        },
        {
          "author": "Kalesh Singh",
          "summary": "The reviewer, Kalesh Singh, raised a concern that the personality(2) call may be too late to establish larger VMA alignment, suggesting an early_param or prctl/personality flag for global enforcement and per-process opt-in.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Fri, Feb 20, 2026 at 4:10AM Kiryl Shutsemau <kas@kernel.org> wrote:\n>\n> On Thu, Feb 19, 2026 at 03:24:37PM -0800, Kalesh Singh wrote:\n> > On Thu, Feb 19, 2026 at 7:39AM David Hildenbrand (Arm)\n> > <david@kernel.org> wrote:\n> > >\n> > > On 2/19/26 16:08, Kiryl Shutsemau wrote:\n> > > > No, there's no new hardware (that I know of). I want to explore what page size\n> > > > means.\n> > > >\n> > > > The kernel uses the same value - PAGE_SIZE - for two things:\n> > > >\n> > > >    - the order-0 buddy allocation size;\n> > > >\n> > > >    - the granularity of virtual address space mapping;\n> > > >\n> > > > I think we can benefit from separating these two meanings and allowing\n> > > > order-0 allocations to be larger than the virtual address space covered by a\n> > > > PTE entry.\n> > > >\n> > > > The main motivation is scalability. Managing memory on multi-terabyte\n> > > > machines in 4k is suboptimal, to say the least.\n> > > >\n> > > > Potential benefits of the approach (assuming 64k pages):\n> > > >\n> > > >    - The order-0 page size cuts struct page overhead by a factor of 16. From\n> > > >      ~1.6% of RAM to ~0.1%;\n> > > >\n> > > >    - TLB wins on machines with TLB coalescing as long as mapping is naturally\n> > > >      aligned;\n> > > >\n> > > >    - Order-5 allocation is 2M, resulting in less pressure on the zone lock;\n> > > >\n> > > >    - 1G pages are within possibility for the buddy allocator - order-14\n> > > >      allocation. It can open the road to 1G THPs.\n> > > >\n> > > >    - As with THP, fewer pages - less pressure on the LRU lock;\n> > > >\n> > > >    - ...\n> > > >\n> > > > The trade-off is memory waste (similar to what we have on architectures with\n> > > > native 64k pages today) and complexity, mostly in the core-MM code.\n> > > >\n> > > > == Design considerations ==\n> > > >\n> > > > I want to split PAGE_SIZE into two distinct values:\n> > > >\n> > > >    - PTE_SIZE defines the virtual address space granularity;\n> > > >\n> > > >    - PG_SIZE defines the size of the order-0 buddy allocation;\n> > > >\n> > > > PAGE_SIZE is only defined if PTE_SIZE == PG_SIZE. It will flag which code\n> > > > requires conversion, and keep existing code working while conversion is in\n> > > > progress.\n> > > >\n> > > > The same split happens for other page-related macros: mask, shift,\n> > > > alignment helpers, etc.\n> > > >\n> > > > PFNs are in PTE_SIZE units.\n> > > >\n> > > > The buddy allocator and page cache (as well as all I/O) operate in PG_SIZE\n> > > > units.\n> > > >\n> > > > Userspace mappings are maintained with PTE_SIZE granularity. No ABI changes\n> > > > for userspace. But we might want to communicate PG_SIZE to userspace to\n> > > > get the optimal results for userspace that cares.\n> > > >\n> > > > PTE_SIZE granularity requires a substantial rework of page fault and VMA\n> > > > handling:\n> > > >\n> > > >    - A struct page pointer and pgprot_t are not enough to create a PTE entry.\n> > > >      We also need the offset within the page we are creating the PTE for.\n> > > >\n> > > >    - Since the VMA start can be aligned arbitrarily with respect to the\n> > > >      underlying page, vma->vm_pgoff has to be changed to vma->vm_pteoff,\n> > > >      which is in PTE_SIZE units.\n> > > >\n> > > >    - The page fault handler needs to handle PTE_SIZE < PG_SIZE, including\n> > > >      misaligned cases;\n> > > >\n> > > > Page faults into file mappings are relatively simple to handle as we\n> > > > always have the page cache to refer to. So you can map only the part of the\n> > > > page that fits in the page table, similarly to fault-around.\n> > > >\n> > > > Anonymous and file-CoW faults should also be simple as long as the VMA is\n> > > > aligned to PG_SIZE in both the virtual address space and with respect to\n> > > > vm_pgoff. We might waste some memory on the ends of the VMA, but it is\n> > > > tolerable.\n> > > >\n> > > > Misaligned anonymous and file-CoW faults are a pain. Specifically, mapping\n> > > > pages across a page table boundary. In the worst case, a page is mapped across\n> > > > a PGD entry boundary and PTEs for the page have to be put in two separate\n> > > > subtrees of page tables.\n> > > >\n> > > > A naive implementation would map different pages on different sides of a\n> > > > page table boundary and accept the waste of one page per page table crossing.\n> > > > The hope is that misaligned mappings are rare, but this is suboptimal.\n> > > >\n> > > > mremap(2) is the ultimate stress test for the design.\n> > > >\n> > > > On x86, page tables are allocated from the buddy allocator and if PG_SIZE\n> > > > is greater than 4 KB, we need a way to pack multiple page tables into a\n> > > > single page. We could use the slab allocator for this, but it would\n> > > > require relocating the page-table metadata out of struct page.\n> > >\n> > > When discussing per-process page sizes with Ryan and Dev, I mentioned\n> > > that having a larger emulated page size could be interesting for other\n> > > architectures as well.\n> > >\n> > > That is, we would emulate a 64K page size on Intel for user space as\n> > > well, but let the OS work with 4K pages.\n> > >\n> > > We'd only allocate+map large folios into user space + pagecache, but\n> > > still allow for page tables etc. to not waste memory.\n> > >\n> > > So \"most\" of your allocations in the system would actually be at least\n> > > 64k, reducing zone lock contention etc.\n> > >\n> > >\n> > > It doesn't solve all the problems you wanted to tackle on your list\n> > > (e.g., \"struct page\" overhead, which will be sorted out by memdescs).\n> >\n> > Hi Kiryl,\n> >\n> > I'd be interested to discuss this at LSFMM.\n> >\n> > On Android, we have a separate but related use case: we emulate the\n> > userspace page size on x86, primarily to enable app developers to\n> > conduct compatibility testing of their apps for 16KB Android devices.\n> > [1]\n> >\n> > It mainly works by enforcing a larger granularity on the VMAs to\n> > emulate a userspace page size, somewhat similar to what David\n> > mentioned, while the underlying kernel still operates on a 4KB\n> > granularity. [2]\n> >\n> > IIUC the current design would not enfore the larger granularity /\n> > alignment for VMAs to avoid breaking ABI. However, I'd be interest to\n> > discuss whether it can be extended to cover this usecase as well.\n>\n> I don't want to break ABI, but might add a knob (maybe personality(2) ?)\n> for enforcement to see what breaks.\n\nI think personality(2) may be too late? By the time a process invokes\nit, the initial userspace mappings (executable, linker for init, etc)\nare already established with the default granularity.\n\nTo handle this, I've been using an early_param to enforce the larger\nVMA alignment system-wide right from boot.\n\nPerhaps, something for global enforcement (Kconfig/early param) and a\nprctl/personality flag for per-process opt in?\n\n>\n> In general, I would prefer to advertise a new value to userspace that\n> would mean preferred virtual address space granularity.\n\nThis makes sense for maintaining ABI compatibility. Userspace\nallocators might want to optimize their layouts to match PG_SIZE while\nstill being able to operate at PTE_SIZE when needed.\n\n-- Kalesh\n\n>\n> >\n> > [1]  https://developer.android.com/guide/practices/page-sizes#16kb-emulator\n> > [2] https://source.android.com/docs/core/architecture/16kb-page-size/getting-started-cf-x86-64-pgagnostic\n> >\n> > Thanks,\n> > Kalesh\n> >\n> >\n> >\n> >\n> > >\n> > > --\n> > > Cheers,\n> > >\n> > > David\n> > >\n>\n> --\n>   Kiryl Shutsemau / Kirill A. Shutemov\n",
          "reply_to": "Kiryl Shutsemau"
        }
      ],
      "analysis_source": "llm"
    }
  }
}