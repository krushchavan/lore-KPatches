{
  "thread_id": "aZk5CVRELS2qo92c@gourry-fedora-PF4VCD3F",
  "subject": "Re: [PATCH v23 10/22] cxl: Export function for unwinding cxl by accelerators",
  "url": "https://lore.kernel.org/all/aZk5CVRELS2qo92c@gourry-fedora-PF4VCD3F/",
  "dates": {
    "2026-02-01": {
      "report_file": "2026-02-21_ollama_llama3.1-8b.html",
      "developer": "Gregory Price",
      "reviews": [
        {
          "author": "alejandro.lucero-palau (author)",
          "summary": "The author addressed a concern about differentiating between CXL memory expanders and device accelerators, explaining that they will add a new function for initializing cxl_dev_state and a macro to help accel drivers embed it in their private structs.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nDifferentiate CXL memory expanders (type 3) from CXL device accelerators\n(type 2) with a new function for initializing cxl_dev_state and a macro\nfor helping accel drivers to embed cxl_dev_state inside a private\nstruct.\n\nMove structs to include/cxl as the size of the accel driver private\nstruct embedding cxl_dev_state needs to know the size of this struct.\n\nUse same new initialization with the type3 pci driver.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Alison Schofield <alison.schofield@intel.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n---\n drivers/cxl/core/mbox.c      |  12 +-\n drivers/cxl/core/memdev.c    |  32 +++++\n drivers/cxl/cxl.h            |  97 +--------------\n drivers/cxl/cxlmem.h         |  86 +------------\n drivers/cxl/pci.c            |  14 +--\n include/cxl/cxl.h            | 226 +++++++++++++++++++++++++++++++++++\n tools/testing/cxl/test/mem.c |   3 +-\n 7 files changed, 274 insertions(+), 196 deletions(-)\n create mode 100644 include/cxl/cxl.h\n\ndiff --git a/drivers/cxl/core/mbox.c b/drivers/cxl/core/mbox.c\nindex fa6dd0c94656..bee84d0101d1 100644\n--- a/drivers/cxl/core/mbox.c\n+++ b/drivers/cxl/core/mbox.c\n@@ -1514,23 +1514,21 @@ int cxl_mailbox_init(struct cxl_mailbox *cxl_mbox, struct device *host)\n }\n EXPORT_SYMBOL_NS_GPL(cxl_mailbox_init, \"CXL\");\n \n-struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev)\n+struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev, u64 serial,\n+\t\t\t\t\t\t u16 dvsec)\n {\n \tstruct cxl_memdev_state *mds;\n \tint rc;\n \n-\tmds = devm_kzalloc(dev, sizeof(*mds), GFP_KERNEL);\n+\tmds = devm_cxl_dev_state_create(dev, CXL_DEVTYPE_CLASSMEM, serial,\n+\t\t\t\t\tdvsec, struct cxl_memdev_state, cxlds,\n+\t\t\t\t\ttrue);\n \tif (!mds) {\n \t\tdev_err(dev, \"No memory available\\n\");\n \t\treturn ERR_PTR(-ENOMEM);\n \t}\n \n \tmutex_init(&mds->event.log_lock);\n-\tmds->cxlds.dev = dev;\n-\tmds->cxlds.reg_map.host = dev;\n-\tmds->cxlds.cxl_mbox.host = dev;\n-\tmds->cxlds.reg_map.resource = CXL_RESOURCE_NONE;\n-\tmds->cxlds.type = CXL_DEVTYPE_CLASSMEM;\n \n \trc = devm_cxl_register_mce_notifier(dev, &mds->mce_notifier);\n \tif (rc == -EOPNOTSUPP)\ndiff --git a/drivers/cxl/core/memdev.c b/drivers/cxl/core/memdev.c\nindex af3d0cc65138..22d156f25305 100644\n--- a/drivers/cxl/core/memdev.c\n+++ b/drivers/cxl/core/memdev.c\n@@ -656,6 +656,38 @@ static void detach_memdev(struct work_struct *work)\n \n static struct lock_class_key cxl_memdev_key;\n \n+static void cxl_dev_state_init(struct cxl_dev_state *cxlds, struct device *dev,\n+\t\t\t       enum cxl_devtype type, u64 serial, u16 dvsec,\n+\t\t\t       bool has_mbox)\n+{\n+\t*cxlds = (struct cxl_dev_state) {\n+\t\t.dev = dev,\n+\t\t.type = type,\n+\t\t.serial = serial,\n+\t\t.cxl_dvsec = dvsec,\n+\t\t.reg_map.host = dev,\n+\t\t.reg_map.resource = CXL_RESOURCE_NONE,\n+\t};\n+\n+\tif (has_mbox)\n+\t\tcxlds->cxl_mbox.host = dev;\n+}\n+\n+struct cxl_dev_state *_devm_cxl_dev_state_create(struct device *dev,\n+\t\t\t\t\t\t enum cxl_devtype type,\n+\t\t\t\t\t\t u64 serial, u16 dvsec,\n+\t\t\t\t\t\t size_t size, bool has_mbox)\n+{\n+\tstruct cxl_dev_state *cxlds = devm_kzalloc(dev, size, GFP_KERNEL);\n+\n+\tif (!cxlds)\n+\t\treturn NULL;\n+\n+\tcxl_dev_state_init(cxlds, dev, type, serial, dvsec, has_mbox);\n+\treturn cxlds;\n+}\n+EXPORT_SYMBOL_NS_GPL(_devm_cxl_dev_state_create, \"CXL\");\n+\n static struct cxl_memdev *cxl_memdev_alloc(struct cxl_dev_state *cxlds,\n \t\t\t\t\t   const struct file_operations *fops,\n \t\t\t\t\t   const struct cxl_memdev_attach *attach)\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex e1d47062e1d3..3eaa353e430b 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -12,6 +12,7 @@\n #include <linux/node.h>\n #include <linux/io.h>\n #include <linux/range.h>\n+#include <cxl/cxl.h>\n \n extern const struct nvdimm_security_ops *cxl_security_ops;\n \n@@ -201,97 +202,6 @@ static inline int ways_to_eiw(unsigned int ways, u8 *eiw)\n #define   CXLDEV_MBOX_BG_CMD_COMMAND_VENDOR_MASK GENMASK_ULL(63, 48)\n #define CXLDEV_MBOX_PAYLOAD_OFFSET 0x20\n \n-/*\n- * Using struct_group() allows for per register-block-type helper routines,\n- * without requiring block-type agnostic code to include the prefix.\n- */\n-struct cxl_regs {\n-\t/*\n-\t * Common set of CXL Component register block base pointers\n-\t * @hdm_decoder: CXL 2.0 8.2.5.12 CXL HDM Decoder Capability Structure\n-\t * @ras: CXL 2.0 8.2.5.9 CXL RAS Capability Structure\n-\t */\n-\tstruct_group_tagged(cxl_component_regs, component,\n-\t\tvoid __iomem *hdm_decoder;\n-\t\tvoid __iomem *ras;\n-\t);\n-\t/*\n-\t * Common set of CXL Device register block base pointers\n-\t * @status: CXL 2.0 8.2.8.3 Device Status Registers\n-\t * @mbox: CXL 2.0 8.2.8.4 Mailbox Registers\n-\t * @memdev: CXL 2.0 8.2.8.5 Memory Device Registers\n-\t */\n-\tstruct_group_tagged(cxl_device_regs, device_regs,\n-\t\tvoid __iomem *status, *mbox, *memdev;\n-\t);\n-\n-\tstruct_group_tagged(cxl_pmu_regs, pmu_regs,\n-\t\tvoid __iomem *pmu;\n-\t);\n-\n-\t/*\n-\t * RCH downstream port specific RAS register\n-\t * @aer: CXL 3.0 8.2.1.1 RCH Downstream Port RCRB\n-\t */\n-\tstruct_group_tagged(cxl_rch_regs, rch_regs,\n-\t\tvoid __iomem *dport_aer;\n-\t);\n-\n-\t/*\n-\t * RCD upstream port specific PCIe cap register\n-\t * @pcie_cap: CXL 3.0 8.2.1.2 RCD Upstream Port RCRB\n-\t */\n-\tstruct_group_tagged(cxl_rcd_regs, rcd_regs,\n-\t\tvoid __iomem *rcd_pcie_cap;\n-\t);\n-};\n-\n-struct cxl_reg_map {\n-\tbool valid;\n-\tint id;\n-\tunsigned long offset;\n-\tunsigned long size;\n-};\n-\n-struct cxl_component_reg_map {\n-\tstruct cxl_reg_map hdm_decoder;\n-\tstruct cxl_reg_map ras;\n-};\n-\n-struct cxl_device_reg_map {\n-\tstruct cxl_reg_map status;\n-\tstruct cxl_reg_map mbox;\n-\tstruct cxl_reg_map memdev;\n-};\n-\n-struct cxl_pmu_reg_map {\n-\tstruct cxl_reg_map pmu;\n-};\n-\n-/**\n- * struct cxl_register_map - DVSEC harvested register block mapping parameters\n- * @host: device for devm operations and logging\n- * @base: virtual base of the register-block-BAR + @block_offset\n- * @resource: physical resource base of the register block\n- * @max_size: maximum mapping size to perform register search\n- * @reg_type: see enum cxl_regloc_type\n- * @component_map: cxl_reg_map for component registers\n- * @device_map: cxl_reg_maps for device registers\n- * @pmu_map: cxl_reg_maps for CXL Performance Monitoring Units\n- */\n-struct cxl_register_map {\n-\tstruct device *host;\n-\tvoid __iomem *base;\n-\tresource_size_t resource;\n-\tresource_size_t max_size;\n-\tu8 reg_type;\n-\tunion {\n-\t\tstruct cxl_component_reg_map component_map;\n-\t\tstruct cxl_device_reg_map device_map;\n-\t\tstruct cxl_pmu_reg_map pmu_map;\n-\t};\n-};\n-\n void cxl_probe_component_regs(struct device *dev, void __iomem *base,\n \t\t\t      struct cxl_component_reg_map *map);\n void cxl_probe_device_regs(struct device *dev, void __iomem *base,\n@@ -497,11 +407,6 @@ struct cxl_region_params {\n \tresource_size_t cache_size;\n };\n \n-enum cxl_partition_mode {\n-\tCXL_PARTMODE_RAM,\n-\tCXL_PARTMODE_PMEM,\n-};\n-\n /*\n  * Indicate whether this region has been assembled by autodetection or\n  * userspace assembly. Prevent endpoint decoders outside of automatic\ndiff --git a/drivers/cxl/cxlmem.h b/drivers/cxl/cxlmem.h\nindex ef202b34e5ea..281546de426e 100644\n--- a/drivers/cxl/cxlmem.h\n+++ b/drivers/cxl/cxlmem.h\n@@ -113,8 +113,6 @@ int devm_cxl_dpa_reserve(struct cxl_endpoint_decoder *cxled,\n \t\t\t resource_size_t base, resource_size_t len,\n \t\t\t resource_size_t skipped);\n \n-#define CXL_NR_PARTITIONS_MAX 2\n-\n struct cxl_dpa_info {\n \tu64 size;\n \tstruct cxl_dpa_part_info {\n@@ -373,87 +371,6 @@ struct cxl_security_state {\n \tstruct kernfs_node *sanitize_node;\n };\n \n-/*\n- * enum cxl_devtype - delineate type-2 from a generic type-3 device\n- * @CXL_DEVTYPE_DEVMEM - Vendor specific CXL Type-2 device implementing HDM-D or\n- *\t\t\t HDM-DB, no requirement that this device implements a\n- *\t\t\t mailbox, or other memory-device-standard manageability\n- *\t\t\t flows.\n- * @CXL_DEVTYPE_CLASSMEM - Common class definition of a CXL Type-3 device with\n- *\t\t\t   HDM-H and class-mandatory memory device registers\n- */\n-enum cxl_devtype {\n-\tCXL_DEVTYPE_DEVMEM,\n-\tCXL_DEVTYPE_CLASSMEM,\n-};\n-\n-/**\n- * struct cxl_dpa_perf - DPA performance property entry\n- * @dpa_range: range for DPA address\n- * @coord: QoS performance data (i.e. latency, bandwidth)\n- * @cdat_coord: raw QoS performance data from CDAT\n- * @qos_class: QoS Class cookies\n- */\n-struct cxl_dpa_perf {\n-\tstruct range dpa_range;\n-\tstruct access_coordinate coord[ACCESS_COORDINATE_MAX];\n-\tstruct access_coordinate cdat_coord[ACCESS_COORDINATE_MAX];\n-\tint qos_class;\n-};\n-\n-/**\n- * struct cxl_dpa_partition - DPA partition descriptor\n- * @res: shortcut to the partition in the DPA resource tree (cxlds->dpa_res)\n- * @perf: performance attributes of the partition from CDAT\n- * @mode: operation mode for the DPA capacity, e.g. ram, pmem, dynamic...\n- */\n-struct cxl_dpa_partition {\n-\tstruct resource res;\n-\tstruct cxl_dpa_perf perf;\n-\tenum cxl_partition_mode mode;\n-};\n-\n-/**\n- * struct cxl_dev_state - The driver device state\n- *\n- * cxl_dev_state represents the CXL driver/device state.  It provides an\n- * interface to mailbox commands as well as some cached data about the device.\n- * Currently only memory devices are represented.\n- *\n- * @dev: The device associated with this CXL state\n- * @cxlmd: The device representing the CXL.mem capabilities of @dev\n- * @reg_map: component and ras register mapping parameters\n- * @regs: Parsed register blocks\n- * @cxl_dvsec: Offset to the PCIe device DVSEC\n- * @rcd: operating in RCD mode (CXL 3.0 9.11.8 CXL Devices Attached to an RCH)\n- * @media_ready: Indicate whether the device media is usable\n- * @dpa_res: Overall DPA resource tree for the device\n- * @part: DPA partition array\n- * @nr_partitions: Number of DPA partitions\n- * @serial: PCIe Device Serial Number\n- * @type: Generic Memory Class device or Vendor Specific Memory device\n- * @cxl_mbox: CXL mailbox context\n- * @cxlfs: CXL features context\n- */\n-struct cxl_dev_state {\n-\tstruct device *dev;\n-\tstruct cxl_memdev *cxlmd;\n-\tstruct cxl_register_map reg_map;\n-\tstruct cxl_regs regs;\n-\tint cxl_dvsec;\n-\tbool rcd;\n-\tbool media_ready;\n-\tstruct resource dpa_res;\n-\tstruct cxl_dpa_partition part[CXL_NR_PARTITIONS_MAX];\n-\tunsigned int nr_partitions;\n-\tu64 serial;\n-\tenum cxl_devtype type;\n-\tstruct cxl_mailbox cxl_mbox;\n-#ifdef CONFIG_CXL_FEATURES\n-\tstruct cxl_features_state *cxlfs;\n-#endif\n-};\n-\n static inline resource_size_t cxl_pmem_size(struct cxl_dev_state *cxlds)\n {\n \t/*\n@@ -858,7 +775,8 @@ int cxl_dev_state_identify(struct cxl_memdev_state *mds);\n int cxl_await_media_ready(struct cxl_dev_state *cxlds);\n int cxl_enumerate_cmds(struct cxl_memdev_state *mds);\n int cxl_mem_dpa_fetch(struct cxl_memdev_state *mds, struct cxl_dpa_info *info);\n-struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev);\n+struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev, u64 serial,\n+\t\t\t\t\t\t u16 dvsec);\n void set_exclusive_cxl_commands(struct cxl_memdev_state *mds,\n \t\t\t\tunsigned long *cmds);\n void clear_exclusive_cxl_commands(struct cxl_memdev_state *mds,\ndiff --git a/drivers/cxl/pci.c b/drivers/cxl/pci.c\nindex 1cf232220873..24179cc702bf 100644\n--- a/drivers/cxl/pci.c\n+++ b/drivers/cxl/pci.c\n@@ -911,25 +911,25 @@ static int cxl_pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n \tint rc, pmu_count;\n \tunsigned int i;\n \tbool irq_avail;\n+\tu16 dvsec;\n \n \trc = pcim_enable_device(pdev);\n \tif (rc)\n \t\treturn rc;\n \tpci_set_master(pdev);\n \n-\tmds = cxl_memdev_state_create(&pdev->dev);\n+\tdvsec = pci_find_dvsec_capability(pdev, PCI_VENDOR_ID_CXL,\n+\t\t\t\t\t  PCI_DVSEC_CXL_DEVICE);\n+\tif (!dvsec)\n+\t\tpci_warn(pdev, \"Device DVSEC not present, skip CXL.mem init\\n\");\n+\n+\tmds = cxl_memdev_state_create(&pdev->dev, pci_get_dsn(pdev), dvsec);\n \tif (IS_ERR(mds))\n \t\treturn PTR_ERR(mds);\n \tcxlds = &mds->cxlds;\n \tpci_set_drvdata(pdev, cxlds);\n \n \tcxlds->rcd = is_cxl_restricted(pdev);\n-\tcxlds->serial = pci_get_dsn(pdev);\n-\tcxlds->cxl_dvsec = pci_find_dvsec_capability(\n-\t\tpdev, PCI_VENDOR_ID_CXL, PCI_DVSEC_CXL_DEVICE);\n-\tif (!cxlds->cxl_dvsec)\n-\t\tdev_warn(&pdev->dev,\n-\t\t\t \"Device DVSEC not present, skip CXL.mem init\\n\");\n \n \trc = cxl_pci_setup_regs(pdev, CXL_REGLOC_RBI_MEMDEV, &map);\n \tif (rc)\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nnew file mode 100644\nindex 000000000000..13d448686189\n--- /dev/null\n+++ b/include/cxl/cxl.h\n@@ -0,0 +1,226 @@\n+/* SPDX-License-Identifier: GPL-2.0 */\n+/* Copyright(c) 2020 Intel Corporation. */\n+/* Copyright(c) 2025 Advanced Micro Devices, Inc. */\n+\n+#ifndef __CXL_CXL_H__\n+#define __CXL_CXL_H__\n+\n+#include <linux/node.h>\n+#include <linux/ioport.h>\n+#include <cxl/mailbox.h>\n+\n+/**\n+ * enum cxl_devtype - delineate type-2 from a generic type-3 device\n+ * @CXL_DEVTYPE_DEVMEM: Vendor specific CXL Type-2 device implementing HDM-D or\n+ *\t\t\t HDM-DB, no requirement that this device implements a\n+ *\t\t\t mailbox, or other memory-device-standard manageability\n+ *\t\t\t flows.\n+ * @CXL_DEVTYPE_CLASSMEM: Common class definition of a CXL Type-3 device with\n+ *\t\t\t   HDM-H and class-mandatory memory device registers\n+ */\n+enum cxl_devtype {\n+\tCXL_DEVTYPE_DEVMEM,\n+\tCXL_DEVTYPE_CLASSMEM,\n+};\n+\n+struct device;\n+\n+/*\n+ * Using struct_group() allows for per register-block-type helper routines,\n+ * without requiring block-type agnostic code to include the prefix.\n+ */\n+struct cxl_regs {\n+\t/*\n+\t * Common set of CXL Component register block base pointers\n+\t * @hdm_decoder: CXL 2.0 8.2.5.12 CXL HDM Decoder Capability Structure\n+\t * @ras: CXL 2.0 8.2.5.9 CXL RAS Capability Structure\n+\t */\n+\tstruct_group_tagged(cxl_component_regs, component,\n+\t\tvoid __iomem *hdm_decoder;\n+\t\tvoid __iomem *ras;\n+\t);\n+\t/*\n+\t * Common set of CXL Device register block base pointers\n+\t * @status: CXL 2.0 8.2.8.3 Device Status Registers\n+\t * @mbox: CXL 2.0 8.2.8.4 Mailbox Registers\n+\t * @memdev: CXL 2.0 8.2.8.5 Memory Device Registers\n+\t */\n+\tstruct_group_tagged(cxl_device_regs, device_regs,\n+\t\tvoid __iomem *status, *mbox, *memdev;\n+\t);\n+\n+\tstruct_group_tagged(cxl_pmu_regs, pmu_regs,\n+\t\tvoid __iomem *pmu;\n+\t);\n+\n+\t/*\n+\t * RCH downstream port specific RAS register\n+\t * @aer: CXL 3.0 8.2.1.1 RCH Downstream Port RCRB\n+\t */\n+\tstruct_group_tagged(cxl_rch_regs, rch_regs,\n+\t\tvoid __iomem *dport_aer;\n+\t);\n+\n+\t/*\n+\t * RCD upstream port specific PCIe cap register\n+\t * @pcie_cap: CXL 3.0 8.2.1.2 RCD Upstream Port RCRB\n+\t */\n+\tstruct_group_tagged(cxl_rcd_regs, rcd_regs,\n+\t\tvoid __iomem *rcd_pcie_cap;\n+\t);\n+};\n+\n+struct cxl_reg_map {\n+\tbool valid;\n+\tint id;\n+\tunsigned long offset;\n+\tunsigned long size;\n+};\n+\n+struct cxl_component_reg_map {\n+\tstruct cxl_reg_map hdm_decoder;\n+\tstruct cxl_reg_map ras;\n+};\n+\n+struct cxl_device_reg_map {\n+\tstruct cxl_reg_map status;\n+\tstruct cxl_reg_map mbox;\n+\tstruct cxl_reg_map memdev;\n+};\n+\n+struct cxl_pmu_reg_map {\n+\tstruct cxl_reg_map pmu;\n+};\n+\n+/**\n+ * struct cxl_register_map - DVSEC harvested register block mapping parameters\n+ * @host: device for devm operations and logging\n+ * @base: virtual base of the register-block-BAR + @block_offset\n+ * @resource: physical resource base of the register block\n+ * @max_size: maximum mapping size to perform register search\n+ * @reg_type: see enum cxl_regloc_type\n+ * @component_map: cxl_reg_map for component registers\n+ * @device_map: cxl_reg_maps for device registers\n+ * @pmu_map: cxl_reg_maps for CXL Performance Monitoring Units\n+ */\n+struct cxl_register_map {\n+\tstruct device *host;\n+\tvoid __iomem *base;\n+\tresource_size_t resource;\n+\tresource_size_t max_size;\n+\tu8 reg_type;\n+\tunion {\n+\t\tstruct cxl_component_reg_map component_map;\n+\t\tstruct cxl_device_reg_map device_map;\n+\t\tstruct cxl_pmu_reg_map pmu_map;\n+\t};\n+};\n+\n+/**\n+ * struct cxl_dpa_perf - DPA performance property entry\n+ * @dpa_range: range for DPA address\n+ * @coord: QoS performance data (i.e. latency, bandwidth)\n+ * @cdat_coord: raw QoS performance data from CDAT\n+ * @qos_class: QoS Class cookies\n+ */\n+struct cxl_dpa_perf {\n+\tstruct range dpa_range;\n+\tstruct access_coordinate coord[ACCESS_COORDINATE_MAX];\n+\tstruct access_coordinate cdat_coord[ACCESS_COORDINATE_MAX];\n+\tint qos_class;\n+};\n+\n+enum cxl_partition_mode {\n+\tCXL_PARTMODE_RAM,\n+\tCXL_PARTMODE_PMEM,\n+};\n+\n+/**\n+ * struct cxl_dpa_partition - DPA partition descriptor\n+ * @res: shortcut to the partition in the DPA resource tree (cxlds->dpa_res)\n+ * @perf: performance attributes of the partition from CDAT\n+ * @mode: operation mode for the DPA capacity, e.g. ram, pmem, dynamic...\n+ */\n+struct cxl_dpa_partition {\n+\tstruct resource res;\n+\tstruct cxl_dpa_perf perf;\n+\tenum cxl_partition_mode mode;\n+};\n+\n+#define CXL_NR_PARTITIONS_MAX 2\n+\n+/**\n+ * struct cxl_dev_state - The driver device state\n+ *\n+ * cxl_dev_state represents the CXL driver/device state.  It provides an\n+ * interface to mailbox commands as well as some cached data about the device.\n+ * Currently only memory devices are represented.\n+ *\n+ * @dev: The device associated with this CXL state\n+ * @cxlmd: The device representing the CXL.mem capabilities of @dev\n+ * @reg_map: component and ras register mapping parameters\n+ * @regs: Parsed register blocks\n+ * @cxl_dvsec: Offset to the PCIe device DVSEC\n+ * @rcd: operating in RCD mode (CXL 3.0 9.11.8 CXL Devices Attached to an RCH)\n+ * @media_ready: Indicate whether the device media is usable\n+ * @dpa_res: Overall DPA resource tree for the device\n+ * @part: DPA partition array\n+ * @nr_partitions: Number of DPA partitions\n+ * @serial: PCIe Device Serial Number\n+ * @type: Generic Memory Class device or Vendor Specific Memory device\n+ * @cxl_mbox: CXL mailbox context\n+ * @cxlfs: CXL features context\n+ */\n+struct cxl_dev_state {\n+\t/* public for Type2 drivers */\n+\tstruct device *dev;\n+\tstruct cxl_memdev *cxlmd;\n+\n+\t/* private for Type2 drivers */\n+\tstruct cxl_register_map reg_map;\n+\tstruct cxl_regs regs;\n+\tint cxl_dvsec;\n+\tbool rcd;\n+\tbool media_ready;\n+\tstruct resource dpa_res;\n+\tstruct cxl_dpa_partition part[CXL_NR_PARTITIONS_MAX];\n+\tunsigned int nr_partitions;\n+\tu64 serial;\n+\tenum cxl_devtype type;\n+\tstruct cxl_mailbox cxl_mbox;\n+#ifdef CONFIG_CXL_FEATURES\n+\tstruct cxl_features_state *cxlfs;\n+#endif\n+};\n+\n+struct cxl_dev_state *_devm_cxl_dev_state_create(struct device *dev,\n+\t\t\t\t\t\t enum cxl_devtype type,\n+\t\t\t\t\t\t u64 serial, u16 dvsec,\n+\t\t\t\t\t\t size_t size, bool has_mbox);\n+\n+/**\n+ * cxl_dev_state_create - safely create and cast a cxl dev state embedded in a\n+ * driver specific struct.\n+ *\n+ * @parent: device behind the request\n+ * @type: CXL device type\n+ * @serial: device identification\n+ * @dvsec: dvsec capability offset\n+ * @drv_struct: driver struct embedding a cxl_dev_state struct\n+ * @member: drv_struct member as cxl_dev_state\n+ * @mbox: true if mailbox supported\n+ *\n+ * Returns a pointer to the drv_struct allocated and embedding a cxl_dev_state\n+ * struct initialized.\n+ *\n+ * Introduced for Type2 driver support.\n+ */\n+#define devm_cxl_dev_state_create(parent, type, serial, dvsec, drv_struct, member, mbox)\t\\\n+\t({\t\t\t\t\t\t\t\t\t\t\\\n+\t\tstatic_assert(__same_type(struct cxl_dev_state,\t\t\t\t\\\n+\t\t\t      ((drv_struct *)NULL)->member));\t\t\t\t\\\n+\t\tstatic_assert(offsetof(drv_struct, member) == 0);\t\t\t\\\n+\t\t(drv_struct *)_devm_cxl_dev_state_create(parent, type, serial, dvsec,\t\\\n+\t\t\t\t\t\t      sizeof(drv_struct), mbox);\t\\\n+\t})\n+#endif /* __CXL_CXL_H__ */\ndiff --git a/tools/testing/cxl/test/mem.c b/tools/testing/cxl/test/mem.c\nindex cb87e8c0e63c..79f42f4474d4 100644\n--- a/tools/testing/cxl/test/mem.c\n+++ b/tools/testing/cxl/test/mem.c\n@@ -1716,7 +1716,7 @@ static int cxl_mock_mem_probe(struct platform_device *pdev)\n \tif (rc)\n \t\treturn rc;\n \n-\tmds = cxl_memdev_state_create(dev);\n+\tmds = cxl_memdev_state_create(dev, pdev->id + 1, 0);\n \tif (IS_ERR(mds))\n \t\treturn PTR_ERR(mds);\n \n@@ -1732,7 +1732,6 @@ static int cxl_mock_mem_probe(struct platform_device *pdev)\n \tmds->event.buf = (struct cxl_get_event_payload *) mdata->event_buf;\n \tINIT_DELAYED_WORK(&mds->security.poll_dwork, cxl_mockmem_sanitize_work);\n \n-\tcxlds->serial = pdev->id + 1;\n \tif (is_rcd(pdev))\n \t\tcxlds->rcd = true;\n \n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-02-01"
        },
        {
          "author": "alejandro.lucero-palau (author)",
          "summary": "The author is addressing a concern about the dependency on Smita's patchset, specifically [PATCH v5 3/7] cxl/region: Skip decoder reset on detach for autodiscovered regions. The author confirms that this patch is needed to support the default behavior with current BIOS configuration and explains that it will be supported in follow-up works.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nThis patchset should be applied on the cxl next branch using the base\nspecified at the end of this cover letter.\n\nDependencies on Dan's work has gone and also on Terry's as the only\npatch required is now in next. The other dependency is on Smita patchset\nbut it does not exist such a dependency as that work will not avoid the\nproblem with Type2 and DAX/hmem if soft reserved memory. This needs to\nbe solved by the BIOS and Type2 UEFI driver for populating the CXL.mem\nrange as EFI_RESERVED_TYPE instead of default EFI_CONVENTIONAL_MEMORY\nwith the EFI_MEMORY_SP attribute. There exists though a dependency on\none Smita's patches:\n\n[PATCH v5 3/7] cxl/region: Skip decoder reset on detach for autodiscovered regions\n\nThis is needed for the default behaviour with current BIOS configuration\nwhere the HDM Type2 decoders will be kept unreset when driver unloads.\nThis is the main change introduced in v23: committed decoders will not\nbe reset. Previous v22 functionality supported first driver load finding\ncommitted decoders but resetting them at unload and supporting\nuncommitted decoders in next driver loads. This will be suported in\nfollow-up works.\n\nv23 changes:\n\n  patch 11: fixing minor issues and droping change in\n\t    should_emulate_decoders (Jonathan Cameron)\n\n  patch13: refactoring unregister_region for safety type in Type2 API\n\n  sfc changes: slight modifications to error path\n\n\nv22 changes:\n\n  patch 1-3 from Dan's branch without any changes.\n\n  patch 11: new\n  \n  patch 12: moved here from v21 patch 22\n\n  patch 13-14: new\n\n  patch 23: move check ahead of type3 only checks\n\n  All patches with sfc changes adapted to support both options.\n\nv21 changes;\n\n  patch1-2: v20 patch1 splitted up doing the code move in the second\n\t    patch in v21. (Jonathan)\n \n  patch1-4: adding my Signed-off tag along with Dan's\n\n  patch5: fix duplication of CXL_NR_PARTITION definition\n\n  patch7: dropped the cxl test fixes removing unused function. It was\n\t  sent independently ahead of this version.\n\n  patch12: optimization for max free space calculation (Jonathan)\n\n  patch19: optimization for returning on error (Jonathan)\n\n\nv20 changes:\n\n  patch 1: using release helps (Jonathan).\n\n  patch 6: minor fix in comments (Jonathan).\n\n  patch 7 & 8: change commit mentioning sfc changes\n\n  patch 11:\t Fix interleave_ways setting (Jonathan)\n\t\tChange assignament location (Dave)\n\n  patch 13:  \tchanging error return order (Jonathan)\n\t\tremoving blank line (Dave)\n\n  patch 18:\tAdd check for only supporting uncommitted decoders\n\t\t\t(Ben, Dave)\n\t\tAdd check for returned value (Dave)\n\nv19 changes:\n\n  Removal of cxl_acquire_endpoint and driver callback for unexpected cxl\n  module removal. Dan's patches made them unnecessary.\n\n  patch 4: remove code already moved by Terry's patches (Ben Cheatham)\n\n  patch 6: removed unrelated change (Ben Cheatham)\n\n  patch 7: fix error report inconsistencies (Jonathan, Dave)\n\n  patch 9: remove unnecessary comment (Ben Cheatham)\n\n  patch 11: fix __free usage (Jonathan Cameron, Ben Cheatham)\n\n  patch 13: style fixes (Jonathan Cameron, Dave Jiag)\n\n  patch 14: move code to previous patch (Jonathan Cameron)\n\n  patch 18: group code in one locking (Dave Jian)\n\t    use __free helper (Ben Cheatham)\n\n\nv18 changes:\n\n  patch 1: minor changes and fixing docs generation (Jonathan, Dan)\n \n  patch4: merged with v17 patch5\n\n  patch 5: merging v17 patches 6 and 7\n\n  patch 6: adding helpers for clarity\n\n  patch 9:\n\t- minor changes (Dave)\n\t- simplifying flags check (Dan)\n\n  patch 10: minor changes (Jonathan)\n\n  patch 11:\n\t- minor changes (Dave)\n\t- fix mess (Jonathan, Dave)\n\n  patch 18: minor changes (Jonathan, Dan)\n  \nv17 changes: (Dan Williams review)\n - use devm for cxl_dev_state allocation\n - using current cxl struct for checking capability registers found by\n   the driver.\n - simplify dpa initialization without a mailbox not supporting pmem\n - add cxl_acquire_endpoint for protection during initialization\n - add callback/action to cxl_create_region for a driver notified about cxl\n   core kernel modules removal.\n - add sfc function to disable CXL-based PIO buffers if such a callback\n   is invoked.\n - Always manage a Type2 created region as private not allowing DAX.\n\nv16 changes:\n - rebase against rc4 (Dave Jiang)\n - remove duplicate line (Ben Cheatham)\n\nv15 changes:\n - remove reference to unused header file (Jonathan Cameron)\n - add proper kernel docs to exported functions (Alison Schofield)\n - using an array to map the enums to strings (Alison Schofield)\n - clarify comment when using bitmap_subset (Jonathan Cameron)\n - specify link to type2 support in all patches (Alison Schofield)\n\n  Patches changed (minor): 4, 11\n\nv14 changes:\n - static null initialization of bitmaps (Jonathan Cameron)\n - Fixing cxl tests (Alison Schofield)\n - Fixing robot compilation problems\n\n  Patches changed (minor): 1, 4, 6, 13\n\nv13 changes:\n - using names for headers checking more consistent (Jonathan Cameron)\n - using helper for caps bit setting (Jonathan Cameron)\n - provide generic function for reporting missing capabilities (Jonathan Cameron)\n - rename cxl_pci_setup_memdev_regs to cxl_pci_accel_setup_memdev_regs (Jonathan Cameron)\n - cxl_dpa_info size to be set by the Type2 driver (Jonathan Cameron)\n - avoiding rc variable when possible (Jonathan Cameron)\n - fix spelling (Simon Horman)\n - use scoped_guard (Dave Jiang)\n - use enum instead of bool (Dave Jiang)\n - dropping patch with hardware symbols\n\nv12 changes:\n - use new macro cxl_dev_state_create in pci driver (Ben Cheatham)\n - add public/private sections in now exported cxl_dev_state struct (Ben\n   Cheatham)\n - fix cxl/pci.h regarding file name for checking if defined\n - Clarify capabilities found vs expected in error message. (Ben\n   Cheatham)\n - Clarify new CXL_DECODER_F flag (Ben Cheatham)\n - Fix changes about cxl memdev creation support moving code to the\n   proper patch. (Ben Cheatham)\n - Avoid debug and function duplications (Ben Cheatham)\n\nv11 changes:\n - Dropping the use of cxl_memdev_state and going back to using\n   cxl_dev_state.\n - Using a helper for an accel driver to allocate its own cxl-related\n   struct embedding cxl_dev_state.\n - Exporting the required structs in include/cxl/cxl.h for an accel\n   driver being able to know the cxl_dev_state size required in the\n   previously mentioned helper for allocation.\n - Avoid using any struct for dpa initialization by the accel driver\n   adding a specific function for creating dpa partitions by accel\n   drivers without a mailbox.\n\nv10 changes:\n - Using cxl_memdev_state instead of cxl_dev_state for type2 which has a\n   memory after all and facilitates the setup.\n - Adapt core for using cxl_memdev_state allowing accel drivers to work\n   with them without further awareness of internal cxl structs.\n - Using last DPA changes for creating DPA partitions with accel driver\n   hardcoding mds values when no mailbox.\n - capabilities not a new field but built up when current register maps\n   is performed and returned to the caller for checking.\n - HPA free space supporting interleaving.\n - DPA free space droping max-min for a simple alloc size.\n\nv9 changes:\n - adding forward definitions (Jonathan Cameron)\n - using set_bit instead of bitmap_set (Jonathan Cameron)\n - fix rebase problem (Jonathan Cameron)\n - Improve error path (Jonathan Cameron)\n - fix build problems with cxl region dependency (robot)\n - fix error path (Simon Horman)\n\nv8 changes:\n - Change error path labeling inside sfc cxl code (Edward Cree)\n - Properly handling checks and error in sfc cxl code (Simon Horman)\n - Fix bug when checking resource_size (Simon Horman)\n - Avoid bisect problems reordering patches (Edward Cree)\n - Fix buffer allocation size in sfc (Simon Horman)\n\nv7 changes:\n\n - fixing kernel test robot complains\n - fix type with Type3 mandatory capabilities (Zhi Wang)\n - optimize code in cxl_request_resource (Kalesh Anakkur Purayil)\n - add sanity check when dealing with resources arithmetics (Fan Ni)\n - fix typos and blank lines (Fan Ni)\n - keep previous log errors/warnings in sfc driver (Martin Habets)\n - add WARN_ON_ONCE if region given is NULL\n\nv6 changes:\n\n - update sfc mcdi_pcol.h with full hardware changes most not related to\n   this patchset. This is an automatic file created from hardware design\n   changes and not touched by software. It is updated from time to time\n   and it required update for the sfc driver CXL support.\n - remove CXL capabilities definitions not used by the patchset or\n   previous kernel code. (Dave Jiang, Jonathan Cameron)\n - Use bitmap_subset instead of reinventing the wheel ... (Ben Cheatham)\n - Use cxl_accel_memdev for new device_type created (Ben Cheatham)\n - Fix construct_region use of rwsem (Zhi Wang)\n - Obtain region range instead of region params (Allison Schofield, Dave\n   Jiang)\n\nv5 changes:\n\n - Fix SFC configuration based on kernel CXL configuration\n - Add subset check for capabilities.\n - fix region creation when HDM decoders programmed by firmware/BIOS (Ben\n   Cheatham)\n - Add option for creating dax region based on driver decission (Ben\n   Cheatham)\n - Using sfc probe_data struct for keeping sfc cxl data\n\nv4 changes:\n\n - Use bitmap for capabilities new field (Jonathan Cameron)\n - Use cxl_mem attributes for sysfs based on device type (Dave Jian)\n - Add conditional cxl sfc compilation relying on kernel CXL config (kernel test robot)\n - Add sfc changes in different patches for facilitating backport (Jonathan Cameron)\n - Remove patch for dealing with cxl modules dependencies and using sfc kconfig plus\n   MODULE_SOFTDEP instead.\n\nv3 changes:\n\n - cxl_dev_state not defined as opaque but only manipulated by accel drivers\n   through accessors.\n - accessors names not identified as only for accel drivers.\n - move pci code from pci driver (drivers/cxl/pci.c) to generic pci code\n   (drivers/cxl/core/pci.c).\n - capabilities field from u8 to u32 and initialised by CXL regs discovering\n   code.\n - add capabilities check and removing current check by CXL regs discovering\n   code.\n - Not fail if CXL Device Registers not found. Not mandatory for Type2.\n - add timeout in acquire_endpoint for solving a race with the endpoint port\n   creation.\n - handle EPROBE_DEFER by sfc driver.\n - Limiting interleave ways to 1 for accel driver HPA/DPA requests.\n - factoring out interleave ways and granularity helpers from type2 region\n   creation patch.\n - restricting region_creation for type2 to one endpoint decoder.\n\nv2 changes:\n\nI have removed the introduction about the concerns with BIOS/UEFI after the\ndiscussion leading to confirm the need of the functionality implemented, at\nleast is some scenarios.\n\nThere are two main changes from the RFC:\n\n1) Following concerns about drivers using CXL core without restrictions, the CXL\nstruct to work with is opaque to those drivers, therefore functions are\nimplemented for modifying or reading those structs indirectly.\n\n2) The driver for using the added functionality is not a test driver but a real\none: the SFC ethernet network driver. It uses the CXL region mapped for PIO\nbuffers instead of regions inside PCIe BARs.\n\nRFC:\n\nCurrent CXL kernel code is focused on supporting Type3 CXL devices, aka memory\nexpanders. Type2 CXL devices, aka device accelerators, share some functionalities\nbut require some special handling.\n\nFirst of all, Type2 are by definition specific to drivers doing something and not just\na memory expander, so it is expected to work with the CXL specifics. This implies the CXL\nsetup needs to be done by such a driver instead of by a generic CXL PCI driver\nas for memory expanders. Most of such setup needs to use current CXL core code\nand therefore needs to be accessible to those vendor drivers. This is accomplished\nexporting opaque CXL structs and adding and exporting functions for working with\nthose structs indirectly.\n\nSome of the patches are based on a patchset sent by Dan Williams [1] which was just\npartially integrated, most related to making things ready for Type2 but none\nrelated to specific Type2 support. Those patches based on Dans work have Dans\nsigning as co-developer, and a link to the original patch.\n\nA final note about CXL.cache is needed. This patchset does not cover it at all,\nalthough the emulated Type2 device advertises it. From the kernel point of view\nsupporting CXL.cache will imply to be sure the CXL path supports what the Type2\ndevice needs. A device accelerator will likely be connected to a Root Switch,\nbut other configurations can not be discarded. Therefore the kernel will need to\ncheck not just HPA, DPA, interleave and granularity, but also the available\nCXL.cache support and resources in each switch in the CXL path to the Type2\ndevice. I expect to contribute to this support in the following months, and\nit would be good to discuss about it when possible.\n\n[1] https://lore.kernel.org/linux-cxl/98b1f61a-e6c2-71d4-c368-50d958501b0c@intel.com/T/\n\nAlejandro Lucero (22):\n  cxl: Add type2 device basic support\n  sfc: add cxl support\n  cxl: Move pci generic code\n  cxl/sfc: Map cxl component regs\n  cxl/sfc: Initialize dpa without a mailbox\n  cxl: Prepare memdev creation for type2\n  sfc: create type2 cxl memdev\n  cxl/hdm: Add support for getting region from committed decoder\n  cxl: Add function for obtaining region range\n  cxl: Export function for unwinding cxl by accelerators\n  sfc: obtain decoder and region if committed by firmware\n  cxl: Define a driver interface for HPA free space enumeration\n  sfc: get root decoder\n  cxl: Define a driver interface for DPA allocation\n  sfc: get endpoint decoder\n  cxl: Make region type based on endpoint type\n  cxl/region: Factor out interleave ways setup\n  cxl/region: Factor out interleave granularity setup\n  cxl: Allow region creation by type2 drivers\n  cxl: Avoid dax creation for accelerators\n  sfc: create cxl region\n  sfc: support pio mapping based on cxl\n\n drivers/cxl/core/core.h               |   5 +-\n drivers/cxl/core/hdm.c                | 123 ++++++++\n drivers/cxl/core/mbox.c               |  63 +---\n drivers/cxl/core/memdev.c             | 113 ++++++-\n drivers/cxl/core/pci.c                |  63 ++++\n drivers/cxl/core/port.c               |   1 +\n drivers/cxl/core/region.c             | 434 +++++++++++++++++++++++---\n drivers/cxl/core/regs.c               |   2 +-\n drivers/cxl/cxl.h                     | 125 +-------\n drivers/cxl/cxlmem.h                  |  92 +-----\n drivers/cxl/cxlpci.h                  |  21 +-\n drivers/cxl/mem.c                     |  45 ++-\n drivers/cxl/pci.c                     |  85 +----\n drivers/net/ethernet/sfc/Kconfig      |  10 +\n drivers/net/ethernet/sfc/Makefile     |   1 +\n drivers/net/ethernet/sfc/ef10.c       |  50 ++-\n drivers/net/ethernet/sfc/efx.c        |  15 +-\n drivers/net/ethernet/sfc/efx_cxl.c    | 186 +++++++++++\n drivers/net/ethernet/sfc/efx_cxl.h    |  41 +++\n drivers/net/ethernet/sfc/net_driver.h |  12 +\n drivers/net/ethernet/sfc/nic.h        |   3 +\n include/cxl/cxl.h                     | 287 +++++++++++++++++\n include/cxl/pci.h                     |  21 ++\n tools/testing/cxl/test/mem.c          |   3 +-\n 24 files changed, 1376 insertions(+), 425 deletions(-)\n create mode 100644 drivers/net/ethernet/sfc/efx_cxl.c\n create mode 100644 drivers/net/ethernet/sfc/efx_cxl.h\n create mode 100644 include/cxl/cxl.h\n create mode 100644 include/cxl/pci.h\n\n\nbase-commit: 3f7938b1aec7f06d5b23adca83e4542fcf027001\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-02-01"
        },
        {
          "author": "alejandro.lucero-palau (author)",
          "summary": "The author addressed a concern about the export of cxl core functions for Type2 driver discovery and mapping, explained that they are being used in sfc driver cxl initialization, and confirmed that this is the correct approach.",
          "sentiment": "positive",
          "sentiment_signals": [
            "confirmed_correct_approach"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nExport cxl core functions for a Type2 driver being able to discover and\nmap the device component registers.\n\nUse it in sfc driver cxl initialization.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Dan Williams <dan.j.williams@intel.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n---\n drivers/cxl/core/pci.c             |  1 +\n drivers/cxl/core/port.c            |  1 +\n drivers/cxl/core/regs.c            |  1 +\n drivers/cxl/cxl.h                  |  7 ------\n drivers/cxl/cxlpci.h               | 12 ----------\n drivers/cxl/pci.c                  |  1 +\n drivers/net/ethernet/sfc/efx_cxl.c | 35 ++++++++++++++++++++++++++++++\n include/cxl/cxl.h                  | 19 ++++++++++++++++\n include/cxl/pci.h                  | 21 ++++++++++++++++++\n 9 files changed, 79 insertions(+), 19 deletions(-)\n create mode 100644 include/cxl/pci.h\n\ndiff --git a/drivers/cxl/core/pci.c b/drivers/cxl/core/pci.c\nindex 6b7e50858d56..ba2d393c540a 100644\n--- a/drivers/cxl/core/pci.c\n+++ b/drivers/cxl/core/pci.c\n@@ -6,6 +6,7 @@\n #include <linux/delay.h>\n #include <linux/pci.h>\n #include <linux/pci-doe.h>\n+#include <cxl/pci.h>\n #include <linux/aer.h>\n #include <cxlpci.h>\n #include <cxlmem.h>\ndiff --git a/drivers/cxl/core/port.c b/drivers/cxl/core/port.c\nindex 54f72452fb06..385588b8b30b 100644\n--- a/drivers/cxl/core/port.c\n+++ b/drivers/cxl/core/port.c\n@@ -11,6 +11,7 @@\n #include <linux/idr.h>\n #include <linux/node.h>\n #include <cxl/einj.h>\n+#include <cxl/pci.h>\n #include <cxlmem.h>\n #include <cxlpci.h>\n #include <cxl.h>\ndiff --git a/drivers/cxl/core/regs.c b/drivers/cxl/core/regs.c\nindex 93710cf4f0a6..20c2d9fbcfe7 100644\n--- a/drivers/cxl/core/regs.c\n+++ b/drivers/cxl/core/regs.c\n@@ -4,6 +4,7 @@\n #include <linux/device.h>\n #include <linux/slab.h>\n #include <linux/pci.h>\n+#include <cxl/pci.h>\n #include <cxlmem.h>\n #include <cxlpci.h>\n #include <pmu.h>\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex 5d111980d879..944c5d1ccceb 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -39,10 +39,6 @@ extern const struct nvdimm_security_ops *cxl_security_ops;\n #define   CXL_CM_CAP_HDR_ARRAY_SIZE_MASK GENMASK(31, 24)\n #define CXL_CM_CAP_PTR_MASK GENMASK(31, 20)\n \n-#define   CXL_CM_CAP_CAP_ID_RAS 0x2\n-#define   CXL_CM_CAP_CAP_ID_HDM 0x5\n-#define   CXL_CM_CAP_CAP_HDM_VERSION 1\n-\n /* HDM decoders CXL 2.0 8.2.5.12 CXL HDM Decoder Capability Structure */\n #define CXL_HDM_DECODER_CAP_OFFSET 0x0\n #define   CXL_HDM_DECODER_COUNT_MASK GENMASK(3, 0)\n@@ -206,9 +202,6 @@ void cxl_probe_component_regs(struct device *dev, void __iomem *base,\n \t\t\t      struct cxl_component_reg_map *map);\n void cxl_probe_device_regs(struct device *dev, void __iomem *base,\n \t\t\t   struct cxl_device_reg_map *map);\n-int cxl_map_component_regs(const struct cxl_register_map *map,\n-\t\t\t   struct cxl_component_regs *regs,\n-\t\t\t   unsigned long map_mask);\n int cxl_map_device_regs(const struct cxl_register_map *map,\n \t\t\tstruct cxl_device_regs *regs);\n int cxl_map_pmu_regs(struct cxl_register_map *map, struct cxl_pmu_regs *regs);\ndiff --git a/drivers/cxl/cxlpci.h b/drivers/cxl/cxlpci.h\nindex d879120b2780..93df1b1fa326 100644\n--- a/drivers/cxl/cxlpci.h\n+++ b/drivers/cxl/cxlpci.h\n@@ -13,16 +13,6 @@\n  */\n #define CXL_PCI_DEFAULT_MAX_VECTORS 16\n \n-/* Register Block Identifier (RBI) */\n-enum cxl_regloc_type {\n-\tCXL_REGLOC_RBI_EMPTY = 0,\n-\tCXL_REGLOC_RBI_COMPONENT,\n-\tCXL_REGLOC_RBI_VIRT,\n-\tCXL_REGLOC_RBI_MEMDEV,\n-\tCXL_REGLOC_RBI_PMU,\n-\tCXL_REGLOC_RBI_TYPES\n-};\n-\n /*\n  * Table Access DOE, CDAT Read Entry Response\n  *\n@@ -106,6 +96,4 @@ static inline void cxl_dport_init_ras_reporting(struct cxl_dport *dport,\n \t\t\t\t\t\tstruct device *host) { }\n #endif\n \n-int cxl_pci_setup_regs(struct pci_dev *pdev, enum cxl_regloc_type type,\n-\t\t       struct cxl_register_map *map);\n #endif /* __CXL_PCI_H__ */\ndiff --git a/drivers/cxl/pci.c b/drivers/cxl/pci.c\nindex 668d44eb1bf5..7b4699fb8870 100644\n--- a/drivers/cxl/pci.c\n+++ b/drivers/cxl/pci.c\n@@ -11,6 +11,7 @@\n #include <linux/pci.h>\n #include <linux/aer.h>\n #include <linux/io.h>\n+#include <cxl/pci.h>\n #include <cxl/mailbox.h>\n #include \"cxlmem.h\"\n #include \"cxlpci.h\"\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\nindex 8e0481d8dced..34126bc4826c 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.c\n+++ b/drivers/net/ethernet/sfc/efx_cxl.c\n@@ -7,6 +7,8 @@\n \n #include <linux/pci.h>\n \n+#include <cxl/cxl.h>\n+#include <cxl/pci.h>\n #include \"net_driver.h\"\n #include \"efx_cxl.h\"\n \n@@ -18,6 +20,7 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \tstruct pci_dev *pci_dev = efx->pci_dev;\n \tstruct efx_cxl *cxl;\n \tu16 dvsec;\n+\tint rc;\n \n \tprobe_data->cxl_pio_initialised = false;\n \n@@ -44,6 +47,38 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \tif (!cxl)\n \t\treturn -ENOMEM;\n \n+\trc = cxl_pci_setup_regs(pci_dev, CXL_REGLOC_RBI_COMPONENT,\n+\t\t\t\t&cxl->cxlds.reg_map);\n+\tif (rc) {\n+\t\tpci_err(pci_dev, \"No component registers\\n\");\n+\t\treturn rc;\n+\t}\n+\n+\tif (!cxl->cxlds.reg_map.component_map.hdm_decoder.valid) {\n+\t\tpci_err(pci_dev, \"Expected HDM component register not found\\n\");\n+\t\treturn -ENODEV;\n+\t}\n+\n+\tif (!cxl->cxlds.reg_map.component_map.ras.valid) {\n+\t\tpci_err(pci_dev, \"Expected RAS component register not found\\n\");\n+\t\treturn -ENODEV;\n+\t}\n+\n+\trc = cxl_map_component_regs(&cxl->cxlds.reg_map,\n+\t\t\t\t    &cxl->cxlds.regs.component,\n+\t\t\t\t    BIT(CXL_CM_CAP_CAP_ID_RAS));\n+\tif (rc) {\n+\t\tpci_err(pci_dev, \"Failed to map RAS capability.\\n\");\n+\t\treturn rc;\n+\t}\n+\n+\t/*\n+\t * Set media ready explicitly as there are neither mailbox for checking\n+\t * this state nor the CXL register involved, both not mandatory for\n+\t * type2.\n+\t */\n+\tcxl->cxlds.media_ready = true;\n+\n \tprobe_data->cxl = cxl;\n \n \treturn 0;\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 13d448686189..7f2e23bce1f7 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -70,6 +70,10 @@ struct cxl_regs {\n \t);\n };\n \n+#define   CXL_CM_CAP_CAP_ID_RAS 0x2\n+#define   CXL_CM_CAP_CAP_ID_HDM 0x5\n+#define   CXL_CM_CAP_CAP_HDM_VERSION 1\n+\n struct cxl_reg_map {\n \tbool valid;\n \tint id;\n@@ -223,4 +227,19 @@ struct cxl_dev_state *_devm_cxl_dev_state_create(struct device *dev,\n \t\t(drv_struct *)_devm_cxl_dev_state_create(parent, type, serial, dvsec,\t\\\n \t\t\t\t\t\t      sizeof(drv_struct), mbox);\t\\\n \t})\n+\n+/**\n+ * cxl_map_component_regs - map cxl component registers\n+ *\n+ * @map: cxl register map to update with the mappings\n+ * @regs: cxl component registers to work with\n+ * @map_mask: cxl component regs to map\n+ *\n+ * Returns integer: success (0) or error (-ENOMEM)\n+ *\n+ * Made public for Type2 driver support.\n+ */\n+int cxl_map_component_regs(const struct cxl_register_map *map,\n+\t\t\t   struct cxl_component_regs *regs,\n+\t\t\t   unsigned long map_mask);\n #endif /* __CXL_CXL_H__ */\ndiff --git a/include/cxl/pci.h b/include/cxl/pci.h\nnew file mode 100644\nindex 000000000000..a172439f08c6\n--- /dev/null\n+++ b/include/cxl/pci.h\n@@ -0,0 +1,21 @@\n+/* SPDX-License-Identifier: GPL-2.0-only */\n+/* Copyright(c) 2020 Intel Corporation. All rights reserved. */\n+\n+#ifndef __CXL_CXL_PCI_H__\n+#define __CXL_CXL_PCI_H__\n+\n+/* Register Block Identifier (RBI) */\n+enum cxl_regloc_type {\n+\tCXL_REGLOC_RBI_EMPTY = 0,\n+\tCXL_REGLOC_RBI_COMPONENT,\n+\tCXL_REGLOC_RBI_VIRT,\n+\tCXL_REGLOC_RBI_MEMDEV,\n+\tCXL_REGLOC_RBI_PMU,\n+\tCXL_REGLOC_RBI_TYPES\n+};\n+\n+struct cxl_register_map;\n+\n+int cxl_pci_setup_regs(struct pci_dev *pdev, enum cxl_regloc_type type,\n+\t\t       struct cxl_register_map *map);\n+#endif\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-02-01"
        },
        {
          "author": "alejandro.lucero-palau (author)",
          "summary": "The author addressed a concern about the cxl_pci_setup_regs function needing to handle both RCRB and RCiEP cases, explained that they moved helper functions from cxl/pci_drv.c to cxl/core/pci.c to be exported and shared with CXL Type2 device initialization, and confirmed that this change addresses the issue.",
          "sentiment": "positive",
          "sentiment_signals": [
            "acknowledged a fix is needed",
            "confirmed the issue is resolved"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nInside cxl/core/pci.c there are helpers for CXL PCIe initialization\nmeanwhile cxl/pci_drv.c implements the functionality for a Type3 device\ninitialization.\n\nMove helper functions from cxl/core/pci_drv.c to cxl/core/pci.c in order\nto be exported and shared with CXL Type2 device initialization.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\nReviewed-by: Fan Ni <fan.ni@samsung.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Alison Schofield <alison.schofield@intel.com>\nReviewed-by: Dan Williams <dan.j.williams@intel.com>\n---\n drivers/cxl/core/core.h |  3 +-\n drivers/cxl/core/pci.c  | 62 ++++++++++++++++++++++++++++++++++++\n drivers/cxl/core/regs.c |  1 -\n drivers/cxl/cxl.h       |  2 --\n drivers/cxl/cxlpci.h    | 13 ++++++++\n drivers/cxl/pci.c       | 70 -----------------------------------------\n 6 files changed, 77 insertions(+), 74 deletions(-)\n\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex 422531799af2..256799d39361 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -187,5 +187,6 @@ int cxl_set_feature(struct cxl_mailbox *cxl_mbox, const uuid_t *feat_uuid,\n \t\t    size_t feat_data_size, u32 feat_flag, u16 offset,\n \t\t    u16 *return_code);\n #endif\n-\n+resource_size_t cxl_rcd_component_reg_phys(struct device *dev,\n+\t\t\t\t\t   struct cxl_dport *dport);\n #endif /* __CXL_CORE_H__ */\ndiff --git a/drivers/cxl/core/pci.c b/drivers/cxl/core/pci.c\nindex b838c59d7a3c..6b7e50858d56 100644\n--- a/drivers/cxl/core/pci.c\n+++ b/drivers/cxl/core/pci.c\n@@ -696,6 +696,68 @@ bool cxl_endpoint_decoder_reset_detected(struct cxl_port *port)\n }\n EXPORT_SYMBOL_NS_GPL(cxl_endpoint_decoder_reset_detected, \"CXL\");\n \n+static int cxl_rcrb_get_comp_regs(struct pci_dev *pdev,\n+\t\t\t\t  struct cxl_register_map *map,\n+\t\t\t\t  struct cxl_dport *dport)\n+{\n+\tresource_size_t component_reg_phys;\n+\n+\t*map = (struct cxl_register_map) {\n+\t\t.host = &pdev->dev,\n+\t\t.resource = CXL_RESOURCE_NONE,\n+\t};\n+\n+\tstruct cxl_port *port __free(put_cxl_port) =\n+\t\tcxl_pci_find_port(pdev, &dport);\n+\tif (!port)\n+\t\treturn -EPROBE_DEFER;\n+\n+\tcomponent_reg_phys = cxl_rcd_component_reg_phys(&pdev->dev, dport);\n+\tif (component_reg_phys == CXL_RESOURCE_NONE)\n+\t\treturn -ENXIO;\n+\n+\tmap->resource = component_reg_phys;\n+\tmap->reg_type = CXL_REGLOC_RBI_COMPONENT;\n+\tmap->max_size = CXL_COMPONENT_REG_BLOCK_SIZE;\n+\n+\treturn 0;\n+}\n+\n+int cxl_pci_setup_regs(struct pci_dev *pdev, enum cxl_regloc_type type,\n+\t\t\tstruct cxl_register_map *map)\n+{\n+\tint rc;\n+\n+\trc = cxl_find_regblock(pdev, type, map);\n+\n+\t/*\n+\t * If the Register Locator DVSEC does not exist, check if it\n+\t * is an RCH and try to extract the Component Registers from\n+\t * an RCRB.\n+\t */\n+\tif (rc && type == CXL_REGLOC_RBI_COMPONENT && is_cxl_restricted(pdev)) {\n+\t\tstruct cxl_dport *dport;\n+\t\tstruct cxl_port *port __free(put_cxl_port) =\n+\t\t\tcxl_pci_find_port(pdev, &dport);\n+\t\tif (!port)\n+\t\t\treturn -EPROBE_DEFER;\n+\n+\t\trc = cxl_rcrb_get_comp_regs(pdev, map, dport);\n+\t\tif (rc)\n+\t\t\treturn rc;\n+\n+\t\trc = cxl_dport_map_rcd_linkcap(pdev, dport);\n+\t\tif (rc)\n+\t\t\treturn rc;\n+\n+\t} else if (rc) {\n+\t\treturn rc;\n+\t}\n+\n+\treturn cxl_setup_regs(map);\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_pci_setup_regs, \"CXL\");\n+\n int cxl_pci_get_bandwidth(struct pci_dev *pdev, struct access_coordinate *c)\n {\n \tint speed, bw;\ndiff --git a/drivers/cxl/core/regs.c b/drivers/cxl/core/regs.c\nindex a010b3214342..93710cf4f0a6 100644\n--- a/drivers/cxl/core/regs.c\n+++ b/drivers/cxl/core/regs.c\n@@ -641,4 +641,3 @@ resource_size_t cxl_rcd_component_reg_phys(struct device *dev,\n \t\treturn CXL_RESOURCE_NONE;\n \treturn __rcrb_to_component(dev, &dport->rcrb, CXL_RCRB_UPSTREAM);\n }\n-EXPORT_SYMBOL_NS_GPL(cxl_rcd_component_reg_phys, \"CXL\");\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex 3eaa353e430b..5d111980d879 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -222,8 +222,6 @@ int cxl_find_regblock(struct pci_dev *pdev, enum cxl_regloc_type type,\n \t\t      struct cxl_register_map *map);\n int cxl_setup_regs(struct cxl_register_map *map);\n struct cxl_dport;\n-resource_size_t cxl_rcd_component_reg_phys(struct device *dev,\n-\t\t\t\t\t   struct cxl_dport *dport);\n int cxl_dport_map_rcd_linkcap(struct pci_dev *pdev, struct cxl_dport *dport);\n \n #define CXL_RESOURCE_NONE ((resource_size_t) -1)\ndiff --git a/drivers/cxl/cxlpci.h b/drivers/cxl/cxlpci.h\nindex 6f9c78886fd9..d879120b2780 100644\n--- a/drivers/cxl/cxlpci.h\n+++ b/drivers/cxl/cxlpci.h\n@@ -74,6 +74,17 @@ static inline bool cxl_pci_flit_256(struct pci_dev *pdev)\n \treturn lnksta2 & PCI_EXP_LNKSTA2_FLIT;\n }\n \n+/*\n+ * Assume that the caller has already validated that @pdev has CXL\n+ * capabilities, any RCiEP with CXL capabilities is treated as a\n+ * Restricted CXL Device (RCD) and finds upstream port and endpoint\n+ * registers in a Root Complex Register Block (RCRB).\n+ */\n+static inline bool is_cxl_restricted(struct pci_dev *pdev)\n+{\n+\treturn pci_pcie_type(pdev) == PCI_EXP_TYPE_RC_END;\n+}\n+\n struct cxl_dev_state;\n void read_cdat_data(struct cxl_port *port);\n \n@@ -95,4 +106,6 @@ static inline void cxl_dport_init_ras_reporting(struct cxl_dport *dport,\n \t\t\t\t\t\tstruct device *host) { }\n #endif\n \n+int cxl_pci_setup_regs(struct pci_dev *pdev, enum cxl_regloc_type type,\n+\t\t       struct cxl_register_map *map);\n #endif /* __CXL_PCI_H__ */\ndiff --git a/drivers/cxl/pci.c b/drivers/cxl/pci.c\nindex 24179cc702bf..668d44eb1bf5 100644\n--- a/drivers/cxl/pci.c\n+++ b/drivers/cxl/pci.c\n@@ -465,76 +465,6 @@ static int cxl_pci_setup_mailbox(struct cxl_memdev_state *mds, bool irq_avail)\n \treturn 0;\n }\n \n-/*\n- * Assume that any RCIEP that emits the CXL memory expander class code\n- * is an RCD\n- */\n-static bool is_cxl_restricted(struct pci_dev *pdev)\n-{\n-\treturn pci_pcie_type(pdev) == PCI_EXP_TYPE_RC_END;\n-}\n-\n-static int cxl_rcrb_get_comp_regs(struct pci_dev *pdev,\n-\t\t\t\t  struct cxl_register_map *map,\n-\t\t\t\t  struct cxl_dport *dport)\n-{\n-\tresource_size_t component_reg_phys;\n-\n-\t*map = (struct cxl_register_map) {\n-\t\t.host = &pdev->dev,\n-\t\t.resource = CXL_RESOURCE_NONE,\n-\t};\n-\n-\tstruct cxl_port *port __free(put_cxl_port) =\n-\t\tcxl_pci_find_port(pdev, &dport);\n-\tif (!port)\n-\t\treturn -EPROBE_DEFER;\n-\n-\tcomponent_reg_phys = cxl_rcd_component_reg_phys(&pdev->dev, dport);\n-\tif (component_reg_phys == CXL_RESOURCE_NONE)\n-\t\treturn -ENXIO;\n-\n-\tmap->resource = component_reg_phys;\n-\tmap->reg_type = CXL_REGLOC_RBI_COMPONENT;\n-\tmap->max_size = CXL_COMPONENT_REG_BLOCK_SIZE;\n-\n-\treturn 0;\n-}\n-\n-static int cxl_pci_setup_regs(struct pci_dev *pdev, enum cxl_regloc_type type,\n-\t\t\t      struct cxl_register_map *map)\n-{\n-\tint rc;\n-\n-\trc = cxl_find_regblock(pdev, type, map);\n-\n-\t/*\n-\t * If the Register Locator DVSEC does not exist, check if it\n-\t * is an RCH and try to extract the Component Registers from\n-\t * an RCRB.\n-\t */\n-\tif (rc && type == CXL_REGLOC_RBI_COMPONENT && is_cxl_restricted(pdev)) {\n-\t\tstruct cxl_dport *dport;\n-\t\tstruct cxl_port *port __free(put_cxl_port) =\n-\t\t\tcxl_pci_find_port(pdev, &dport);\n-\t\tif (!port)\n-\t\t\treturn -EPROBE_DEFER;\n-\n-\t\trc = cxl_rcrb_get_comp_regs(pdev, map, dport);\n-\t\tif (rc)\n-\t\t\treturn rc;\n-\n-\t\trc = cxl_dport_map_rcd_linkcap(pdev, dport);\n-\t\tif (rc)\n-\t\t\treturn rc;\n-\n-\t} else if (rc) {\n-\t\treturn rc;\n-\t}\n-\n-\treturn cxl_setup_regs(map);\n-}\n-\n static int cxl_pci_ras_unmask(struct pci_dev *pdev)\n {\n \tstruct cxl_dev_state *cxlds = pci_get_drvdata(pdev);\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-02-01"
        },
        {
          "author": "alejandro.lucero-palau (author)",
          "summary": "The author is addressing a concern about the cxl_mem_get_partition_info function being moved to memdev.c, which was previously in mbox.c. The author explains that this move allows Type2 drivers to initialize DPA by giving the size of their volatile hardware partition, and adds the sfc driver as a client. No fix is planned for the current patch.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nType3 relies on mailbox CXL_MBOX_OP_IDENTIFY command for initializing\nmemdev state params which end up being used for DPA initialization.\n\nAllow a Type2 driver to initialize DPA simply by giving the size of its\nvolatile hardware partition.\n\nMove related functions to memdev.\n\nAdd sfc driver as the client.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Dan Williams <dan.j.williams@intel.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n---\n drivers/cxl/core/core.h            |  2 +\n drivers/cxl/core/mbox.c            | 51 +----------------------\n drivers/cxl/core/memdev.c          | 66 ++++++++++++++++++++++++++++++\n drivers/net/ethernet/sfc/efx_cxl.c |  5 +++\n include/cxl/cxl.h                  |  1 +\n 5 files changed, 75 insertions(+), 50 deletions(-)\n\ndiff --git a/drivers/cxl/core/core.h b/drivers/cxl/core/core.h\nindex 256799d39361..e3c85ceda248 100644\n--- a/drivers/cxl/core/core.h\n+++ b/drivers/cxl/core/core.h\n@@ -89,6 +89,8 @@ void __iomem *devm_cxl_iomap_block(struct device *dev, resource_size_t addr,\n struct dentry *cxl_debugfs_create_dir(const char *dir);\n int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n \t\t     enum cxl_partition_mode mode);\n+struct cxl_memdev_state;\n+int cxl_mem_get_partition_info(struct cxl_memdev_state *mds);\n int cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size);\n int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n resource_size_t cxl_dpa_size(struct cxl_endpoint_decoder *cxled);\ndiff --git a/drivers/cxl/core/mbox.c b/drivers/cxl/core/mbox.c\nindex bee84d0101d1..d57a0c2d39fb 100644\n--- a/drivers/cxl/core/mbox.c\n+++ b/drivers/cxl/core/mbox.c\n@@ -1144,7 +1144,7 @@ EXPORT_SYMBOL_NS_GPL(cxl_mem_get_event_records, \"CXL\");\n  *\n  * See CXL @8.2.9.5.2.1 Get Partition Info\n  */\n-static int cxl_mem_get_partition_info(struct cxl_memdev_state *mds)\n+int cxl_mem_get_partition_info(struct cxl_memdev_state *mds)\n {\n \tstruct cxl_mailbox *cxl_mbox = &mds->cxlds.cxl_mbox;\n \tstruct cxl_mbox_get_partition_info pi;\n@@ -1300,55 +1300,6 @@ int cxl_mem_sanitize(struct cxl_memdev *cxlmd, u16 cmd)\n \treturn -EBUSY;\n }\n \n-static void add_part(struct cxl_dpa_info *info, u64 start, u64 size, enum cxl_partition_mode mode)\n-{\n-\tint i = info->nr_partitions;\n-\n-\tif (size == 0)\n-\t\treturn;\n-\n-\tinfo->part[i].range = (struct range) {\n-\t\t.start = start,\n-\t\t.end = start + size - 1,\n-\t};\n-\tinfo->part[i].mode = mode;\n-\tinfo->nr_partitions++;\n-}\n-\n-int cxl_mem_dpa_fetch(struct cxl_memdev_state *mds, struct cxl_dpa_info *info)\n-{\n-\tstruct cxl_dev_state *cxlds = &mds->cxlds;\n-\tstruct device *dev = cxlds->dev;\n-\tint rc;\n-\n-\tif (!cxlds->media_ready) {\n-\t\tinfo->size = 0;\n-\t\treturn 0;\n-\t}\n-\n-\tinfo->size = mds->total_bytes;\n-\n-\tif (mds->partition_align_bytes == 0) {\n-\t\tadd_part(info, 0, mds->volatile_only_bytes, CXL_PARTMODE_RAM);\n-\t\tadd_part(info, mds->volatile_only_bytes,\n-\t\t\t mds->persistent_only_bytes, CXL_PARTMODE_PMEM);\n-\t\treturn 0;\n-\t}\n-\n-\trc = cxl_mem_get_partition_info(mds);\n-\tif (rc) {\n-\t\tdev_err(dev, \"Failed to query partition information\\n\");\n-\t\treturn rc;\n-\t}\n-\n-\tadd_part(info, 0, mds->active_volatile_bytes, CXL_PARTMODE_RAM);\n-\tadd_part(info, mds->active_volatile_bytes, mds->active_persistent_bytes,\n-\t\t CXL_PARTMODE_PMEM);\n-\n-\treturn 0;\n-}\n-EXPORT_SYMBOL_NS_GPL(cxl_mem_dpa_fetch, \"CXL\");\n-\n int cxl_get_dirty_count(struct cxl_memdev_state *mds, u32 *count)\n {\n \tstruct cxl_mailbox *cxl_mbox = &mds->cxlds.cxl_mbox;\ndiff --git a/drivers/cxl/core/memdev.c b/drivers/cxl/core/memdev.c\nindex 22d156f25305..2c5dd72f43ca 100644\n--- a/drivers/cxl/core/memdev.c\n+++ b/drivers/cxl/core/memdev.c\n@@ -582,6 +582,72 @@ bool is_cxl_memdev(const struct device *dev)\n }\n EXPORT_SYMBOL_NS_GPL(is_cxl_memdev, \"CXL\");\n \n+static void add_part(struct cxl_dpa_info *info, u64 start, u64 size, enum cxl_partition_mode mode)\n+{\n+\tint i = info->nr_partitions;\n+\n+\tif (size == 0)\n+\t\treturn;\n+\n+\tinfo->part[i].range = (struct range) {\n+\t\t.start = start,\n+\t\t.end = start + size - 1,\n+\t};\n+\tinfo->part[i].mode = mode;\n+\tinfo->nr_partitions++;\n+}\n+\n+int cxl_mem_dpa_fetch(struct cxl_memdev_state *mds, struct cxl_dpa_info *info)\n+{\n+\tstruct cxl_dev_state *cxlds = &mds->cxlds;\n+\tstruct device *dev = cxlds->dev;\n+\tint rc;\n+\n+\tif (!cxlds->media_ready) {\n+\t\tinfo->size = 0;\n+\t\treturn 0;\n+\t}\n+\n+\tinfo->size = mds->total_bytes;\n+\n+\tif (mds->partition_align_bytes == 0) {\n+\t\tadd_part(info, 0, mds->volatile_only_bytes, CXL_PARTMODE_RAM);\n+\t\tadd_part(info, mds->volatile_only_bytes,\n+\t\t\t mds->persistent_only_bytes, CXL_PARTMODE_PMEM);\n+\t\treturn 0;\n+\t}\n+\n+\trc = cxl_mem_get_partition_info(mds);\n+\tif (rc) {\n+\t\tdev_err(dev, \"Failed to query partition information\\n\");\n+\t\treturn rc;\n+\t}\n+\n+\tadd_part(info, 0, mds->active_volatile_bytes, CXL_PARTMODE_RAM);\n+\tadd_part(info, mds->active_volatile_bytes, mds->active_persistent_bytes,\n+\t\t CXL_PARTMODE_PMEM);\n+\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_mem_dpa_fetch, \"CXL\");\n+\n+/**\n+ * cxl_set_capacity: initialize dpa by a driver without a mailbox.\n+ *\n+ * @cxlds: pointer to cxl_dev_state\n+ * @capacity: device volatile memory size\n+ */\n+int cxl_set_capacity(struct cxl_dev_state *cxlds, u64 capacity)\n+{\n+\tstruct cxl_dpa_info range_info = {\n+\t\t.size = capacity,\n+\t};\n+\n+\tadd_part(&range_info, 0, capacity, CXL_PARTMODE_RAM);\n+\treturn cxl_dpa_setup(cxlds, &range_info);\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_set_capacity, \"CXL\");\n+\n /**\n  * set_exclusive_cxl_commands() - atomically disable user cxl commands\n  * @mds: The device state to operate on\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\nindex 34126bc4826c..0b10a2e6aceb 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.c\n+++ b/drivers/net/ethernet/sfc/efx_cxl.c\n@@ -79,6 +79,11 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \t */\n \tcxl->cxlds.media_ready = true;\n \n+\tif (cxl_set_capacity(&cxl->cxlds, EFX_CTPIO_BUFFER_SIZE)) {\n+\t\tpci_err(pci_dev, \"dpa capacity setup failed\\n\");\n+\t\treturn -ENODEV;\n+\t}\n+\n \tprobe_data->cxl = cxl;\n \n \treturn 0;\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 7f2e23bce1f7..fb2f8f2395d5 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -242,4 +242,5 @@ struct cxl_dev_state *_devm_cxl_dev_state_create(struct device *dev,\n int cxl_map_component_regs(const struct cxl_register_map *map,\n \t\t\t   struct cxl_component_regs *regs,\n \t\t\t   unsigned long map_mask);\n+int cxl_set_capacity(struct cxl_dev_state *cxlds, u64 capacity);\n #endif /* __CXL_CXL_H__ */\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-02-01"
        },
        {
          "author": "alejandro.lucero-palau (author)",
          "summary": "The author addressed a concern that the current CXL core relies on a specific device type when creating a memdev, leading to issues with obtaining cxl_memdev_state references from a different device type. The author modified the check for obtaining cxl_memdev_state to add support for the CXL_DEVTYPE_DEVMEM type and made devm_cxl_add_memdev accessible from an accel driver.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged a fix is needed"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nCurrent cxl core is relying on a CXL_DEVTYPE_CLASSMEM type device when\ncreating a memdev leading to problems when obtaining cxl_memdev_state\nreferences from a CXL_DEVTYPE_DEVMEM type.\n\nModify check for obtaining cxl_memdev_state adding CXL_DEVTYPE_DEVMEM\nsupport.\n\nMake devm_cxl_add_memdev accessible from an accel driver.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Alison Schofield <alison.schofield@intel.com>\nReviewed-by: Dan Williams <dan.j.williams@intel.com>\n---\n drivers/cxl/core/memdev.c | 15 +++++++++++--\n drivers/cxl/cxlmem.h      |  6 ------\n drivers/cxl/mem.c         | 45 +++++++++++++++++++++++++++++----------\n include/cxl/cxl.h         |  6 ++++++\n 4 files changed, 53 insertions(+), 19 deletions(-)\n\ndiff --git a/drivers/cxl/core/memdev.c b/drivers/cxl/core/memdev.c\nindex 2c5dd72f43ca..1b43763b8e20 100644\n--- a/drivers/cxl/core/memdev.c\n+++ b/drivers/cxl/core/memdev.c\n@@ -7,6 +7,7 @@\n #include <linux/slab.h>\n #include <linux/idr.h>\n #include <linux/pci.h>\n+#include <cxl/cxl.h>\n #include <cxlmem.h>\n #include \"trace.h\"\n #include \"core.h\"\n@@ -576,9 +577,16 @@ static const struct device_type cxl_memdev_type = {\n \t.groups = cxl_memdev_attribute_groups,\n };\n \n+static const struct device_type cxl_accel_memdev_type = {\n+\t.name = \"cxl_accel_memdev\",\n+\t.release = cxl_memdev_release,\n+\t.devnode = cxl_memdev_devnode,\n+};\n+\n bool is_cxl_memdev(const struct device *dev)\n {\n-\treturn dev->type == &cxl_memdev_type;\n+\treturn (dev->type == &cxl_memdev_type ||\n+\t\tdev->type == &cxl_accel_memdev_type);\n }\n EXPORT_SYMBOL_NS_GPL(is_cxl_memdev, \"CXL\");\n \n@@ -781,7 +789,10 @@ static struct cxl_memdev *cxl_memdev_alloc(struct cxl_dev_state *cxlds,\n \tdev->parent = cxlds->dev;\n \tdev->bus = &cxl_bus_type;\n \tdev->devt = MKDEV(cxl_mem_major, cxlmd->id);\n-\tdev->type = &cxl_memdev_type;\n+\tif (cxlds->type == CXL_DEVTYPE_DEVMEM)\n+\t\tdev->type = &cxl_accel_memdev_type;\n+\telse\n+\t\tdev->type = &cxl_memdev_type;\n \tdevice_set_pm_not_required(dev);\n \tINIT_WORK(&cxlmd->detach_work, detach_memdev);\n \ndiff --git a/drivers/cxl/cxlmem.h b/drivers/cxl/cxlmem.h\nindex 281546de426e..c98db6f18aa2 100644\n--- a/drivers/cxl/cxlmem.h\n+++ b/drivers/cxl/cxlmem.h\n@@ -34,10 +34,6 @@\n \t(FIELD_GET(CXLMDEV_RESET_NEEDED_MASK, status) !=                       \\\n \t CXLMDEV_RESET_NEEDED_NOT)\n \n-struct cxl_memdev_attach {\n-\tint (*probe)(struct cxl_memdev *cxlmd);\n-};\n-\n /**\n  * struct cxl_memdev - CXL bus object representing a Type-3 Memory Device\n  * @dev: driver core device object\n@@ -103,8 +99,6 @@ static inline bool is_cxl_endpoint(struct cxl_port *port)\n \n struct cxl_memdev *__devm_cxl_add_memdev(struct cxl_dev_state *cxlds,\n \t\t\t\t\t const struct cxl_memdev_attach *attach);\n-struct cxl_memdev *devm_cxl_add_memdev(struct cxl_dev_state *cxlds,\n-\t\t\t\t       const struct cxl_memdev_attach *attach);\n int devm_cxl_sanitize_setup_notifier(struct device *host,\n \t\t\t\t     struct cxl_memdev *cxlmd);\n struct cxl_memdev_state;\ndiff --git a/drivers/cxl/mem.c b/drivers/cxl/mem.c\nindex 0958bea915ac..39687baedd1a 100644\n--- a/drivers/cxl/mem.c\n+++ b/drivers/cxl/mem.c\n@@ -65,6 +65,26 @@ static int cxl_debugfs_poison_clear(void *data, u64 dpa)\n DEFINE_DEBUGFS_ATTRIBUTE(cxl_poison_clear_fops, NULL,\n \t\t\t cxl_debugfs_poison_clear, \"%llx\\n\");\n \n+static void cxl_memdev_poison_enable(struct cxl_memdev_state *mds,\n+\t\t\t\t     struct cxl_memdev *cxlmd,\n+\t\t\t\t     struct dentry *dentry)\n+{\n+\t/*\n+\t * Avoid poison debugfs for DEVMEM aka accelerators as they rely on\n+\t * cxl_memdev_state.\n+\t */\n+\tif (!mds)\n+\t\treturn;\n+\n+\tif (test_bit(CXL_POISON_ENABLED_INJECT, mds->poison.enabled_cmds))\n+\t\tdebugfs_create_file(\"inject_poison\", 0200, dentry, cxlmd,\n+\t\t\t\t    &cxl_poison_inject_fops);\n+\n+\tif (test_bit(CXL_POISON_ENABLED_CLEAR, mds->poison.enabled_cmds))\n+\t\tdebugfs_create_file(\"clear_poison\", 0200, dentry, cxlmd,\n+\t\t\t\t    &cxl_poison_clear_fops);\n+}\n+\n static int cxl_mem_probe(struct device *dev)\n {\n \tstruct cxl_memdev *cxlmd = to_cxl_memdev(dev);\n@@ -92,12 +112,7 @@ static int cxl_mem_probe(struct device *dev)\n \tdentry = cxl_debugfs_create_dir(dev_name(dev));\n \tdebugfs_create_devm_seqfile(dev, \"dpamem\", dentry, cxl_mem_dpa_show);\n \n-\tif (test_bit(CXL_POISON_ENABLED_INJECT, mds->poison.enabled_cmds))\n-\t\tdebugfs_create_file(\"inject_poison\", 0200, dentry, cxlmd,\n-\t\t\t\t    &cxl_poison_inject_fops);\n-\tif (test_bit(CXL_POISON_ENABLED_CLEAR, mds->poison.enabled_cmds))\n-\t\tdebugfs_create_file(\"clear_poison\", 0200, dentry, cxlmd,\n-\t\t\t\t    &cxl_poison_clear_fops);\n+\tcxl_memdev_poison_enable(mds, cxlmd, dentry);\n \n \trc = devm_add_action_or_reset(dev, remove_debugfs, dentry);\n \tif (rc)\n@@ -208,16 +223,24 @@ static ssize_t trigger_poison_list_store(struct device *dev,\n }\n static DEVICE_ATTR_WO(trigger_poison_list);\n \n-static umode_t cxl_mem_visible(struct kobject *kobj, struct attribute *a, int n)\n+static bool cxl_poison_attr_visible(struct kobject *kobj, struct attribute *a)\n {\n \tstruct device *dev = kobj_to_dev(kobj);\n \tstruct cxl_memdev *cxlmd = to_cxl_memdev(dev);\n \tstruct cxl_memdev_state *mds = to_cxl_memdev_state(cxlmd->cxlds);\n \n-\tif (a == &dev_attr_trigger_poison_list.attr)\n-\t\tif (!test_bit(CXL_POISON_ENABLED_LIST,\n-\t\t\t      mds->poison.enabled_cmds))\n-\t\t\treturn 0;\n+\tif (!mds ||\n+\t    !test_bit(CXL_POISON_ENABLED_LIST, mds->poison.enabled_cmds))\n+\t\treturn false;\n+\n+\treturn true;\n+}\n+\n+static umode_t cxl_mem_visible(struct kobject *kobj, struct attribute *a, int n)\n+{\n+\tif (a == &dev_attr_trigger_poison_list.attr &&\n+\t    !cxl_poison_attr_visible(kobj, a))\n+\t\treturn 0;\n \n \treturn a->mode;\n }\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex fb2f8f2395d5..6f8d365067af 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -153,6 +153,10 @@ struct cxl_dpa_partition {\n \n #define CXL_NR_PARTITIONS_MAX 2\n \n+struct cxl_memdev_attach {\n+\tint (*probe)(struct cxl_memdev *cxlmd);\n+};\n+\n /**\n  * struct cxl_dev_state - The driver device state\n  *\n@@ -243,4 +247,6 @@ int cxl_map_component_regs(const struct cxl_register_map *map,\n \t\t\t   struct cxl_component_regs *regs,\n \t\t\t   unsigned long map_mask);\n int cxl_set_capacity(struct cxl_dev_state *cxlds, u64 capacity);\n+struct cxl_memdev *devm_cxl_add_memdev(struct cxl_dev_state *cxlds,\n+\t\t\t\t       const struct cxl_memdev_attach *attach);\n #endif /* __CXL_CXL_H__ */\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-02-01"
        },
        {
          "author": "alejandro.lucero-palau (author)",
          "summary": "The author is addressing a concern about the cxl memory device creation, specifically that it was not using the type2 cxl_dev_state struct as required by the new CXL API. The author has modified the code to use this struct and added error handling for creating the cxl memory device.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged a fix is needed"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nUse cxl API for creating a cxl memory device using the type2\ncxl_dev_state struct.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Martin Habets <habetsm.xilinx@gmail.com>\nReviewed-by: Fan Ni <fan.ni@samsung.com>\nAcked-by: Edward Cree <ecree.xilinx@gmail.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\n---\n drivers/net/ethernet/sfc/efx_cxl.c | 6 ++++++\n 1 file changed, 6 insertions(+)\n\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\nindex 0b10a2e6aceb..a77ef4783fcb 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.c\n+++ b/drivers/net/ethernet/sfc/efx_cxl.c\n@@ -84,6 +84,12 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \t\treturn -ENODEV;\n \t}\n \n+\tcxl->cxlmd = devm_cxl_add_memdev(&cxl->cxlds, NULL);\n+\tif (IS_ERR(cxl->cxlmd)) {\n+\t\tpci_err(pci_dev, \"CXL accel memdev creation failed\");\n+\t\treturn PTR_ERR(cxl->cxlmd);\n+\t}\n+\n \tprobe_data->cxl = cxl;\n \n \treturn 0;\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-02-01"
        },
        {
          "author": "alejandro.lucero-palau (author)",
          "summary": "The author is addressing a concern about the HDM (Host Data Management) being committed by the BIOS for Type2 devices, which can cause issues when trying to create a CXL region. The author has added a new function cxl_get_committed_decoder() that checks if the HDM is already committed after memdev creation and returns the corresponding decoder if it is. This change allows Type2 drivers to work with the HPA (Host Physical Address) and DPA (Device Physical Address) space even when the HDM is not committed.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "added new function",
            "addressed concern"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nA Type2 device configured by the BIOS can already have its HDM\ncommitted. Add a cxl_get_committed_decoder() function for cheking\nso after memdev creation. A CXL region should have been created\nduring memdev initialization, therefore a Type2 driver can ask for\nsuch a region for working with the HPA. If the HDM is not committed,\na Type2 driver will create the region after obtaining proper HPA\nand DPA space.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\n---\n drivers/cxl/core/hdm.c | 39 +++++++++++++++++++++++++++++++++++++++\n include/cxl/cxl.h      |  3 +++\n 2 files changed, 42 insertions(+)\n\ndiff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\nindex 6e516c69b2d2..a172ce4e9b19 100644\n--- a/drivers/cxl/core/hdm.c\n+++ b/drivers/cxl/core/hdm.c\n@@ -686,6 +686,45 @@ int cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size)\n \treturn devm_add_action_or_reset(&port->dev, cxl_dpa_release, cxled);\n }\n \n+static int find_committed_endpoint_decoder(struct device *dev, const void *data)\n+{\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tstruct cxl_port *port;\n+\n+\tif (!is_endpoint_decoder(dev))\n+\t\treturn 0;\n+\n+\tcxled = to_cxl_endpoint_decoder(dev);\n+\tport = cxled_to_port(cxled);\n+\n+\treturn cxled->cxld.id == port->hdm_end;\n+}\n+\n+struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n+\t\t\t\t\t\t       struct cxl_region **cxlr)\n+{\n+\tstruct cxl_port *endpoint = cxlmd->endpoint;\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tstruct device *cxled_dev;\n+\n+\tif (!endpoint)\n+\t\treturn NULL;\n+\n+\tguard(rwsem_read)(&cxl_rwsem.dpa);\n+\tcxled_dev = device_find_child(&endpoint->dev, NULL,\n+\t\t\t\t      find_committed_endpoint_decoder);\n+\n+\tif (!cxled_dev)\n+\t\treturn NULL;\n+\n+\tcxled = to_cxl_endpoint_decoder(cxled_dev);\n+\t*cxlr = cxled->cxld.region;\n+\n+\tput_device(cxled_dev);\n+\treturn cxled;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_get_committed_decoder, \"CXL\");\n+\n static void cxld_set_interleave(struct cxl_decoder *cxld, u32 *ctrl)\n {\n \tu16 eig;\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 6f8d365067af..928276dba952 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -249,4 +249,7 @@ int cxl_map_component_regs(const struct cxl_register_map *map,\n int cxl_set_capacity(struct cxl_dev_state *cxlds, u64 capacity);\n struct cxl_memdev *devm_cxl_add_memdev(struct cxl_dev_state *cxlds,\n \t\t\t\t       const struct cxl_memdev_attach *attach);\n+struct cxl_region;\n+struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n+\t\t\t\t\t\t       struct cxl_region **cxlr);\n #endif /* __CXL_CXL_H__ */\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-02-01"
        },
        {
          "author": "alejandro.lucero-palau (author)",
          "summary": "The author addressed a concern about the accelerator driver API not having a clean exit mechanism, and added cxl_unregister_region() to handle this. The function is exported for use by other drivers.",
          "sentiment": "positive",
          "sentiment_signals": [
            "acknowledged fix needed",
            "agreed with approach"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nAdd cxl_unregister_region() to the accelerator driver API\nfor a clean exit.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\n---\n drivers/cxl/core/region.c | 17 ++++++++++++-----\n include/cxl/cxl.h         |  1 +\n 2 files changed, 13 insertions(+), 5 deletions(-)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex acf29ba3b205..954b8fcdbac6 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -2438,9 +2438,8 @@ static struct cxl_region *to_cxl_region(struct device *dev)\n \treturn container_of(dev, struct cxl_region, dev);\n }\n \n-static void unregister_region(void *_cxlr)\n+void cxl_unregister_region(struct cxl_region *cxlr)\n {\n-\tstruct cxl_region *cxlr = _cxlr;\n \tstruct cxl_region_params *p = &cxlr->params;\n \tint i;\n \n@@ -2457,6 +2456,14 @@ static void unregister_region(void *_cxlr)\n \tcxl_region_iomem_release(cxlr);\n \tput_device(&cxlr->dev);\n }\n+EXPORT_SYMBOL_NS_GPL(cxl_unregister_region, \"CXL\");\n+\n+static void __unregister_region(void *_cxlr)\n+{\n+\tstruct cxl_region *cxlr = _cxlr;\n+\n+\treturn cxl_unregister_region(cxlr);\n+}\n \n static struct lock_class_key cxl_region_key;\n \n@@ -2608,7 +2615,7 @@ static struct cxl_region *devm_cxl_add_region(struct cxl_root_decoder *cxlrd,\n \tif (rc)\n \t\tgoto err;\n \n-\trc = devm_add_action_or_reset(port->uport_dev, unregister_region, cxlr);\n+\trc = devm_add_action_or_reset(port->uport_dev, __unregister_region, cxlr);\n \tif (rc)\n \t\treturn ERR_PTR(rc);\n \n@@ -2762,7 +2769,7 @@ static ssize_t delete_region_store(struct device *dev,\n \tif (IS_ERR(cxlr))\n \t\treturn PTR_ERR(cxlr);\n \n-\tdevm_release_action(port->uport_dev, unregister_region, cxlr);\n+\tdevm_release_action(port->uport_dev, __unregister_region, cxlr);\n \tput_device(&cxlr->dev);\n \n \treturn len;\n@@ -3878,7 +3885,7 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n \n \trc = __construct_region(cxlr, cxlrd, cxled);\n \tif (rc) {\n-\t\tdevm_release_action(port->uport_dev, unregister_region, cxlr);\n+\t\tdevm_release_action(port->uport_dev, __unregister_region, cxlr);\n \t\treturn ERR_PTR(rc);\n \t}\n \ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 906065e0d2a6..92880c26b2d5 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -254,4 +254,5 @@ struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n \t\t\t\t\t\t       struct cxl_region **cxlr);\n struct range;\n int cxl_get_region_range(struct cxl_region *region, struct range *range);\n+void cxl_unregister_region(struct cxl_region *cxlr);\n #endif /* __CXL_CXL_H__ */\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-02-01"
        },
        {
          "author": "alejandro.lucero-palau (author)",
          "summary": "The author addressed a concern about Type2 drivers accessing CXL region structs by adding a function to get the cxl region range, which will be used for mapping memory ranges. The function is exported and added to the cxl.h header file.",
          "sentiment": "positive",
          "sentiment_signals": [
            "acknowledged fix needed",
            "agreed to restructure"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nA CXL region struct contains the physical address to work with.\n\nType2 drivers can create a CXL region but have not access to the\nrelated struct as it is defined as private by the kernel CXL core.\nAdd a function for getting the cxl region range to be used for mapping\nsuch memory range by a Type2 driver.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Zhi Wang <zhiw@nvidia.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\n---\n drivers/cxl/core/region.c | 23 +++++++++++++++++++++++\n include/cxl/cxl.h         |  2 ++\n 2 files changed, 25 insertions(+)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 96888d87a8df..acf29ba3b205 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -2621,6 +2621,29 @@ static struct cxl_region *devm_cxl_add_region(struct cxl_root_decoder *cxlrd,\n \treturn ERR_PTR(rc);\n }\n \n+/**\n+ * cxl_get_region_range - obtain range linked to a CXL region\n+ *\n+ * @region: a pointer to struct cxl_region\n+ * @range: a pointer to a struct range to be set\n+ *\n+ * Returns 0 or error.\n+ */\n+int cxl_get_region_range(struct cxl_region *region, struct range *range)\n+{\n+\tif (WARN_ON_ONCE(!region))\n+\t\treturn -ENODEV;\n+\n+\tif (!region->params.res)\n+\t\treturn -ENOSPC;\n+\n+\trange->start = region->params.res->start;\n+\trange->end = region->params.res->end;\n+\n+\treturn 0;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_get_region_range, \"CXL\");\n+\n static ssize_t __create_region_show(struct cxl_root_decoder *cxlrd, char *buf)\n {\n \treturn sysfs_emit(buf, \"region%u\\n\", atomic_read(&cxlrd->region_id));\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 928276dba952..906065e0d2a6 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -252,4 +252,6 @@ struct cxl_memdev *devm_cxl_add_memdev(struct cxl_dev_state *cxlds,\n struct cxl_region;\n struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n \t\t\t\t\t\t       struct cxl_region **cxlr);\n+struct range;\n+int cxl_get_region_range(struct cxl_region *region, struct range *range);\n #endif /* __CXL_CXL_H__ */\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-02-01"
        },
        {
          "author": "alejandro.lucero-palau (author)",
          "summary": "The author addressed a concern about the complexity of finding a suitable root decoder for CXL region creation, explaining that it involves determining available Host Physical Address (HPA) and allocating capacity from Device Physical Address (DPA). They added a new API to retrieve a root decoder with specific constraints and capacity, along with a complementary function to release the reference to such a decoder. The author did not mention any plans for revising the patch in response to this feedback.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nCXL region creation involves allocating capacity from Device Physical\nAddress (DPA) and assigning it to decode a given Host Physical Address\n(HPA). Before determining how much DPA to allocate the amount of available\nHPA must be determined. Also, not all HPA is created equal, some HPA\ntargets RAM, some targets PMEM, some is prepared for device-memory flows\nlike HDM-D and HDM-DB, and some is HDM-H (host-only).\n\nIn order to support Type2 CXL devices, wrap all of those concerns into\nan API that retrieves a root decoder (platform CXL window) that fits the\nspecified constraints and the capacity available for a new region.\n\nAdd a complementary function for releasing the reference to such root\ndecoder.\n\nBased on https://lore.kernel.org/linux-cxl/168592159290.1948938.13522227102445462976.stgit@dwillia2-xfh.jf.intel.com/\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n---\n drivers/cxl/core/region.c | 164 ++++++++++++++++++++++++++++++++++++++\n drivers/cxl/cxl.h         |   3 +\n include/cxl/cxl.h         |   6 ++\n 3 files changed, 173 insertions(+)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 954b8fcdbac6..bdefd088f5f1 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -705,6 +705,170 @@ static int free_hpa(struct cxl_region *cxlr)\n \treturn 0;\n }\n \n+struct cxlrd_max_context {\n+\tstruct device * const *host_bridges;\n+\tint interleave_ways;\n+\tunsigned long flags;\n+\tresource_size_t max_hpa;\n+\tstruct cxl_root_decoder *cxlrd;\n+};\n+\n+static int find_max_hpa(struct device *dev, void *data)\n+{\n+\tstruct cxlrd_max_context *ctx = data;\n+\tstruct cxl_switch_decoder *cxlsd;\n+\tstruct cxl_root_decoder *cxlrd;\n+\tstruct resource *res, *prev;\n+\tstruct cxl_decoder *cxld;\n+\tresource_size_t free = 0;\n+\tresource_size_t max;\n+\tint found = 0;\n+\n+\tif (!is_root_decoder(dev))\n+\t\treturn 0;\n+\n+\tcxlrd = to_cxl_root_decoder(dev);\n+\tcxlsd = &cxlrd->cxlsd;\n+\tcxld = &cxlsd->cxld;\n+\n+\tif ((cxld->flags & ctx->flags) != ctx->flags) {\n+\t\tdev_dbg(dev, \"flags not matching: %08lx vs %08lx\\n\",\n+\t\t\tcxld->flags, ctx->flags);\n+\t\treturn 0;\n+\t}\n+\n+\tfor (int i = 0; i < ctx->interleave_ways; i++) {\n+\t\tfor (int j = 0; j < ctx->interleave_ways; j++) {\n+\t\t\tif (ctx->host_bridges[i] == cxlsd->target[j]->dport_dev) {\n+\t\t\t\tfound++;\n+\t\t\t\tbreak;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tif (found != ctx->interleave_ways) {\n+\t\tdev_dbg(dev,\n+\t\t\t\"Not enough host bridges. Found %d for %d interleave ways requested\\n\",\n+\t\t\tfound, ctx->interleave_ways);\n+\t\treturn 0;\n+\t}\n+\n+\t/*\n+\t * Walk the root decoder resource range relying on cxl_rwsem.region to\n+\t * preclude sibling arrival/departure and find the largest free space\n+\t * gap.\n+\t */\n+\tlockdep_assert_held_read(&cxl_rwsem.region);\n+\tres = cxlrd->res->child;\n+\n+\t/* With no resource child the whole parent resource is available */\n+\tif (!res)\n+\t\tmax = resource_size(cxlrd->res);\n+\telse\n+\t\tmax = 0;\n+\n+\tfor (prev = NULL; res; prev = res, res = res->sibling) {\n+\t\tif (!prev && res->start == cxlrd->res->start &&\n+\t\t    res->end == cxlrd->res->end) {\n+\t\t\tmax = resource_size(cxlrd->res);\n+\t\t\tbreak;\n+\t\t}\n+\t\t/*\n+\t\t * Sanity check for preventing arithmetic problems below as a\n+\t\t * resource with size 0 could imply using the end field below\n+\t\t * when set to unsigned zero - 1 or all f in hex.\n+\t\t */\n+\t\tif (prev && !resource_size(prev))\n+\t\t\tcontinue;\n+\n+\t\tif (!prev && res->start > cxlrd->res->start) {\n+\t\t\tfree = res->start - cxlrd->res->start;\n+\t\t\tmax = max(free, max);\n+\t\t}\n+\t\tif (prev && res->start > prev->end + 1) {\n+\t\t\tfree = res->start - prev->end + 1;\n+\t\t\tmax = max(free, max);\n+\t\t}\n+\t}\n+\n+\tif (prev && prev->end + 1 < cxlrd->res->end + 1) {\n+\t\tfree = cxlrd->res->end + 1 - prev->end + 1;\n+\t\tmax = max(free, max);\n+\t}\n+\n+\tdev_dbg(cxlrd_dev(cxlrd), \"found %pa bytes of free space\\n\", &max);\n+\tif (max > ctx->max_hpa) {\n+\t\tif (ctx->cxlrd)\n+\t\t\tput_device(cxlrd_dev(ctx->cxlrd));\n+\t\tget_device(cxlrd_dev(cxlrd));\n+\t\tctx->cxlrd = cxlrd;\n+\t\tctx->max_hpa = max;\n+\t}\n+\treturn 0;\n+}\n+\n+/**\n+ * cxl_get_hpa_freespace - find a root decoder with free capacity per constraints\n+ * @cxlmd: the mem device requiring the HPA\n+ * @interleave_ways: number of entries in @host_bridges\n+ * @flags: CXL_DECODER_F flags for selecting RAM vs PMEM, and Type2 device\n+ * @max_avail_contig: output parameter of max contiguous bytes available in the\n+ *\t\t      returned decoder\n+ *\n+ * Returns a pointer to a struct cxl_root_decoder\n+ *\n+ * The return tuple of a 'struct cxl_root_decoder' and 'bytes available given\n+ * in (@max_avail_contig))' is a point in time snapshot. If by the time the\n+ * caller goes to use this decoder and its capacity is reduced then caller needs\n+ * to loop and retry.\n+ *\n+ * The returned root decoder has an elevated reference count that needs to be\n+ * put with cxl_put_root_decoder(cxlrd).\n+ */\n+struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n+\t\t\t\t\t       int interleave_ways,\n+\t\t\t\t\t       unsigned long flags,\n+\t\t\t\t\t       resource_size_t *max_avail_contig)\n+{\n+\tstruct cxlrd_max_context ctx = {\n+\t\t.flags = flags,\n+\t\t.interleave_ways = interleave_ways,\n+\t};\n+\tstruct cxl_port *root_port;\n+\tstruct cxl_port *endpoint;\n+\n+\tendpoint = cxlmd->endpoint;\n+\tif (!endpoint) {\n+\t\tdev_dbg(&cxlmd->dev, \"endpoint not linked to memdev\\n\");\n+\t\treturn ERR_PTR(-ENXIO);\n+\t}\n+\n+\tctx.host_bridges = &endpoint->host_bridge;\n+\n+\tstruct cxl_root *root __free(put_cxl_root) = find_cxl_root(endpoint);\n+\tif (!root) {\n+\t\tdev_dbg(&endpoint->dev, \"endpoint is not related to a root port\\n\");\n+\t\treturn ERR_PTR(-ENXIO);\n+\t}\n+\n+\troot_port = &root->port;\n+\tscoped_guard(rwsem_read, &cxl_rwsem.region)\n+\t\tdevice_for_each_child(&root_port->dev, &ctx, find_max_hpa);\n+\n+\tif (!ctx.cxlrd)\n+\t\treturn ERR_PTR(-ENOMEM);\n+\n+\t*max_avail_contig = ctx.max_hpa;\n+\treturn ctx.cxlrd;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_get_hpa_freespace, \"CXL\");\n+\n+void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd)\n+{\n+\tput_device(cxlrd_dev(cxlrd));\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_put_root_decoder, \"CXL\");\n+\n static ssize_t size_store(struct device *dev, struct device_attribute *attr,\n \t\t\t  const char *buf, size_t len)\n {\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex 944c5d1ccceb..c7d9b2c2908f 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -706,6 +706,9 @@ struct cxl_root_decoder *to_cxl_root_decoder(struct device *dev);\n struct cxl_switch_decoder *to_cxl_switch_decoder(struct device *dev);\n struct cxl_endpoint_decoder *to_cxl_endpoint_decoder(struct device *dev);\n bool is_root_decoder(struct device *dev);\n+\n+#define cxlrd_dev(cxlrd) (&(cxlrd)->cxlsd.cxld.dev)\n+\n bool is_switch_decoder(struct device *dev);\n bool is_endpoint_decoder(struct device *dev);\n struct cxl_root_decoder *cxl_root_decoder_alloc(struct cxl_port *port,\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 92880c26b2d5..834dc7e78934 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -255,4 +255,10 @@ struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n struct range;\n int cxl_get_region_range(struct cxl_region *region, struct range *range);\n void cxl_unregister_region(struct cxl_region *cxlr);\n+struct cxl_port;\n+struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n+\t\t\t\t\t       int interleave_ways,\n+\t\t\t\t\t       unsigned long flags,\n+\t\t\t\t\t       resource_size_t *max);\n+void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n #endif /* __CXL_CXL_H__ */\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-02-01"
        },
        {
          "author": "alejandro.lucero-palau (author)",
          "summary": "Author acknowledged that the current code only supports Type3 or CXL_DECODER_HOSTONLYMEM devices and agreed to modify the region type based on the endpoint type HDM-D[B] for Type2 support.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged a limitation",
            "agreed to modify"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nCurrent code is expecting Type3 or CXL_DECODER_HOSTONLYMEM devices only.\nSupport for Type2 implies region type needs to be based on the endpoint\ntype HDM-D[B] instead.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Zhi Wang <zhiw@nvidia.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\nReviewed-by: Alison Schofield <alison.schofield@intel.com>\nReviewed-by: Davidlohr Bueso <daves@stgolabs.net>\n---\n drivers/cxl/core/region.c | 10 ++++++----\n 1 file changed, 6 insertions(+), 4 deletions(-)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex bdefd088f5f1..f53b2e9fd9e6 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -2833,7 +2833,8 @@ static ssize_t create_ram_region_show(struct device *dev,\n }\n \n static struct cxl_region *__create_region(struct cxl_root_decoder *cxlrd,\n-\t\t\t\t\t  enum cxl_partition_mode mode, int id)\n+\t\t\t\t\t  enum cxl_partition_mode mode, int id,\n+\t\t\t\t\t  enum cxl_decoder_type target_type)\n {\n \tint rc;\n \n@@ -2855,7 +2856,7 @@ static struct cxl_region *__create_region(struct cxl_root_decoder *cxlrd,\n \t\treturn ERR_PTR(-EBUSY);\n \t}\n \n-\treturn devm_cxl_add_region(cxlrd, id, mode, CXL_DECODER_HOSTONLYMEM);\n+\treturn devm_cxl_add_region(cxlrd, id, mode, target_type);\n }\n \n static ssize_t create_region_store(struct device *dev, const char *buf,\n@@ -2869,7 +2870,7 @@ static ssize_t create_region_store(struct device *dev, const char *buf,\n \tif (rc != 1)\n \t\treturn -EINVAL;\n \n-\tcxlr = __create_region(cxlrd, mode, id);\n+\tcxlr = __create_region(cxlrd, mode, id, CXL_DECODER_HOSTONLYMEM);\n \tif (IS_ERR(cxlr))\n \t\treturn PTR_ERR(cxlr);\n \n@@ -4036,7 +4037,8 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n \n \tdo {\n \t\tcxlr = __create_region(cxlrd, cxlds->part[part].mode,\n-\t\t\t\t       atomic_read(&cxlrd->region_id));\n+\t\t\t\t       atomic_read(&cxlrd->region_id),\n+\t\t\t\t       cxled->cxld.target_type);\n \t} while (IS_ERR(cxlr) && PTR_ERR(cxlr) == -EBUSY);\n \n \tif (IS_ERR(cxlr)) {\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-02-01"
        },
        {
          "author": "alejandro.lucero-palau (author)",
          "summary": "The author addressed a concern about using the cxl API for getting HPA (Host Physical Address) from a CXL root decoder. They modified the code to use cxl_get_hpa_freespace() instead of directly accessing the hdm control register, and added checks for sufficient free HPA space. The patch will need further revision.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "patch modification",
            "acknowledgment of concern"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nUse cxl api for getting HPA (Host Physical Address) to use from a\nCXL root decoder.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Martin Habets <habetsm.xilinx@gmail.com>\nAcked-by: Edward Cree <ecree.xilinx@gmail.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n---\n drivers/cxl/cxl.h                  | 15 ---------------\n drivers/net/ethernet/sfc/Kconfig   |  1 +\n drivers/net/ethernet/sfc/efx_cxl.c | 26 +++++++++++++++++++++++---\n drivers/net/ethernet/sfc/efx_cxl.h |  1 +\n include/cxl/cxl.h                  | 15 +++++++++++++++\n 5 files changed, 40 insertions(+), 18 deletions(-)\n\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex c7d9b2c2908f..d1b010e5e1d0 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -220,21 +220,6 @@ int cxl_dport_map_rcd_linkcap(struct pci_dev *pdev, struct cxl_dport *dport);\n #define CXL_RESOURCE_NONE ((resource_size_t) -1)\n #define CXL_TARGET_STRLEN 20\n \n-/*\n- * cxl_decoder flags that define the type of memory / devices this\n- * decoder supports as well as configuration lock status See \"CXL 2.0\n- * 8.2.5.12.7 CXL HDM Decoder 0 Control Register\" for details.\n- * Additionally indicate whether decoder settings were autodetected,\n- * user customized.\n- */\n-#define CXL_DECODER_F_RAM   BIT(0)\n-#define CXL_DECODER_F_PMEM  BIT(1)\n-#define CXL_DECODER_F_TYPE2 BIT(2)\n-#define CXL_DECODER_F_TYPE3 BIT(3)\n-#define CXL_DECODER_F_LOCK  BIT(4)\n-#define CXL_DECODER_F_ENABLE    BIT(5)\n-#define CXL_DECODER_F_MASK  GENMASK(5, 0)\n-\n enum cxl_decoder_type {\n \tCXL_DECODER_DEVMEM = 2,\n \tCXL_DECODER_HOSTONLYMEM = 3,\ndiff --git a/drivers/net/ethernet/sfc/Kconfig b/drivers/net/ethernet/sfc/Kconfig\nindex 979f2801e2a8..e959d9b4f4ce 100644\n--- a/drivers/net/ethernet/sfc/Kconfig\n+++ b/drivers/net/ethernet/sfc/Kconfig\n@@ -69,6 +69,7 @@ config SFC_MCDI_LOGGING\n config SFC_CXL\n \tbool \"Solarflare SFC9100-family CXL support\"\n \tdepends on SFC && CXL_BUS >= SFC\n+\tdepends on CXL_REGION\n \tdefault SFC\n \thelp\n \t  This enables SFC CXL support if the kernel is configuring CXL for\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\nindex 3536eccf1b2a..1a4c1097c315 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.c\n+++ b/drivers/net/ethernet/sfc/efx_cxl.c\n@@ -18,6 +18,7 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n {\n \tstruct efx_nic *efx = &probe_data->efx;\n \tstruct pci_dev *pci_dev = efx->pci_dev;\n+\tresource_size_t max_size;\n \tstruct efx_cxl *cxl;\n \tstruct range range;\n \tu16 dvsec;\n@@ -110,9 +111,24 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \t\t\treturn -ENOMEM;\n \t\t}\n \n-\t\tprobe_data->cxl = cxl;\n+\t\tcxl->hdm_was_committed = true;\n+\t} else {\n+\t\tcxl->cxlrd = cxl_get_hpa_freespace(cxl->cxlmd, 1, CXL_DECODER_F_RAM |\n+\t\t\t\t\t\t   CXL_DECODER_F_TYPE2, &max_size);\n+\t\tif (IS_ERR(cxl->cxlrd)) {\n+\t\t\tdev_err(&pci_dev->dev, \"cxl_get_hpa_freespace failed\\n\");\n+\t\t\treturn PTR_ERR(cxl->cxlrd);\n+\t\t}\n+\n+\t\tif (max_size < EFX_CTPIO_BUFFER_SIZE) {\n+\t\t\tdev_err(&pci_dev->dev, \"%s: not enough free HPA space %pap < %u\\n\",\n+\t\t\t\t__func__, &max_size, EFX_CTPIO_BUFFER_SIZE);\n+\t\t\tcxl_put_root_decoder(cxl->cxlrd);\n+\t\t\treturn -ENOSPC;\n+\t\t}\n \t}\n \n+\tprobe_data->cxl = cxl;\n \treturn 0;\n }\n \n@@ -121,8 +137,12 @@ void efx_cxl_exit(struct efx_probe_data *probe_data)\n \tif (!probe_data->cxl)\n \t\treturn;\n \n-\tiounmap(probe_data->cxl->ctpio_cxl);\n-\tcxl_unregister_region(probe_data->cxl->efx_region);\n+\tif (probe_data->cxl->hdm_was_committed) {\n+\t\tiounmap(probe_data->cxl->ctpio_cxl);\n+\t\tcxl_unregister_region(probe_data->cxl->efx_region);\n+\t} else {\n+\t\tcxl_put_root_decoder(probe_data->cxl->cxlrd);\n+\t}\n }\n \n MODULE_IMPORT_NS(\"CXL\");\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.h b/drivers/net/ethernet/sfc/efx_cxl.h\nindex 961639cef692..9a92e386695b 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.h\n+++ b/drivers/net/ethernet/sfc/efx_cxl.h\n@@ -27,6 +27,7 @@ struct efx_cxl {\n \tstruct cxl_root_decoder *cxlrd;\n \tstruct cxl_port *endpoint;\n \tstruct cxl_endpoint_decoder *cxled;\n+\tbool hdm_was_committed;\n \tstruct cxl_region *efx_region;\n \tvoid __iomem *ctpio_cxl;\n };\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 834dc7e78934..783ad570a6eb 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -153,6 +153,21 @@ struct cxl_dpa_partition {\n \n #define CXL_NR_PARTITIONS_MAX 2\n \n+/*\n+ * cxl_decoder flags that define the type of memory / devices this\n+ * decoder supports as well as configuration lock status See \"CXL 2.0\n+ * 8.2.5.12.7 CXL HDM Decoder 0 Control Register\" for details.\n+ * Additionally indicate whether decoder settings were autodetected,\n+ * user customized.\n+ */\n+#define CXL_DECODER_F_RAM   BIT(0)\n+#define CXL_DECODER_F_PMEM  BIT(1)\n+#define CXL_DECODER_F_TYPE2 BIT(2)\n+#define CXL_DECODER_F_TYPE3 BIT(3)\n+#define CXL_DECODER_F_LOCK  BIT(4)\n+#define CXL_DECODER_F_ENABLE    BIT(5)\n+#define CXL_DECODER_F_MASK  GENMASK(5, 0)\n+\n struct cxl_memdev_attach {\n \tint (*probe)(struct cxl_memdev *cxlmd);\n };\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-02-01"
        },
        {
          "author": "alejandro.lucero-palau (author)",
          "summary": "Author acknowledged that the CXL region should exist if a device HDM is already committed during firmware/BIOS initialization, and agreed to add code to check for this condition in efx_cxl_init().",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged fix needed"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nCheck if device HDM is already committed during firmware/BIOS\ninitialization.\n\nA CXL region should exist if so after memdev allocation/initialization.\nGet HPA from region and map it.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\n---\n drivers/net/ethernet/sfc/efx_cxl.c | 28 +++++++++++++++++++++++++++-\n 1 file changed, 27 insertions(+), 1 deletion(-)\n\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\nindex a77ef4783fcb..3536eccf1b2a 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.c\n+++ b/drivers/net/ethernet/sfc/efx_cxl.c\n@@ -19,6 +19,7 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \tstruct efx_nic *efx = &probe_data->efx;\n \tstruct pci_dev *pci_dev = efx->pci_dev;\n \tstruct efx_cxl *cxl;\n+\tstruct range range;\n \tu16 dvsec;\n \tint rc;\n \n@@ -90,13 +91,38 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \t\treturn PTR_ERR(cxl->cxlmd);\n \t}\n \n-\tprobe_data->cxl = cxl;\n+\tcxl->cxled = cxl_get_committed_decoder(cxl->cxlmd, &cxl->efx_region);\n+\tif (cxl->cxled) {\n+\t\tif (!cxl->efx_region) {\n+\t\t\tpci_err(pci_dev, \"CXL found committed decoder without a region\");\n+\t\t\treturn -ENODEV;\n+\t\t}\n+\t\trc = cxl_get_region_range(cxl->efx_region, &range);\n+\t\tif (rc) {\n+\t\t\tpci_err(pci_dev,\n+\t\t\t\t\"CXL getting regions params from a committed decoder failed\");\n+\t\t\treturn rc;\n+\t\t}\n+\n+\t\tcxl->ctpio_cxl = ioremap(range.start, range.end - range.start + 1);\n+\t\tif (!cxl->ctpio_cxl) {\n+\t\t\tpci_err(pci_dev, \"CXL ioremap region (%pra) failed\", &range);\n+\t\t\treturn -ENOMEM;\n+\t\t}\n+\n+\t\tprobe_data->cxl = cxl;\n+\t}\n \n \treturn 0;\n }\n \n void efx_cxl_exit(struct efx_probe_data *probe_data)\n {\n+\tif (!probe_data->cxl)\n+\t\treturn;\n+\n+\tiounmap(probe_data->cxl->ctpio_cxl);\n+\tcxl_unregister_region(probe_data->cxl->efx_region);\n }\n \n MODULE_IMPORT_NS(\"CXL\");\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-02-01"
        },
        {
          "author": "alejandro.lucero-palau (author)",
          "summary": "Author addressed a concern about the cxl_create_region() call in efx_cxl_init(), explained that it should be used to create a region using the endpoint decoder related to a DPA range, and confirmed that this change will be included in the patch.",
          "sentiment": "positive",
          "sentiment_signals": [
            "acknowledged concern",
            "confirmed fix"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nUse cxl api for creating a region using the endpoint decoder related to\na DPA range.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\n---\n drivers/net/ethernet/sfc/efx_cxl.c | 10 +++++++++-\n 1 file changed, 9 insertions(+), 1 deletion(-)\n\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\nindex 2cfd0a46225f..4d5f3974e51d 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.c\n+++ b/drivers/net/ethernet/sfc/efx_cxl.c\n@@ -134,6 +134,14 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \t\t\tcxl_put_root_decoder(cxl->cxlrd);\n \t\t\treturn PTR_ERR(cxl->cxled);\n \t\t}\n+\n+\t\tcxl->efx_region = cxl_create_region(cxl->cxlrd, &cxl->cxled, 1);\n+\t\tif (IS_ERR(cxl->efx_region)) {\n+\t\t\tpci_err(pci_dev, \"CXL accel create region failed\");\n+\t\t\tcxl_put_root_decoder(cxl->cxlrd);\n+\t\t\tcxl_dpa_free(cxl->cxled);\n+\t\t\treturn PTR_ERR(cxl->efx_region);\n+\t\t}\n \t}\n \n \tprobe_data->cxl = cxl;\n@@ -147,11 +155,11 @@ void efx_cxl_exit(struct efx_probe_data *probe_data)\n \n \tif (probe_data->cxl->hdm_was_committed) {\n \t\tiounmap(probe_data->cxl->ctpio_cxl);\n-\t\tcxl_unregister_region(probe_data->cxl->efx_region);\n \t} else {\n \t\tcxl_dpa_free(probe_data->cxl->cxled);\n \t\tcxl_put_root_decoder(probe_data->cxl->cxlrd);\n \t}\n+\tcxl_unregister_region(probe_data->cxl->efx_region);\n }\n \n MODULE_IMPORT_NS(\"CXL\");\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-02-01"
        },
        {
          "author": "alejandro.lucero-palau (author)",
          "summary": "The author addressed a concern about the interleave ways store function, which was previously acquiring the region rwsem lock for write and then immediately releasing it. The author refactored the code to use the set_interleave_ways helper function, which acquires the lock only once and calls the helper function. This change is intended to improve performance by reducing lock contention.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "refactoring"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nRegion creation based on Type3 devices is triggered from user space\nallowing memory combination through interleaving.\n\nIn preparation for kernel driven region creation, that is Type2 drivers\ntriggering region creation backed with its advertised CXL memory, factor\nout a common helper from the user-sysfs region setup for interleave ways.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Zhi Wang <zhiw@nvidia.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\nReviewed-by: Alison Schofield <alison.schofield@intel.com>\n---\n drivers/cxl/core/region.c | 43 ++++++++++++++++++++++++---------------\n 1 file changed, 27 insertions(+), 16 deletions(-)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex f53b2e9fd9e6..ece1d3df7cf1 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -485,22 +485,14 @@ static ssize_t interleave_ways_show(struct device *dev,\n \n static const struct attribute_group *get_cxl_region_target_group(void);\n \n-static ssize_t interleave_ways_store(struct device *dev,\n-\t\t\t\t     struct device_attribute *attr,\n-\t\t\t\t     const char *buf, size_t len)\n+static int set_interleave_ways(struct cxl_region *cxlr, int val)\n {\n-\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(dev->parent);\n+\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n-\tstruct cxl_region *cxlr = to_cxl_region(dev);\n \tstruct cxl_region_params *p = &cxlr->params;\n-\tunsigned int val, save;\n-\tint rc;\n+\tint save, rc;\n \tu8 iw;\n \n-\trc = kstrtouint(buf, 0, &val);\n-\tif (rc)\n-\t\treturn rc;\n-\n \trc = ways_to_eiw(val, &iw);\n \tif (rc)\n \t\treturn rc;\n@@ -515,9 +507,7 @@ static ssize_t interleave_ways_store(struct device *dev,\n \t\treturn -EINVAL;\n \t}\n \n-\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n-\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n-\t\treturn rc;\n+\tlockdep_assert_held_write(&cxl_rwsem.region);\n \n \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n \t\treturn -EBUSY;\n@@ -525,10 +515,31 @@ static ssize_t interleave_ways_store(struct device *dev,\n \tsave = p->interleave_ways;\n \tp->interleave_ways = val;\n \trc = sysfs_update_group(&cxlr->dev.kobj, get_cxl_region_target_group());\n-\tif (rc) {\n+\tif (rc)\n \t\tp->interleave_ways = save;\n+\n+\treturn rc;\n+}\n+\n+static ssize_t interleave_ways_store(struct device *dev,\n+\t\t\t\t     struct device_attribute *attr,\n+\t\t\t\t     const char *buf, size_t len)\n+{\n+\tstruct cxl_region *cxlr = to_cxl_region(dev);\n+\tunsigned int val;\n+\tint rc;\n+\n+\trc = kstrtouint(buf, 0, &val);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n+\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n+\t\treturn rc;\n+\n+\trc = set_interleave_ways(cxlr, val);\n+\tif (rc)\n \t\treturn rc;\n-\t}\n \n \treturn len;\n }\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-02-01"
        },
        {
          "author": "alejandro.lucero-palau (author)",
          "summary": "The author addressed the concern about finding available DPA (device-physical-address) capacity to map into HPA (host-physical-address) space for CXL Type2 devices, and provided a new API cxl_request_dpa() that tries to allocate the required DPA memory. The author explained how this API works and included code changes to implement it.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "new API implementation",
            "code changes"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nRegion creation involves finding available DPA (device-physical-address)\ncapacity to map into HPA (host-physical-address) space.\n\nIn order to support CXL Type2 devices, define an API, cxl_request_dpa(),\nthat tries to allocate the DPA memory the driver requires to operate.The\nmemory requested should not be bigger than the max available HPA obtained\npreviously with cxl_get_hpa_freespace().\n\nBased on https://lore.kernel.org/linux-cxl/168592158743.1948938.7622563891193802610.stgit@dwillia2-xfh.jf.intel.com/\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\n---\n drivers/cxl/core/hdm.c | 84 ++++++++++++++++++++++++++++++++++++++++++\n drivers/cxl/cxl.h      |  1 +\n include/cxl/cxl.h      |  5 +++\n 3 files changed, 90 insertions(+)\n\ndiff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\nindex a172ce4e9b19..d60a697f12cc 100644\n--- a/drivers/cxl/core/hdm.c\n+++ b/drivers/cxl/core/hdm.c\n@@ -3,6 +3,7 @@\n #include <linux/seq_file.h>\n #include <linux/device.h>\n #include <linux/delay.h>\n+#include <cxl/cxl.h>\n \n #include \"cxlmem.h\"\n #include \"core.h\"\n@@ -546,6 +547,12 @@ bool cxl_resource_contains_addr(const struct resource *res, const resource_size_\n \treturn resource_contains(res, &_addr);\n }\n \n+/**\n+ * cxl_dpa_free - release DPA (Device Physical Address)\n+ * @cxled: endpoint decoder linked to the DPA\n+ *\n+ * Returns 0 or error.\n+ */\n int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n {\n \tstruct cxl_port *port = cxled_to_port(cxled);\n@@ -572,6 +579,7 @@ int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n \tdevm_cxl_dpa_release(cxled);\n \treturn 0;\n }\n+EXPORT_SYMBOL_NS_GPL(cxl_dpa_free, \"CXL\");\n \n int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n \t\t     enum cxl_partition_mode mode)\n@@ -603,6 +611,82 @@ int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n \treturn 0;\n }\n \n+static int find_free_decoder(struct device *dev, const void *data)\n+{\n+\tstruct cxl_endpoint_decoder *cxled;\n+\tstruct cxl_port *port;\n+\n+\tif (!is_endpoint_decoder(dev))\n+\t\treturn 0;\n+\n+\tcxled = to_cxl_endpoint_decoder(dev);\n+\tport = cxled_to_port(cxled);\n+\n+\treturn cxled->cxld.id == (port->hdm_end + 1);\n+}\n+\n+static struct cxl_endpoint_decoder *\n+cxl_find_free_decoder(struct cxl_memdev *cxlmd)\n+{\n+\tstruct cxl_port *endpoint = cxlmd->endpoint;\n+\tstruct device *dev;\n+\n+\tguard(rwsem_read)(&cxl_rwsem.dpa);\n+\tdev = device_find_child(&endpoint->dev, NULL,\n+\t\t\t\tfind_free_decoder);\n+\tif (!dev)\n+\t\treturn NULL;\n+\n+\treturn to_cxl_endpoint_decoder(dev);\n+}\n+\n+/**\n+ * cxl_request_dpa - search and reserve DPA given input constraints\n+ * @cxlmd: memdev with an endpoint port with available decoders\n+ * @mode: CXL partition mode (ram vs pmem)\n+ * @alloc: dpa size required\n+ *\n+ * Returns a pointer to a 'struct cxl_endpoint_decoder' on success or\n+ * an errno encoded pointer on failure.\n+ *\n+ * Given that a region needs to allocate from limited HPA capacity it\n+ * may be the case that a device has more mappable DPA capacity than\n+ * available HPA. The expectation is that @alloc is a driver known\n+ * value based on the device capacity but which could not be fully\n+ * available due to HPA constraints.\n+ *\n+ * Returns a pinned cxl_decoder with at least @alloc bytes of capacity\n+ * reserved, or an error pointer. The caller is also expected to own the\n+ * lifetime of the memdev registration associated with the endpoint to\n+ * pin the decoder registered as well.\n+ */\n+struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n+\t\t\t\t\t     enum cxl_partition_mode mode,\n+\t\t\t\t\t     resource_size_t alloc)\n+{\n+\tint rc;\n+\n+\tif (!IS_ALIGNED(alloc, SZ_256M))\n+\t\treturn ERR_PTR(-EINVAL);\n+\n+\tstruct cxl_endpoint_decoder *cxled __free(put_cxled) =\n+\t\tcxl_find_free_decoder(cxlmd);\n+\n+\tif (!cxled)\n+\t\treturn ERR_PTR(-ENODEV);\n+\n+\trc = cxl_dpa_set_part(cxled, mode);\n+\tif (rc)\n+\t\treturn ERR_PTR(rc);\n+\n+\trc = cxl_dpa_alloc(cxled, alloc);\n+\tif (rc)\n+\t\treturn ERR_PTR(rc);\n+\n+\treturn no_free_ptr(cxled);\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_request_dpa, \"CXL\");\n+\n static int __cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size)\n {\n \tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\ndiff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\nindex d1b010e5e1d0..2b1f7d687a0e 100644\n--- a/drivers/cxl/cxl.h\n+++ b/drivers/cxl/cxl.h\n@@ -667,6 +667,7 @@ struct cxl_root *find_cxl_root(struct cxl_port *port);\n \n DEFINE_FREE(put_cxl_root, struct cxl_root *, if (_T) put_device(&_T->port.dev))\n DEFINE_FREE(put_cxl_port, struct cxl_port *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n+DEFINE_FREE(put_cxled, struct cxl_endpoint_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxld.dev))\n DEFINE_FREE(put_cxl_root_decoder, struct cxl_root_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxlsd.cxld.dev))\n DEFINE_FREE(put_cxl_region, struct cxl_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n \ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 783ad570a6eb..4802371db00e 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -7,6 +7,7 @@\n \n #include <linux/node.h>\n #include <linux/ioport.h>\n+#include <linux/range.h>\n #include <cxl/mailbox.h>\n \n /**\n@@ -276,4 +277,8 @@ struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n \t\t\t\t\t       unsigned long flags,\n \t\t\t\t\t       resource_size_t *max);\n void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n+struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n+\t\t\t\t\t     enum cxl_partition_mode mode,\n+\t\t\t\t\t     resource_size_t alloc);\n+int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n #endif /* __CXL_CXL_H__ */\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-02-01"
        },
        {
          "author": "alejandro.lucero-palau (author)",
          "summary": "The author addressed a concern about interleaving granularity being set in user space, agreeing to factor out a common helper for kernel-driven region creation and acknowledging the need to restructure the code to handle this scenario.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged fix needed",
            "agreed to restructure"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nRegion creation based on Type3 devices is triggered from user space\nallowing memory combination through interleaving.\n\nIn preparation for kernel driven region creation, that is Type2 drivers\ntriggering region creation backed with its advertised CXL memory, factor\nout a common helper from the user-sysfs region setup forinterleave\ngranularity.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Zhi Wang <zhiw@nvidia.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\nReviewed-by: Alison Schofield <alison.schofield@intel.com>\n---\n drivers/cxl/core/region.c | 39 +++++++++++++++++++++++++--------------\n 1 file changed, 25 insertions(+), 14 deletions(-)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex ece1d3df7cf1..63c2aeb2ee1f 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -559,21 +559,14 @@ static ssize_t interleave_granularity_show(struct device *dev,\n \treturn sysfs_emit(buf, \"%d\\n\", p->interleave_granularity);\n }\n \n-static ssize_t interleave_granularity_store(struct device *dev,\n-\t\t\t\t\t    struct device_attribute *attr,\n-\t\t\t\t\t    const char *buf, size_t len)\n+static int set_interleave_granularity(struct cxl_region *cxlr, int val)\n {\n-\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(dev->parent);\n+\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n-\tstruct cxl_region *cxlr = to_cxl_region(dev);\n \tstruct cxl_region_params *p = &cxlr->params;\n-\tint rc, val;\n+\tint rc;\n \tu16 ig;\n \n-\trc = kstrtoint(buf, 0, &val);\n-\tif (rc)\n-\t\treturn rc;\n-\n \trc = granularity_to_eig(val, &ig);\n \tif (rc)\n \t\treturn rc;\n@@ -589,14 +582,32 @@ static ssize_t interleave_granularity_store(struct device *dev,\n \tif (cxld->interleave_ways > 1 && val != cxld->interleave_granularity)\n \t\treturn -EINVAL;\n \n-\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n-\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n-\t\treturn rc;\n-\n+\tlockdep_assert_held_write(&cxl_rwsem.region);\n \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n \t\treturn -EBUSY;\n \n \tp->interleave_granularity = val;\n+\treturn 0;\n+}\n+\n+static ssize_t interleave_granularity_store(struct device *dev,\n+\t\t\t\t\t    struct device_attribute *attr,\n+\t\t\t\t\t    const char *buf, size_t len)\n+{\n+\tstruct cxl_region *cxlr = to_cxl_region(dev);\n+\tint rc, val;\n+\n+\trc = kstrtoint(buf, 0, &val);\n+\tif (rc)\n+\t\treturn rc;\n+\n+\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n+\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n+\t\treturn rc;\n+\n+\trc = set_interleave_granularity(cxlr, val);\n+\tif (rc)\n+\t\treturn rc;\n \n \treturn len;\n }\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-02-01"
        },
        {
          "author": "alejandro.lucero-palau (author)",
          "summary": "Author addressed a concern about using the CXL API to get the Device Physical Address (DPA) and agreed to use it through an endpoint decoder.",
          "sentiment": "positive",
          "sentiment_signals": [
            "agreed",
            "implemented"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nUse cxl api for getting DPA (Device Physical Address) to use through an\nendpoint decoder.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Martin Habets <habetsm.xilinx@gmail.com>\nAcked-by: Edward Cree <ecree.xilinx@gmail.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\n---\n drivers/net/ethernet/sfc/efx_cxl.c | 9 +++++++++\n 1 file changed, 9 insertions(+)\n\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\nindex 1a4c1097c315..2cfd0a46225f 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.c\n+++ b/drivers/net/ethernet/sfc/efx_cxl.c\n@@ -126,6 +126,14 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \t\t\tcxl_put_root_decoder(cxl->cxlrd);\n \t\t\treturn -ENOSPC;\n \t\t}\n+\n+\t\tcxl->cxled = cxl_request_dpa(cxl->cxlmd, CXL_PARTMODE_RAM,\n+\t\t\t\t\t     EFX_CTPIO_BUFFER_SIZE);\n+\t\tif (IS_ERR(cxl->cxled)) {\n+\t\t\tpci_err(pci_dev, \"CXL accel request DPA failed\");\n+\t\t\tcxl_put_root_decoder(cxl->cxlrd);\n+\t\t\treturn PTR_ERR(cxl->cxled);\n+\t\t}\n \t}\n \n \tprobe_data->cxl = cxl;\n@@ -141,6 +149,7 @@ void efx_cxl_exit(struct efx_probe_data *probe_data)\n \t\tiounmap(probe_data->cxl->ctpio_cxl);\n \t\tcxl_unregister_region(probe_data->cxl->efx_region);\n \t} else {\n+\t\tcxl_dpa_free(probe_data->cxl->cxled);\n \t\tcxl_put_root_decoder(probe_data->cxl->cxlrd);\n \t}\n }\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-02-01"
        },
        {
          "author": "alejandro.lucero-palau (author)",
          "summary": "The author addressed a concern about type2 CXL devices using host-managed memory, explaining that it should not be available to other uses and adding code to skip device-dax registration for such regions.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nBy definition a type2 cxl device will use the host managed memory for\nspecific functionality, therefore it should not be available to other\nuses.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Davidlohr Bueso <daves@stgolabs.net>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n---\n drivers/cxl/core/region.c | 7 +++++++\n 1 file changed, 7 insertions(+)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 293e63dfef22..12df717cc881 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -4441,6 +4441,13 @@ static int cxl_region_probe(struct device *dev)\n \tif (rc)\n \t\treturn rc;\n \n+\t/*\n+\t * HDM-D[B] (device-memory) regions have accelerator specific usage.\n+\t * Skip device-dax registration.\n+\t */\n+\tif (cxlr->type == CXL_DECODER_DEVMEM)\n+\t\treturn 0;\n+\n \t/*\n \t * From this point on any path that changes the region's state away from\n \t * CXL_CONFIG_COMMIT is also responsible for releasing the driver.\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-02-01"
        },
        {
          "author": "alejandro.lucero-palau (author)",
          "summary": "Author acknowledged that the CXL endpoint removal callback needs to be handled by disabling CXL-based PIO buffers, agreed to add this handling in a future patch.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged fix needed"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nA PIO buffer is a region of device memory to which the driver can write a\npacket for TX, with the device handling the transmit doorbell without\nrequiring a DMA for getting the packet data, which helps reducing latency\nin certain exchanges. With CXL mem protocol this latency can be lowered\nfurther.\n\nWith a device supporting CXL and successfully initialised, use the cxl\nregion to map the memory range and use this mapping for PIO buffers.\n\nAdd the disabling of those CXL-based PIO buffers if the callback for\npotential cxl endpoint removal by the CXL code happens.\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\n---\n drivers/net/ethernet/sfc/ef10.c       | 50 +++++++++++++++++++++++----\n drivers/net/ethernet/sfc/efx_cxl.c    | 33 ++++++++++++++----\n drivers/net/ethernet/sfc/net_driver.h |  2 ++\n drivers/net/ethernet/sfc/nic.h        |  3 ++\n 4 files changed, 75 insertions(+), 13 deletions(-)\n\ndiff --git a/drivers/net/ethernet/sfc/ef10.c b/drivers/net/ethernet/sfc/ef10.c\nindex fcec81f862ec..2bb6d3136c7c 100644\n--- a/drivers/net/ethernet/sfc/ef10.c\n+++ b/drivers/net/ethernet/sfc/ef10.c\n@@ -24,6 +24,7 @@\n #include <linux/wait.h>\n #include <linux/workqueue.h>\n #include <net/udp_tunnel.h>\n+#include \"efx_cxl.h\"\n \n /* Hardware control for EF10 architecture including 'Huntington'. */\n \n@@ -106,7 +107,7 @@ static int efx_ef10_get_vf_index(struct efx_nic *efx)\n \n static int efx_ef10_init_datapath_caps(struct efx_nic *efx)\n {\n-\tMCDI_DECLARE_BUF(outbuf, MC_CMD_GET_CAPABILITIES_V4_OUT_LEN);\n+\tMCDI_DECLARE_BUF(outbuf, MC_CMD_GET_CAPABILITIES_V7_OUT_LEN);\n \tstruct efx_ef10_nic_data *nic_data = efx->nic_data;\n \tsize_t outlen;\n \tint rc;\n@@ -177,6 +178,12 @@ static int efx_ef10_init_datapath_caps(struct efx_nic *efx)\n \t\t\t  efx->num_mac_stats);\n \t}\n \n+\tif (outlen < MC_CMD_GET_CAPABILITIES_V7_OUT_LEN)\n+\t\tnic_data->datapath_caps3 = 0;\n+\telse\n+\t\tnic_data->datapath_caps3 = MCDI_DWORD(outbuf,\n+\t\t\t\t\t\t      GET_CAPABILITIES_V7_OUT_FLAGS3);\n+\n \treturn 0;\n }\n \n@@ -919,6 +926,9 @@ static void efx_ef10_forget_old_piobufs(struct efx_nic *efx)\n static void efx_ef10_remove(struct efx_nic *efx)\n {\n \tstruct efx_ef10_nic_data *nic_data = efx->nic_data;\n+#ifdef CONFIG_SFC_CXL\n+\tstruct efx_probe_data *probe_data;\n+#endif\n \tint rc;\n \n #ifdef CONFIG_SFC_SRIOV\n@@ -949,7 +959,12 @@ static void efx_ef10_remove(struct efx_nic *efx)\n \n \tefx_mcdi_rx_free_indir_table(efx);\n \n+#ifdef CONFIG_SFC_CXL\n+\tprobe_data = container_of(efx, struct efx_probe_data, efx);\n+\tif (nic_data->wc_membase && !probe_data->cxl_pio_in_use)\n+#else\n \tif (nic_data->wc_membase)\n+#endif\n \t\tiounmap(nic_data->wc_membase);\n \n \trc = efx_mcdi_free_vis(efx);\n@@ -1140,6 +1155,9 @@ static int efx_ef10_dimension_resources(struct efx_nic *efx)\n \tunsigned int channel_vis, pio_write_vi_base, max_vis;\n \tstruct efx_ef10_nic_data *nic_data = efx->nic_data;\n \tunsigned int uc_mem_map_size, wc_mem_map_size;\n+#ifdef CONFIG_SFC_CXL\n+\tstruct efx_probe_data *probe_data;\n+#endif\n \tvoid __iomem *membase;\n \tint rc;\n \n@@ -1263,8 +1281,25 @@ static int efx_ef10_dimension_resources(struct efx_nic *efx)\n \tiounmap(efx->membase);\n \tefx->membase = membase;\n \n-\t/* Set up the WC mapping if needed */\n-\tif (wc_mem_map_size) {\n+\tif (!wc_mem_map_size)\n+\t\tgoto skip_pio;\n+\n+\t/* Set up the WC mapping */\n+\n+#ifdef CONFIG_SFC_CXL\n+\tprobe_data = container_of(efx, struct efx_probe_data, efx);\n+\tif ((nic_data->datapath_caps3 &\n+\t    (1 << MC_CMD_GET_CAPABILITIES_V7_OUT_CXL_CONFIG_ENABLE_LBN)) &&\n+\t    probe_data->cxl_pio_initialised) {\n+\t\t/* Using PIO through CXL mapping? */\n+\t\tnic_data->pio_write_base = probe_data->cxl->ctpio_cxl +\n+\t\t\t\t\t   (pio_write_vi_base * efx->vi_stride +\n+\t\t\t\t\t    ER_DZ_TX_PIOBUF - uc_mem_map_size);\n+\t\tprobe_data->cxl_pio_in_use = true;\n+\t} else\n+#endif\n+\t{\n+\t\t/* Using legacy PIO BAR mapping */\n \t\tnic_data->wc_membase = ioremap_wc(efx->membase_phys +\n \t\t\t\t\t\t  uc_mem_map_size,\n \t\t\t\t\t\t  wc_mem_map_size);\n@@ -1279,12 +1314,13 @@ static int efx_ef10_dimension_resources(struct efx_nic *efx)\n \t\t\tnic_data->wc_membase +\n \t\t\t(pio_write_vi_base * efx->vi_stride + ER_DZ_TX_PIOBUF -\n \t\t\t uc_mem_map_size);\n-\n-\t\trc = efx_ef10_link_piobufs(efx);\n-\t\tif (rc)\n-\t\t\tefx_ef10_free_piobufs(efx);\n \t}\n \n+\trc = efx_ef10_link_piobufs(efx);\n+\tif (rc)\n+\t\tefx_ef10_free_piobufs(efx);\n+\n+skip_pio:\n \tnetif_dbg(efx, probe, efx->net_dev,\n \t\t  \"memory BAR at %pa (virtual %p+%x UC, %p+%x WC)\\n\",\n \t\t  &efx->membase_phys, efx->membase, uc_mem_map_size,\ndiff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\nindex 4d5f3974e51d..c13e1f2bf7ea 100644\n--- a/drivers/net/ethernet/sfc/efx_cxl.c\n+++ b/drivers/net/ethernet/sfc/efx_cxl.c\n@@ -11,6 +11,7 @@\n #include <cxl/pci.h>\n #include \"net_driver.h\"\n #include \"efx_cxl.h\"\n+#include \"efx.h\"\n \n #define EFX_CTPIO_BUFFER_SIZE\tSZ_256M\n \n@@ -138,14 +139,34 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n \t\tcxl->efx_region = cxl_create_region(cxl->cxlrd, &cxl->cxled, 1);\n \t\tif (IS_ERR(cxl->efx_region)) {\n \t\t\tpci_err(pci_dev, \"CXL accel create region failed\");\n-\t\t\tcxl_put_root_decoder(cxl->cxlrd);\n-\t\t\tcxl_dpa_free(cxl->cxled);\n-\t\t\treturn PTR_ERR(cxl->efx_region);\n+\t\t\trc = PTR_ERR(cxl->efx_region);\n+\t\t\tgoto err_region;\n+\t\t}\n+\n+\t\trc = cxl_get_region_range(cxl->efx_region, &range);\n+\t\tif (rc) {\n+\t\t\tpci_err(pci_dev, \"CXL getting regions params failed\");\n+\t\t\tgoto err_map;\n+\t\t}\n+\n+\t\tcxl->ctpio_cxl = ioremap(range.start, range.end - range.start + 1);\n+\t\tif (!cxl->ctpio_cxl) {\n+\t\t\tpci_err(pci_dev, \"CXL ioremap region (%pra) failed\", &range);\n+\t\t\trc = -ENOMEM;\n+\t\t\tgoto err_map;\n \t\t}\n \t}\n \n \tprobe_data->cxl = cxl;\n+\tprobe_data->cxl_pio_initialised = true;\n \treturn 0;\n+\n+err_map:\n+\tcxl_unregister_region(cxl->efx_region);\n+err_region:\n+\tcxl_put_root_decoder(cxl->cxlrd);\n+\tcxl_dpa_free(cxl->cxled);\n+\treturn rc;\n }\n \n void efx_cxl_exit(struct efx_probe_data *probe_data)\n@@ -153,9 +174,9 @@ void efx_cxl_exit(struct efx_probe_data *probe_data)\n \tif (!probe_data->cxl)\n \t\treturn;\n \n-\tif (probe_data->cxl->hdm_was_committed) {\n-\t\tiounmap(probe_data->cxl->ctpio_cxl);\n-\t} else {\n+\tiounmap(probe_data->cxl->ctpio_cxl);\n+\n+\tif (!probe_data->cxl->hdm_was_committed) {\n \t\tcxl_dpa_free(probe_data->cxl->cxled);\n \t\tcxl_put_root_decoder(probe_data->cxl->cxlrd);\n \t}\ndiff --git a/drivers/net/ethernet/sfc/net_driver.h b/drivers/net/ethernet/sfc/net_driver.h\nindex 3964b2c56609..bea4eecdf842 100644\n--- a/drivers/net/ethernet/sfc/net_driver.h\n+++ b/drivers/net/ethernet/sfc/net_driver.h\n@@ -1207,6 +1207,7 @@ struct efx_cxl;\n  * @efx: Efx NIC details\n  * @cxl: details of related cxl objects\n  * @cxl_pio_initialised: cxl initialization outcome.\n+ * @cxl_pio_in_use: PIO using CXL mapping\n  */\n struct efx_probe_data {\n \tstruct pci_dev *pci_dev;\n@@ -1214,6 +1215,7 @@ struct efx_probe_data {\n #ifdef CONFIG_SFC_CXL\n \tstruct efx_cxl *cxl;\n \tbool cxl_pio_initialised;\n+\tbool cxl_pio_in_use;\n #endif\n };\n \ndiff --git a/drivers/net/ethernet/sfc/nic.h b/drivers/net/ethernet/sfc/nic.h\nindex 9fa5c4c713ab..c87cc9214690 100644\n--- a/drivers/net/ethernet/sfc/nic.h\n+++ b/drivers/net/ethernet/sfc/nic.h\n@@ -152,6 +152,8 @@ enum {\n  *\t%MC_CMD_GET_CAPABILITIES response)\n  * @datapath_caps2: Further Capabilities of datapath firmware (FLAGS2 field of\n  * %MC_CMD_GET_CAPABILITIES response)\n+ * @datapath_caps3: Further Capabilities of datapath firmware (FLAGS3 field of\n+ * %MC_CMD_GET_CAPABILITIES response)\n  * @rx_dpcpu_fw_id: Firmware ID of the RxDPCPU\n  * @tx_dpcpu_fw_id: Firmware ID of the TxDPCPU\n  * @must_probe_vswitching: Flag: vswitching has yet to be setup after MC reboot\n@@ -186,6 +188,7 @@ struct efx_ef10_nic_data {\n \tbool must_check_datapath_caps;\n \tu32 datapath_caps;\n \tu32 datapath_caps2;\n+\tu32 datapath_caps3;\n \tunsigned int rx_dpcpu_fw_id;\n \tunsigned int tx_dpcpu_fw_id;\n \tbool must_probe_vswitching;\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-02-01"
        },
        {
          "author": "alejandro.lucero-palau (author)",
          "summary": "Author acknowledged that type2 support should allow accelerator drivers to create CXL regions from kernel code, explained how this would be achieved by adding functionality and integrating it with current memory expander support.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "acknowledged",
            "explained"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "From: Alejandro Lucero <alucerop@amd.com>\n\nCreating a CXL region requires userspace intervention through the cxl\nsysfs files. Type2 support should allow accelerator drivers to create\nsuch cxl region from kernel code.\n\nAdding that functionality and integrating it with current support for\nmemory expanders.\n\nBased on https://lore.kernel.org/linux-cxl/168592159835.1948938.1647215579839222774.stgit@dwillia2-xfh.jf.intel.com/\n\nSigned-off-by: Alejandro Lucero <alucerop@amd.com>\nReviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\n---\n drivers/cxl/core/region.c | 131 ++++++++++++++++++++++++++++++++++++--\n include/cxl/cxl.h         |   3 +\n 2 files changed, 127 insertions(+), 7 deletions(-)\n\ndiff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\nindex 63c2aeb2ee1f..293e63dfef22 100644\n--- a/drivers/cxl/core/region.c\n+++ b/drivers/cxl/core/region.c\n@@ -2944,6 +2944,14 @@ cxl_find_region_by_name(struct cxl_root_decoder *cxlrd, const char *name)\n \treturn to_cxl_region(region_dev);\n }\n \n+static void drop_region(struct cxl_region *cxlr)\n+{\n+\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n+\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n+\n+\tdevm_release_action(port->uport_dev, __unregister_region, cxlr);\n+}\n+\n static ssize_t delete_region_store(struct device *dev,\n \t\t\t\t   struct device_attribute *attr,\n \t\t\t\t   const char *buf, size_t len)\n@@ -4047,14 +4055,12 @@ static int __construct_region(struct cxl_region *cxlr,\n \treturn 0;\n }\n \n-/* Establish an empty region covering the given HPA range */\n-static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n-\t\t\t\t\t   struct cxl_endpoint_decoder *cxled)\n+static struct cxl_region *construct_region_begin(struct cxl_root_decoder *cxlrd,\n+\t\t\t\t\t\t struct cxl_endpoint_decoder *cxled)\n {\n \tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n-\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n \tstruct cxl_dev_state *cxlds = cxlmd->cxlds;\n-\tint rc, part = READ_ONCE(cxled->part);\n+\tint part = READ_ONCE(cxled->part);\n \tstruct cxl_region *cxlr;\n \n \tdo {\n@@ -4063,13 +4069,26 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n \t\t\t\t       cxled->cxld.target_type);\n \t} while (IS_ERR(cxlr) && PTR_ERR(cxlr) == -EBUSY);\n \n-\tif (IS_ERR(cxlr)) {\n+\tif (IS_ERR(cxlr))\n \t\tdev_err(cxlmd->dev.parent,\n \t\t\t\"%s:%s: %s failed assign region: %ld\\n\",\n \t\t\tdev_name(&cxlmd->dev), dev_name(&cxled->cxld.dev),\n \t\t\t__func__, PTR_ERR(cxlr));\n+\n+\treturn cxlr;\n+}\n+\n+/* Establish an empty region covering the given HPA range */\n+static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n+\t\t\t\t\t   struct cxl_endpoint_decoder *cxled)\n+{\n+\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n+\tstruct cxl_region *cxlr;\n+\tint rc;\n+\n+\tcxlr = construct_region_begin(cxlrd, cxled);\n+\tif (IS_ERR(cxlr))\n \t\treturn cxlr;\n-\t}\n \n \trc = __construct_region(cxlr, cxlrd, cxled);\n \tif (rc) {\n@@ -4080,6 +4099,104 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n \treturn cxlr;\n }\n \n+DEFINE_FREE(cxl_region_drop, struct cxl_region *, if (_T) drop_region(_T))\n+\n+static struct cxl_region *\n+__construct_new_region(struct cxl_root_decoder *cxlrd,\n+\t\t       struct cxl_endpoint_decoder **cxled, int ways)\n+{\n+\tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled[0]);\n+\tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n+\tstruct cxl_region_params *p;\n+\tresource_size_t size = 0;\n+\tint rc, i;\n+\n+\tstruct cxl_region *cxlr __free(cxl_region_drop) =\n+\t\tconstruct_region_begin(cxlrd, cxled[0]);\n+\tif (IS_ERR(cxlr))\n+\t\treturn cxlr;\n+\n+\tguard(rwsem_write)(&cxl_rwsem.region);\n+\n+\t/*\n+\t * Sanity check. This should not happen with an accel driver handling\n+\t * the region creation.\n+\t */\n+\tp = &cxlr->params;\n+\tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE) {\n+\t\tdev_err(cxlmd->dev.parent,\n+\t\t\t\"%s:%s: %s  unexpected region state\\n\",\n+\t\t\tdev_name(&cxlmd->dev), dev_name(&cxled[0]->cxld.dev),\n+\t\t\t__func__);\n+\t\treturn ERR_PTR(-EBUSY);\n+\t}\n+\n+\trc = set_interleave_ways(cxlr, ways);\n+\tif (rc)\n+\t\treturn ERR_PTR(rc);\n+\n+\trc = set_interleave_granularity(cxlr, cxld->interleave_granularity);\n+\tif (rc)\n+\t\treturn ERR_PTR(rc);\n+\n+\tscoped_guard(rwsem_read, &cxl_rwsem.dpa) {\n+\t\tfor (i = 0; i < ways; i++) {\n+\t\t\tif (!cxled[i]->dpa_res)\n+\t\t\t\treturn ERR_PTR(-EINVAL);\n+\t\t\tsize += resource_size(cxled[i]->dpa_res);\n+\t\t}\n+\n+\t\trc = alloc_hpa(cxlr, size);\n+\t\tif (rc)\n+\t\t\treturn ERR_PTR(rc);\n+\n+\t\tfor (i = 0; i < ways; i++) {\n+\t\t\trc = cxl_region_attach(cxlr, cxled[i], 0);\n+\t\t\tif (rc)\n+\t\t\t\treturn ERR_PTR(rc);\n+\t\t}\n+\t}\n+\n+\trc = cxl_region_decode_commit(cxlr);\n+\tif (rc)\n+\t\treturn ERR_PTR(rc);\n+\n+\tp->state = CXL_CONFIG_COMMIT;\n+\n+\treturn no_free_ptr(cxlr);\n+}\n+\n+/**\n+ * cxl_create_region - Establish a region given an endpoint decoder\n+ * @cxlrd: root decoder to allocate HPA\n+ * @cxled: endpoint decoders with reserved DPA capacity\n+ * @ways: interleave ways required\n+ *\n+ * Returns a fully formed region in the commit state and attached to the\n+ * cxl_region driver.\n+ */\n+struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n+\t\t\t\t     struct cxl_endpoint_decoder **cxled,\n+\t\t\t\t     int ways)\n+{\n+\tstruct cxl_region *cxlr;\n+\n+\tmutex_lock(&cxlrd->range_lock);\n+\tcxlr = __construct_new_region(cxlrd, cxled, ways);\n+\tmutex_unlock(&cxlrd->range_lock);\n+\tif (IS_ERR(cxlr))\n+\t\treturn cxlr;\n+\n+\tif (device_attach(&cxlr->dev) <= 0) {\n+\t\tdev_err(&cxlr->dev, \"failed to create region\\n\");\n+\t\tdrop_region(cxlr);\n+\t\treturn ERR_PTR(-ENODEV);\n+\t}\n+\n+\treturn cxlr;\n+}\n+EXPORT_SYMBOL_NS_GPL(cxl_create_region, \"CXL\");\n+\n static struct cxl_region *\n cxl_find_region_by_range(struct cxl_root_decoder *cxlrd, struct range *hpa)\n {\ndiff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\nindex 4802371db00e..50acbd13bcf8 100644\n--- a/include/cxl/cxl.h\n+++ b/include/cxl/cxl.h\n@@ -281,4 +281,7 @@ struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n \t\t\t\t\t     enum cxl_partition_mode mode,\n \t\t\t\t\t     resource_size_t alloc);\n int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n+struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n+\t\t\t\t     struct cxl_endpoint_decoder **cxled,\n+\t\t\t\t     int ways);\n #endif /* __CXL_CXL_H__ */\n-- \n2.34.1",
          "reply_to": "",
          "message_date": "2026-02-01"
        }
      ],
      "analysis_source": "llm"
    },
    "2026-02-11": {
      "report_file": "2026-02-21_ollama_llama3.1-8b.html",
      "developer": "Gregory Price",
      "reviews": [
        {
          "author": "Cheatham, Benjamin",
          "summary": "Reviewer Cheatham suggested moving the function call to be the first thing in the function, which would avoid acquiring a lock in cxl_region_can_probe() above.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "minor optimization"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Minor nit: Should probably move this to be the first thing in the function. It would save\nhaving to acquire a lock in cxl_region_can_probe() above. Keep my reviewed-by either way,\nit's really just a minor optimization.",
          "reply_to": "alejandro.lucero-palau",
          "message_date": "2026-02-11"
        },
        {
          "author": "Cheatham, Benjamin",
          "summary": "Reviewer Cheatham questioned the complexity of the code and suggested simplification by removing the outer loop in cxl_get_hpa_freespace() since ctx->host_bridges is only set to one host bridge at that point, and also proposed changing ctx->host_bridges to a struct device * const.\n\nThe reviewer noted that interleave_ways is hardcoded to 1 and suggested removing this portion of the function or adding a doc comment explaining its current unused state.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "complexity",
            "simplification",
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "This may be over complicated. I'm not quite sure how it works (I'm just slow today I guess), but I understand\nwhat the intention is based on the debug print below. My issue is that ctx->host_bridges is only set to 1 host\nbridge (endpoint->host_bridge) in cxl_get_hpa_freespace(), which is the only caller of this function. At that\npoint, why have the outer loop at all? At that point, you could also simplify ctx->host_bridges to only\nbe a struct device * const.\n\nMaybe this gets called elsewhere later on in the series? I haven't looked at the rest yet. If I'm wrong, then\nI'd probably add a comment saying what the cxlsd->target[] entries are supposed to be pointing at.\n\n---\n\nMentioned earlier, interleave_ways is effectively hardcoded to 1 (unless I'm misunderstanding\nsomething). I think what you want here is to go to the CXL root and pass in the children (i.e. host bridges)?\nI'm not sure of what the fix is to get the intended behavior.\n\nIt may be worth getting rid of the interleave_ways portion of this function and\nadd it later when someone needs it. You could also explain it's hard coded to 1/unused\nin the doc comment if you know of an immediate need for it.",
          "reply_to": "alejandro.lucero-palau",
          "message_date": "2026-02-11"
        },
        {
          "author": "Cheatham, Benjamin",
          "summary": "Gave Reviewed-by",
          "sentiment": "positive",
          "sentiment_signals": [],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> By definition a type2 cxl device will use the host managed memory for\n> specific functionality, therefore it should not be available to other\n> uses.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Davidlohr Bueso <daves@stgolabs.net>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> ---\n>  drivers/cxl/core/region.c | 7 +++++++\n>  1 file changed, 7 insertions(+)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index 293e63dfef22..12df717cc881 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -4441,6 +4441,13 @@ static int cxl_region_probe(struct device *dev)\n>  \tif (rc)\n>  \t\treturn rc;\n>  \n> +\t/*\n> +\t * HDM-D[B] (device-memory) regions have accelerator specific usage.\n> +\t * Skip device-dax registration.\n> +\t */\n> +\tif (cxlr->type == CXL_DECODER_DEVMEM)\n> +\t\treturn 0;\n\nMinor nit: Should probably move this to be the first thing in the function. It would save\nhaving to acquire a lock in cxl_region_can_probe() above. Keep my reviewed-by either way,\nit's really just a minor optimization.\n> +\n>  \t/*\n>  \t * From this point on any path that changes the region's state away from\n>  \t * CXL_CONFIG_COMMIT is also responsible for releasing the driver.\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> CXL region creation involves allocating capacity from Device Physical\n> Address (DPA) and assigning it to decode a given Host Physical Address\n> (HPA). Before determining how much DPA to allocate the amount of available\n> HPA must be determined. Also, not all HPA is created equal, some HPA\n> targets RAM, some targets PMEM, some is prepared for device-memory flows\n> like HDM-D and HDM-DB, and some is HDM-H (host-only).\n> \n> In order to support Type2 CXL devices, wrap all of those concerns into\n> an API that retrieves a root decoder (platform CXL window) that fits the\n> specified constraints and the capacity available for a new region.\n> \n> Add a complementary function for releasing the reference to such root\n> decoder.\n> \n> Based on https://lore.kernel.org/linux-cxl/168592159290.1948938.13522227102445462976.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> ---\n>  drivers/cxl/core/region.c | 164 ++++++++++++++++++++++++++++++++++++++\n>  drivers/cxl/cxl.h         |   3 +\n>  include/cxl/cxl.h         |   6 ++\n>  3 files changed, 173 insertions(+)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index 954b8fcdbac6..bdefd088f5f1 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -705,6 +705,170 @@ static int free_hpa(struct cxl_region *cxlr)\n>  \treturn 0;\n>  }\n>  \n> +struct cxlrd_max_context {\n> +\tstruct device * const *host_bridges;\n> +\tint interleave_ways;\n> +\tunsigned long flags;\n> +\tresource_size_t max_hpa;\n> +\tstruct cxl_root_decoder *cxlrd;\n> +};\n> +\n> +static int find_max_hpa(struct device *dev, void *data)\n> +{\n> +\tstruct cxlrd_max_context *ctx = data;\n> +\tstruct cxl_switch_decoder *cxlsd;\n> +\tstruct cxl_root_decoder *cxlrd;\n> +\tstruct resource *res, *prev;\n> +\tstruct cxl_decoder *cxld;\n> +\tresource_size_t free = 0;\n> +\tresource_size_t max;\n> +\tint found = 0;\n> +\n> +\tif (!is_root_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxlrd = to_cxl_root_decoder(dev);\n> +\tcxlsd = &cxlrd->cxlsd;\n> +\tcxld = &cxlsd->cxld;\n> +\n> +\tif ((cxld->flags & ctx->flags) != ctx->flags) {\n> +\t\tdev_dbg(dev, \"flags not matching: %08lx vs %08lx\\n\",\n> +\t\t\tcxld->flags, ctx->flags);\n> +\t\treturn 0;\n> +\t}\n> +\n> +\tfor (int i = 0; i < ctx->interleave_ways; i++) {\n> +\t\tfor (int j = 0; j < ctx->interleave_ways; j++) {\n> +\t\t\tif (ctx->host_bridges[i] == cxlsd->target[j]->dport_dev) {\n> +\t\t\t\tfound++;\n> +\t\t\t\tbreak;\n> +\t\t\t}\n> +\t\t}\n> +\t}\n\nThis may be over complicated. I'm not quite sure how it works (I'm just slow today I guess), but I understand\nwhat the intention is based on the debug print below. My issue is that ctx->host_bridges is only set to 1 host\nbridge (endpoint->host_bridge) in cxl_get_hpa_freespace(), which is the only caller of this function. At that\npoint, why have the outer loop at all? At that point, you could also simplify ctx->host_bridges to only\nbe a struct device * const.\n\nMaybe this gets called elsewhere later on in the series? I haven't looked at the rest yet. If I'm wrong, then\nI'd probably add a comment saying what the cxlsd->target[] entries are supposed to be pointing at.\n> +\n> +\tif (found != ctx->interleave_ways) {\n> +\t\tdev_dbg(dev,\n> +\t\t\t\"Not enough host bridges. Found %d for %d interleave ways requested\\n\",\n> +\t\t\tfound, ctx->interleave_ways);\n> +\t\treturn 0;\n> +\t}\n> +\n> +\t/*\n> +\t * Walk the root decoder resource range relying on cxl_rwsem.region to\n> +\t * preclude sibling arrival/departure and find the largest free space\n> +\t * gap.\n> +\t */\n> +\tlockdep_assert_held_read(&cxl_rwsem.region);\n> +\tres = cxlrd->res->child;\n> +\n> +\t/* With no resource child the whole parent resource is available */\n> +\tif (!res)\n> +\t\tmax = resource_size(cxlrd->res);\n> +\telse\n> +\t\tmax = 0;\n> +\n> +\tfor (prev = NULL; res; prev = res, res = res->sibling) {\n> +\t\tif (!prev && res->start == cxlrd->res->start &&\n> +\t\t    res->end == cxlrd->res->end) {\n> +\t\t\tmax = resource_size(cxlrd->res);\n> +\t\t\tbreak;\n> +\t\t}\n> +\t\t/*\n> +\t\t * Sanity check for preventing arithmetic problems below as a\n> +\t\t * resource with size 0 could imply using the end field below\n> +\t\t * when set to unsigned zero - 1 or all f in hex.\n> +\t\t */\n> +\t\tif (prev && !resource_size(prev))\n> +\t\t\tcontinue;\n> +\n> +\t\tif (!prev && res->start > cxlrd->res->start) {\n> +\t\t\tfree = res->start - cxlrd->res->start;\n> +\t\t\tmax = max(free, max);\n> +\t\t}\n> +\t\tif (prev && res->start > prev->end + 1) {\n> +\t\t\tfree = res->start - prev->end + 1;\n> +\t\t\tmax = max(free, max);\n> +\t\t}\n> +\t}\n> +\n> +\tif (prev && prev->end + 1 < cxlrd->res->end + 1) {\n> +\t\tfree = cxlrd->res->end + 1 - prev->end + 1;\n> +\t\tmax = max(free, max);\n> +\t}\n> +\n> +\tdev_dbg(cxlrd_dev(cxlrd), \"found %pa bytes of free space\\n\", &max);\n> +\tif (max > ctx->max_hpa) {\n> +\t\tif (ctx->cxlrd)\n> +\t\t\tput_device(cxlrd_dev(ctx->cxlrd));\n> +\t\tget_device(cxlrd_dev(cxlrd));\n> +\t\tctx->cxlrd = cxlrd;\n> +\t\tctx->max_hpa = max;\n> +\t}\n> +\treturn 0;\n> +}\n> +\n> +/**\n> + * cxl_get_hpa_freespace - find a root decoder with free capacity per constraints\n> + * @cxlmd: the mem device requiring the HPA\n> + * @interleave_ways: number of entries in @host_bridges\n> + * @flags: CXL_DECODER_F flags for selecting RAM vs PMEM, and Type2 device\n> + * @max_avail_contig: output parameter of max contiguous bytes available in the\n> + *\t\t      returned decoder\n> + *\n> + * Returns a pointer to a struct cxl_root_decoder\n> + *\n> + * The return tuple of a 'struct cxl_root_decoder' and 'bytes available given\n> + * in (@max_avail_contig))' is a point in time snapshot. If by the time the\n> + * caller goes to use this decoder and its capacity is reduced then caller needs\n> + * to loop and retry.\n> + *\n> + * The returned root decoder has an elevated reference count that needs to be\n> + * put with cxl_put_root_decoder(cxlrd).\n> + */\n> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t       int interleave_ways,\n> +\t\t\t\t\t       unsigned long flags,\n> +\t\t\t\t\t       resource_size_t *max_avail_contig)\n> +{\n> +\tstruct cxlrd_max_context ctx = {\n> +\t\t.flags = flags,\n> +\t\t.interleave_ways = interleave_ways,\n> +\t};\n> +\tstruct cxl_port *root_port;\n> +\tstruct cxl_port *endpoint;\n> +\n> +\tendpoint = cxlmd->endpoint;\n> +\tif (!endpoint) {\n> +\t\tdev_dbg(&cxlmd->dev, \"endpoint not linked to memdev\\n\");\n> +\t\treturn ERR_PTR(-ENXIO);\n> +\t}\n> +\n> +\tctx.host_bridges = &endpoint->host_bridge;\n\nMentioned earlier, interleave_ways is effectively hardcoded to 1 (unless I'm misunderstanding\nsomething). I think what you want here is to go to the CXL root and pass in the children (i.e. host bridges)?\nI'm not sure of what the fix is to get the intended behavior.\n\nIt may be worth getting rid of the interleave_ways portion of this function and\nadd it later when someone needs it. You could also explain it's hard coded to 1/unused\nin the doc comment if you know of an immediate need for it.\n\n> +\n> +\tstruct cxl_root *root __free(put_cxl_root) = find_cxl_root(endpoint);\n> +\tif (!root) {\n> +\t\tdev_dbg(&endpoint->dev, \"endpoint is not related to a root port\\n\");\n> +\t\treturn ERR_PTR(-ENXIO);\n> +\t}\n> +\n> +\troot_port = &root->port;\n> +\tscoped_guard(rwsem_read, &cxl_rwsem.region)\n> +\t\tdevice_for_each_child(&root_port->dev, &ctx, find_max_hpa);\n\nCan just use a guard() here.\n\n> +\n> +\tif (!ctx.cxlrd)\n> +\t\treturn ERR_PTR(-ENOMEM);\n> +\n> +\t*max_avail_contig = ctx.max_hpa;\n> +\treturn ctx.cxlrd;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_get_hpa_freespace, \"CXL\");\n> +\n> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd)\n> +{\n> +\tput_device(cxlrd_dev(cxlrd));\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_put_root_decoder, \"CXL\");\n> +\n>  static ssize_t size_store(struct device *dev, struct device_attribute *attr,\n>  \t\t\t  const char *buf, size_t len)\n>  {\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index 944c5d1ccceb..c7d9b2c2908f 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -706,6 +706,9 @@ struct cxl_root_decoder *to_cxl_root_decoder(struct device *dev);\n>  struct cxl_switch_decoder *to_cxl_switch_decoder(struct device *dev);\n>  struct cxl_endpoint_decoder *to_cxl_endpoint_decoder(struct device *dev);\n>  bool is_root_decoder(struct device *dev);\n> +\n> +#define cxlrd_dev(cxlrd) (&(cxlrd)->cxlsd.cxld.dev)\n> +\n>  bool is_switch_decoder(struct device *dev);\n>  bool is_endpoint_decoder(struct device *dev);\n>  struct cxl_root_decoder *cxl_root_decoder_alloc(struct cxl_port *port,\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 92880c26b2d5..834dc7e78934 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -255,4 +255,10 @@ struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n>  struct range;\n>  int cxl_get_region_range(struct cxl_region *region, struct range *range);\n>  void cxl_unregister_region(struct cxl_region *cxlr);\n> +struct cxl_port;\n> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t       int interleave_ways,\n> +\t\t\t\t\t       unsigned long flags,\n> +\t\t\t\t\t       resource_size_t *max);\n> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Check if device HDM is already committed during firmware/BIOS\n> initialization.\n> \n> A CXL region should exist if so after memdev allocation/initialization.\n> Get HPA from region and map it.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> ---\n>  drivers/net/ethernet/sfc/efx_cxl.c | 28 +++++++++++++++++++++++++++-\n>  1 file changed, 27 insertions(+), 1 deletion(-)\n> \n> diff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\n> index a77ef4783fcb..3536eccf1b2a 100644\n> --- a/drivers/net/ethernet/sfc/efx_cxl.c\n> +++ b/drivers/net/ethernet/sfc/efx_cxl.c\n> @@ -19,6 +19,7 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n>  \tstruct efx_nic *efx = &probe_data->efx;\n>  \tstruct pci_dev *pci_dev = efx->pci_dev;\n>  \tstruct efx_cxl *cxl;\n> +\tstruct range range;\n>  \tu16 dvsec;\n>  \tint rc;\n>  \n> @@ -90,13 +91,38 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n>  \t\treturn PTR_ERR(cxl->cxlmd);\n>  \t}\n>  \n> -\tprobe_data->cxl = cxl;\n> +\tcxl->cxled = cxl_get_committed_decoder(cxl->cxlmd, &cxl->efx_region);\n> +\tif (cxl->cxled) {\n> +\t\tif (!cxl->efx_region) {\n> +\t\t\tpci_err(pci_dev, \"CXL found committed decoder without a region\");\n> +\t\t\treturn -ENODEV;\n> +\t\t}\n> +\t\trc = cxl_get_region_range(cxl->efx_region, &range);\n\nMissing an empty line above.\n\n> +\t\tif (rc) {\n> +\t\t\tpci_err(pci_dev,\n> +\t\t\t\t\"CXL getting regions params from a committed decoder failed\");\n> +\t\t\treturn rc;\n> +\t\t}\n> +\n> +\t\tcxl->ctpio_cxl = ioremap(range.start, range.end - range.start + 1);\n\nMaybe use range_len() instead for the second parameter?\n\n> +\t\tif (!cxl->ctpio_cxl) {\n> +\t\t\tpci_err(pci_dev, \"CXL ioremap region (%pra) failed\", &range);\n> +\t\t\treturn -ENOMEM;\n> +\t\t}\n> +\n> +\t\tprobe_data->cxl = cxl;\n> +\t}\n>  \n>  \treturn 0;\n>  }\n>  \n>  void efx_cxl_exit(struct efx_probe_data *probe_data)\n>  {\n> +\tif (!probe_data->cxl)\n> +\t\treturn;\n> +\n> +\tiounmap(probe_data->cxl->ctpio_cxl);\n> +\tcxl_unregister_region(probe_data->cxl->efx_region);\n>  }\n>  \n>  MODULE_IMPORT_NS(\"CXL\");\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Region creation involves finding available DPA (device-physical-address)\n> capacity to map into HPA (host-physical-address) space.\n> \n> In order to support CXL Type2 devices, define an API, cxl_request_dpa(),\n> that tries to allocate the DPA memory the driver requires to operate.The\n> memory requested should not be bigger than the max available HPA obtained\n> previously with cxl_get_hpa_freespace().\n> \n> Based on https://lore.kernel.org/linux-cxl/168592158743.1948938.7622563891193802610.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> ---\n>  drivers/cxl/core/hdm.c | 84 ++++++++++++++++++++++++++++++++++++++++++\n>  drivers/cxl/cxl.h      |  1 +\n>  include/cxl/cxl.h      |  5 +++\n>  3 files changed, 90 insertions(+)\n> \n> diff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\n> index a172ce4e9b19..d60a697f12cc 100644\n> --- a/drivers/cxl/core/hdm.c\n> +++ b/drivers/cxl/core/hdm.c\n> @@ -3,6 +3,7 @@\n>  #include <linux/seq_file.h>\n>  #include <linux/device.h>\n>  #include <linux/delay.h>\n> +#include <cxl/cxl.h>\n>  \n>  #include \"cxlmem.h\"\n>  #include \"core.h\"\n> @@ -546,6 +547,12 @@ bool cxl_resource_contains_addr(const struct resource *res, const resource_size_\n>  \treturn resource_contains(res, &_addr);\n>  }\n>  \n> +/**\n> + * cxl_dpa_free - release DPA (Device Physical Address)\n> + * @cxled: endpoint decoder linked to the DPA\n> + *\n> + * Returns 0 or error.\n> + */\n>  int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n>  {\n>  \tstruct cxl_port *port = cxled_to_port(cxled);\n> @@ -572,6 +579,7 @@ int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n>  \tdevm_cxl_dpa_release(cxled);\n>  \treturn 0;\n>  }\n> +EXPORT_SYMBOL_NS_GPL(cxl_dpa_free, \"CXL\");\n>  \n>  int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n>  \t\t     enum cxl_partition_mode mode)\n> @@ -603,6 +611,82 @@ int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n>  \treturn 0;\n>  }\n>  \n> +static int find_free_decoder(struct device *dev, const void *data)\n> +{\n> +\tstruct cxl_endpoint_decoder *cxled;\n> +\tstruct cxl_port *port;\n> +\n> +\tif (!is_endpoint_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxled = to_cxl_endpoint_decoder(dev);\n> +\tport = cxled_to_port(cxled);\n> +\n> +\treturn cxled->cxld.id == (port->hdm_end + 1);\n> +}\n> +\n> +static struct cxl_endpoint_decoder *\n> +cxl_find_free_decoder(struct cxl_memdev *cxlmd)\n> +{\n> +\tstruct cxl_port *endpoint = cxlmd->endpoint;\n> +\tstruct device *dev;\n> +\n> +\tguard(rwsem_read)(&cxl_rwsem.dpa);\n> +\tdev = device_find_child(&endpoint->dev, NULL,\n> +\t\t\t\tfind_free_decoder);\n> +\tif (!dev)\n> +\t\treturn NULL;\n> +\n> +\treturn to_cxl_endpoint_decoder(dev);\n> +}\n> +\n> +/**\n> + * cxl_request_dpa - search and reserve DPA given input constraints\n> + * @cxlmd: memdev with an endpoint port with available decoders\n> + * @mode: CXL partition mode (ram vs pmem)\n> + * @alloc: dpa size required\n> + *\n> + * Returns a pointer to a 'struct cxl_endpoint_decoder' on success or\n> + * an errno encoded pointer on failure.\n> + *\n> + * Given that a region needs to allocate from limited HPA capacity it\n> + * may be the case that a device has more mappable DPA capacity than\n> + * available HPA. The expectation is that @alloc is a driver known\n> + * value based on the device capacity but which could not be fully\n> + * available due to HPA constraints.\n> + *\n> + * Returns a pinned cxl_decoder with at least @alloc bytes of capacity\n> + * reserved, or an error pointer. The caller is also expected to own the\n> + * lifetime of the memdev registration associated with the endpoint to\n> + * pin the decoder registered as well.\n> + */\n> +struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t     enum cxl_partition_mode mode,\n> +\t\t\t\t\t     resource_size_t alloc)\n> +{\n> +\tint rc;\n> +\n> +\tif (!IS_ALIGNED(alloc, SZ_256M))\n> +\t\treturn ERR_PTR(-EINVAL);\n> +\n> +\tstruct cxl_endpoint_decoder *cxled __free(put_cxled) =\n> +\t\tcxl_find_free_decoder(cxlmd);\n> +\n> +\tif (!cxled)\n> +\t\treturn ERR_PTR(-ENODEV);\n> +\n> +\trc = cxl_dpa_set_part(cxled, mode);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\trc = cxl_dpa_alloc(cxled, alloc);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n\nShould cxl_dpa_set_part() be unwound here, or does it not matter? If it doesn't matter:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> +\n> +\treturn no_free_ptr(cxled);\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_request_dpa, \"CXL\");\n> +\n>  static int __cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size)\n>  {\n>  \tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index d1b010e5e1d0..2b1f7d687a0e 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -667,6 +667,7 @@ struct cxl_root *find_cxl_root(struct cxl_port *port);\n>  \n>  DEFINE_FREE(put_cxl_root, struct cxl_root *, if (_T) put_device(&_T->port.dev))\n>  DEFINE_FREE(put_cxl_port, struct cxl_port *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n> +DEFINE_FREE(put_cxled, struct cxl_endpoint_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxld.dev))\n>  DEFINE_FREE(put_cxl_root_decoder, struct cxl_root_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxlsd.cxld.dev))\n>  DEFINE_FREE(put_cxl_region, struct cxl_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n>  \n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 783ad570a6eb..4802371db00e 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -7,6 +7,7 @@\n>  \n>  #include <linux/node.h>\n>  #include <linux/ioport.h>\n> +#include <linux/range.h>\n>  #include <cxl/mailbox.h>\n>  \n>  /**\n> @@ -276,4 +277,8 @@ struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n>  \t\t\t\t\t       unsigned long flags,\n>  \t\t\t\t\t       resource_size_t *max);\n>  void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n> +struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t     enum cxl_partition_mode mode,\n> +\t\t\t\t\t     resource_size_t alloc);\n> +int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Current code is expecting Type3 or CXL_DECODER_HOSTONLYMEM devices only.\n> Support for Type2 implies region type needs to be based on the endpoint\n> type HDM-D[B] instead.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Zhi Wang <zhiw@nvidia.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n> Reviewed-by: Davidlohr Bueso <daves@stgolabs.net>\n> ---\n>  drivers/cxl/core/region.c | 10 ++++++----\n>  1 file changed, 6 insertions(+), 4 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index bdefd088f5f1..f53b2e9fd9e6 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -2833,7 +2833,8 @@ static ssize_t create_ram_region_show(struct device *dev,\n>  }\n>  \n>  static struct cxl_region *__create_region(struct cxl_root_decoder *cxlrd,\n> -\t\t\t\t\t  enum cxl_partition_mode mode, int id)\n> +\t\t\t\t\t  enum cxl_partition_mode mode, int id,\n> +\t\t\t\t\t  enum cxl_decoder_type target_type)\n>  {\n>  \tint rc;\n>  \n> @@ -2855,7 +2856,7 @@ static struct cxl_region *__create_region(struct cxl_root_decoder *cxlrd,\n>  \t\treturn ERR_PTR(-EBUSY);\n>  \t}\n>  \n> -\treturn devm_cxl_add_region(cxlrd, id, mode, CXL_DECODER_HOSTONLYMEM);\n> +\treturn devm_cxl_add_region(cxlrd, id, mode, target_type);\n>  }\n>  \n>  static ssize_t create_region_store(struct device *dev, const char *buf,\n> @@ -2869,7 +2870,7 @@ static ssize_t create_region_store(struct device *dev, const char *buf,\n>  \tif (rc != 1)\n>  \t\treturn -EINVAL;\n>  \n> -\tcxlr = __create_region(cxlrd, mode, id);\n> +\tcxlr = __create_region(cxlrd, mode, id, CXL_DECODER_HOSTONLYMEM);\n\nI haven't read the ABI docs, but would it be worthwhile to update the documentation for this attribute\nto mention it only makes type 3 regions? I'm flip-flopping on whether it's worth the trouble but thought\nI should mention it.\n\nEither way:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n\n>  \tif (IS_ERR(cxlr))\n>  \t\treturn PTR_ERR(cxlr);\n>  \n> @@ -4036,7 +4037,8 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n>  \n>  \tdo {\n>  \t\tcxlr = __create_region(cxlrd, cxlds->part[part].mode,\n> -\t\t\t\t       atomic_read(&cxlrd->region_id));\n> +\t\t\t\t       atomic_read(&cxlrd->region_id),\n> +\t\t\t\t       cxled->cxld.target_type);\n>  \t} while (IS_ERR(cxlr) && PTR_ERR(cxlr) == -EBUSY);\n>  \n>  \tif (IS_ERR(cxlr)) {\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Region creation based on Type3 devices is triggered from user space\n> allowing memory combination through interleaving.\n> \n> In preparation for kernel driven region creation, that is Type2 drivers\n> triggering region creation backed with its advertised CXL memory, factor\n> out a common helper from the user-sysfs region setup for interleave ways.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Zhi Wang <zhiw@nvidia.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n> ---\n>  drivers/cxl/core/region.c | 43 ++++++++++++++++++++++++---------------\n>  1 file changed, 27 insertions(+), 16 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index f53b2e9fd9e6..ece1d3df7cf1 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -485,22 +485,14 @@ static ssize_t interleave_ways_show(struct device *dev,\n>  \n>  static const struct attribute_group *get_cxl_region_target_group(void);\n>  \n> -static ssize_t interleave_ways_store(struct device *dev,\n> -\t\t\t\t     struct device_attribute *attr,\n> -\t\t\t\t     const char *buf, size_t len)\n> +static int set_interleave_ways(struct cxl_region *cxlr, int val)\n\n@val should probably stay an unsigned int. You pass an unsigned int in the sysfs function, and the\nfunction was originally coded with that in mind (same with @save below). With that cleaned up:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n\n>  {\n> -\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(dev->parent);\n> +\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n>  \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n> -\tstruct cxl_region *cxlr = to_cxl_region(dev);\n>  \tstruct cxl_region_params *p = &cxlr->params;\n> -\tunsigned int val, save;\n> -\tint rc;\n> +\tint save, rc;\n>  \tu8 iw;\n>  \n> -\trc = kstrtouint(buf, 0, &val);\n> -\tif (rc)\n> -\t\treturn rc;\n> -\n>  \trc = ways_to_eiw(val, &iw);\n>  \tif (rc)\n>  \t\treturn rc;\n> @@ -515,9 +507,7 @@ static ssize_t interleave_ways_store(struct device *dev,\n>  \t\treturn -EINVAL;\n>  \t}\n>  \n> -\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n> -\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n> -\t\treturn rc;\n> +\tlockdep_assert_held_write(&cxl_rwsem.region);\n>  \n>  \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n>  \t\treturn -EBUSY;\n> @@ -525,10 +515,31 @@ static ssize_t interleave_ways_store(struct device *dev,\n>  \tsave = p->interleave_ways;\n>  \tp->interleave_ways = val;\n>  \trc = sysfs_update_group(&cxlr->dev.kobj, get_cxl_region_target_group());\n> -\tif (rc) {\n> +\tif (rc)\n>  \t\tp->interleave_ways = save;\n> +\n> +\treturn rc;\n> +}\n> +\n> +static ssize_t interleave_ways_store(struct device *dev,\n> +\t\t\t\t     struct device_attribute *attr,\n> +\t\t\t\t     const char *buf, size_t len)\n> +{\n> +\tstruct cxl_region *cxlr = to_cxl_region(dev);\n> +\tunsigned int val;\n> +\tint rc;\n> +\n> +\trc = kstrtouint(buf, 0, &val);\n> +\tif (rc)\n> +\t\treturn rc;\n> +\n> +\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n> +\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n> +\t\treturn rc;\n> +\n> +\trc = set_interleave_ways(cxlr, val);\n> +\tif (rc)\n>  \t\treturn rc;\n> -\t}\n>  \n>  \treturn len;\n>  }\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Region creation based on Type3 devices is triggered from user space\n> allowing memory combination through interleaving.\n> \n> In preparation for kernel driven region creation, that is Type2 drivers\n> triggering region creation backed with its advertised CXL memory, factor\n> out a common helper from the user-sysfs region setup forinterleave\n> granularity.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Zhi Wang <zhiw@nvidia.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n> ---\n>  drivers/cxl/core/region.c | 39 +++++++++++++++++++++++++--------------\n>  1 file changed, 25 insertions(+), 14 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index ece1d3df7cf1..63c2aeb2ee1f 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -559,21 +559,14 @@ static ssize_t interleave_granularity_show(struct device *dev,\n>  \treturn sysfs_emit(buf, \"%d\\n\", p->interleave_granularity);\n>  }\n>  \n> -static ssize_t interleave_granularity_store(struct device *dev,\n> -\t\t\t\t\t    struct device_attribute *attr,\n> -\t\t\t\t\t    const char *buf, size_t len)\n> +static int set_interleave_granularity(struct cxl_region *cxlr, int val)\n\nSame thing as last patch. Assuming it's fixed:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n>  {\n> -\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(dev->parent);\n> +\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n>  \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n> -\tstruct cxl_region *cxlr = to_cxl_region(dev);\n>  \tstruct cxl_region_params *p = &cxlr->params;\n> -\tint rc, val;\n> +\tint rc;\n>  \tu16 ig;\n>  \n> -\trc = kstrtoint(buf, 0, &val);\n> -\tif (rc)\n> -\t\treturn rc;\n> -\n>  \trc = granularity_to_eig(val, &ig);\n>  \tif (rc)\n>  \t\treturn rc;\n> @@ -589,14 +582,32 @@ static ssize_t interleave_granularity_store(struct device *dev,\n>  \tif (cxld->interleave_ways > 1 && val != cxld->interleave_granularity)\n>  \t\treturn -EINVAL;\n>  \n> -\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n> -\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n> -\t\treturn rc;\n> -\n> +\tlockdep_assert_held_write(&cxl_rwsem.region);\n>  \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n>  \t\treturn -EBUSY;\n>  \n>  \tp->interleave_granularity = val;\n> +\treturn 0;\n> +}\n> +\n> +static ssize_t interleave_granularity_store(struct device *dev,\n> +\t\t\t\t\t    struct device_attribute *attr,\n> +\t\t\t\t\t    const char *buf, size_t len)\n> +{\n> +\tstruct cxl_region *cxlr = to_cxl_region(dev);\n> +\tint rc, val;\n> +\n> +\trc = kstrtoint(buf, 0, &val);\n> +\tif (rc)\n> +\t\treturn rc;\n> +\n> +\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n> +\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n> +\t\treturn rc;\n> +\n> +\trc = set_interleave_granularity(cxlr, val);\n> +\tif (rc)\n> +\t\treturn rc;\n>  \n>  \treturn len;\n>  }\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Creating a CXL region requires userspace intervention through the cxl\n> sysfs files. Type2 support should allow accelerator drivers to create\n> such cxl region from kernel code.\n> \n> Adding that functionality and integrating it with current support for\n> memory expanders.\n> \n> Based on https://lore.kernel.org/linux-cxl/168592159835.1948938.1647215579839222774.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> ---\n>  drivers/cxl/core/region.c | 131 ++++++++++++++++++++++++++++++++++++--\n>  include/cxl/cxl.h         |   3 +\n>  2 files changed, 127 insertions(+), 7 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index 63c2aeb2ee1f..293e63dfef22 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -2944,6 +2944,14 @@ cxl_find_region_by_name(struct cxl_root_decoder *cxlrd, const char *name)\n>  \treturn to_cxl_region(region_dev);\n>  }\n>  \n> +static void drop_region(struct cxl_region *cxlr)\n> +{\n> +\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n> +\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n> +\n> +\tdevm_release_action(port->uport_dev, __unregister_region, cxlr);\n> +}\n> +\n>  static ssize_t delete_region_store(struct device *dev,\n>  \t\t\t\t   struct device_attribute *attr,\n>  \t\t\t\t   const char *buf, size_t len)\n> @@ -4047,14 +4055,12 @@ static int __construct_region(struct cxl_region *cxlr,\n>  \treturn 0;\n>  }\n>  \n> -/* Establish an empty region covering the given HPA range */\n> -static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n> -\t\t\t\t\t   struct cxl_endpoint_decoder *cxled)\n> +static struct cxl_region *construct_region_begin(struct cxl_root_decoder *cxlrd,\n> +\t\t\t\t\t\t struct cxl_endpoint_decoder *cxled)\n>  {\n>  \tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n> -\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n>  \tstruct cxl_dev_state *cxlds = cxlmd->cxlds;\n> -\tint rc, part = READ_ONCE(cxled->part);\n> +\tint part = READ_ONCE(cxled->part);\n>  \tstruct cxl_region *cxlr;\n>  \n>  \tdo {\n> @@ -4063,13 +4069,26 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n>  \t\t\t\t       cxled->cxld.target_type);\n>  \t} while (IS_ERR(cxlr) && PTR_ERR(cxlr) == -EBUSY);\n>  \n> -\tif (IS_ERR(cxlr)) {\n> +\tif (IS_ERR(cxlr))\n>  \t\tdev_err(cxlmd->dev.parent,\n>  \t\t\t\"%s:%s: %s failed assign region: %ld\\n\",\n>  \t\t\tdev_name(&cxlmd->dev), dev_name(&cxled->cxld.dev),\n>  \t\t\t__func__, PTR_ERR(cxlr));\n> +\n> +\treturn cxlr;\n> +}\n> +\n> +/* Establish an empty region covering the given HPA range */\n> +static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n> +\t\t\t\t\t   struct cxl_endpoint_decoder *cxled)\n> +{\n> +\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n> +\tstruct cxl_region *cxlr;\n> +\tint rc;\n> +\n> +\tcxlr = construct_region_begin(cxlrd, cxled);\n> +\tif (IS_ERR(cxlr))\n>  \t\treturn cxlr;\n> -\t}\n>  \n>  \trc = __construct_region(cxlr, cxlrd, cxled);\n>  \tif (rc) {\n> @@ -4080,6 +4099,104 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n>  \treturn cxlr;\n>  }\n>  \n> +DEFINE_FREE(cxl_region_drop, struct cxl_region *, if (_T) drop_region(_T))\n\nThis needs to be \"if (!IS_ERR_OR_NULL(_T) drop_region(_T)\". If construct_region_begin() returns an\nerror pointer, drop_region() will be called with it as of now leading to a garbage pointer deref.\n\n> +\n> +static struct cxl_region *\n> +__construct_new_region(struct cxl_root_decoder *cxlrd,\n> +\t\t       struct cxl_endpoint_decoder **cxled, int ways)\n> +{\n> +\tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled[0]);\n> +\tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n> +\tstruct cxl_region_params *p;\n> +\tresource_size_t size = 0;\n> +\tint rc, i;\n> +\n> +\tstruct cxl_region *cxlr __free(cxl_region_drop) =\n> +\t\tconstruct_region_begin(cxlrd, cxled[0]);\n> +\tif (IS_ERR(cxlr))\n> +\t\treturn cxlr;\n> +\n> +\tguard(rwsem_write)(&cxl_rwsem.region);\n> +\n> +\t/*\n> +\t * Sanity check. This should not happen with an accel driver handling\n> +\t * the region creation.\n> +\t */\n> +\tp = &cxlr->params;\n> +\tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE) {\n> +\t\tdev_err(cxlmd->dev.parent,\n> +\t\t\t\"%s:%s: %s  unexpected region state\\n\",\n> +\t\t\tdev_name(&cxlmd->dev), dev_name(&cxled[0]->cxld.dev),\n> +\t\t\t__func__);\n> +\t\treturn ERR_PTR(-EBUSY);\n> +\t}\n> +\n> +\trc = set_interleave_ways(cxlr, ways);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\trc = set_interleave_granularity(cxlr, cxld->interleave_granularity);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\tscoped_guard(rwsem_read, &cxl_rwsem.dpa) {\n> +\t\tfor (i = 0; i < ways; i++) {\n> +\t\t\tif (!cxled[i]->dpa_res)\n> +\t\t\t\treturn ERR_PTR(-EINVAL);\n> +\t\t\tsize += resource_size(cxled[i]->dpa_res);\n> +\t\t}\n> +\n> +\t\trc = alloc_hpa(cxlr, size);\n> +\t\tif (rc)\n> +\t\t\treturn ERR_PTR(rc);\n> +\n> +\t\tfor (i = 0; i < ways; i++) {\n> +\t\t\trc = cxl_region_attach(cxlr, cxled[i], 0);\n\nPosition parameter is hardcoded to 0. It should be set to i, right? This kind of goes back to my\nissues in patch 12/22; the interleaving functionality is there but it looks unused.\n\n> +\t\t\tif (rc)\n> +\t\t\t\treturn ERR_PTR(rc);\n> +\t\t}\n> +\t}\n> +\n> +\trc = cxl_region_decode_commit(cxlr);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\tp->state = CXL_CONFIG_COMMIT;\n> +\n> +\treturn no_free_ptr(cxlr);\n> +}\n> +\n> +/**\n> + * cxl_create_region - Establish a region given an endpoint decoder\n> + * @cxlrd: root decoder to allocate HPA\n> + * @cxled: endpoint decoders with reserved DPA capacity\n> + * @ways: interleave ways required\n> + *\n> + * Returns a fully formed region in the commit state and attached to the\n> + * cxl_region driver.\n> + */\n> +struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n> +\t\t\t\t     struct cxl_endpoint_decoder **cxled,\n> +\t\t\t\t     int ways)\n> +{\n> +\tstruct cxl_region *cxlr;\n> +\n> +\tmutex_lock(&cxlrd->range_lock);\n> +\tcxlr = __construct_new_region(cxlrd, cxled, ways);\n> +\tmutex_unlock(&cxlrd->range_lock);\n> +\tif (IS_ERR(cxlr))\n> +\t\treturn cxlr;\n> +\n> +\tif (device_attach(&cxlr->dev) <= 0) {\n> +\t\tdev_err(&cxlr->dev, \"failed to create region\\n\");\n> +\t\tdrop_region(cxlr);\n> +\t\treturn ERR_PTR(-ENODEV);\n> +\t}\n> +\n> +\treturn cxlr;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_create_region, \"CXL\");\n> +\n>  static struct cxl_region *\n>  cxl_find_region_by_range(struct cxl_root_decoder *cxlrd, struct range *hpa)\n>  {\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 4802371db00e..50acbd13bcf8 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -281,4 +281,7 @@ struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n>  \t\t\t\t\t     enum cxl_partition_mode mode,\n>  \t\t\t\t\t     resource_size_t alloc);\n>  int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n> +struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n> +\t\t\t\t     struct cxl_endpoint_decoder **cxled,\n> +\t\t\t\t     int ways);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Differentiate CXL memory expanders (type 3) from CXL device accelerators\n> (type 2) with a new function for initializing cxl_dev_state and a macro\n> for helping accel drivers to embed cxl_dev_state inside a private\n> struct.\n> \n> Move structs to include/cxl as the size of the accel driver private\n> struct embedding cxl_dev_state needs to know the size of this struct.\n> \n> Use same new initialization with the type3 pci driver.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> ---\n>  drivers/cxl/core/mbox.c      |  12 +-\n>  drivers/cxl/core/memdev.c    |  32 +++++\n>  drivers/cxl/cxl.h            |  97 +--------------\n>  drivers/cxl/cxlmem.h         |  86 +------------\n>  drivers/cxl/pci.c            |  14 +--\n>  include/cxl/cxl.h            | 226 +++++++++++++++++++++++++++++++++++\n>  tools/testing/cxl/test/mem.c |   3 +-\n>  7 files changed, 274 insertions(+), 196 deletions(-)\n>  create mode 100644 include/cxl/cxl.h\n> \n> diff --git a/drivers/cxl/core/mbox.c b/drivers/cxl/core/mbox.c\n> index fa6dd0c94656..bee84d0101d1 100644\n> --- a/drivers/cxl/core/mbox.c\n> +++ b/drivers/cxl/core/mbox.c\n> @@ -1514,23 +1514,21 @@ int cxl_mailbox_init(struct cxl_mailbox *cxl_mbox, struct device *host)\n>  }\n>  EXPORT_SYMBOL_NS_GPL(cxl_mailbox_init, \"CXL\");\n>  \n> -struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev)\n> +struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev, u64 serial,\n> +\t\t\t\t\t\t u16 dvsec)\n>  {\n>  \tstruct cxl_memdev_state *mds;\n>  \tint rc;\n>  \n> -\tmds = devm_kzalloc(dev, sizeof(*mds), GFP_KERNEL);\n> +\tmds = devm_cxl_dev_state_create(dev, CXL_DEVTYPE_CLASSMEM, serial,\n> +\t\t\t\t\tdvsec, struct cxl_memdev_state, cxlds,\n> +\t\t\t\t\ttrue);\n>  \tif (!mds) {\n>  \t\tdev_err(dev, \"No memory available\\n\");\n>  \t\treturn ERR_PTR(-ENOMEM);\n>  \t}\n>  \n>  \tmutex_init(&mds->event.log_lock);\n> -\tmds->cxlds.dev = dev;\n> -\tmds->cxlds.reg_map.host = dev;\n> -\tmds->cxlds.cxl_mbox.host = dev;\n> -\tmds->cxlds.reg_map.resource = CXL_RESOURCE_NONE;\n> -\tmds->cxlds.type = CXL_DEVTYPE_CLASSMEM;\n>  \n>  \trc = devm_cxl_register_mce_notifier(dev, &mds->mce_notifier);\n>  \tif (rc == -EOPNOTSUPP)\n> diff --git a/drivers/cxl/core/memdev.c b/drivers/cxl/core/memdev.c\n> index af3d0cc65138..22d156f25305 100644\n> --- a/drivers/cxl/core/memdev.c\n> +++ b/drivers/cxl/core/memdev.c\n> @@ -656,6 +656,38 @@ static void detach_memdev(struct work_struct *work)\n>  \n>  static struct lock_class_key cxl_memdev_key;\n>  \n> +static void cxl_dev_state_init(struct cxl_dev_state *cxlds, struct device *dev,\n> +\t\t\t       enum cxl_devtype type, u64 serial, u16 dvsec,\n> +\t\t\t       bool has_mbox)\n> +{\n> +\t*cxlds = (struct cxl_dev_state) {\n> +\t\t.dev = dev,\n> +\t\t.type = type,\n> +\t\t.serial = serial,\n> +\t\t.cxl_dvsec = dvsec,\n> +\t\t.reg_map.host = dev,\n> +\t\t.reg_map.resource = CXL_RESOURCE_NONE,\n> +\t};\n> +\n> +\tif (has_mbox)\n> +\t\tcxlds->cxl_mbox.host = dev;\n> +}\n> +\n> +struct cxl_dev_state *_devm_cxl_dev_state_create(struct device *dev,\n> +\t\t\t\t\t\t enum cxl_devtype type,\n> +\t\t\t\t\t\t u64 serial, u16 dvsec,\n> +\t\t\t\t\t\t size_t size, bool has_mbox)\n> +{\n> +\tstruct cxl_dev_state *cxlds = devm_kzalloc(dev, size, GFP_KERNEL);\n> +\n> +\tif (!cxlds)\n> +\t\treturn NULL;\n> +\n> +\tcxl_dev_state_init(cxlds, dev, type, serial, dvsec, has_mbox);\n\nNit: Having a second function to do the init seems overkill here, especially since cxl_dev_state_init() isn't called outside this\nfunction. I'd fold it into this function instead, but I'm fine with it either way (especially if you were told otherwise before).\n\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> +\treturn cxlds;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(_devm_cxl_dev_state_create, \"CXL\");\n> +\n>  static struct cxl_memdev *cxl_memdev_alloc(struct cxl_dev_state *cxlds,\n>  \t\t\t\t\t   const struct file_operations *fops,\n>  \t\t\t\t\t   const struct cxl_memdev_attach *attach)\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index e1d47062e1d3..3eaa353e430b 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -12,6 +12,7 @@\n>  #include <linux/node.h>\n>  #include <linux/io.h>\n>  #include <linux/range.h>\n> +#include <cxl/cxl.h>\n>  \n>  extern const struct nvdimm_security_ops *cxl_security_ops;\n>  \n> @@ -201,97 +202,6 @@ static inline int ways_to_eiw(unsigned int ways, u8 *eiw)\n>  #define   CXLDEV_MBOX_BG_CMD_COMMAND_VENDOR_MASK GENMASK_ULL(63, 48)\n>  #define CXLDEV_MBOX_PAYLOAD_OFFSET 0x20\n>  \n> -/*\n> - * Using struct_group() allows for per register-block-type helper routines,\n> - * without requiring block-type agnostic code to include the prefix.\n> - */\n> -struct cxl_regs {\n> -\t/*\n> -\t * Common set of CXL Component register block base pointers\n> -\t * @hdm_decoder: CXL 2.0 8.2.5.12 CXL HDM Decoder Capability Structure\n> -\t * @ras: CXL 2.0 8.2.5.9 CXL RAS Capability Structure\n> -\t */\n> -\tstruct_group_tagged(cxl_component_regs, component,\n> -\t\tvoid __iomem *hdm_decoder;\n> -\t\tvoid __iomem *ras;\n> -\t);\n> -\t/*\n> -\t * Common set of CXL Device register block base pointers\n> -\t * @status: CXL 2.0 8.2.8.3 Device Status Registers\n> -\t * @mbox: CXL 2.0 8.2.8.4 Mailbox Registers\n> -\t * @memdev: CXL 2.0 8.2.8.5 Memory Device Registers\n> -\t */\n> -\tstruct_group_tagged(cxl_device_regs, device_regs,\n> -\t\tvoid __iomem *status, *mbox, *memdev;\n> -\t);\n> -\n> -\tstruct_group_tagged(cxl_pmu_regs, pmu_regs,\n> -\t\tvoid __iomem *pmu;\n> -\t);\n> -\n> -\t/*\n> -\t * RCH downstream port specific RAS register\n> -\t * @aer: CXL 3.0 8.2.1.1 RCH Downstream Port RCRB\n> -\t */\n> -\tstruct_group_tagged(cxl_rch_regs, rch_regs,\n> -\t\tvoid __iomem *dport_aer;\n> -\t);\n> -\n> -\t/*\n> -\t * RCD upstream port specific PCIe cap register\n> -\t * @pcie_cap: CXL 3.0 8.2.1.2 RCD Upstream Port RCRB\n> -\t */\n> -\tstruct_group_tagged(cxl_rcd_regs, rcd_regs,\n> -\t\tvoid __iomem *rcd_pcie_cap;\n> -\t);\n> -};\n> -\n> -struct cxl_reg_map {\n> -\tbool valid;\n> -\tint id;\n> -\tunsigned long offset;\n> -\tunsigned long size;\n> -};\n> -\n> -struct cxl_component_reg_map {\n> -\tstruct cxl_reg_map hdm_decoder;\n> -\tstruct cxl_reg_map ras;\n> -};\n> -\n> -struct cxl_device_reg_map {\n> -\tstruct cxl_reg_map status;\n> -\tstruct cxl_reg_map mbox;\n> -\tstruct cxl_reg_map memdev;\n> -};\n> -\n> -struct cxl_pmu_reg_map {\n> -\tstruct cxl_reg_map pmu;\n> -};\n> -\n> -/**\n> - * struct cxl_register_map - DVSEC harvested register block mapping parameters\n> - * @host: device for devm operations and logging\n> - * @base: virtual base of the register-block-BAR + @block_offset\n> - * @resource: physical resource base of the register block\n> - * @max_size: maximum mapping size to perform register search\n> - * @reg_type: see enum cxl_regloc_type\n> - * @component_map: cxl_reg_map for component registers\n> - * @device_map: cxl_reg_maps for device registers\n> - * @pmu_map: cxl_reg_maps for CXL Performance Monitoring Units\n> - */\n> -struct cxl_register_map {\n> -\tstruct device *host;\n> -\tvoid __iomem *base;\n> -\tresource_size_t resource;\n> -\tresource_size_t max_size;\n> -\tu8 reg_type;\n> -\tunion {\n> -\t\tstruct cxl_component_reg_map component_map;\n> -\t\tstruct cxl_device_reg_map device_map;\n> -\t\tstruct cxl_pmu_reg_map pmu_map;\n> -\t};\n> -};\n> -\n>  void cxl_probe_component_regs(struct device *dev, void __iomem *base,\n>  \t\t\t      struct cxl_component_reg_map *map);\n>  void cxl_probe_device_regs(struct device *dev, void __iomem *base,\n> @@ -497,11 +407,6 @@ struct cxl_region_params {\n>  \tresource_size_t cache_size;\n>  };\n>  \n> -enum cxl_partition_mode {\n> -\tCXL_PARTMODE_RAM,\n> -\tCXL_PARTMODE_PMEM,\n> -};\n> -\n>  /*\n>   * Indicate whether this region has been assembled by autodetection or\n>   * userspace assembly. Prevent endpoint decoders outside of automatic\n> diff --git a/drivers/cxl/cxlmem.h b/drivers/cxl/cxlmem.h\n> index ef202b34e5ea..281546de426e 100644\n> --- a/drivers/cxl/cxlmem.h\n> +++ b/drivers/cxl/cxlmem.h\n> @@ -113,8 +113,6 @@ int devm_cxl_dpa_reserve(struct cxl_endpoint_decoder *cxled,\n>  \t\t\t resource_size_t base, resource_size_t len,\n>  \t\t\t resource_size_t skipped);\n>  \n> -#define CXL_NR_PARTITIONS_MAX 2\n> -\n>  struct cxl_dpa_info {\n>  \tu64 size;\n>  \tstruct cxl_dpa_part_info {\n> @@ -373,87 +371,6 @@ struct cxl_security_state {\n>  \tstruct kernfs_node *sanitize_node;\n>  };\n>  \n> -/*\n> - * enum cxl_devtype - delineate type-2 from a generic type-3 device\n> - * @CXL_DEVTYPE_DEVMEM - Vendor specific CXL Type-2 device implementing HDM-D or\n> - *\t\t\t HDM-DB, no requirement that this device implements a\n> - *\t\t\t mailbox, or other memory-device-standard manageability\n> - *\t\t\t flows.\n> - * @CXL_DEVTYPE_CLASSMEM - Common class definition of a CXL Type-3 device with\n> - *\t\t\t   HDM-H and class-mandatory memory device registers\n> - */\n> -enum cxl_devtype {\n> -\tCXL_DEVTYPE_DEVMEM,\n> -\tCXL_DEVTYPE_CLASSMEM,\n> -};\n> -\n> -/**\n> - * struct cxl_dpa_perf - DPA performance property entry\n> - * @dpa_range: range for DPA address\n> - * @coord: QoS performance data (i.e. latency, bandwidth)\n> - * @cdat_coord: raw QoS performance data from CDAT\n> - * @qos_class: QoS Class cookies\n> - */\n> -struct cxl_dpa_perf {\n> -\tstruct range dpa_range;\n> -\tstruct access_coordinate coord[ACCESS_COORDINATE_MAX];\n> -\tstruct access_coordinate cdat_coord[ACCESS_COORDINATE_MAX];\n> -\tint qos_class;\n> -};\n> -\n> -/**\n> - * struct cxl_dpa_partition - DPA partition descriptor\n> - * @res: shortcut to the partition in the DPA resource tree (cxlds->dpa_res)\n> - * @perf: performance attributes of the partition from CDAT\n> - * @mode: operation mode for the DPA capacity, e.g. ram, pmem, dynamic...\n> - */\n> -struct cxl_dpa_partition {\n> -\tstruct resource res;\n> -\tstruct cxl_dpa_perf perf;\n> -\tenum cxl_partition_mode mode;\n> -};\n> -\n> -/**\n> - * struct cxl_dev_state - The driver device state\n> - *\n> - * cxl_dev_state represents the CXL driver/device state.  It provides an\n> - * interface to mailbox commands as well as some cached data about the device.\n> - * Currently only memory devices are represented.\n> - *\n> - * @dev: The device associated with this CXL state\n> - * @cxlmd: The device representing the CXL.mem capabilities of @dev\n> - * @reg_map: component and ras register mapping parameters\n> - * @regs: Parsed register blocks\n> - * @cxl_dvsec: Offset to the PCIe device DVSEC\n> - * @rcd: operating in RCD mode (CXL 3.0 9.11.8 CXL Devices Attached to an RCH)\n> - * @media_ready: Indicate whether the device media is usable\n> - * @dpa_res: Overall DPA resource tree for the device\n> - * @part: DPA partition array\n> - * @nr_partitions: Number of DPA partitions\n> - * @serial: PCIe Device Serial Number\n> - * @type: Generic Memory Class device or Vendor Specific Memory device\n> - * @cxl_mbox: CXL mailbox context\n> - * @cxlfs: CXL features context\n> - */\n> -struct cxl_dev_state {\n> -\tstruct device *dev;\n> -\tstruct cxl_memdev *cxlmd;\n> -\tstruct cxl_register_map reg_map;\n> -\tstruct cxl_regs regs;\n> -\tint cxl_dvsec;\n> -\tbool rcd;\n> -\tbool media_ready;\n> -\tstruct resource dpa_res;\n> -\tstruct cxl_dpa_partition part[CXL_NR_PARTITIONS_MAX];\n> -\tunsigned int nr_partitions;\n> -\tu64 serial;\n> -\tenum cxl_devtype type;\n> -\tstruct cxl_mailbox cxl_mbox;\n> -#ifdef CONFIG_CXL_FEATURES\n> -\tstruct cxl_features_state *cxlfs;\n> -#endif\n> -};\n> -\n>  static inline resource_size_t cxl_pmem_size(struct cxl_dev_state *cxlds)\n>  {\n>  \t/*\n> @@ -858,7 +775,8 @@ int cxl_dev_state_identify(struct cxl_memdev_state *mds);\n>  int cxl_await_media_ready(struct cxl_dev_state *cxlds);\n>  int cxl_enumerate_cmds(struct cxl_memdev_state *mds);\n>  int cxl_mem_dpa_fetch(struct cxl_memdev_state *mds, struct cxl_dpa_info *info);\n> -struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev);\n> +struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev, u64 serial,\n> +\t\t\t\t\t\t u16 dvsec);\n>  void set_exclusive_cxl_commands(struct cxl_memdev_state *mds,\n>  \t\t\t\tunsigned long *cmds);\n>  void clear_exclusive_cxl_commands(struct cxl_memdev_state *mds,\n> diff --git a/drivers/cxl/pci.c b/drivers/cxl/pci.c\n> index 1cf232220873..24179cc702bf 100644\n> --- a/drivers/cxl/pci.c\n> +++ b/drivers/cxl/pci.c\n> @@ -911,25 +911,25 @@ static int cxl_pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n>  \tint rc, pmu_count;\n>  \tunsigned int i;\n>  \tbool irq_avail;\n> +\tu16 dvsec;\n>  \n>  \trc = pcim_enable_device(pdev);\n>  \tif (rc)\n>  \t\treturn rc;\n>  \tpci_set_master(pdev);\n>  \n> -\tmds = cxl_memdev_state_create(&pdev->dev);\n> +\tdvsec = pci_find_dvsec_capability(pdev, PCI_VENDOR_ID_CXL,\n> +\t\t\t\t\t  PCI_DVSEC_CXL_DEVICE);\n> +\tif (!dvsec)\n> +\t\tpci_warn(pdev, \"Device DVSEC not present, skip CXL.mem init\\n\");\n> +\n> +\tmds = cxl_memdev_state_create(&pdev->dev, pci_get_dsn(pdev), dvsec);\n>  \tif (IS_ERR(mds))\n>  \t\treturn PTR_ERR(mds);\n>  \tcxlds = &mds->cxlds;\n>  \tpci_set_drvdata(pdev, cxlds);\n>  \n>  \tcxlds->rcd = is_cxl_restricted(pdev);\n> -\tcxlds->serial = pci_get_dsn(pdev);\n> -\tcxlds->cxl_dvsec = pci_find_dvsec_capability(\n> -\t\tpdev, PCI_VENDOR_ID_CXL, PCI_DVSEC_CXL_DEVICE);\n> -\tif (!cxlds->cxl_dvsec)\n> -\t\tdev_warn(&pdev->dev,\n> -\t\t\t \"Device DVSEC not present, skip CXL.mem init\\n\");\n>  \n>  \trc = cxl_pci_setup_regs(pdev, CXL_REGLOC_RBI_MEMDEV, &map);\n>  \tif (rc)\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> new file mode 100644\n> index 000000000000..13d448686189\n> --- /dev/null\n> +++ b/include/cxl/cxl.h\n> @@ -0,0 +1,226 @@\n> +/* SPDX-License-Identifier: GPL-2.0 */\n> +/* Copyright(c) 2020 Intel Corporation. */\n> +/* Copyright(c) 2025 Advanced Micro Devices, Inc. */\n> +\n> +#ifndef __CXL_CXL_H__\n> +#define __CXL_CXL_H__\n> +\n> +#include <linux/node.h>\n> +#include <linux/ioport.h>\n> +#include <cxl/mailbox.h>\n> +\n> +/**\n> + * enum cxl_devtype - delineate type-2 from a generic type-3 device\n> + * @CXL_DEVTYPE_DEVMEM: Vendor specific CXL Type-2 device implementing HDM-D or\n> + *\t\t\t HDM-DB, no requirement that this device implements a\n> + *\t\t\t mailbox, or other memory-device-standard manageability\n> + *\t\t\t flows.\n> + * @CXL_DEVTYPE_CLASSMEM: Common class definition of a CXL Type-3 device with\n> + *\t\t\t   HDM-H and class-mandatory memory device registers\n> + */\n> +enum cxl_devtype {\n> +\tCXL_DEVTYPE_DEVMEM,\n> +\tCXL_DEVTYPE_CLASSMEM,\n> +};\n> +\n> +struct device;\n> +\n> +/*\n> + * Using struct_group() allows for per register-block-type helper routines,\n> + * without requiring block-type agnostic code to include the prefix.\n> + */\n> +struct cxl_regs {\n> +\t/*\n> +\t * Common set of CXL Component register block base pointers\n> +\t * @hdm_decoder: CXL 2.0 8.2.5.12 CXL HDM Decoder Capability Structure\n> +\t * @ras: CXL 2.0 8.2.5.9 CXL RAS Capability Structure\n> +\t */\n> +\tstruct_group_tagged(cxl_component_regs, component,\n> +\t\tvoid __iomem *hdm_decoder;\n> +\t\tvoid __iomem *ras;\n> +\t);\n> +\t/*\n> +\t * Common set of CXL Device register block base pointers\n> +\t * @status: CXL 2.0 8.2.8.3 Device Status Registers\n> +\t * @mbox: CXL 2.0 8.2.8.4 Mailbox Registers\n> +\t * @memdev: CXL 2.0 8.2.8.5 Memory Device Registers\n> +\t */\n> +\tstruct_group_tagged(cxl_device_regs, device_regs,\n> +\t\tvoid __iomem *status, *mbox, *memdev;\n> +\t);\n> +\n> +\tstruct_group_tagged(cxl_pmu_regs, pmu_regs,\n> +\t\tvoid __iomem *pmu;\n> +\t);\n> +\n> +\t/*\n> +\t * RCH downstream port specific RAS register\n> +\t * @aer: CXL 3.0 8.2.1.1 RCH Downstream Port RCRB\n> +\t */\n> +\tstruct_group_tagged(cxl_rch_regs, rch_regs,\n> +\t\tvoid __iomem *dport_aer;\n> +\t);\n> +\n> +\t/*\n> +\t * RCD upstream port specific PCIe cap register\n> +\t * @pcie_cap: CXL 3.0 8.2.1.2 RCD Upstream Port RCRB\n> +\t */\n> +\tstruct_group_tagged(cxl_rcd_regs, rcd_regs,\n> +\t\tvoid __iomem *rcd_pcie_cap;\n> +\t);\n> +};\n> +\n> +struct cxl_reg_map {\n> +\tbool valid;\n> +\tint id;\n> +\tunsigned long offset;\n> +\tunsigned long size;\n> +};\n> +\n> +struct cxl_component_reg_map {\n> +\tstruct cxl_reg_map hdm_decoder;\n> +\tstruct cxl_reg_map ras;\n> +};\n> +\n> +struct cxl_device_reg_map {\n> +\tstruct cxl_reg_map status;\n> +\tstruct cxl_reg_map mbox;\n> +\tstruct cxl_reg_map memdev;\n> +};\n> +\n> +struct cxl_pmu_reg_map {\n> +\tstruct cxl_reg_map pmu;\n> +};\n> +\n> +/**\n> + * struct cxl_register_map - DVSEC harvested register block mapping parameters\n> + * @host: device for devm operations and logging\n> + * @base: virtual base of the register-block-BAR + @block_offset\n> + * @resource: physical resource base of the register block\n> + * @max_size: maximum mapping size to perform register search\n> + * @reg_type: see enum cxl_regloc_type\n> + * @component_map: cxl_reg_map for component registers\n> + * @device_map: cxl_reg_maps for device registers\n> + * @pmu_map: cxl_reg_maps for CXL Performance Monitoring Units\n> + */\n> +struct cxl_register_map {\n> +\tstruct device *host;\n> +\tvoid __iomem *base;\n> +\tresource_size_t resource;\n> +\tresource_size_t max_size;\n> +\tu8 reg_type;\n> +\tunion {\n> +\t\tstruct cxl_component_reg_map component_map;\n> +\t\tstruct cxl_device_reg_map device_map;\n> +\t\tstruct cxl_pmu_reg_map pmu_map;\n> +\t};\n> +};\n> +\n> +/**\n> + * struct cxl_dpa_perf - DPA performance property entry\n> + * @dpa_range: range for DPA address\n> + * @coord: QoS performance data (i.e. latency, bandwidth)\n> + * @cdat_coord: raw QoS performance data from CDAT\n> + * @qos_class: QoS Class cookies\n> + */\n> +struct cxl_dpa_perf {\n> +\tstruct range dpa_range;\n> +\tstruct access_coordinate coord[ACCESS_COORDINATE_MAX];\n> +\tstruct access_coordinate cdat_coord[ACCESS_COORDINATE_MAX];\n> +\tint qos_class;\n> +};\n> +\n> +enum cxl_partition_mode {\n> +\tCXL_PARTMODE_RAM,\n> +\tCXL_PARTMODE_PMEM,\n> +};\n> +\n> +/**\n> + * struct cxl_dpa_partition - DPA partition descriptor\n> + * @res: shortcut to the partition in the DPA resource tree (cxlds->dpa_res)\n> + * @perf: performance attributes of the partition from CDAT\n> + * @mode: operation mode for the DPA capacity, e.g. ram, pmem, dynamic...\n> + */\n> +struct cxl_dpa_partition {\n> +\tstruct resource res;\n> +\tstruct cxl_dpa_perf perf;\n> +\tenum cxl_partition_mode mode;\n> +};\n> +\n> +#define CXL_NR_PARTITIONS_MAX 2\n> +\n> +/**\n> + * struct cxl_dev_state - The driver device state\n> + *\n> + * cxl_dev_state represents the CXL driver/device state.  It provides an\n> + * interface to mailbox commands as well as some cached data about the device.\n> + * Currently only memory devices are represented.\n> + *\n> + * @dev: The device associated with this CXL state\n> + * @cxlmd: The device representing the CXL.mem capabilities of @dev\n> + * @reg_map: component and ras register mapping parameters\n> + * @regs: Parsed register blocks\n> + * @cxl_dvsec: Offset to the PCIe device DVSEC\n> + * @rcd: operating in RCD mode (CXL 3.0 9.11.8 CXL Devices Attached to an RCH)\n> + * @media_ready: Indicate whether the device media is usable\n> + * @dpa_res: Overall DPA resource tree for the device\n> + * @part: DPA partition array\n> + * @nr_partitions: Number of DPA partitions\n> + * @serial: PCIe Device Serial Number\n> + * @type: Generic Memory Class device or Vendor Specific Memory device\n> + * @cxl_mbox: CXL mailbox context\n> + * @cxlfs: CXL features context\n> + */\n> +struct cxl_dev_state {\n> +\t/* public for Type2 drivers */\n> +\tstruct device *dev;\n> +\tstruct cxl_memdev *cxlmd;\n> +\n> +\t/* private for Type2 drivers */\n> +\tstruct cxl_register_map reg_map;\n> +\tstruct cxl_regs regs;\n> +\tint cxl_dvsec;\n> +\tbool rcd;\n> +\tbool media_ready;\n> +\tstruct resource dpa_res;\n> +\tstruct cxl_dpa_partition part[CXL_NR_PARTITIONS_MAX];\n> +\tunsigned int nr_partitions;\n> +\tu64 serial;\n> +\tenum cxl_devtype type;\n> +\tstruct cxl_mailbox cxl_mbox;\n> +#ifdef CONFIG_CXL_FEATURES\n> +\tstruct cxl_features_state *cxlfs;\n> +#endif\n> +};\n> +\n> +struct cxl_dev_state *_devm_cxl_dev_state_create(struct device *dev,\n> +\t\t\t\t\t\t enum cxl_devtype type,\n> +\t\t\t\t\t\t u64 serial, u16 dvsec,\n> +\t\t\t\t\t\t size_t size, bool has_mbox);\n> +\n> +/**\n> + * cxl_dev_state_create - safely create and cast a cxl dev state embedded in a\n> + * driver specific struct.\n> + *\n> + * @parent: device behind the request\n> + * @type: CXL device type\n> + * @serial: device identification\n> + * @dvsec: dvsec capability offset\n> + * @drv_struct: driver struct embedding a cxl_dev_state struct\n> + * @member: drv_struct member as cxl_dev_state\n> + * @mbox: true if mailbox supported\n> + *\n> + * Returns a pointer to the drv_struct allocated and embedding a cxl_dev_state\n> + * struct initialized.\n> + *\n> + * Introduced for Type2 driver support.\n> + */\n> +#define devm_cxl_dev_state_create(parent, type, serial, dvsec, drv_struct, member, mbox)\t\\\n> +\t({\t\t\t\t\t\t\t\t\t\t\\\n> +\t\tstatic_assert(__same_type(struct cxl_dev_state,\t\t\t\t\\\n> +\t\t\t      ((drv_struct *)NULL)->member));\t\t\t\t\\\n> +\t\tstatic_assert(offsetof(drv_struct, member) == 0);\t\t\t\\\n> +\t\t(drv_struct *)_devm_cxl_dev_state_create(parent, type, serial, dvsec,\t\\\n> +\t\t\t\t\t\t      sizeof(drv_struct), mbox);\t\\\n> +\t})\n> +#endif /* __CXL_CXL_H__ */\n> diff --git a/tools/testing/cxl/test/mem.c b/tools/testing/cxl/test/mem.c\n> index cb87e8c0e63c..79f42f4474d4 100644\n> --- a/tools/testing/cxl/test/mem.c\n> +++ b/tools/testing/cxl/test/mem.c\n> @@ -1716,7 +1716,7 @@ static int cxl_mock_mem_probe(struct platform_device *pdev)\n>  \tif (rc)\n>  \t\treturn rc;\n>  \n> -\tmds = cxl_memdev_state_create(dev);\n> +\tmds = cxl_memdev_state_create(dev, pdev->id + 1, 0);\n>  \tif (IS_ERR(mds))\n>  \t\treturn PTR_ERR(mds);\n>  \n> @@ -1732,7 +1732,6 @@ static int cxl_mock_mem_probe(struct platform_device *pdev)\n>  \tmds->event.buf = (struct cxl_get_event_payload *) mdata->event_buf;\n>  \tINIT_DELAYED_WORK(&mds->security.poll_dwork, cxl_mockmem_sanitize_work);\n>  \n> -\tcxlds->serial = pdev->id + 1;\n>  \tif (is_rcd(pdev))\n>  \t\tcxlds->rcd = true;\n>  \n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> A Type2 device configured by the BIOS can already have its HDM\n> committed. Add a cxl_get_committed_decoder() function for cheking\n> so after memdev creation. A CXL region should have been created\n> during memdev initialization, therefore a Type2 driver can ask for\n> such a region for working with the HPA. If the HDM is not committed,\n> a Type2 driver will create the region after obtaining proper HPA\n> and DPA space.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> ---\n>  drivers/cxl/core/hdm.c | 39 +++++++++++++++++++++++++++++++++++++++\n>  include/cxl/cxl.h      |  3 +++\n>  2 files changed, 42 insertions(+)\n> \n> diff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\n> index 6e516c69b2d2..a172ce4e9b19 100644\n> --- a/drivers/cxl/core/hdm.c\n> +++ b/drivers/cxl/core/hdm.c\n> @@ -686,6 +686,45 @@ int cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size)\n>  \treturn devm_add_action_or_reset(&port->dev, cxl_dpa_release, cxled);\n>  }\n>  \n> +static int find_committed_endpoint_decoder(struct device *dev, const void *data)\n> +{\n> +\tstruct cxl_endpoint_decoder *cxled;\n> +\tstruct cxl_port *port;\n> +\n> +\tif (!is_endpoint_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxled = to_cxl_endpoint_decoder(dev);\n> +\tport = cxled_to_port(cxled);\n> +\n> +\treturn cxled->cxld.id == port->hdm_end;\n\nIs this the way you're supposed to check if a decoder is committed? The doc comment for @hdm_end in\nstruct cxl_port says it's just the last allocated decoder. If allocated decoders are always committed then\nI'm fine with this, otherwise I think you'd want to a register read or something to find the commit state.\n> +}\n> +\n> +struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t\t       struct cxl_region **cxlr)\n> +{\n> +\tstruct cxl_port *endpoint = cxlmd->endpoint;\n> +\tstruct cxl_endpoint_decoder *cxled;\n> +\tstruct device *cxled_dev;\n> +\n> +\tif (!endpoint)\n> +\t\treturn NULL;\n> +\n> +\tguard(rwsem_read)(&cxl_rwsem.dpa);\n> +\tcxled_dev = device_find_child(&endpoint->dev, NULL,\n> +\t\t\t\t      find_committed_endpoint_decoder);\n> +\n> +\tif (!cxled_dev)\n> +\t\treturn NULL;\n> +\n> +\tcxled = to_cxl_endpoint_decoder(cxled_dev);\n> +\t*cxlr = cxled->cxld.region;\n> +\n> +\tput_device(cxled_dev);\n> +\treturn cxled;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_get_committed_decoder, \"CXL\");\n> +\n>  static void cxld_set_interleave(struct cxl_decoder *cxld, u32 *ctrl)\n>  {\n>  \tu16 eig;\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 6f8d365067af..928276dba952 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -249,4 +249,7 @@ int cxl_map_component_regs(const struct cxl_register_map *map,\n>  int cxl_set_capacity(struct cxl_dev_state *cxlds, u64 capacity);\n>  struct cxl_memdev *devm_cxl_add_memdev(struct cxl_dev_state *cxlds,\n>  \t\t\t\t       const struct cxl_memdev_attach *attach);\n> +struct cxl_region;\n> +struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t\t       struct cxl_region **cxlr);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> This patchset should be applied on the cxl next branch using the base\n> specified at the end of this cover letter.\n> \n> Dependencies on Dan's work has gone and also on Terry's as the only\n> patch required is now in next. The other dependency is on Smita patchset\n> but it does not exist such a dependency as that work will not avoid the\n> problem with Type2 and DAX/hmem if soft reserved memory. This needs to\n> be solved by the BIOS and Type2 UEFI driver for populating the CXL.mem\n> range as EFI_RESERVED_TYPE instead of default EFI_CONVENTIONAL_MEMORY\n> with the EFI_MEMORY_SP attribute. There exists though a dependency on\n> one Smita's patches:\n> \n> [PATCH v5 3/7] cxl/region: Skip decoder reset on detach for autodiscovered regions\n> \n> This is needed for the default behaviour with current BIOS configuration\n> where the HDM Type2 decoders will be kept unreset when driver unloads.\n> This is the main change introduced in v23: committed decoders will not\n> be reset. Previous v22 functionality supported first driver load finding\n> committed decoders but resetting them at unload and supporting\n> uncommitted decoders in next driver loads. This will be suported in\n> follow-up works.\n> \n> v23 changes:\n> \n>   patch 11: fixing minor issues and droping change in\n> \t    should_emulate_decoders (Jonathan Cameron)\n> \n>   patch13: refactoring unregister_region for safety type in Type2 API\n> \n>   sfc changes: slight modifications to error path\n> \n\nThis cover letter is really long, I'd remove the change logs for anything more\nthan 3 revisions back (assuming a v24 is needed). After that you could leave\na lore link for older revisions if you want, but it's not needed imo.\nAlso, feel free to add my Reviewed-by for anything I didn't leave a comment on\n(felt I should cut down on the mail).\n\nThanks,\nBen\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Region creation involves finding available DPA (device-physical-address)\n> capacity to map into HPA (host-physical-address) space.\n> \n> In order to support CXL Type2 devices, define an API, cxl_request_dpa(),\n> that tries to allocate the DPA memory the driver requires to operate.The\n> memory requested should not be bigger than the max available HPA obtained\n> previously with cxl_get_hpa_freespace().\n> \n> Based on https://lore.kernel.org/linux-cxl/168592158743.1948938.7622563891193802610.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> ---\n>  drivers/cxl/core/hdm.c | 84 ++++++++++++++++++++++++++++++++++++++++++\n>  drivers/cxl/cxl.h      |  1 +\n>  include/cxl/cxl.h      |  5 +++\n>  3 files changed, 90 insertions(+)\n> \n> diff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\n> index a172ce4e9b19..d60a697f12cc 100644\n> --- a/drivers/cxl/core/hdm.c\n> +++ b/drivers/cxl/core/hdm.c\n> @@ -3,6 +3,7 @@\n>  #include <linux/seq_file.h>\n>  #include <linux/device.h>\n>  #include <linux/delay.h>\n> +#include <cxl/cxl.h>\n>  \n>  #include \"cxlmem.h\"\n>  #include \"core.h\"\n> @@ -546,6 +547,12 @@ bool cxl_resource_contains_addr(const struct resource *res, const resource_size_\n>  \treturn resource_contains(res, &_addr);\n>  }\n>  \n> +/**\n> + * cxl_dpa_free - release DPA (Device Physical Address)\n> + * @cxled: endpoint decoder linked to the DPA\n> + *\n> + * Returns 0 or error.\n> + */\n>  int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n>  {\n>  \tstruct cxl_port *port = cxled_to_port(cxled);\n> @@ -572,6 +579,7 @@ int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n>  \tdevm_cxl_dpa_release(cxled);\n>  \treturn 0;\n>  }\n> +EXPORT_SYMBOL_NS_GPL(cxl_dpa_free, \"CXL\");\n>  \n>  int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n>  \t\t     enum cxl_partition_mode mode)\n> @@ -603,6 +611,82 @@ int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n>  \treturn 0;\n>  }\n>  \n> +static int find_free_decoder(struct device *dev, const void *data)\n> +{\n> +\tstruct cxl_endpoint_decoder *cxled;\n> +\tstruct cxl_port *port;\n> +\n> +\tif (!is_endpoint_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxled = to_cxl_endpoint_decoder(dev);\n> +\tport = cxled_to_port(cxled);\n> +\n> +\treturn cxled->cxld.id == (port->hdm_end + 1);\n> +}\n> +\n> +static struct cxl_endpoint_decoder *\n> +cxl_find_free_decoder(struct cxl_memdev *cxlmd)\n> +{\n> +\tstruct cxl_port *endpoint = cxlmd->endpoint;\n> +\tstruct device *dev;\n> +\n> +\tguard(rwsem_read)(&cxl_rwsem.dpa);\n> +\tdev = device_find_child(&endpoint->dev, NULL,\n> +\t\t\t\tfind_free_decoder);\n> +\tif (!dev)\n> +\t\treturn NULL;\n> +\n> +\treturn to_cxl_endpoint_decoder(dev);\n> +}\n> +\n> +/**\n> + * cxl_request_dpa - search and reserve DPA given input constraints\n> + * @cxlmd: memdev with an endpoint port with available decoders\n> + * @mode: CXL partition mode (ram vs pmem)\n> + * @alloc: dpa size required\n> + *\n> + * Returns a pointer to a 'struct cxl_endpoint_decoder' on success or\n> + * an errno encoded pointer on failure.\n> + *\n> + * Given that a region needs to allocate from limited HPA capacity it\n> + * may be the case that a device has more mappable DPA capacity than\n> + * available HPA. The expectation is that @alloc is a driver known\n> + * value based on the device capacity but which could not be fully\n> + * available due to HPA constraints.\n> + *\n> + * Returns a pinned cxl_decoder with at least @alloc bytes of capacity\n> + * reserved, or an error pointer. The caller is also expected to own the\n> + * lifetime of the memdev registration associated with the endpoint to\n> + * pin the decoder registered as well.\n> + */\n> +struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t     enum cxl_partition_mode mode,\n> +\t\t\t\t\t     resource_size_t alloc)\n> +{\n> +\tint rc;\n> +\n> +\tif (!IS_ALIGNED(alloc, SZ_256M))\n> +\t\treturn ERR_PTR(-EINVAL);\n> +\n> +\tstruct cxl_endpoint_decoder *cxled __free(put_cxled) =\n> +\t\tcxl_find_free_decoder(cxlmd);\n> +\n> +\tif (!cxled)\n> +\t\treturn ERR_PTR(-ENODEV);\n> +\n> +\trc = cxl_dpa_set_part(cxled, mode);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\trc = cxl_dpa_alloc(cxled, alloc);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n\nShould cxl_dpa_set_part() be unwound here, or does it not matter? If it doesn't matter:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> +\n> +\treturn no_free_ptr(cxled);\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_request_dpa, \"CXL\");\n> +\n>  static int __cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size)\n>  {\n>  \tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index d1b010e5e1d0..2b1f7d687a0e 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -667,6 +667,7 @@ struct cxl_root *find_cxl_root(struct cxl_port *port);\n>  \n>  DEFINE_FREE(put_cxl_root, struct cxl_root *, if (_T) put_device(&_T->port.dev))\n>  DEFINE_FREE(put_cxl_port, struct cxl_port *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n> +DEFINE_FREE(put_cxled, struct cxl_endpoint_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxld.dev))\n>  DEFINE_FREE(put_cxl_root_decoder, struct cxl_root_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxlsd.cxld.dev))\n>  DEFINE_FREE(put_cxl_region, struct cxl_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n>  \n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 783ad570a6eb..4802371db00e 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -7,6 +7,7 @@\n>  \n>  #include <linux/node.h>\n>  #include <linux/ioport.h>\n> +#include <linux/range.h>\n>  #include <cxl/mailbox.h>\n>  \n>  /**\n> @@ -276,4 +277,8 @@ struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n>  \t\t\t\t\t       unsigned long flags,\n>  \t\t\t\t\t       resource_size_t *max);\n>  void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n> +struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t     enum cxl_partition_mode mode,\n> +\t\t\t\t\t     resource_size_t alloc);\n> +int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\n\n\nOn 2/19/2026 4:40 AM, Alejandro Lucero Palau wrote:\n> \n> On 2/11/26 22:11, Cheatham, Benjamin wrote:\n>> On 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n>>> From: Alejandro Lucero <alucerop@amd.com>\n>>>\n>>> Region creation based on Type3 devices is triggered from user space\n>>> allowing memory combination through interleaving.\n>>>\n>>> In preparation for kernel driven region creation, that is Type2 drivers\n>>> triggering region creation backed with its advertised CXL memory, factor\n>>> out a common helper from the user-sysfs region setup for interleave ways.\n>>>\n>>> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n>>> Reviewed-by: Zhi Wang <zhiw@nvidia.com>\n>>> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n>>> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n>>> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n>>> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n>>> ---\n>>>  drivers/cxl/core/region.c | 43 ++++++++++++++++++++++++---------------\n>>>  1 file changed, 27 insertions(+), 16 deletions(-)\n>>>\n>>> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n>>> index f53b2e9fd9e6..ece1d3df7cf1 100644\n>>> --- a/drivers/cxl/core/region.c\n>>> +++ b/drivers/cxl/core/region.c\n>>> @@ -485,22 +485,14 @@ static ssize_t interleave_ways_show(struct device *dev,\n>>>   static const struct attribute_group *get_cxl_region_target_group(void);\n>>>  -static ssize_t interleave_ways_store(struct device *dev,\n>>> - struct device_attribute *attr,\n>>> - const char *buf, size_t len)\n>>> +static int set_interleave_ways(struct cxl_region *cxlr, int val)\n>> @val should probably stay an unsigned int. You pass an unsigned int in the sysfs function, and the\n>> function was originally coded with that in mind (same with @save below).\n> \n> Good catch. I wonder if I should just change the way the value is obtained, using kstrtoint instead of kstrtouint, as those values are used for cxl_region_params fields defined as int. In other words, it seems doing that simpler than changing all the other places you mention and the structs involved. I can not see a reason for using unsigned int so I think I will follow that approach. Tell me if you think otherwise.\n> \n\nIf I had to guess unsigned int was used because a negative interleave granularity/ways makes no sense. I think your suggestion is fine though since no one\nin their right mind would give anything but a (relatively) small and positive value for these.\n\nThanks,\nBen\n\n> \n> Thank you\n> \n> \n>> With that cleaned up:\n>> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n>>\n>>>  {\n>>> - struct cxl_root_decoder *cxlrd = to_cxl_root_decoder(dev->parent);\n>>> + struct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n>>>  struct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n>>> - struct cxl_region *cxlr = to_cxl_region(dev);\n>>>  struct cxl_region_params *p = &cxlr->params;\n>>> - unsigned int val, save;\n>>> - int rc;\n>>> + int save, rc;\n>>>  u8 iw;\n>>>  - rc = kstrtouint(buf, 0, &val);\n>>> - if (rc)\n>>> - return rc;\n>>> -\n>>>  rc = ways_to_eiw(val, &iw);\n>>>  if (rc)\n>>>  return rc;\n>>> @@ -515,9 +507,7 @@ static ssize_t interleave_ways_store(struct device *dev,\n>>>  return -EINVAL;\n>>>  }\n>>>  - ACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n>>> - if ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n>>> - return rc;\n>>> + lockdep_assert_held_write(&cxl_rwsem.region);\n>>>   if (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n>>>  return -EBUSY;\n>>> @@ -525,10 +515,31 @@ static ssize_t interleave_ways_store(struct device *dev,\n>>>  save = p->interleave_ways;\n>>>  p->interleave_ways = val;\n>>>  rc = sysfs_update_group(&cxlr->dev.kobj, get_cxl_region_target_group());\n>>> - if (rc) {\n>>> + if (rc)\n>>>  p->interleave_ways = save;\n>>> +\n>>> + return rc;\n>>> +}\n>>> +\n>>> +static ssize_t interleave_ways_store(struct device *dev,\n>>> + struct device_attribute *attr,\n>>> + const char *buf, size_t len)\n>>> +{\n>>> + struct cxl_region *cxlr = to_cxl_region(dev);\n>>> + unsigned int val;\n>>> + int rc;\n>>> +\n>>> + rc = kstrtouint(buf, 0, &val);\n>>> + if (rc)\n>>> + return rc;\n>>> +\n>>> + ACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n>>> + if ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n>>> + return rc;\n>>> +\n>>> + rc = set_interleave_ways(cxlr, val);\n>>> + if (rc)\n>>>  return rc;\n>>> - }\n>>>   return len;\n>>>  }\n\n\n\n---\n\nOn 2/19/2026 3:58 AM, Alejandro Lucero Palau wrote:\n> \n> On 2/11/26 22:10, Cheatham, Benjamin wrote:\n>> On 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n>>> From: Alejandro Lucero <alucerop@amd.com>\n>>>\n>>> CXL region creation involves allocating capacity from Device Physical\n>>> Address (DPA) and assigning it to decode a given Host Physical Address\n>>> (HPA). Before determining how much DPA to allocate the amount of available\n>>> HPA must be determined. Also, not all HPA is created equal, some HPA\n>>> targets RAM, some targets PMEM, some is prepared for device-memory flows\n>>> like HDM-D and HDM-DB, and some is HDM-H (host-only).\n>>>\n>>> In order to support Type2 CXL devices, wrap all of those concerns into\n>>> an API that retrieves a root decoder (platform CXL window) that fits the\n>>> specified constraints and the capacity available for a new region.\n>>>\n>>> Add a complementary function for releasing the reference to such root\n>>> decoder.\n>>>\n>>> Based on https://lore.kernel.org/linux-cxl/168592159290.1948938.13522227102445462976.stgit@dwillia2-xfh.jf.intel.com/\n>>>\n>>> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n>>> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n>>> ---\n>>>  drivers/cxl/core/region.c | 164 ++++++++++++++++++++++++++++++++++++++\n>>>  drivers/cxl/cxl.h | 3 +\n>>>  include/cxl/cxl.h | 6 ++\n>>>  3 files changed, 173 insertions(+)\n>>>\n>>> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n>>> index 954b8fcdbac6..bdefd088f5f1 100644\n>>> --- a/drivers/cxl/core/region.c\n>>> +++ b/drivers/cxl/core/region.c\n>>> @@ -705,6 +705,170 @@ static int free_hpa(struct cxl_region *cxlr)\n>>>  return 0;\n>>>  }\n>>>  +struct cxlrd_max_context {\n>>> + struct device * const *host_bridges;\n>>> + int interleave_ways;\n>>> + unsigned long flags;\n>>> + resource_size_t max_hpa;\n>>> + struct cxl_root_decoder *cxlrd;\n>>> +};\n>>> +\n>>> +static int find_max_hpa(struct device *dev, void *data)\n>>> +{\n>>> + struct cxlrd_max_context *ctx = data;\n>>> + struct cxl_switch_decoder *cxlsd;\n>>> + struct cxl_root_decoder *cxlrd;\n>>> + struct resource *res, *prev;\n>>> + struct cxl_decoder *cxld;\n>>> + resource_size_t free = 0;\n>>> + resource_size_t max;\n>>> + int found = 0;\n>>> +\n>>> + if (!is_root_decoder(dev))\n>>> + return 0;\n>>> +\n>>> + cxlrd = to_cxl_root_decoder(dev);\n>>> + cxlsd = &cxlrd->cxlsd;\n>>> + cxld = &cxlsd->cxld;\n>>> +\n>>> + if ((cxld->flags & ctx->flags) != ctx->flags) {\n>>> + dev_dbg(dev, \"flags not matching: %08lx vs %08lx\\n\",\n>>> + cxld->flags, ctx->flags);\n>>> + return 0;\n>>> + }\n>>> +\n>>> + for (int i = 0; i < ctx->interleave_ways; i++) {\n>>> + for (int j = 0; j < ctx->interleave_ways; j++) {\n>>> + if (ctx->host_bridges[i] == cxlsd->target[j]->dport_dev) {\n>>> + found++;\n>>> + break;\n>>> + }\n>>> + }\n>>> + }\n>> This may be over complicated. I'm not quite sure how it works (I'm just slow today I guess), but I understand\n>> what the intention is based on the debug print below. My issue is that ctx->host_bridges is only set to 1 host\n>> bridge (endpoint->host_bridge) in cxl_get_hpa_freespace(), which is the only caller of this function. At that\n>> point, why have the outer loop at all? At that point, you could also simplify ctx->host_bridges to only\n>> be a struct device * const.\n>>\n>> Maybe this gets called elsewhere later on in the series? I haven't looked at the rest yet. If I'm wrong, then\n>> I'd probably add a comment saying what the cxlsd->target[] entries are supposed to be pointing at.\n> \n> \n> Hi Ben,\n> \n> \n> I do remember this one.\n> \n> \n> Dan's original patches had this support for interleaving, then I removed it as the case for Type2 and interleaving is quite unlikely, at least right now and likely in the near future. But I was told why do not support it as it was trivial to do so. FWIW, If I think only about the use case coming with the patchset, I agree with you, but because those previous discussions, I think I have to leave it.\n> \n\nI'm fine with that, but I would at least do the fix with the decoder position in 19/22 and make a note that the\ninterleave_ways parameter in cxl_get_hpa_freespace() below is currently unused (unless I'm misunderstanding\nthe endpoint->host_bridge member).\n\nThat way, the support is mostly there and just requires a small, previously noted, addition to enable. If you're\nfine with that then feel free to add my Reviewed-by after implementing in v24.\n\nThanks,\nBen\n\n> \n> Thank you\n> \n> \n>>> +\n>>> + if (found != ctx->interleave_ways) {\n>>> + dev_dbg(dev,\n>>> + \"Not enough host bridges. Found %d for %d interleave ways requested\\n\",\n>>> + found, ctx->interleave_ways);\n>>> + return 0;\n>>> + }\n>>> +\n>>> + /*\n>>> + * Walk the root decoder resource range relying on cxl_rwsem.region to\n>>> + * preclude sibling arrival/departure and find the largest free space\n>>> + * gap.\n>>> + */\n>>> + lockdep_assert_held_read(&cxl_rwsem.region);\n>>> + res = cxlrd->res->child;\n>>> +\n>>> + /* With no resource child the whole parent resource is available */\n>>> + if (!res)\n>>> + max = resource_size(cxlrd->res);\n>>> + else\n>>> + max = 0;\n>>> +\n>>> + for (prev = NULL; res; prev = res, res = res->sibling) {\n>>> + if (!prev && res->start == cxlrd->res->start &&\n>>> + res->end == cxlrd->res->end) {\n>>> + max = resource_size(cxlrd->res);\n>>> + break;\n>>> + }\n>>> + /*\n>>> + * Sanity check for preventing arithmetic problems below as a\n>>> + * resource with size 0 could imply using the end field below\n>>> + * when set to unsigned zero - 1 or all f in hex.\n>>> + */\n>>> + if (prev && !resource_size(prev))\n>>> + continue;\n>>> +\n>>> + if (!prev && res->start > cxlrd->res->start) {\n>>> + free = res->start - cxlrd->res->start;\n>>> + max = max(free, max);\n>>> + }\n>>> + if (prev && res->start > prev->end + 1) {\n>>> + free = res->start - prev->end + 1;\n>>> + max = max(free, max);\n>>> + }\n>>> + }\n>>> +\n>>> + if (prev && prev->end + 1 < cxlrd->res->end + 1) {\n>>> + free = cxlrd->res->end + 1 - prev->end + 1;\n>>> + max = max(free, max);\n>>> + }\n>>> +\n>>> + dev_dbg(cxlrd_dev(cxlrd), \"found %pa bytes of free space\\n\", &max);\n>>> + if (max > ctx->max_hpa) {\n>>> + if (ctx->cxlrd)\n>>> + put_device(cxlrd_dev(ctx->cxlrd));\n>>> + get_device(cxlrd_dev(cxlrd));\n>>> + ctx->cxlrd = cxlrd;\n>>> + ctx->max_hpa = max;\n>>> + }\n>>> + return 0;\n>>> +}\n>>> +\n>>> +/**\n>>> + * cxl_get_hpa_freespace - find a root decoder with free capacity per constraints\n>>> + * @cxlmd: the mem device requiring the HPA\n>>> + * @interleave_ways: number of entries in @host_bridges\n>>> + * @flags: CXL_DECODER_F flags for selecting RAM vs PMEM, and Type2 device\n>>> + * @max_avail_contig: output parameter of max contiguous bytes available in the\n>>> + * returned decoder\n>>> + *\n>>> + * Returns a pointer to a struct cxl_root_decoder\n>>> + *\n>>> + * The return tuple of a 'struct cxl_root_decoder' and 'bytes available given\n>>> + * in (@max_avail_contig))' is a point in time snapshot. If by the time the\n>>> + * caller goes to use this decoder and its capacity is reduced then caller needs\n>>> + * to loop and retry.\n>>> + *\n>>> + * The returned root decoder has an elevated reference count that needs to be\n>>> + * put with cxl_put_root_decoder(cxlrd).\n>>> + */\n>>> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n>>> + int interleave_ways,\n>>> + unsigned long flags,\n>>> + resource_size_t *max_avail_contig)\n>>> +{\n>>> + struct cxlrd_max_context ctx = {\n>>> + .flags = flags,\n>>> + .interleave_ways = interleave_ways,\n>>> + };\n>>> + struct cxl_port *root_port;\n>>> + struct cxl_port *endpoint;\n>>> +\n>>> + endpoint = cxlmd->endpoint;\n>>> + if (!endpoint) {\n>>> + dev_dbg(&cxlmd->dev, \"endpoint not linked to memdev\\n\");\n>>> + return ERR_PTR(-ENXIO);\n>>> + }\n>>> +\n>>> + ctx.host_bridges = &endpoint->host_bridge;\n>> Mentioned earlier, interleave_ways is effectively hardcoded to 1 (unless I'm misunderstanding\n>> something). I think what you want here is to go to the CXL root and pass in the children (i.e. host bridges)?\n>> I'm not sure of what the fix is to get the intended behavior.\n>>\n>> It may be worth getting rid of the interleave_ways portion of this function and\n>> add it later when someone needs it. You could also explain it's hard coded to 1/unused\n>> in the doc comment if you know of an immediate need for it.\n>>\n>>> +\n>>> + struct cxl_root *root __free(put_cxl_root) = find_cxl_root(endpoint);\n>>> + if (!root) {\n>>> + dev_dbg(&endpoint->dev, \"endpoint is not related to a root port\\n\");\n>>> + return ERR_PTR(-ENXIO);\n>>> + }\n>>> +\n>>> + root_port = &root->port;\n>>> + scoped_guard(rwsem_read, &cxl_rwsem.region)\n>>> + device_for_each_child(&root_port->dev, &ctx, find_max_hpa);\n>> Can just use a guard() here.\n>>\n>>> +\n>>> + if (!ctx.cxlrd)\n>>> + return ERR_PTR(-ENOMEM);\n>>> +\n>>> + *max_avail_contig = ctx.max_hpa;\n>>> + return ctx.cxlrd;\n>>> +}\n>>> +EXPORT_SYMBOL_NS_GPL(cxl_get_hpa_freespace, \"CXL\");\n>>> +\n>>> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd)\n>>> +{\n>>> + put_device(cxlrd_dev(cxlrd));\n>>> +}\n>>> +EXPORT_SYMBOL_NS_GPL(cxl_put_root_decoder, \"CXL\");\n>>> +\n>>>  static ssize_t size_store(struct device *dev, struct device_attribute *attr,\n>>>  const char *buf, size_t len)\n>>>  {\n>>> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n>>> index 944c5d1ccceb..c7d9b2c2908f 100644\n>>> --- a/drivers/cxl/cxl.h\n>>> +++ b/drivers/cxl/cxl.h\n>>> @@ -706,6 +706,9 @@ struct cxl_root_decoder *to_cxl_root_decoder(struct device *dev);\n>>>  struct cxl_switch_decoder *to_cxl_switch_decoder(struct device *dev);\n>>>  struct cxl_endpoint_decoder *to_cxl_endpoint_decoder(struct device *dev);\n>>>  bool is_root_decoder(struct device *dev);\n>>> +\n>>> +#define cxlrd_dev(cxlrd) (&(cxlrd)->cxlsd.cxld.dev)\n>>> +\n>>>  bool is_switch_decoder(struct device *dev);\n>>>  bool is_endpoint_decoder(struct device *dev);\n>>>  struct cxl_root_decoder *cxl_root_decoder_alloc(struct cxl_port *port,\n>>> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n>>> index 92880c26b2d5..834dc7e78934 100644\n>>> --- a/include/cxl/cxl.h\n>>> +++ b/include/cxl/cxl.h\n>>> @@ -255,4 +255,10 @@ struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n>>>  struct range;\n>>>  int cxl_get_region_range(struct cxl_region *region, struct range *range);\n>>>  void cxl_unregister_region(struct cxl_region *cxlr);\n>>> +struct cxl_port;\n>>> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n>>> + int interleave_ways,\n>>> + unsigned long flags,\n>>> + resource_size_t *max);\n>>> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n>>>  #endif /* __CXL_CXL_H__ */\n\n",
          "reply_to": "alejandro.lucero-palau",
          "message_date": "2026-02-11"
        },
        {
          "author": "Cheatham, Benjamin",
          "summary": "Reviewer suggested updating ABI documentation to explicitly state that this attribute only applies to type 3 regions, and offered a Reviewed-by tag.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "suggested update",
            "considered worthwhile"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "I haven't read the ABI docs, but would it be worthwhile to update the documentation for this attribute\nto mention it only makes type 3 regions? I'm flip-flopping on whether it's worth the trouble but thought\nI should mention it.\n\nEither way:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>",
          "reply_to": "alejandro.lucero-palau",
          "message_date": "2026-02-11"
        },
        {
          "author": "Cheatham, Benjamin",
          "summary": "Reviewer noted that the function parameter @val should remain an unsigned int, as it is passed as such in sysfs and was originally coded to expect this type.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "requested_change"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "@val should probably stay an unsigned int. You pass an unsigned int in the sysfs function, and the\nfunction was originally coded with that in mind (same with @save below). With that cleaned up:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>",
          "reply_to": "alejandro.lucero-palau",
          "message_date": "2026-02-11"
        },
        {
          "author": "Cheatham, Benjamin",
          "summary": "Gave Reviewed-by",
          "sentiment": "positive",
          "sentiment_signals": [],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> By definition a type2 cxl device will use the host managed memory for\n> specific functionality, therefore it should not be available to other\n> uses.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Davidlohr Bueso <daves@stgolabs.net>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> ---\n>  drivers/cxl/core/region.c | 7 +++++++\n>  1 file changed, 7 insertions(+)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index 293e63dfef22..12df717cc881 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -4441,6 +4441,13 @@ static int cxl_region_probe(struct device *dev)\n>  \tif (rc)\n>  \t\treturn rc;\n>  \n> +\t/*\n> +\t * HDM-D[B] (device-memory) regions have accelerator specific usage.\n> +\t * Skip device-dax registration.\n> +\t */\n> +\tif (cxlr->type == CXL_DECODER_DEVMEM)\n> +\t\treturn 0;\n\nMinor nit: Should probably move this to be the first thing in the function. It would save\nhaving to acquire a lock in cxl_region_can_probe() above. Keep my reviewed-by either way,\nit's really just a minor optimization.\n> +\n>  \t/*\n>  \t * From this point on any path that changes the region's state away from\n>  \t * CXL_CONFIG_COMMIT is also responsible for releasing the driver.\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> CXL region creation involves allocating capacity from Device Physical\n> Address (DPA) and assigning it to decode a given Host Physical Address\n> (HPA). Before determining how much DPA to allocate the amount of available\n> HPA must be determined. Also, not all HPA is created equal, some HPA\n> targets RAM, some targets PMEM, some is prepared for device-memory flows\n> like HDM-D and HDM-DB, and some is HDM-H (host-only).\n> \n> In order to support Type2 CXL devices, wrap all of those concerns into\n> an API that retrieves a root decoder (platform CXL window) that fits the\n> specified constraints and the capacity available for a new region.\n> \n> Add a complementary function for releasing the reference to such root\n> decoder.\n> \n> Based on https://lore.kernel.org/linux-cxl/168592159290.1948938.13522227102445462976.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> ---\n>  drivers/cxl/core/region.c | 164 ++++++++++++++++++++++++++++++++++++++\n>  drivers/cxl/cxl.h         |   3 +\n>  include/cxl/cxl.h         |   6 ++\n>  3 files changed, 173 insertions(+)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index 954b8fcdbac6..bdefd088f5f1 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -705,6 +705,170 @@ static int free_hpa(struct cxl_region *cxlr)\n>  \treturn 0;\n>  }\n>  \n> +struct cxlrd_max_context {\n> +\tstruct device * const *host_bridges;\n> +\tint interleave_ways;\n> +\tunsigned long flags;\n> +\tresource_size_t max_hpa;\n> +\tstruct cxl_root_decoder *cxlrd;\n> +};\n> +\n> +static int find_max_hpa(struct device *dev, void *data)\n> +{\n> +\tstruct cxlrd_max_context *ctx = data;\n> +\tstruct cxl_switch_decoder *cxlsd;\n> +\tstruct cxl_root_decoder *cxlrd;\n> +\tstruct resource *res, *prev;\n> +\tstruct cxl_decoder *cxld;\n> +\tresource_size_t free = 0;\n> +\tresource_size_t max;\n> +\tint found = 0;\n> +\n> +\tif (!is_root_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxlrd = to_cxl_root_decoder(dev);\n> +\tcxlsd = &cxlrd->cxlsd;\n> +\tcxld = &cxlsd->cxld;\n> +\n> +\tif ((cxld->flags & ctx->flags) != ctx->flags) {\n> +\t\tdev_dbg(dev, \"flags not matching: %08lx vs %08lx\\n\",\n> +\t\t\tcxld->flags, ctx->flags);\n> +\t\treturn 0;\n> +\t}\n> +\n> +\tfor (int i = 0; i < ctx->interleave_ways; i++) {\n> +\t\tfor (int j = 0; j < ctx->interleave_ways; j++) {\n> +\t\t\tif (ctx->host_bridges[i] == cxlsd->target[j]->dport_dev) {\n> +\t\t\t\tfound++;\n> +\t\t\t\tbreak;\n> +\t\t\t}\n> +\t\t}\n> +\t}\n\nThis may be over complicated. I'm not quite sure how it works (I'm just slow today I guess), but I understand\nwhat the intention is based on the debug print below. My issue is that ctx->host_bridges is only set to 1 host\nbridge (endpoint->host_bridge) in cxl_get_hpa_freespace(), which is the only caller of this function. At that\npoint, why have the outer loop at all? At that point, you could also simplify ctx->host_bridges to only\nbe a struct device * const.\n\nMaybe this gets called elsewhere later on in the series? I haven't looked at the rest yet. If I'm wrong, then\nI'd probably add a comment saying what the cxlsd->target[] entries are supposed to be pointing at.\n> +\n> +\tif (found != ctx->interleave_ways) {\n> +\t\tdev_dbg(dev,\n> +\t\t\t\"Not enough host bridges. Found %d for %d interleave ways requested\\n\",\n> +\t\t\tfound, ctx->interleave_ways);\n> +\t\treturn 0;\n> +\t}\n> +\n> +\t/*\n> +\t * Walk the root decoder resource range relying on cxl_rwsem.region to\n> +\t * preclude sibling arrival/departure and find the largest free space\n> +\t * gap.\n> +\t */\n> +\tlockdep_assert_held_read(&cxl_rwsem.region);\n> +\tres = cxlrd->res->child;\n> +\n> +\t/* With no resource child the whole parent resource is available */\n> +\tif (!res)\n> +\t\tmax = resource_size(cxlrd->res);\n> +\telse\n> +\t\tmax = 0;\n> +\n> +\tfor (prev = NULL; res; prev = res, res = res->sibling) {\n> +\t\tif (!prev && res->start == cxlrd->res->start &&\n> +\t\t    res->end == cxlrd->res->end) {\n> +\t\t\tmax = resource_size(cxlrd->res);\n> +\t\t\tbreak;\n> +\t\t}\n> +\t\t/*\n> +\t\t * Sanity check for preventing arithmetic problems below as a\n> +\t\t * resource with size 0 could imply using the end field below\n> +\t\t * when set to unsigned zero - 1 or all f in hex.\n> +\t\t */\n> +\t\tif (prev && !resource_size(prev))\n> +\t\t\tcontinue;\n> +\n> +\t\tif (!prev && res->start > cxlrd->res->start) {\n> +\t\t\tfree = res->start - cxlrd->res->start;\n> +\t\t\tmax = max(free, max);\n> +\t\t}\n> +\t\tif (prev && res->start > prev->end + 1) {\n> +\t\t\tfree = res->start - prev->end + 1;\n> +\t\t\tmax = max(free, max);\n> +\t\t}\n> +\t}\n> +\n> +\tif (prev && prev->end + 1 < cxlrd->res->end + 1) {\n> +\t\tfree = cxlrd->res->end + 1 - prev->end + 1;\n> +\t\tmax = max(free, max);\n> +\t}\n> +\n> +\tdev_dbg(cxlrd_dev(cxlrd), \"found %pa bytes of free space\\n\", &max);\n> +\tif (max > ctx->max_hpa) {\n> +\t\tif (ctx->cxlrd)\n> +\t\t\tput_device(cxlrd_dev(ctx->cxlrd));\n> +\t\tget_device(cxlrd_dev(cxlrd));\n> +\t\tctx->cxlrd = cxlrd;\n> +\t\tctx->max_hpa = max;\n> +\t}\n> +\treturn 0;\n> +}\n> +\n> +/**\n> + * cxl_get_hpa_freespace - find a root decoder with free capacity per constraints\n> + * @cxlmd: the mem device requiring the HPA\n> + * @interleave_ways: number of entries in @host_bridges\n> + * @flags: CXL_DECODER_F flags for selecting RAM vs PMEM, and Type2 device\n> + * @max_avail_contig: output parameter of max contiguous bytes available in the\n> + *\t\t      returned decoder\n> + *\n> + * Returns a pointer to a struct cxl_root_decoder\n> + *\n> + * The return tuple of a 'struct cxl_root_decoder' and 'bytes available given\n> + * in (@max_avail_contig))' is a point in time snapshot. If by the time the\n> + * caller goes to use this decoder and its capacity is reduced then caller needs\n> + * to loop and retry.\n> + *\n> + * The returned root decoder has an elevated reference count that needs to be\n> + * put with cxl_put_root_decoder(cxlrd).\n> + */\n> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t       int interleave_ways,\n> +\t\t\t\t\t       unsigned long flags,\n> +\t\t\t\t\t       resource_size_t *max_avail_contig)\n> +{\n> +\tstruct cxlrd_max_context ctx = {\n> +\t\t.flags = flags,\n> +\t\t.interleave_ways = interleave_ways,\n> +\t};\n> +\tstruct cxl_port *root_port;\n> +\tstruct cxl_port *endpoint;\n> +\n> +\tendpoint = cxlmd->endpoint;\n> +\tif (!endpoint) {\n> +\t\tdev_dbg(&cxlmd->dev, \"endpoint not linked to memdev\\n\");\n> +\t\treturn ERR_PTR(-ENXIO);\n> +\t}\n> +\n> +\tctx.host_bridges = &endpoint->host_bridge;\n\nMentioned earlier, interleave_ways is effectively hardcoded to 1 (unless I'm misunderstanding\nsomething). I think what you want here is to go to the CXL root and pass in the children (i.e. host bridges)?\nI'm not sure of what the fix is to get the intended behavior.\n\nIt may be worth getting rid of the interleave_ways portion of this function and\nadd it later when someone needs it. You could also explain it's hard coded to 1/unused\nin the doc comment if you know of an immediate need for it.\n\n> +\n> +\tstruct cxl_root *root __free(put_cxl_root) = find_cxl_root(endpoint);\n> +\tif (!root) {\n> +\t\tdev_dbg(&endpoint->dev, \"endpoint is not related to a root port\\n\");\n> +\t\treturn ERR_PTR(-ENXIO);\n> +\t}\n> +\n> +\troot_port = &root->port;\n> +\tscoped_guard(rwsem_read, &cxl_rwsem.region)\n> +\t\tdevice_for_each_child(&root_port->dev, &ctx, find_max_hpa);\n\nCan just use a guard() here.\n\n> +\n> +\tif (!ctx.cxlrd)\n> +\t\treturn ERR_PTR(-ENOMEM);\n> +\n> +\t*max_avail_contig = ctx.max_hpa;\n> +\treturn ctx.cxlrd;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_get_hpa_freespace, \"CXL\");\n> +\n> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd)\n> +{\n> +\tput_device(cxlrd_dev(cxlrd));\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_put_root_decoder, \"CXL\");\n> +\n>  static ssize_t size_store(struct device *dev, struct device_attribute *attr,\n>  \t\t\t  const char *buf, size_t len)\n>  {\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index 944c5d1ccceb..c7d9b2c2908f 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -706,6 +706,9 @@ struct cxl_root_decoder *to_cxl_root_decoder(struct device *dev);\n>  struct cxl_switch_decoder *to_cxl_switch_decoder(struct device *dev);\n>  struct cxl_endpoint_decoder *to_cxl_endpoint_decoder(struct device *dev);\n>  bool is_root_decoder(struct device *dev);\n> +\n> +#define cxlrd_dev(cxlrd) (&(cxlrd)->cxlsd.cxld.dev)\n> +\n>  bool is_switch_decoder(struct device *dev);\n>  bool is_endpoint_decoder(struct device *dev);\n>  struct cxl_root_decoder *cxl_root_decoder_alloc(struct cxl_port *port,\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 92880c26b2d5..834dc7e78934 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -255,4 +255,10 @@ struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n>  struct range;\n>  int cxl_get_region_range(struct cxl_region *region, struct range *range);\n>  void cxl_unregister_region(struct cxl_region *cxlr);\n> +struct cxl_port;\n> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t       int interleave_ways,\n> +\t\t\t\t\t       unsigned long flags,\n> +\t\t\t\t\t       resource_size_t *max);\n> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Check if device HDM is already committed during firmware/BIOS\n> initialization.\n> \n> A CXL region should exist if so after memdev allocation/initialization.\n> Get HPA from region and map it.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> ---\n>  drivers/net/ethernet/sfc/efx_cxl.c | 28 +++++++++++++++++++++++++++-\n>  1 file changed, 27 insertions(+), 1 deletion(-)\n> \n> diff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\n> index a77ef4783fcb..3536eccf1b2a 100644\n> --- a/drivers/net/ethernet/sfc/efx_cxl.c\n> +++ b/drivers/net/ethernet/sfc/efx_cxl.c\n> @@ -19,6 +19,7 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n>  \tstruct efx_nic *efx = &probe_data->efx;\n>  \tstruct pci_dev *pci_dev = efx->pci_dev;\n>  \tstruct efx_cxl *cxl;\n> +\tstruct range range;\n>  \tu16 dvsec;\n>  \tint rc;\n>  \n> @@ -90,13 +91,38 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n>  \t\treturn PTR_ERR(cxl->cxlmd);\n>  \t}\n>  \n> -\tprobe_data->cxl = cxl;\n> +\tcxl->cxled = cxl_get_committed_decoder(cxl->cxlmd, &cxl->efx_region);\n> +\tif (cxl->cxled) {\n> +\t\tif (!cxl->efx_region) {\n> +\t\t\tpci_err(pci_dev, \"CXL found committed decoder without a region\");\n> +\t\t\treturn -ENODEV;\n> +\t\t}\n> +\t\trc = cxl_get_region_range(cxl->efx_region, &range);\n\nMissing an empty line above.\n\n> +\t\tif (rc) {\n> +\t\t\tpci_err(pci_dev,\n> +\t\t\t\t\"CXL getting regions params from a committed decoder failed\");\n> +\t\t\treturn rc;\n> +\t\t}\n> +\n> +\t\tcxl->ctpio_cxl = ioremap(range.start, range.end - range.start + 1);\n\nMaybe use range_len() instead for the second parameter?\n\n> +\t\tif (!cxl->ctpio_cxl) {\n> +\t\t\tpci_err(pci_dev, \"CXL ioremap region (%pra) failed\", &range);\n> +\t\t\treturn -ENOMEM;\n> +\t\t}\n> +\n> +\t\tprobe_data->cxl = cxl;\n> +\t}\n>  \n>  \treturn 0;\n>  }\n>  \n>  void efx_cxl_exit(struct efx_probe_data *probe_data)\n>  {\n> +\tif (!probe_data->cxl)\n> +\t\treturn;\n> +\n> +\tiounmap(probe_data->cxl->ctpio_cxl);\n> +\tcxl_unregister_region(probe_data->cxl->efx_region);\n>  }\n>  \n>  MODULE_IMPORT_NS(\"CXL\");\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Region creation involves finding available DPA (device-physical-address)\n> capacity to map into HPA (host-physical-address) space.\n> \n> In order to support CXL Type2 devices, define an API, cxl_request_dpa(),\n> that tries to allocate the DPA memory the driver requires to operate.The\n> memory requested should not be bigger than the max available HPA obtained\n> previously with cxl_get_hpa_freespace().\n> \n> Based on https://lore.kernel.org/linux-cxl/168592158743.1948938.7622563891193802610.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> ---\n>  drivers/cxl/core/hdm.c | 84 ++++++++++++++++++++++++++++++++++++++++++\n>  drivers/cxl/cxl.h      |  1 +\n>  include/cxl/cxl.h      |  5 +++\n>  3 files changed, 90 insertions(+)\n> \n> diff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\n> index a172ce4e9b19..d60a697f12cc 100644\n> --- a/drivers/cxl/core/hdm.c\n> +++ b/drivers/cxl/core/hdm.c\n> @@ -3,6 +3,7 @@\n>  #include <linux/seq_file.h>\n>  #include <linux/device.h>\n>  #include <linux/delay.h>\n> +#include <cxl/cxl.h>\n>  \n>  #include \"cxlmem.h\"\n>  #include \"core.h\"\n> @@ -546,6 +547,12 @@ bool cxl_resource_contains_addr(const struct resource *res, const resource_size_\n>  \treturn resource_contains(res, &_addr);\n>  }\n>  \n> +/**\n> + * cxl_dpa_free - release DPA (Device Physical Address)\n> + * @cxled: endpoint decoder linked to the DPA\n> + *\n> + * Returns 0 or error.\n> + */\n>  int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n>  {\n>  \tstruct cxl_port *port = cxled_to_port(cxled);\n> @@ -572,6 +579,7 @@ int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n>  \tdevm_cxl_dpa_release(cxled);\n>  \treturn 0;\n>  }\n> +EXPORT_SYMBOL_NS_GPL(cxl_dpa_free, \"CXL\");\n>  \n>  int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n>  \t\t     enum cxl_partition_mode mode)\n> @@ -603,6 +611,82 @@ int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n>  \treturn 0;\n>  }\n>  \n> +static int find_free_decoder(struct device *dev, const void *data)\n> +{\n> +\tstruct cxl_endpoint_decoder *cxled;\n> +\tstruct cxl_port *port;\n> +\n> +\tif (!is_endpoint_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxled = to_cxl_endpoint_decoder(dev);\n> +\tport = cxled_to_port(cxled);\n> +\n> +\treturn cxled->cxld.id == (port->hdm_end + 1);\n> +}\n> +\n> +static struct cxl_endpoint_decoder *\n> +cxl_find_free_decoder(struct cxl_memdev *cxlmd)\n> +{\n> +\tstruct cxl_port *endpoint = cxlmd->endpoint;\n> +\tstruct device *dev;\n> +\n> +\tguard(rwsem_read)(&cxl_rwsem.dpa);\n> +\tdev = device_find_child(&endpoint->dev, NULL,\n> +\t\t\t\tfind_free_decoder);\n> +\tif (!dev)\n> +\t\treturn NULL;\n> +\n> +\treturn to_cxl_endpoint_decoder(dev);\n> +}\n> +\n> +/**\n> + * cxl_request_dpa - search and reserve DPA given input constraints\n> + * @cxlmd: memdev with an endpoint port with available decoders\n> + * @mode: CXL partition mode (ram vs pmem)\n> + * @alloc: dpa size required\n> + *\n> + * Returns a pointer to a 'struct cxl_endpoint_decoder' on success or\n> + * an errno encoded pointer on failure.\n> + *\n> + * Given that a region needs to allocate from limited HPA capacity it\n> + * may be the case that a device has more mappable DPA capacity than\n> + * available HPA. The expectation is that @alloc is a driver known\n> + * value based on the device capacity but which could not be fully\n> + * available due to HPA constraints.\n> + *\n> + * Returns a pinned cxl_decoder with at least @alloc bytes of capacity\n> + * reserved, or an error pointer. The caller is also expected to own the\n> + * lifetime of the memdev registration associated with the endpoint to\n> + * pin the decoder registered as well.\n> + */\n> +struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t     enum cxl_partition_mode mode,\n> +\t\t\t\t\t     resource_size_t alloc)\n> +{\n> +\tint rc;\n> +\n> +\tif (!IS_ALIGNED(alloc, SZ_256M))\n> +\t\treturn ERR_PTR(-EINVAL);\n> +\n> +\tstruct cxl_endpoint_decoder *cxled __free(put_cxled) =\n> +\t\tcxl_find_free_decoder(cxlmd);\n> +\n> +\tif (!cxled)\n> +\t\treturn ERR_PTR(-ENODEV);\n> +\n> +\trc = cxl_dpa_set_part(cxled, mode);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\trc = cxl_dpa_alloc(cxled, alloc);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n\nShould cxl_dpa_set_part() be unwound here, or does it not matter? If it doesn't matter:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> +\n> +\treturn no_free_ptr(cxled);\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_request_dpa, \"CXL\");\n> +\n>  static int __cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size)\n>  {\n>  \tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index d1b010e5e1d0..2b1f7d687a0e 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -667,6 +667,7 @@ struct cxl_root *find_cxl_root(struct cxl_port *port);\n>  \n>  DEFINE_FREE(put_cxl_root, struct cxl_root *, if (_T) put_device(&_T->port.dev))\n>  DEFINE_FREE(put_cxl_port, struct cxl_port *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n> +DEFINE_FREE(put_cxled, struct cxl_endpoint_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxld.dev))\n>  DEFINE_FREE(put_cxl_root_decoder, struct cxl_root_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxlsd.cxld.dev))\n>  DEFINE_FREE(put_cxl_region, struct cxl_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n>  \n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 783ad570a6eb..4802371db00e 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -7,6 +7,7 @@\n>  \n>  #include <linux/node.h>\n>  #include <linux/ioport.h>\n> +#include <linux/range.h>\n>  #include <cxl/mailbox.h>\n>  \n>  /**\n> @@ -276,4 +277,8 @@ struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n>  \t\t\t\t\t       unsigned long flags,\n>  \t\t\t\t\t       resource_size_t *max);\n>  void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n> +struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t     enum cxl_partition_mode mode,\n> +\t\t\t\t\t     resource_size_t alloc);\n> +int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Current code is expecting Type3 or CXL_DECODER_HOSTONLYMEM devices only.\n> Support for Type2 implies region type needs to be based on the endpoint\n> type HDM-D[B] instead.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Zhi Wang <zhiw@nvidia.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n> Reviewed-by: Davidlohr Bueso <daves@stgolabs.net>\n> ---\n>  drivers/cxl/core/region.c | 10 ++++++----\n>  1 file changed, 6 insertions(+), 4 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index bdefd088f5f1..f53b2e9fd9e6 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -2833,7 +2833,8 @@ static ssize_t create_ram_region_show(struct device *dev,\n>  }\n>  \n>  static struct cxl_region *__create_region(struct cxl_root_decoder *cxlrd,\n> -\t\t\t\t\t  enum cxl_partition_mode mode, int id)\n> +\t\t\t\t\t  enum cxl_partition_mode mode, int id,\n> +\t\t\t\t\t  enum cxl_decoder_type target_type)\n>  {\n>  \tint rc;\n>  \n> @@ -2855,7 +2856,7 @@ static struct cxl_region *__create_region(struct cxl_root_decoder *cxlrd,\n>  \t\treturn ERR_PTR(-EBUSY);\n>  \t}\n>  \n> -\treturn devm_cxl_add_region(cxlrd, id, mode, CXL_DECODER_HOSTONLYMEM);\n> +\treturn devm_cxl_add_region(cxlrd, id, mode, target_type);\n>  }\n>  \n>  static ssize_t create_region_store(struct device *dev, const char *buf,\n> @@ -2869,7 +2870,7 @@ static ssize_t create_region_store(struct device *dev, const char *buf,\n>  \tif (rc != 1)\n>  \t\treturn -EINVAL;\n>  \n> -\tcxlr = __create_region(cxlrd, mode, id);\n> +\tcxlr = __create_region(cxlrd, mode, id, CXL_DECODER_HOSTONLYMEM);\n\nI haven't read the ABI docs, but would it be worthwhile to update the documentation for this attribute\nto mention it only makes type 3 regions? I'm flip-flopping on whether it's worth the trouble but thought\nI should mention it.\n\nEither way:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n\n>  \tif (IS_ERR(cxlr))\n>  \t\treturn PTR_ERR(cxlr);\n>  \n> @@ -4036,7 +4037,8 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n>  \n>  \tdo {\n>  \t\tcxlr = __create_region(cxlrd, cxlds->part[part].mode,\n> -\t\t\t\t       atomic_read(&cxlrd->region_id));\n> +\t\t\t\t       atomic_read(&cxlrd->region_id),\n> +\t\t\t\t       cxled->cxld.target_type);\n>  \t} while (IS_ERR(cxlr) && PTR_ERR(cxlr) == -EBUSY);\n>  \n>  \tif (IS_ERR(cxlr)) {\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Region creation based on Type3 devices is triggered from user space\n> allowing memory combination through interleaving.\n> \n> In preparation for kernel driven region creation, that is Type2 drivers\n> triggering region creation backed with its advertised CXL memory, factor\n> out a common helper from the user-sysfs region setup for interleave ways.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Zhi Wang <zhiw@nvidia.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n> ---\n>  drivers/cxl/core/region.c | 43 ++++++++++++++++++++++++---------------\n>  1 file changed, 27 insertions(+), 16 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index f53b2e9fd9e6..ece1d3df7cf1 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -485,22 +485,14 @@ static ssize_t interleave_ways_show(struct device *dev,\n>  \n>  static const struct attribute_group *get_cxl_region_target_group(void);\n>  \n> -static ssize_t interleave_ways_store(struct device *dev,\n> -\t\t\t\t     struct device_attribute *attr,\n> -\t\t\t\t     const char *buf, size_t len)\n> +static int set_interleave_ways(struct cxl_region *cxlr, int val)\n\n@val should probably stay an unsigned int. You pass an unsigned int in the sysfs function, and the\nfunction was originally coded with that in mind (same with @save below). With that cleaned up:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n\n>  {\n> -\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(dev->parent);\n> +\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n>  \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n> -\tstruct cxl_region *cxlr = to_cxl_region(dev);\n>  \tstruct cxl_region_params *p = &cxlr->params;\n> -\tunsigned int val, save;\n> -\tint rc;\n> +\tint save, rc;\n>  \tu8 iw;\n>  \n> -\trc = kstrtouint(buf, 0, &val);\n> -\tif (rc)\n> -\t\treturn rc;\n> -\n>  \trc = ways_to_eiw(val, &iw);\n>  \tif (rc)\n>  \t\treturn rc;\n> @@ -515,9 +507,7 @@ static ssize_t interleave_ways_store(struct device *dev,\n>  \t\treturn -EINVAL;\n>  \t}\n>  \n> -\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n> -\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n> -\t\treturn rc;\n> +\tlockdep_assert_held_write(&cxl_rwsem.region);\n>  \n>  \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n>  \t\treturn -EBUSY;\n> @@ -525,10 +515,31 @@ static ssize_t interleave_ways_store(struct device *dev,\n>  \tsave = p->interleave_ways;\n>  \tp->interleave_ways = val;\n>  \trc = sysfs_update_group(&cxlr->dev.kobj, get_cxl_region_target_group());\n> -\tif (rc) {\n> +\tif (rc)\n>  \t\tp->interleave_ways = save;\n> +\n> +\treturn rc;\n> +}\n> +\n> +static ssize_t interleave_ways_store(struct device *dev,\n> +\t\t\t\t     struct device_attribute *attr,\n> +\t\t\t\t     const char *buf, size_t len)\n> +{\n> +\tstruct cxl_region *cxlr = to_cxl_region(dev);\n> +\tunsigned int val;\n> +\tint rc;\n> +\n> +\trc = kstrtouint(buf, 0, &val);\n> +\tif (rc)\n> +\t\treturn rc;\n> +\n> +\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n> +\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n> +\t\treturn rc;\n> +\n> +\trc = set_interleave_ways(cxlr, val);\n> +\tif (rc)\n>  \t\treturn rc;\n> -\t}\n>  \n>  \treturn len;\n>  }\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Region creation based on Type3 devices is triggered from user space\n> allowing memory combination through interleaving.\n> \n> In preparation for kernel driven region creation, that is Type2 drivers\n> triggering region creation backed with its advertised CXL memory, factor\n> out a common helper from the user-sysfs region setup forinterleave\n> granularity.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Zhi Wang <zhiw@nvidia.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n> ---\n>  drivers/cxl/core/region.c | 39 +++++++++++++++++++++++++--------------\n>  1 file changed, 25 insertions(+), 14 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index ece1d3df7cf1..63c2aeb2ee1f 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -559,21 +559,14 @@ static ssize_t interleave_granularity_show(struct device *dev,\n>  \treturn sysfs_emit(buf, \"%d\\n\", p->interleave_granularity);\n>  }\n>  \n> -static ssize_t interleave_granularity_store(struct device *dev,\n> -\t\t\t\t\t    struct device_attribute *attr,\n> -\t\t\t\t\t    const char *buf, size_t len)\n> +static int set_interleave_granularity(struct cxl_region *cxlr, int val)\n\nSame thing as last patch. Assuming it's fixed:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n>  {\n> -\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(dev->parent);\n> +\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n>  \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n> -\tstruct cxl_region *cxlr = to_cxl_region(dev);\n>  \tstruct cxl_region_params *p = &cxlr->params;\n> -\tint rc, val;\n> +\tint rc;\n>  \tu16 ig;\n>  \n> -\trc = kstrtoint(buf, 0, &val);\n> -\tif (rc)\n> -\t\treturn rc;\n> -\n>  \trc = granularity_to_eig(val, &ig);\n>  \tif (rc)\n>  \t\treturn rc;\n> @@ -589,14 +582,32 @@ static ssize_t interleave_granularity_store(struct device *dev,\n>  \tif (cxld->interleave_ways > 1 && val != cxld->interleave_granularity)\n>  \t\treturn -EINVAL;\n>  \n> -\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n> -\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n> -\t\treturn rc;\n> -\n> +\tlockdep_assert_held_write(&cxl_rwsem.region);\n>  \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n>  \t\treturn -EBUSY;\n>  \n>  \tp->interleave_granularity = val;\n> +\treturn 0;\n> +}\n> +\n> +static ssize_t interleave_granularity_store(struct device *dev,\n> +\t\t\t\t\t    struct device_attribute *attr,\n> +\t\t\t\t\t    const char *buf, size_t len)\n> +{\n> +\tstruct cxl_region *cxlr = to_cxl_region(dev);\n> +\tint rc, val;\n> +\n> +\trc = kstrtoint(buf, 0, &val);\n> +\tif (rc)\n> +\t\treturn rc;\n> +\n> +\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n> +\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n> +\t\treturn rc;\n> +\n> +\trc = set_interleave_granularity(cxlr, val);\n> +\tif (rc)\n> +\t\treturn rc;\n>  \n>  \treturn len;\n>  }\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Creating a CXL region requires userspace intervention through the cxl\n> sysfs files. Type2 support should allow accelerator drivers to create\n> such cxl region from kernel code.\n> \n> Adding that functionality and integrating it with current support for\n> memory expanders.\n> \n> Based on https://lore.kernel.org/linux-cxl/168592159835.1948938.1647215579839222774.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> ---\n>  drivers/cxl/core/region.c | 131 ++++++++++++++++++++++++++++++++++++--\n>  include/cxl/cxl.h         |   3 +\n>  2 files changed, 127 insertions(+), 7 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index 63c2aeb2ee1f..293e63dfef22 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -2944,6 +2944,14 @@ cxl_find_region_by_name(struct cxl_root_decoder *cxlrd, const char *name)\n>  \treturn to_cxl_region(region_dev);\n>  }\n>  \n> +static void drop_region(struct cxl_region *cxlr)\n> +{\n> +\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n> +\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n> +\n> +\tdevm_release_action(port->uport_dev, __unregister_region, cxlr);\n> +}\n> +\n>  static ssize_t delete_region_store(struct device *dev,\n>  \t\t\t\t   struct device_attribute *attr,\n>  \t\t\t\t   const char *buf, size_t len)\n> @@ -4047,14 +4055,12 @@ static int __construct_region(struct cxl_region *cxlr,\n>  \treturn 0;\n>  }\n>  \n> -/* Establish an empty region covering the given HPA range */\n> -static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n> -\t\t\t\t\t   struct cxl_endpoint_decoder *cxled)\n> +static struct cxl_region *construct_region_begin(struct cxl_root_decoder *cxlrd,\n> +\t\t\t\t\t\t struct cxl_endpoint_decoder *cxled)\n>  {\n>  \tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n> -\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n>  \tstruct cxl_dev_state *cxlds = cxlmd->cxlds;\n> -\tint rc, part = READ_ONCE(cxled->part);\n> +\tint part = READ_ONCE(cxled->part);\n>  \tstruct cxl_region *cxlr;\n>  \n>  \tdo {\n> @@ -4063,13 +4069,26 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n>  \t\t\t\t       cxled->cxld.target_type);\n>  \t} while (IS_ERR(cxlr) && PTR_ERR(cxlr) == -EBUSY);\n>  \n> -\tif (IS_ERR(cxlr)) {\n> +\tif (IS_ERR(cxlr))\n>  \t\tdev_err(cxlmd->dev.parent,\n>  \t\t\t\"%s:%s: %s failed assign region: %ld\\n\",\n>  \t\t\tdev_name(&cxlmd->dev), dev_name(&cxled->cxld.dev),\n>  \t\t\t__func__, PTR_ERR(cxlr));\n> +\n> +\treturn cxlr;\n> +}\n> +\n> +/* Establish an empty region covering the given HPA range */\n> +static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n> +\t\t\t\t\t   struct cxl_endpoint_decoder *cxled)\n> +{\n> +\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n> +\tstruct cxl_region *cxlr;\n> +\tint rc;\n> +\n> +\tcxlr = construct_region_begin(cxlrd, cxled);\n> +\tif (IS_ERR(cxlr))\n>  \t\treturn cxlr;\n> -\t}\n>  \n>  \trc = __construct_region(cxlr, cxlrd, cxled);\n>  \tif (rc) {\n> @@ -4080,6 +4099,104 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n>  \treturn cxlr;\n>  }\n>  \n> +DEFINE_FREE(cxl_region_drop, struct cxl_region *, if (_T) drop_region(_T))\n\nThis needs to be \"if (!IS_ERR_OR_NULL(_T) drop_region(_T)\". If construct_region_begin() returns an\nerror pointer, drop_region() will be called with it as of now leading to a garbage pointer deref.\n\n> +\n> +static struct cxl_region *\n> +__construct_new_region(struct cxl_root_decoder *cxlrd,\n> +\t\t       struct cxl_endpoint_decoder **cxled, int ways)\n> +{\n> +\tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled[0]);\n> +\tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n> +\tstruct cxl_region_params *p;\n> +\tresource_size_t size = 0;\n> +\tint rc, i;\n> +\n> +\tstruct cxl_region *cxlr __free(cxl_region_drop) =\n> +\t\tconstruct_region_begin(cxlrd, cxled[0]);\n> +\tif (IS_ERR(cxlr))\n> +\t\treturn cxlr;\n> +\n> +\tguard(rwsem_write)(&cxl_rwsem.region);\n> +\n> +\t/*\n> +\t * Sanity check. This should not happen with an accel driver handling\n> +\t * the region creation.\n> +\t */\n> +\tp = &cxlr->params;\n> +\tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE) {\n> +\t\tdev_err(cxlmd->dev.parent,\n> +\t\t\t\"%s:%s: %s  unexpected region state\\n\",\n> +\t\t\tdev_name(&cxlmd->dev), dev_name(&cxled[0]->cxld.dev),\n> +\t\t\t__func__);\n> +\t\treturn ERR_PTR(-EBUSY);\n> +\t}\n> +\n> +\trc = set_interleave_ways(cxlr, ways);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\trc = set_interleave_granularity(cxlr, cxld->interleave_granularity);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\tscoped_guard(rwsem_read, &cxl_rwsem.dpa) {\n> +\t\tfor (i = 0; i < ways; i++) {\n> +\t\t\tif (!cxled[i]->dpa_res)\n> +\t\t\t\treturn ERR_PTR(-EINVAL);\n> +\t\t\tsize += resource_size(cxled[i]->dpa_res);\n> +\t\t}\n> +\n> +\t\trc = alloc_hpa(cxlr, size);\n> +\t\tif (rc)\n> +\t\t\treturn ERR_PTR(rc);\n> +\n> +\t\tfor (i = 0; i < ways; i++) {\n> +\t\t\trc = cxl_region_attach(cxlr, cxled[i], 0);\n\nPosition parameter is hardcoded to 0. It should be set to i, right? This kind of goes back to my\nissues in patch 12/22; the interleaving functionality is there but it looks unused.\n\n> +\t\t\tif (rc)\n> +\t\t\t\treturn ERR_PTR(rc);\n> +\t\t}\n> +\t}\n> +\n> +\trc = cxl_region_decode_commit(cxlr);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\tp->state = CXL_CONFIG_COMMIT;\n> +\n> +\treturn no_free_ptr(cxlr);\n> +}\n> +\n> +/**\n> + * cxl_create_region - Establish a region given an endpoint decoder\n> + * @cxlrd: root decoder to allocate HPA\n> + * @cxled: endpoint decoders with reserved DPA capacity\n> + * @ways: interleave ways required\n> + *\n> + * Returns a fully formed region in the commit state and attached to the\n> + * cxl_region driver.\n> + */\n> +struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n> +\t\t\t\t     struct cxl_endpoint_decoder **cxled,\n> +\t\t\t\t     int ways)\n> +{\n> +\tstruct cxl_region *cxlr;\n> +\n> +\tmutex_lock(&cxlrd->range_lock);\n> +\tcxlr = __construct_new_region(cxlrd, cxled, ways);\n> +\tmutex_unlock(&cxlrd->range_lock);\n> +\tif (IS_ERR(cxlr))\n> +\t\treturn cxlr;\n> +\n> +\tif (device_attach(&cxlr->dev) <= 0) {\n> +\t\tdev_err(&cxlr->dev, \"failed to create region\\n\");\n> +\t\tdrop_region(cxlr);\n> +\t\treturn ERR_PTR(-ENODEV);\n> +\t}\n> +\n> +\treturn cxlr;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_create_region, \"CXL\");\n> +\n>  static struct cxl_region *\n>  cxl_find_region_by_range(struct cxl_root_decoder *cxlrd, struct range *hpa)\n>  {\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 4802371db00e..50acbd13bcf8 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -281,4 +281,7 @@ struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n>  \t\t\t\t\t     enum cxl_partition_mode mode,\n>  \t\t\t\t\t     resource_size_t alloc);\n>  int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n> +struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n> +\t\t\t\t     struct cxl_endpoint_decoder **cxled,\n> +\t\t\t\t     int ways);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Differentiate CXL memory expanders (type 3) from CXL device accelerators\n> (type 2) with a new function for initializing cxl_dev_state and a macro\n> for helping accel drivers to embed cxl_dev_state inside a private\n> struct.\n> \n> Move structs to include/cxl as the size of the accel driver private\n> struct embedding cxl_dev_state needs to know the size of this struct.\n> \n> Use same new initialization with the type3 pci driver.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> ---\n>  drivers/cxl/core/mbox.c      |  12 +-\n>  drivers/cxl/core/memdev.c    |  32 +++++\n>  drivers/cxl/cxl.h            |  97 +--------------\n>  drivers/cxl/cxlmem.h         |  86 +------------\n>  drivers/cxl/pci.c            |  14 +--\n>  include/cxl/cxl.h            | 226 +++++++++++++++++++++++++++++++++++\n>  tools/testing/cxl/test/mem.c |   3 +-\n>  7 files changed, 274 insertions(+), 196 deletions(-)\n>  create mode 100644 include/cxl/cxl.h\n> \n> diff --git a/drivers/cxl/core/mbox.c b/drivers/cxl/core/mbox.c\n> index fa6dd0c94656..bee84d0101d1 100644\n> --- a/drivers/cxl/core/mbox.c\n> +++ b/drivers/cxl/core/mbox.c\n> @@ -1514,23 +1514,21 @@ int cxl_mailbox_init(struct cxl_mailbox *cxl_mbox, struct device *host)\n>  }\n>  EXPORT_SYMBOL_NS_GPL(cxl_mailbox_init, \"CXL\");\n>  \n> -struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev)\n> +struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev, u64 serial,\n> +\t\t\t\t\t\t u16 dvsec)\n>  {\n>  \tstruct cxl_memdev_state *mds;\n>  \tint rc;\n>  \n> -\tmds = devm_kzalloc(dev, sizeof(*mds), GFP_KERNEL);\n> +\tmds = devm_cxl_dev_state_create(dev, CXL_DEVTYPE_CLASSMEM, serial,\n> +\t\t\t\t\tdvsec, struct cxl_memdev_state, cxlds,\n> +\t\t\t\t\ttrue);\n>  \tif (!mds) {\n>  \t\tdev_err(dev, \"No memory available\\n\");\n>  \t\treturn ERR_PTR(-ENOMEM);\n>  \t}\n>  \n>  \tmutex_init(&mds->event.log_lock);\n> -\tmds->cxlds.dev = dev;\n> -\tmds->cxlds.reg_map.host = dev;\n> -\tmds->cxlds.cxl_mbox.host = dev;\n> -\tmds->cxlds.reg_map.resource = CXL_RESOURCE_NONE;\n> -\tmds->cxlds.type = CXL_DEVTYPE_CLASSMEM;\n>  \n>  \trc = devm_cxl_register_mce_notifier(dev, &mds->mce_notifier);\n>  \tif (rc == -EOPNOTSUPP)\n> diff --git a/drivers/cxl/core/memdev.c b/drivers/cxl/core/memdev.c\n> index af3d0cc65138..22d156f25305 100644\n> --- a/drivers/cxl/core/memdev.c\n> +++ b/drivers/cxl/core/memdev.c\n> @@ -656,6 +656,38 @@ static void detach_memdev(struct work_struct *work)\n>  \n>  static struct lock_class_key cxl_memdev_key;\n>  \n> +static void cxl_dev_state_init(struct cxl_dev_state *cxlds, struct device *dev,\n> +\t\t\t       enum cxl_devtype type, u64 serial, u16 dvsec,\n> +\t\t\t       bool has_mbox)\n> +{\n> +\t*cxlds = (struct cxl_dev_state) {\n> +\t\t.dev = dev,\n> +\t\t.type = type,\n> +\t\t.serial = serial,\n> +\t\t.cxl_dvsec = dvsec,\n> +\t\t.reg_map.host = dev,\n> +\t\t.reg_map.resource = CXL_RESOURCE_NONE,\n> +\t};\n> +\n> +\tif (has_mbox)\n> +\t\tcxlds->cxl_mbox.host = dev;\n> +}\n> +\n> +struct cxl_dev_state *_devm_cxl_dev_state_create(struct device *dev,\n> +\t\t\t\t\t\t enum cxl_devtype type,\n> +\t\t\t\t\t\t u64 serial, u16 dvsec,\n> +\t\t\t\t\t\t size_t size, bool has_mbox)\n> +{\n> +\tstruct cxl_dev_state *cxlds = devm_kzalloc(dev, size, GFP_KERNEL);\n> +\n> +\tif (!cxlds)\n> +\t\treturn NULL;\n> +\n> +\tcxl_dev_state_init(cxlds, dev, type, serial, dvsec, has_mbox);\n\nNit: Having a second function to do the init seems overkill here, especially since cxl_dev_state_init() isn't called outside this\nfunction. I'd fold it into this function instead, but I'm fine with it either way (especially if you were told otherwise before).\n\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> +\treturn cxlds;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(_devm_cxl_dev_state_create, \"CXL\");\n> +\n>  static struct cxl_memdev *cxl_memdev_alloc(struct cxl_dev_state *cxlds,\n>  \t\t\t\t\t   const struct file_operations *fops,\n>  \t\t\t\t\t   const struct cxl_memdev_attach *attach)\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index e1d47062e1d3..3eaa353e430b 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -12,6 +12,7 @@\n>  #include <linux/node.h>\n>  #include <linux/io.h>\n>  #include <linux/range.h>\n> +#include <cxl/cxl.h>\n>  \n>  extern const struct nvdimm_security_ops *cxl_security_ops;\n>  \n> @@ -201,97 +202,6 @@ static inline int ways_to_eiw(unsigned int ways, u8 *eiw)\n>  #define   CXLDEV_MBOX_BG_CMD_COMMAND_VENDOR_MASK GENMASK_ULL(63, 48)\n>  #define CXLDEV_MBOX_PAYLOAD_OFFSET 0x20\n>  \n> -/*\n> - * Using struct_group() allows for per register-block-type helper routines,\n> - * without requiring block-type agnostic code to include the prefix.\n> - */\n> -struct cxl_regs {\n> -\t/*\n> -\t * Common set of CXL Component register block base pointers\n> -\t * @hdm_decoder: CXL 2.0 8.2.5.12 CXL HDM Decoder Capability Structure\n> -\t * @ras: CXL 2.0 8.2.5.9 CXL RAS Capability Structure\n> -\t */\n> -\tstruct_group_tagged(cxl_component_regs, component,\n> -\t\tvoid __iomem *hdm_decoder;\n> -\t\tvoid __iomem *ras;\n> -\t);\n> -\t/*\n> -\t * Common set of CXL Device register block base pointers\n> -\t * @status: CXL 2.0 8.2.8.3 Device Status Registers\n> -\t * @mbox: CXL 2.0 8.2.8.4 Mailbox Registers\n> -\t * @memdev: CXL 2.0 8.2.8.5 Memory Device Registers\n> -\t */\n> -\tstruct_group_tagged(cxl_device_regs, device_regs,\n> -\t\tvoid __iomem *status, *mbox, *memdev;\n> -\t);\n> -\n> -\tstruct_group_tagged(cxl_pmu_regs, pmu_regs,\n> -\t\tvoid __iomem *pmu;\n> -\t);\n> -\n> -\t/*\n> -\t * RCH downstream port specific RAS register\n> -\t * @aer: CXL 3.0 8.2.1.1 RCH Downstream Port RCRB\n> -\t */\n> -\tstruct_group_tagged(cxl_rch_regs, rch_regs,\n> -\t\tvoid __iomem *dport_aer;\n> -\t);\n> -\n> -\t/*\n> -\t * RCD upstream port specific PCIe cap register\n> -\t * @pcie_cap: CXL 3.0 8.2.1.2 RCD Upstream Port RCRB\n> -\t */\n> -\tstruct_group_tagged(cxl_rcd_regs, rcd_regs,\n> -\t\tvoid __iomem *rcd_pcie_cap;\n> -\t);\n> -};\n> -\n> -struct cxl_reg_map {\n> -\tbool valid;\n> -\tint id;\n> -\tunsigned long offset;\n> -\tunsigned long size;\n> -};\n> -\n> -struct cxl_component_reg_map {\n> -\tstruct cxl_reg_map hdm_decoder;\n> -\tstruct cxl_reg_map ras;\n> -};\n> -\n> -struct cxl_device_reg_map {\n> -\tstruct cxl_reg_map status;\n> -\tstruct cxl_reg_map mbox;\n> -\tstruct cxl_reg_map memdev;\n> -};\n> -\n> -struct cxl_pmu_reg_map {\n> -\tstruct cxl_reg_map pmu;\n> -};\n> -\n> -/**\n> - * struct cxl_register_map - DVSEC harvested register block mapping parameters\n> - * @host: device for devm operations and logging\n> - * @base: virtual base of the register-block-BAR + @block_offset\n> - * @resource: physical resource base of the register block\n> - * @max_size: maximum mapping size to perform register search\n> - * @reg_type: see enum cxl_regloc_type\n> - * @component_map: cxl_reg_map for component registers\n> - * @device_map: cxl_reg_maps for device registers\n> - * @pmu_map: cxl_reg_maps for CXL Performance Monitoring Units\n> - */\n> -struct cxl_register_map {\n> -\tstruct device *host;\n> -\tvoid __iomem *base;\n> -\tresource_size_t resource;\n> -\tresource_size_t max_size;\n> -\tu8 reg_type;\n> -\tunion {\n> -\t\tstruct cxl_component_reg_map component_map;\n> -\t\tstruct cxl_device_reg_map device_map;\n> -\t\tstruct cxl_pmu_reg_map pmu_map;\n> -\t};\n> -};\n> -\n>  void cxl_probe_component_regs(struct device *dev, void __iomem *base,\n>  \t\t\t      struct cxl_component_reg_map *map);\n>  void cxl_probe_device_regs(struct device *dev, void __iomem *base,\n> @@ -497,11 +407,6 @@ struct cxl_region_params {\n>  \tresource_size_t cache_size;\n>  };\n>  \n> -enum cxl_partition_mode {\n> -\tCXL_PARTMODE_RAM,\n> -\tCXL_PARTMODE_PMEM,\n> -};\n> -\n>  /*\n>   * Indicate whether this region has been assembled by autodetection or\n>   * userspace assembly. Prevent endpoint decoders outside of automatic\n> diff --git a/drivers/cxl/cxlmem.h b/drivers/cxl/cxlmem.h\n> index ef202b34e5ea..281546de426e 100644\n> --- a/drivers/cxl/cxlmem.h\n> +++ b/drivers/cxl/cxlmem.h\n> @@ -113,8 +113,6 @@ int devm_cxl_dpa_reserve(struct cxl_endpoint_decoder *cxled,\n>  \t\t\t resource_size_t base, resource_size_t len,\n>  \t\t\t resource_size_t skipped);\n>  \n> -#define CXL_NR_PARTITIONS_MAX 2\n> -\n>  struct cxl_dpa_info {\n>  \tu64 size;\n>  \tstruct cxl_dpa_part_info {\n> @@ -373,87 +371,6 @@ struct cxl_security_state {\n>  \tstruct kernfs_node *sanitize_node;\n>  };\n>  \n> -/*\n> - * enum cxl_devtype - delineate type-2 from a generic type-3 device\n> - * @CXL_DEVTYPE_DEVMEM - Vendor specific CXL Type-2 device implementing HDM-D or\n> - *\t\t\t HDM-DB, no requirement that this device implements a\n> - *\t\t\t mailbox, or other memory-device-standard manageability\n> - *\t\t\t flows.\n> - * @CXL_DEVTYPE_CLASSMEM - Common class definition of a CXL Type-3 device with\n> - *\t\t\t   HDM-H and class-mandatory memory device registers\n> - */\n> -enum cxl_devtype {\n> -\tCXL_DEVTYPE_DEVMEM,\n> -\tCXL_DEVTYPE_CLASSMEM,\n> -};\n> -\n> -/**\n> - * struct cxl_dpa_perf - DPA performance property entry\n> - * @dpa_range: range for DPA address\n> - * @coord: QoS performance data (i.e. latency, bandwidth)\n> - * @cdat_coord: raw QoS performance data from CDAT\n> - * @qos_class: QoS Class cookies\n> - */\n> -struct cxl_dpa_perf {\n> -\tstruct range dpa_range;\n> -\tstruct access_coordinate coord[ACCESS_COORDINATE_MAX];\n> -\tstruct access_coordinate cdat_coord[ACCESS_COORDINATE_MAX];\n> -\tint qos_class;\n> -};\n> -\n> -/**\n> - * struct cxl_dpa_partition - DPA partition descriptor\n> - * @res: shortcut to the partition in the DPA resource tree (cxlds->dpa_res)\n> - * @perf: performance attributes of the partition from CDAT\n> - * @mode: operation mode for the DPA capacity, e.g. ram, pmem, dynamic...\n> - */\n> -struct cxl_dpa_partition {\n> -\tstruct resource res;\n> -\tstruct cxl_dpa_perf perf;\n> -\tenum cxl_partition_mode mode;\n> -};\n> -\n> -/**\n> - * struct cxl_dev_state - The driver device state\n> - *\n> - * cxl_dev_state represents the CXL driver/device state.  It provides an\n> - * interface to mailbox commands as well as some cached data about the device.\n> - * Currently only memory devices are represented.\n> - *\n> - * @dev: The device associated with this CXL state\n> - * @cxlmd: The device representing the CXL.mem capabilities of @dev\n> - * @reg_map: component and ras register mapping parameters\n> - * @regs: Parsed register blocks\n> - * @cxl_dvsec: Offset to the PCIe device DVSEC\n> - * @rcd: operating in RCD mode (CXL 3.0 9.11.8 CXL Devices Attached to an RCH)\n> - * @media_ready: Indicate whether the device media is usable\n> - * @dpa_res: Overall DPA resource tree for the device\n> - * @part: DPA partition array\n> - * @nr_partitions: Number of DPA partitions\n> - * @serial: PCIe Device Serial Number\n> - * @type: Generic Memory Class device or Vendor Specific Memory device\n> - * @cxl_mbox: CXL mailbox context\n> - * @cxlfs: CXL features context\n> - */\n> -struct cxl_dev_state {\n> -\tstruct device *dev;\n> -\tstruct cxl_memdev *cxlmd;\n> -\tstruct cxl_register_map reg_map;\n> -\tstruct cxl_regs regs;\n> -\tint cxl_dvsec;\n> -\tbool rcd;\n> -\tbool media_ready;\n> -\tstruct resource dpa_res;\n> -\tstruct cxl_dpa_partition part[CXL_NR_PARTITIONS_MAX];\n> -\tunsigned int nr_partitions;\n> -\tu64 serial;\n> -\tenum cxl_devtype type;\n> -\tstruct cxl_mailbox cxl_mbox;\n> -#ifdef CONFIG_CXL_FEATURES\n> -\tstruct cxl_features_state *cxlfs;\n> -#endif\n> -};\n> -\n>  static inline resource_size_t cxl_pmem_size(struct cxl_dev_state *cxlds)\n>  {\n>  \t/*\n> @@ -858,7 +775,8 @@ int cxl_dev_state_identify(struct cxl_memdev_state *mds);\n>  int cxl_await_media_ready(struct cxl_dev_state *cxlds);\n>  int cxl_enumerate_cmds(struct cxl_memdev_state *mds);\n>  int cxl_mem_dpa_fetch(struct cxl_memdev_state *mds, struct cxl_dpa_info *info);\n> -struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev);\n> +struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev, u64 serial,\n> +\t\t\t\t\t\t u16 dvsec);\n>  void set_exclusive_cxl_commands(struct cxl_memdev_state *mds,\n>  \t\t\t\tunsigned long *cmds);\n>  void clear_exclusive_cxl_commands(struct cxl_memdev_state *mds,\n> diff --git a/drivers/cxl/pci.c b/drivers/cxl/pci.c\n> index 1cf232220873..24179cc702bf 100644\n> --- a/drivers/cxl/pci.c\n> +++ b/drivers/cxl/pci.c\n> @@ -911,25 +911,25 @@ static int cxl_pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n>  \tint rc, pmu_count;\n>  \tunsigned int i;\n>  \tbool irq_avail;\n> +\tu16 dvsec;\n>  \n>  \trc = pcim_enable_device(pdev);\n>  \tif (rc)\n>  \t\treturn rc;\n>  \tpci_set_master(pdev);\n>  \n> -\tmds = cxl_memdev_state_create(&pdev->dev);\n> +\tdvsec = pci_find_dvsec_capability(pdev, PCI_VENDOR_ID_CXL,\n> +\t\t\t\t\t  PCI_DVSEC_CXL_DEVICE);\n> +\tif (!dvsec)\n> +\t\tpci_warn(pdev, \"Device DVSEC not present, skip CXL.mem init\\n\");\n> +\n> +\tmds = cxl_memdev_state_create(&pdev->dev, pci_get_dsn(pdev), dvsec);\n>  \tif (IS_ERR(mds))\n>  \t\treturn PTR_ERR(mds);\n>  \tcxlds = &mds->cxlds;\n>  \tpci_set_drvdata(pdev, cxlds);\n>  \n>  \tcxlds->rcd = is_cxl_restricted(pdev);\n> -\tcxlds->serial = pci_get_dsn(pdev);\n> -\tcxlds->cxl_dvsec = pci_find_dvsec_capability(\n> -\t\tpdev, PCI_VENDOR_ID_CXL, PCI_DVSEC_CXL_DEVICE);\n> -\tif (!cxlds->cxl_dvsec)\n> -\t\tdev_warn(&pdev->dev,\n> -\t\t\t \"Device DVSEC not present, skip CXL.mem init\\n\");\n>  \n>  \trc = cxl_pci_setup_regs(pdev, CXL_REGLOC_RBI_MEMDEV, &map);\n>  \tif (rc)\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> new file mode 100644\n> index 000000000000..13d448686189\n> --- /dev/null\n> +++ b/include/cxl/cxl.h\n> @@ -0,0 +1,226 @@\n> +/* SPDX-License-Identifier: GPL-2.0 */\n> +/* Copyright(c) 2020 Intel Corporation. */\n> +/* Copyright(c) 2025 Advanced Micro Devices, Inc. */\n> +\n> +#ifndef __CXL_CXL_H__\n> +#define __CXL_CXL_H__\n> +\n> +#include <linux/node.h>\n> +#include <linux/ioport.h>\n> +#include <cxl/mailbox.h>\n> +\n> +/**\n> + * enum cxl_devtype - delineate type-2 from a generic type-3 device\n> + * @CXL_DEVTYPE_DEVMEM: Vendor specific CXL Type-2 device implementing HDM-D or\n> + *\t\t\t HDM-DB, no requirement that this device implements a\n> + *\t\t\t mailbox, or other memory-device-standard manageability\n> + *\t\t\t flows.\n> + * @CXL_DEVTYPE_CLASSMEM: Common class definition of a CXL Type-3 device with\n> + *\t\t\t   HDM-H and class-mandatory memory device registers\n> + */\n> +enum cxl_devtype {\n> +\tCXL_DEVTYPE_DEVMEM,\n> +\tCXL_DEVTYPE_CLASSMEM,\n> +};\n> +\n> +struct device;\n> +\n> +/*\n> + * Using struct_group() allows for per register-block-type helper routines,\n> + * without requiring block-type agnostic code to include the prefix.\n> + */\n> +struct cxl_regs {\n> +\t/*\n> +\t * Common set of CXL Component register block base pointers\n> +\t * @hdm_decoder: CXL 2.0 8.2.5.12 CXL HDM Decoder Capability Structure\n> +\t * @ras: CXL 2.0 8.2.5.9 CXL RAS Capability Structure\n> +\t */\n> +\tstruct_group_tagged(cxl_component_regs, component,\n> +\t\tvoid __iomem *hdm_decoder;\n> +\t\tvoid __iomem *ras;\n> +\t);\n> +\t/*\n> +\t * Common set of CXL Device register block base pointers\n> +\t * @status: CXL 2.0 8.2.8.3 Device Status Registers\n> +\t * @mbox: CXL 2.0 8.2.8.4 Mailbox Registers\n> +\t * @memdev: CXL 2.0 8.2.8.5 Memory Device Registers\n> +\t */\n> +\tstruct_group_tagged(cxl_device_regs, device_regs,\n> +\t\tvoid __iomem *status, *mbox, *memdev;\n> +\t);\n> +\n> +\tstruct_group_tagged(cxl_pmu_regs, pmu_regs,\n> +\t\tvoid __iomem *pmu;\n> +\t);\n> +\n> +\t/*\n> +\t * RCH downstream port specific RAS register\n> +\t * @aer: CXL 3.0 8.2.1.1 RCH Downstream Port RCRB\n> +\t */\n> +\tstruct_group_tagged(cxl_rch_regs, rch_regs,\n> +\t\tvoid __iomem *dport_aer;\n> +\t);\n> +\n> +\t/*\n> +\t * RCD upstream port specific PCIe cap register\n> +\t * @pcie_cap: CXL 3.0 8.2.1.2 RCD Upstream Port RCRB\n> +\t */\n> +\tstruct_group_tagged(cxl_rcd_regs, rcd_regs,\n> +\t\tvoid __iomem *rcd_pcie_cap;\n> +\t);\n> +};\n> +\n> +struct cxl_reg_map {\n> +\tbool valid;\n> +\tint id;\n> +\tunsigned long offset;\n> +\tunsigned long size;\n> +};\n> +\n> +struct cxl_component_reg_map {\n> +\tstruct cxl_reg_map hdm_decoder;\n> +\tstruct cxl_reg_map ras;\n> +};\n> +\n> +struct cxl_device_reg_map {\n> +\tstruct cxl_reg_map status;\n> +\tstruct cxl_reg_map mbox;\n> +\tstruct cxl_reg_map memdev;\n> +};\n> +\n> +struct cxl_pmu_reg_map {\n> +\tstruct cxl_reg_map pmu;\n> +};\n> +\n> +/**\n> + * struct cxl_register_map - DVSEC harvested register block mapping parameters\n> + * @host: device for devm operations and logging\n> + * @base: virtual base of the register-block-BAR + @block_offset\n> + * @resource: physical resource base of the register block\n> + * @max_size: maximum mapping size to perform register search\n> + * @reg_type: see enum cxl_regloc_type\n> + * @component_map: cxl_reg_map for component registers\n> + * @device_map: cxl_reg_maps for device registers\n> + * @pmu_map: cxl_reg_maps for CXL Performance Monitoring Units\n> + */\n> +struct cxl_register_map {\n> +\tstruct device *host;\n> +\tvoid __iomem *base;\n> +\tresource_size_t resource;\n> +\tresource_size_t max_size;\n> +\tu8 reg_type;\n> +\tunion {\n> +\t\tstruct cxl_component_reg_map component_map;\n> +\t\tstruct cxl_device_reg_map device_map;\n> +\t\tstruct cxl_pmu_reg_map pmu_map;\n> +\t};\n> +};\n> +\n> +/**\n> + * struct cxl_dpa_perf - DPA performance property entry\n> + * @dpa_range: range for DPA address\n> + * @coord: QoS performance data (i.e. latency, bandwidth)\n> + * @cdat_coord: raw QoS performance data from CDAT\n> + * @qos_class: QoS Class cookies\n> + */\n> +struct cxl_dpa_perf {\n> +\tstruct range dpa_range;\n> +\tstruct access_coordinate coord[ACCESS_COORDINATE_MAX];\n> +\tstruct access_coordinate cdat_coord[ACCESS_COORDINATE_MAX];\n> +\tint qos_class;\n> +};\n> +\n> +enum cxl_partition_mode {\n> +\tCXL_PARTMODE_RAM,\n> +\tCXL_PARTMODE_PMEM,\n> +};\n> +\n> +/**\n> + * struct cxl_dpa_partition - DPA partition descriptor\n> + * @res: shortcut to the partition in the DPA resource tree (cxlds->dpa_res)\n> + * @perf: performance attributes of the partition from CDAT\n> + * @mode: operation mode for the DPA capacity, e.g. ram, pmem, dynamic...\n> + */\n> +struct cxl_dpa_partition {\n> +\tstruct resource res;\n> +\tstruct cxl_dpa_perf perf;\n> +\tenum cxl_partition_mode mode;\n> +};\n> +\n> +#define CXL_NR_PARTITIONS_MAX 2\n> +\n> +/**\n> + * struct cxl_dev_state - The driver device state\n> + *\n> + * cxl_dev_state represents the CXL driver/device state.  It provides an\n> + * interface to mailbox commands as well as some cached data about the device.\n> + * Currently only memory devices are represented.\n> + *\n> + * @dev: The device associated with this CXL state\n> + * @cxlmd: The device representing the CXL.mem capabilities of @dev\n> + * @reg_map: component and ras register mapping parameters\n> + * @regs: Parsed register blocks\n> + * @cxl_dvsec: Offset to the PCIe device DVSEC\n> + * @rcd: operating in RCD mode (CXL 3.0 9.11.8 CXL Devices Attached to an RCH)\n> + * @media_ready: Indicate whether the device media is usable\n> + * @dpa_res: Overall DPA resource tree for the device\n> + * @part: DPA partition array\n> + * @nr_partitions: Number of DPA partitions\n> + * @serial: PCIe Device Serial Number\n> + * @type: Generic Memory Class device or Vendor Specific Memory device\n> + * @cxl_mbox: CXL mailbox context\n> + * @cxlfs: CXL features context\n> + */\n> +struct cxl_dev_state {\n> +\t/* public for Type2 drivers */\n> +\tstruct device *dev;\n> +\tstruct cxl_memdev *cxlmd;\n> +\n> +\t/* private for Type2 drivers */\n> +\tstruct cxl_register_map reg_map;\n> +\tstruct cxl_regs regs;\n> +\tint cxl_dvsec;\n> +\tbool rcd;\n> +\tbool media_ready;\n> +\tstruct resource dpa_res;\n> +\tstruct cxl_dpa_partition part[CXL_NR_PARTITIONS_MAX];\n> +\tunsigned int nr_partitions;\n> +\tu64 serial;\n> +\tenum cxl_devtype type;\n> +\tstruct cxl_mailbox cxl_mbox;\n> +#ifdef CONFIG_CXL_FEATURES\n> +\tstruct cxl_features_state *cxlfs;\n> +#endif\n> +};\n> +\n> +struct cxl_dev_state *_devm_cxl_dev_state_create(struct device *dev,\n> +\t\t\t\t\t\t enum cxl_devtype type,\n> +\t\t\t\t\t\t u64 serial, u16 dvsec,\n> +\t\t\t\t\t\t size_t size, bool has_mbox);\n> +\n> +/**\n> + * cxl_dev_state_create - safely create and cast a cxl dev state embedded in a\n> + * driver specific struct.\n> + *\n> + * @parent: device behind the request\n> + * @type: CXL device type\n> + * @serial: device identification\n> + * @dvsec: dvsec capability offset\n> + * @drv_struct: driver struct embedding a cxl_dev_state struct\n> + * @member: drv_struct member as cxl_dev_state\n> + * @mbox: true if mailbox supported\n> + *\n> + * Returns a pointer to the drv_struct allocated and embedding a cxl_dev_state\n> + * struct initialized.\n> + *\n> + * Introduced for Type2 driver support.\n> + */\n> +#define devm_cxl_dev_state_create(parent, type, serial, dvsec, drv_struct, member, mbox)\t\\\n> +\t({\t\t\t\t\t\t\t\t\t\t\\\n> +\t\tstatic_assert(__same_type(struct cxl_dev_state,\t\t\t\t\\\n> +\t\t\t      ((drv_struct *)NULL)->member));\t\t\t\t\\\n> +\t\tstatic_assert(offsetof(drv_struct, member) == 0);\t\t\t\\\n> +\t\t(drv_struct *)_devm_cxl_dev_state_create(parent, type, serial, dvsec,\t\\\n> +\t\t\t\t\t\t      sizeof(drv_struct), mbox);\t\\\n> +\t})\n> +#endif /* __CXL_CXL_H__ */\n> diff --git a/tools/testing/cxl/test/mem.c b/tools/testing/cxl/test/mem.c\n> index cb87e8c0e63c..79f42f4474d4 100644\n> --- a/tools/testing/cxl/test/mem.c\n> +++ b/tools/testing/cxl/test/mem.c\n> @@ -1716,7 +1716,7 @@ static int cxl_mock_mem_probe(struct platform_device *pdev)\n>  \tif (rc)\n>  \t\treturn rc;\n>  \n> -\tmds = cxl_memdev_state_create(dev);\n> +\tmds = cxl_memdev_state_create(dev, pdev->id + 1, 0);\n>  \tif (IS_ERR(mds))\n>  \t\treturn PTR_ERR(mds);\n>  \n> @@ -1732,7 +1732,6 @@ static int cxl_mock_mem_probe(struct platform_device *pdev)\n>  \tmds->event.buf = (struct cxl_get_event_payload *) mdata->event_buf;\n>  \tINIT_DELAYED_WORK(&mds->security.poll_dwork, cxl_mockmem_sanitize_work);\n>  \n> -\tcxlds->serial = pdev->id + 1;\n>  \tif (is_rcd(pdev))\n>  \t\tcxlds->rcd = true;\n>  \n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> A Type2 device configured by the BIOS can already have its HDM\n> committed. Add a cxl_get_committed_decoder() function for cheking\n> so after memdev creation. A CXL region should have been created\n> during memdev initialization, therefore a Type2 driver can ask for\n> such a region for working with the HPA. If the HDM is not committed,\n> a Type2 driver will create the region after obtaining proper HPA\n> and DPA space.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> ---\n>  drivers/cxl/core/hdm.c | 39 +++++++++++++++++++++++++++++++++++++++\n>  include/cxl/cxl.h      |  3 +++\n>  2 files changed, 42 insertions(+)\n> \n> diff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\n> index 6e516c69b2d2..a172ce4e9b19 100644\n> --- a/drivers/cxl/core/hdm.c\n> +++ b/drivers/cxl/core/hdm.c\n> @@ -686,6 +686,45 @@ int cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size)\n>  \treturn devm_add_action_or_reset(&port->dev, cxl_dpa_release, cxled);\n>  }\n>  \n> +static int find_committed_endpoint_decoder(struct device *dev, const void *data)\n> +{\n> +\tstruct cxl_endpoint_decoder *cxled;\n> +\tstruct cxl_port *port;\n> +\n> +\tif (!is_endpoint_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxled = to_cxl_endpoint_decoder(dev);\n> +\tport = cxled_to_port(cxled);\n> +\n> +\treturn cxled->cxld.id == port->hdm_end;\n\nIs this the way you're supposed to check if a decoder is committed? The doc comment for @hdm_end in\nstruct cxl_port says it's just the last allocated decoder. If allocated decoders are always committed then\nI'm fine with this, otherwise I think you'd want to a register read or something to find the commit state.\n> +}\n> +\n> +struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t\t       struct cxl_region **cxlr)\n> +{\n> +\tstruct cxl_port *endpoint = cxlmd->endpoint;\n> +\tstruct cxl_endpoint_decoder *cxled;\n> +\tstruct device *cxled_dev;\n> +\n> +\tif (!endpoint)\n> +\t\treturn NULL;\n> +\n> +\tguard(rwsem_read)(&cxl_rwsem.dpa);\n> +\tcxled_dev = device_find_child(&endpoint->dev, NULL,\n> +\t\t\t\t      find_committed_endpoint_decoder);\n> +\n> +\tif (!cxled_dev)\n> +\t\treturn NULL;\n> +\n> +\tcxled = to_cxl_endpoint_decoder(cxled_dev);\n> +\t*cxlr = cxled->cxld.region;\n> +\n> +\tput_device(cxled_dev);\n> +\treturn cxled;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_get_committed_decoder, \"CXL\");\n> +\n>  static void cxld_set_interleave(struct cxl_decoder *cxld, u32 *ctrl)\n>  {\n>  \tu16 eig;\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 6f8d365067af..928276dba952 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -249,4 +249,7 @@ int cxl_map_component_regs(const struct cxl_register_map *map,\n>  int cxl_set_capacity(struct cxl_dev_state *cxlds, u64 capacity);\n>  struct cxl_memdev *devm_cxl_add_memdev(struct cxl_dev_state *cxlds,\n>  \t\t\t\t       const struct cxl_memdev_attach *attach);\n> +struct cxl_region;\n> +struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t\t       struct cxl_region **cxlr);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> This patchset should be applied on the cxl next branch using the base\n> specified at the end of this cover letter.\n> \n> Dependencies on Dan's work has gone and also on Terry's as the only\n> patch required is now in next. The other dependency is on Smita patchset\n> but it does not exist such a dependency as that work will not avoid the\n> problem with Type2 and DAX/hmem if soft reserved memory. This needs to\n> be solved by the BIOS and Type2 UEFI driver for populating the CXL.mem\n> range as EFI_RESERVED_TYPE instead of default EFI_CONVENTIONAL_MEMORY\n> with the EFI_MEMORY_SP attribute. There exists though a dependency on\n> one Smita's patches:\n> \n> [PATCH v5 3/7] cxl/region: Skip decoder reset on detach for autodiscovered regions\n> \n> This is needed for the default behaviour with current BIOS configuration\n> where the HDM Type2 decoders will be kept unreset when driver unloads.\n> This is the main change introduced in v23: committed decoders will not\n> be reset. Previous v22 functionality supported first driver load finding\n> committed decoders but resetting them at unload and supporting\n> uncommitted decoders in next driver loads. This will be suported in\n> follow-up works.\n> \n> v23 changes:\n> \n>   patch 11: fixing minor issues and droping change in\n> \t    should_emulate_decoders (Jonathan Cameron)\n> \n>   patch13: refactoring unregister_region for safety type in Type2 API\n> \n>   sfc changes: slight modifications to error path\n> \n\nThis cover letter is really long, I'd remove the change logs for anything more\nthan 3 revisions back (assuming a v24 is needed). After that you could leave\na lore link for older revisions if you want, but it's not needed imo.\nAlso, feel free to add my Reviewed-by for anything I didn't leave a comment on\n(felt I should cut down on the mail).\n\nThanks,\nBen\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Region creation involves finding available DPA (device-physical-address)\n> capacity to map into HPA (host-physical-address) space.\n> \n> In order to support CXL Type2 devices, define an API, cxl_request_dpa(),\n> that tries to allocate the DPA memory the driver requires to operate.The\n> memory requested should not be bigger than the max available HPA obtained\n> previously with cxl_get_hpa_freespace().\n> \n> Based on https://lore.kernel.org/linux-cxl/168592158743.1948938.7622563891193802610.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> ---\n>  drivers/cxl/core/hdm.c | 84 ++++++++++++++++++++++++++++++++++++++++++\n>  drivers/cxl/cxl.h      |  1 +\n>  include/cxl/cxl.h      |  5 +++\n>  3 files changed, 90 insertions(+)\n> \n> diff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\n> index a172ce4e9b19..d60a697f12cc 100644\n> --- a/drivers/cxl/core/hdm.c\n> +++ b/drivers/cxl/core/hdm.c\n> @@ -3,6 +3,7 @@\n>  #include <linux/seq_file.h>\n>  #include <linux/device.h>\n>  #include <linux/delay.h>\n> +#include <cxl/cxl.h>\n>  \n>  #include \"cxlmem.h\"\n>  #include \"core.h\"\n> @@ -546,6 +547,12 @@ bool cxl_resource_contains_addr(const struct resource *res, const resource_size_\n>  \treturn resource_contains(res, &_addr);\n>  }\n>  \n> +/**\n> + * cxl_dpa_free - release DPA (Device Physical Address)\n> + * @cxled: endpoint decoder linked to the DPA\n> + *\n> + * Returns 0 or error.\n> + */\n>  int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n>  {\n>  \tstruct cxl_port *port = cxled_to_port(cxled);\n> @@ -572,6 +579,7 @@ int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n>  \tdevm_cxl_dpa_release(cxled);\n>  \treturn 0;\n>  }\n> +EXPORT_SYMBOL_NS_GPL(cxl_dpa_free, \"CXL\");\n>  \n>  int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n>  \t\t     enum cxl_partition_mode mode)\n> @@ -603,6 +611,82 @@ int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n>  \treturn 0;\n>  }\n>  \n> +static int find_free_decoder(struct device *dev, const void *data)\n> +{\n> +\tstruct cxl_endpoint_decoder *cxled;\n> +\tstruct cxl_port *port;\n> +\n> +\tif (!is_endpoint_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxled = to_cxl_endpoint_decoder(dev);\n> +\tport = cxled_to_port(cxled);\n> +\n> +\treturn cxled->cxld.id == (port->hdm_end + 1);\n> +}\n> +\n> +static struct cxl_endpoint_decoder *\n> +cxl_find_free_decoder(struct cxl_memdev *cxlmd)\n> +{\n> +\tstruct cxl_port *endpoint = cxlmd->endpoint;\n> +\tstruct device *dev;\n> +\n> +\tguard(rwsem_read)(&cxl_rwsem.dpa);\n> +\tdev = device_find_child(&endpoint->dev, NULL,\n> +\t\t\t\tfind_free_decoder);\n> +\tif (!dev)\n> +\t\treturn NULL;\n> +\n> +\treturn to_cxl_endpoint_decoder(dev);\n> +}\n> +\n> +/**\n> + * cxl_request_dpa - search and reserve DPA given input constraints\n> + * @cxlmd: memdev with an endpoint port with available decoders\n> + * @mode: CXL partition mode (ram vs pmem)\n> + * @alloc: dpa size required\n> + *\n> + * Returns a pointer to a 'struct cxl_endpoint_decoder' on success or\n> + * an errno encoded pointer on failure.\n> + *\n> + * Given that a region needs to allocate from limited HPA capacity it\n> + * may be the case that a device has more mappable DPA capacity than\n> + * available HPA. The expectation is that @alloc is a driver known\n> + * value based on the device capacity but which could not be fully\n> + * available due to HPA constraints.\n> + *\n> + * Returns a pinned cxl_decoder with at least @alloc bytes of capacity\n> + * reserved, or an error pointer. The caller is also expected to own the\n> + * lifetime of the memdev registration associated with the endpoint to\n> + * pin the decoder registered as well.\n> + */\n> +struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t     enum cxl_partition_mode mode,\n> +\t\t\t\t\t     resource_size_t alloc)\n> +{\n> +\tint rc;\n> +\n> +\tif (!IS_ALIGNED(alloc, SZ_256M))\n> +\t\treturn ERR_PTR(-EINVAL);\n> +\n> +\tstruct cxl_endpoint_decoder *cxled __free(put_cxled) =\n> +\t\tcxl_find_free_decoder(cxlmd);\n> +\n> +\tif (!cxled)\n> +\t\treturn ERR_PTR(-ENODEV);\n> +\n> +\trc = cxl_dpa_set_part(cxled, mode);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\trc = cxl_dpa_alloc(cxled, alloc);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n\nShould cxl_dpa_set_part() be unwound here, or does it not matter? If it doesn't matter:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> +\n> +\treturn no_free_ptr(cxled);\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_request_dpa, \"CXL\");\n> +\n>  static int __cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size)\n>  {\n>  \tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index d1b010e5e1d0..2b1f7d687a0e 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -667,6 +667,7 @@ struct cxl_root *find_cxl_root(struct cxl_port *port);\n>  \n>  DEFINE_FREE(put_cxl_root, struct cxl_root *, if (_T) put_device(&_T->port.dev))\n>  DEFINE_FREE(put_cxl_port, struct cxl_port *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n> +DEFINE_FREE(put_cxled, struct cxl_endpoint_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxld.dev))\n>  DEFINE_FREE(put_cxl_root_decoder, struct cxl_root_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxlsd.cxld.dev))\n>  DEFINE_FREE(put_cxl_region, struct cxl_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n>  \n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 783ad570a6eb..4802371db00e 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -7,6 +7,7 @@\n>  \n>  #include <linux/node.h>\n>  #include <linux/ioport.h>\n> +#include <linux/range.h>\n>  #include <cxl/mailbox.h>\n>  \n>  /**\n> @@ -276,4 +277,8 @@ struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n>  \t\t\t\t\t       unsigned long flags,\n>  \t\t\t\t\t       resource_size_t *max);\n>  void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n> +struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t     enum cxl_partition_mode mode,\n> +\t\t\t\t\t     resource_size_t alloc);\n> +int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\n\n\nOn 2/19/2026 4:40 AM, Alejandro Lucero Palau wrote:\n> \n> On 2/11/26 22:11, Cheatham, Benjamin wrote:\n>> On 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n>>> From: Alejandro Lucero <alucerop@amd.com>\n>>>\n>>> Region creation based on Type3 devices is triggered from user space\n>>> allowing memory combination through interleaving.\n>>>\n>>> In preparation for kernel driven region creation, that is Type2 drivers\n>>> triggering region creation backed with its advertised CXL memory, factor\n>>> out a common helper from the user-sysfs region setup for interleave ways.\n>>>\n>>> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n>>> Reviewed-by: Zhi Wang <zhiw@nvidia.com>\n>>> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n>>> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n>>> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n>>> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n>>> ---\n>>>  drivers/cxl/core/region.c | 43 ++++++++++++++++++++++++---------------\n>>>  1 file changed, 27 insertions(+), 16 deletions(-)\n>>>\n>>> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n>>> index f53b2e9fd9e6..ece1d3df7cf1 100644\n>>> --- a/drivers/cxl/core/region.c\n>>> +++ b/drivers/cxl/core/region.c\n>>> @@ -485,22 +485,14 @@ static ssize_t interleave_ways_show(struct device *dev,\n>>>   static const struct attribute_group *get_cxl_region_target_group(void);\n>>>  -static ssize_t interleave_ways_store(struct device *dev,\n>>> - struct device_attribute *attr,\n>>> - const char *buf, size_t len)\n>>> +static int set_interleave_ways(struct cxl_region *cxlr, int val)\n>> @val should probably stay an unsigned int. You pass an unsigned int in the sysfs function, and the\n>> function was originally coded with that in mind (same with @save below).\n> \n> Good catch. I wonder if I should just change the way the value is obtained, using kstrtoint instead of kstrtouint, as those values are used for cxl_region_params fields defined as int. In other words, it seems doing that simpler than changing all the other places you mention and the structs involved. I can not see a reason for using unsigned int so I think I will follow that approach. Tell me if you think otherwise.\n> \n\nIf I had to guess unsigned int was used because a negative interleave granularity/ways makes no sense. I think your suggestion is fine though since no one\nin their right mind would give anything but a (relatively) small and positive value for these.\n\nThanks,\nBen\n\n> \n> Thank you\n> \n> \n>> With that cleaned up:\n>> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n>>\n>>>  {\n>>> - struct cxl_root_decoder *cxlrd = to_cxl_root_decoder(dev->parent);\n>>> + struct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n>>>  struct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n>>> - struct cxl_region *cxlr = to_cxl_region(dev);\n>>>  struct cxl_region_params *p = &cxlr->params;\n>>> - unsigned int val, save;\n>>> - int rc;\n>>> + int save, rc;\n>>>  u8 iw;\n>>>  - rc = kstrtouint(buf, 0, &val);\n>>> - if (rc)\n>>> - return rc;\n>>> -\n>>>  rc = ways_to_eiw(val, &iw);\n>>>  if (rc)\n>>>  return rc;\n>>> @@ -515,9 +507,7 @@ static ssize_t interleave_ways_store(struct device *dev,\n>>>  return -EINVAL;\n>>>  }\n>>>  - ACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n>>> - if ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n>>> - return rc;\n>>> + lockdep_assert_held_write(&cxl_rwsem.region);\n>>>   if (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n>>>  return -EBUSY;\n>>> @@ -525,10 +515,31 @@ static ssize_t interleave_ways_store(struct device *dev,\n>>>  save = p->interleave_ways;\n>>>  p->interleave_ways = val;\n>>>  rc = sysfs_update_group(&cxlr->dev.kobj, get_cxl_region_target_group());\n>>> - if (rc) {\n>>> + if (rc)\n>>>  p->interleave_ways = save;\n>>> +\n>>> + return rc;\n>>> +}\n>>> +\n>>> +static ssize_t interleave_ways_store(struct device *dev,\n>>> + struct device_attribute *attr,\n>>> + const char *buf, size_t len)\n>>> +{\n>>> + struct cxl_region *cxlr = to_cxl_region(dev);\n>>> + unsigned int val;\n>>> + int rc;\n>>> +\n>>> + rc = kstrtouint(buf, 0, &val);\n>>> + if (rc)\n>>> + return rc;\n>>> +\n>>> + ACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n>>> + if ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n>>> + return rc;\n>>> +\n>>> + rc = set_interleave_ways(cxlr, val);\n>>> + if (rc)\n>>>  return rc;\n>>> - }\n>>>   return len;\n>>>  }\n\n\n\n---\n\nOn 2/19/2026 3:58 AM, Alejandro Lucero Palau wrote:\n> \n> On 2/11/26 22:10, Cheatham, Benjamin wrote:\n>> On 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n>>> From: Alejandro Lucero <alucerop@amd.com>\n>>>\n>>> CXL region creation involves allocating capacity from Device Physical\n>>> Address (DPA) and assigning it to decode a given Host Physical Address\n>>> (HPA). Before determining how much DPA to allocate the amount of available\n>>> HPA must be determined. Also, not all HPA is created equal, some HPA\n>>> targets RAM, some targets PMEM, some is prepared for device-memory flows\n>>> like HDM-D and HDM-DB, and some is HDM-H (host-only).\n>>>\n>>> In order to support Type2 CXL devices, wrap all of those concerns into\n>>> an API that retrieves a root decoder (platform CXL window) that fits the\n>>> specified constraints and the capacity available for a new region.\n>>>\n>>> Add a complementary function for releasing the reference to such root\n>>> decoder.\n>>>\n>>> Based on https://lore.kernel.org/linux-cxl/168592159290.1948938.13522227102445462976.stgit@dwillia2-xfh.jf.intel.com/\n>>>\n>>> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n>>> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n>>> ---\n>>>  drivers/cxl/core/region.c | 164 ++++++++++++++++++++++++++++++++++++++\n>>>  drivers/cxl/cxl.h | 3 +\n>>>  include/cxl/cxl.h | 6 ++\n>>>  3 files changed, 173 insertions(+)\n>>>\n>>> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n>>> index 954b8fcdbac6..bdefd088f5f1 100644\n>>> --- a/drivers/cxl/core/region.c\n>>> +++ b/drivers/cxl/core/region.c\n>>> @@ -705,6 +705,170 @@ static int free_hpa(struct cxl_region *cxlr)\n>>>  return 0;\n>>>  }\n>>>  +struct cxlrd_max_context {\n>>> + struct device * const *host_bridges;\n>>> + int interleave_ways;\n>>> + unsigned long flags;\n>>> + resource_size_t max_hpa;\n>>> + struct cxl_root_decoder *cxlrd;\n>>> +};\n>>> +\n>>> +static int find_max_hpa(struct device *dev, void *data)\n>>> +{\n>>> + struct cxlrd_max_context *ctx = data;\n>>> + struct cxl_switch_decoder *cxlsd;\n>>> + struct cxl_root_decoder *cxlrd;\n>>> + struct resource *res, *prev;\n>>> + struct cxl_decoder *cxld;\n>>> + resource_size_t free = 0;\n>>> + resource_size_t max;\n>>> + int found = 0;\n>>> +\n>>> + if (!is_root_decoder(dev))\n>>> + return 0;\n>>> +\n>>> + cxlrd = to_cxl_root_decoder(dev);\n>>> + cxlsd = &cxlrd->cxlsd;\n>>> + cxld = &cxlsd->cxld;\n>>> +\n>>> + if ((cxld->flags & ctx->flags) != ctx->flags) {\n>>> + dev_dbg(dev, \"flags not matching: %08lx vs %08lx\\n\",\n>>> + cxld->flags, ctx->flags);\n>>> + return 0;\n>>> + }\n>>> +\n>>> + for (int i = 0; i < ctx->interleave_ways; i++) {\n>>> + for (int j = 0; j < ctx->interleave_ways; j++) {\n>>> + if (ctx->host_bridges[i] == cxlsd->target[j]->dport_dev) {\n>>> + found++;\n>>> + break;\n>>> + }\n>>> + }\n>>> + }\n>> This may be over complicated. I'm not quite sure how it works (I'm just slow today I guess), but I understand\n>> what the intention is based on the debug print below. My issue is that ctx->host_bridges is only set to 1 host\n>> bridge (endpoint->host_bridge) in cxl_get_hpa_freespace(), which is the only caller of this function. At that\n>> point, why have the outer loop at all? At that point, you could also simplify ctx->host_bridges to only\n>> be a struct device * const.\n>>\n>> Maybe this gets called elsewhere later on in the series? I haven't looked at the rest yet. If I'm wrong, then\n>> I'd probably add a comment saying what the cxlsd->target[] entries are supposed to be pointing at.\n> \n> \n> Hi Ben,\n> \n> \n> I do remember this one.\n> \n> \n> Dan's original patches had this support for interleaving, then I removed it as the case for Type2 and interleaving is quite unlikely, at least right now and likely in the near future. But I was told why do not support it as it was trivial to do so. FWIW, If I think only about the use case coming with the patchset, I agree with you, but because those previous discussions, I think I have to leave it.\n> \n\nI'm fine with that, but I would at least do the fix with the decoder position in 19/22 and make a note that the\ninterleave_ways parameter in cxl_get_hpa_freespace() below is currently unused (unless I'm misunderstanding\nthe endpoint->host_bridge member).\n\nThat way, the support is mostly there and just requires a small, previously noted, addition to enable. If you're\nfine with that then feel free to add my Reviewed-by after implementing in v24.\n\nThanks,\nBen\n\n> \n> Thank you\n> \n> \n>>> +\n>>> + if (found != ctx->interleave_ways) {\n>>> + dev_dbg(dev,\n>>> + \"Not enough host bridges. Found %d for %d interleave ways requested\\n\",\n>>> + found, ctx->interleave_ways);\n>>> + return 0;\n>>> + }\n>>> +\n>>> + /*\n>>> + * Walk the root decoder resource range relying on cxl_rwsem.region to\n>>> + * preclude sibling arrival/departure and find the largest free space\n>>> + * gap.\n>>> + */\n>>> + lockdep_assert_held_read(&cxl_rwsem.region);\n>>> + res = cxlrd->res->child;\n>>> +\n>>> + /* With no resource child the whole parent resource is available */\n>>> + if (!res)\n>>> + max = resource_size(cxlrd->res);\n>>> + else\n>>> + max = 0;\n>>> +\n>>> + for (prev = NULL; res; prev = res, res = res->sibling) {\n>>> + if (!prev && res->start == cxlrd->res->start &&\n>>> + res->end == cxlrd->res->end) {\n>>> + max = resource_size(cxlrd->res);\n>>> + break;\n>>> + }\n>>> + /*\n>>> + * Sanity check for preventing arithmetic problems below as a\n>>> + * resource with size 0 could imply using the end field below\n>>> + * when set to unsigned zero - 1 or all f in hex.\n>>> + */\n>>> + if (prev && !resource_size(prev))\n>>> + continue;\n>>> +\n>>> + if (!prev && res->start > cxlrd->res->start) {\n>>> + free = res->start - cxlrd->res->start;\n>>> + max = max(free, max);\n>>> + }\n>>> + if (prev && res->start > prev->end + 1) {\n>>> + free = res->start - prev->end + 1;\n>>> + max = max(free, max);\n>>> + }\n>>> + }\n>>> +\n>>> + if (prev && prev->end + 1 < cxlrd->res->end + 1) {\n>>> + free = cxlrd->res->end + 1 - prev->end + 1;\n>>> + max = max(free, max);\n>>> + }\n>>> +\n>>> + dev_dbg(cxlrd_dev(cxlrd), \"found %pa bytes of free space\\n\", &max);\n>>> + if (max > ctx->max_hpa) {\n>>> + if (ctx->cxlrd)\n>>> + put_device(cxlrd_dev(ctx->cxlrd));\n>>> + get_device(cxlrd_dev(cxlrd));\n>>> + ctx->cxlrd = cxlrd;\n>>> + ctx->max_hpa = max;\n>>> + }\n>>> + return 0;\n>>> +}\n>>> +\n>>> +/**\n>>> + * cxl_get_hpa_freespace - find a root decoder with free capacity per constraints\n>>> + * @cxlmd: the mem device requiring the HPA\n>>> + * @interleave_ways: number of entries in @host_bridges\n>>> + * @flags: CXL_DECODER_F flags for selecting RAM vs PMEM, and Type2 device\n>>> + * @max_avail_contig: output parameter of max contiguous bytes available in the\n>>> + * returned decoder\n>>> + *\n>>> + * Returns a pointer to a struct cxl_root_decoder\n>>> + *\n>>> + * The return tuple of a 'struct cxl_root_decoder' and 'bytes available given\n>>> + * in (@max_avail_contig))' is a point in time snapshot. If by the time the\n>>> + * caller goes to use this decoder and its capacity is reduced then caller needs\n>>> + * to loop and retry.\n>>> + *\n>>> + * The returned root decoder has an elevated reference count that needs to be\n>>> + * put with cxl_put_root_decoder(cxlrd).\n>>> + */\n>>> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n>>> + int interleave_ways,\n>>> + unsigned long flags,\n>>> + resource_size_t *max_avail_contig)\n>>> +{\n>>> + struct cxlrd_max_context ctx = {\n>>> + .flags = flags,\n>>> + .interleave_ways = interleave_ways,\n>>> + };\n>>> + struct cxl_port *root_port;\n>>> + struct cxl_port *endpoint;\n>>> +\n>>> + endpoint = cxlmd->endpoint;\n>>> + if (!endpoint) {\n>>> + dev_dbg(&cxlmd->dev, \"endpoint not linked to memdev\\n\");\n>>> + return ERR_PTR(-ENXIO);\n>>> + }\n>>> +\n>>> + ctx.host_bridges = &endpoint->host_bridge;\n>> Mentioned earlier, interleave_ways is effectively hardcoded to 1 (unless I'm misunderstanding\n>> something). I think what you want here is to go to the CXL root and pass in the children (i.e. host bridges)?\n>> I'm not sure of what the fix is to get the intended behavior.\n>>\n>> It may be worth getting rid of the interleave_ways portion of this function and\n>> add it later when someone needs it. You could also explain it's hard coded to 1/unused\n>> in the doc comment if you know of an immediate need for it.\n>>\n>>> +\n>>> + struct cxl_root *root __free(put_cxl_root) = find_cxl_root(endpoint);\n>>> + if (!root) {\n>>> + dev_dbg(&endpoint->dev, \"endpoint is not related to a root port\\n\");\n>>> + return ERR_PTR(-ENXIO);\n>>> + }\n>>> +\n>>> + root_port = &root->port;\n>>> + scoped_guard(rwsem_read, &cxl_rwsem.region)\n>>> + device_for_each_child(&root_port->dev, &ctx, find_max_hpa);\n>> Can just use a guard() here.\n>>\n>>> +\n>>> + if (!ctx.cxlrd)\n>>> + return ERR_PTR(-ENOMEM);\n>>> +\n>>> + *max_avail_contig = ctx.max_hpa;\n>>> + return ctx.cxlrd;\n>>> +}\n>>> +EXPORT_SYMBOL_NS_GPL(cxl_get_hpa_freespace, \"CXL\");\n>>> +\n>>> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd)\n>>> +{\n>>> + put_device(cxlrd_dev(cxlrd));\n>>> +}\n>>> +EXPORT_SYMBOL_NS_GPL(cxl_put_root_decoder, \"CXL\");\n>>> +\n>>>  static ssize_t size_store(struct device *dev, struct device_attribute *attr,\n>>>  const char *buf, size_t len)\n>>>  {\n>>> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n>>> index 944c5d1ccceb..c7d9b2c2908f 100644\n>>> --- a/drivers/cxl/cxl.h\n>>> +++ b/drivers/cxl/cxl.h\n>>> @@ -706,6 +706,9 @@ struct cxl_root_decoder *to_cxl_root_decoder(struct device *dev);\n>>>  struct cxl_switch_decoder *to_cxl_switch_decoder(struct device *dev);\n>>>  struct cxl_endpoint_decoder *to_cxl_endpoint_decoder(struct device *dev);\n>>>  bool is_root_decoder(struct device *dev);\n>>> +\n>>> +#define cxlrd_dev(cxlrd) (&(cxlrd)->cxlsd.cxld.dev)\n>>> +\n>>>  bool is_switch_decoder(struct device *dev);\n>>>  bool is_endpoint_decoder(struct device *dev);\n>>>  struct cxl_root_decoder *cxl_root_decoder_alloc(struct cxl_port *port,\n>>> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n>>> index 92880c26b2d5..834dc7e78934 100644\n>>> --- a/include/cxl/cxl.h\n>>> +++ b/include/cxl/cxl.h\n>>> @@ -255,4 +255,10 @@ struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n>>>  struct range;\n>>>  int cxl_get_region_range(struct cxl_region *region, struct range *range);\n>>>  void cxl_unregister_region(struct cxl_region *cxlr);\n>>> +struct cxl_port;\n>>> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n>>> + int interleave_ways,\n>>> + unsigned long flags,\n>>> + resource_size_t *max);\n>>> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n>>>  #endif /* __CXL_CXL_H__ */\n\n",
          "reply_to": "alejandro.lucero-palau",
          "message_date": "2026-02-11"
        },
        {
          "author": "Cheatham, Benjamin",
          "summary": "Reviewer noted that the current implementation calls drop_region() with an error pointer returned by construct_region_begin(), which would lead to a garbage pointer dereference, and suggested changing the if condition to check for IS_ERR_OR_NULL before calling drop_region().\n\nReviewer noted that the position parameter in a function is hardcoded to 0, and suggested setting it to 'i' instead, referencing an earlier patch where interleaving functionality was added but appears unused.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "This needs to be \"if (!IS_ERR_OR_NULL(_T) drop_region(_T)\". If construct_region_begin() returns an\nerror pointer, drop_region() will be called with it as of now leading to a garbage pointer deref.\n\n---\n\nPosition parameter is hardcoded to 0. It should be set to i, right? This kind of goes back to my\nissues in patch 12/22; the interleaving functionality is there but it looks unused.",
          "reply_to": "alejandro.lucero-palau",
          "message_date": "2026-02-11"
        },
        {
          "author": "Cheatham, Benjamin",
          "summary": "Reviewer suggested folding cxl_dev_state_init() into the existing function, citing that it's only called within this function and thus unnecessary to have a separate init function.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "NEEDS_WORK"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Nit: Having a second function to do the init seems overkill here, especially since cxl_dev_state_init() isn't called outside this\nfunction. I'd fold it into this function instead, but I'm fine with it either way (especially if you were told otherwise before).\n\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>",
          "reply_to": "alejandro.lucero-palau",
          "message_date": "2026-02-11"
        },
        {
          "author": "Cheatham, Benjamin",
          "summary": "Reviewer questioned whether checking if a decoder is committed should be done by verifying the last allocated decoder (hdm_end), pointing out that this might not accurately reflect the commit state and suggesting an alternative approach of reading the register directly.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Is this the way you're supposed to check if a decoder is committed? The doc comment for @hdm_end in\nstruct cxl_port says it's just the last allocated decoder. If allocated decoders are always committed then\nI'm fine with this, otherwise I think you'd want to a register read or something to find the commit state.",
          "reply_to": "alejandro.lucero-palau",
          "message_date": "2026-02-11"
        },
        {
          "author": "Cheatham, Benjamin",
          "summary": "Reviewer suggested removing change logs for revisions more than 3 back, and adding a lore link for older revisions if desired.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "Requested changes",
            "Suggested improvement"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "This cover letter is really long, I'd remove the change logs for anything more\nthan 3 revisions back (assuming a v24 is needed). After that you could leave\na lore link for older revisions if you want, but it's not needed imo.\nAlso, feel free to add my Reviewed-by for anything I didn't leave a comment on\n(felt I should cut down on the mail).\n\nThanks,\nBen",
          "reply_to": "alejandro.lucero-palau",
          "message_date": "2026-02-11"
        },
        {
          "author": "Cheatham, Benjamin",
          "summary": "Gave Reviewed-by",
          "sentiment": "positive",
          "sentiment_signals": [],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> By definition a type2 cxl device will use the host managed memory for\n> specific functionality, therefore it should not be available to other\n> uses.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Davidlohr Bueso <daves@stgolabs.net>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> ---\n>  drivers/cxl/core/region.c | 7 +++++++\n>  1 file changed, 7 insertions(+)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index 293e63dfef22..12df717cc881 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -4441,6 +4441,13 @@ static int cxl_region_probe(struct device *dev)\n>  \tif (rc)\n>  \t\treturn rc;\n>  \n> +\t/*\n> +\t * HDM-D[B] (device-memory) regions have accelerator specific usage.\n> +\t * Skip device-dax registration.\n> +\t */\n> +\tif (cxlr->type == CXL_DECODER_DEVMEM)\n> +\t\treturn 0;\n\nMinor nit: Should probably move this to be the first thing in the function. It would save\nhaving to acquire a lock in cxl_region_can_probe() above. Keep my reviewed-by either way,\nit's really just a minor optimization.\n> +\n>  \t/*\n>  \t * From this point on any path that changes the region's state away from\n>  \t * CXL_CONFIG_COMMIT is also responsible for releasing the driver.\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> CXL region creation involves allocating capacity from Device Physical\n> Address (DPA) and assigning it to decode a given Host Physical Address\n> (HPA). Before determining how much DPA to allocate the amount of available\n> HPA must be determined. Also, not all HPA is created equal, some HPA\n> targets RAM, some targets PMEM, some is prepared for device-memory flows\n> like HDM-D and HDM-DB, and some is HDM-H (host-only).\n> \n> In order to support Type2 CXL devices, wrap all of those concerns into\n> an API that retrieves a root decoder (platform CXL window) that fits the\n> specified constraints and the capacity available for a new region.\n> \n> Add a complementary function for releasing the reference to such root\n> decoder.\n> \n> Based on https://lore.kernel.org/linux-cxl/168592159290.1948938.13522227102445462976.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> ---\n>  drivers/cxl/core/region.c | 164 ++++++++++++++++++++++++++++++++++++++\n>  drivers/cxl/cxl.h         |   3 +\n>  include/cxl/cxl.h         |   6 ++\n>  3 files changed, 173 insertions(+)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index 954b8fcdbac6..bdefd088f5f1 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -705,6 +705,170 @@ static int free_hpa(struct cxl_region *cxlr)\n>  \treturn 0;\n>  }\n>  \n> +struct cxlrd_max_context {\n> +\tstruct device * const *host_bridges;\n> +\tint interleave_ways;\n> +\tunsigned long flags;\n> +\tresource_size_t max_hpa;\n> +\tstruct cxl_root_decoder *cxlrd;\n> +};\n> +\n> +static int find_max_hpa(struct device *dev, void *data)\n> +{\n> +\tstruct cxlrd_max_context *ctx = data;\n> +\tstruct cxl_switch_decoder *cxlsd;\n> +\tstruct cxl_root_decoder *cxlrd;\n> +\tstruct resource *res, *prev;\n> +\tstruct cxl_decoder *cxld;\n> +\tresource_size_t free = 0;\n> +\tresource_size_t max;\n> +\tint found = 0;\n> +\n> +\tif (!is_root_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxlrd = to_cxl_root_decoder(dev);\n> +\tcxlsd = &cxlrd->cxlsd;\n> +\tcxld = &cxlsd->cxld;\n> +\n> +\tif ((cxld->flags & ctx->flags) != ctx->flags) {\n> +\t\tdev_dbg(dev, \"flags not matching: %08lx vs %08lx\\n\",\n> +\t\t\tcxld->flags, ctx->flags);\n> +\t\treturn 0;\n> +\t}\n> +\n> +\tfor (int i = 0; i < ctx->interleave_ways; i++) {\n> +\t\tfor (int j = 0; j < ctx->interleave_ways; j++) {\n> +\t\t\tif (ctx->host_bridges[i] == cxlsd->target[j]->dport_dev) {\n> +\t\t\t\tfound++;\n> +\t\t\t\tbreak;\n> +\t\t\t}\n> +\t\t}\n> +\t}\n\nThis may be over complicated. I'm not quite sure how it works (I'm just slow today I guess), but I understand\nwhat the intention is based on the debug print below. My issue is that ctx->host_bridges is only set to 1 host\nbridge (endpoint->host_bridge) in cxl_get_hpa_freespace(), which is the only caller of this function. At that\npoint, why have the outer loop at all? At that point, you could also simplify ctx->host_bridges to only\nbe a struct device * const.\n\nMaybe this gets called elsewhere later on in the series? I haven't looked at the rest yet. If I'm wrong, then\nI'd probably add a comment saying what the cxlsd->target[] entries are supposed to be pointing at.\n> +\n> +\tif (found != ctx->interleave_ways) {\n> +\t\tdev_dbg(dev,\n> +\t\t\t\"Not enough host bridges. Found %d for %d interleave ways requested\\n\",\n> +\t\t\tfound, ctx->interleave_ways);\n> +\t\treturn 0;\n> +\t}\n> +\n> +\t/*\n> +\t * Walk the root decoder resource range relying on cxl_rwsem.region to\n> +\t * preclude sibling arrival/departure and find the largest free space\n> +\t * gap.\n> +\t */\n> +\tlockdep_assert_held_read(&cxl_rwsem.region);\n> +\tres = cxlrd->res->child;\n> +\n> +\t/* With no resource child the whole parent resource is available */\n> +\tif (!res)\n> +\t\tmax = resource_size(cxlrd->res);\n> +\telse\n> +\t\tmax = 0;\n> +\n> +\tfor (prev = NULL; res; prev = res, res = res->sibling) {\n> +\t\tif (!prev && res->start == cxlrd->res->start &&\n> +\t\t    res->end == cxlrd->res->end) {\n> +\t\t\tmax = resource_size(cxlrd->res);\n> +\t\t\tbreak;\n> +\t\t}\n> +\t\t/*\n> +\t\t * Sanity check for preventing arithmetic problems below as a\n> +\t\t * resource with size 0 could imply using the end field below\n> +\t\t * when set to unsigned zero - 1 or all f in hex.\n> +\t\t */\n> +\t\tif (prev && !resource_size(prev))\n> +\t\t\tcontinue;\n> +\n> +\t\tif (!prev && res->start > cxlrd->res->start) {\n> +\t\t\tfree = res->start - cxlrd->res->start;\n> +\t\t\tmax = max(free, max);\n> +\t\t}\n> +\t\tif (prev && res->start > prev->end + 1) {\n> +\t\t\tfree = res->start - prev->end + 1;\n> +\t\t\tmax = max(free, max);\n> +\t\t}\n> +\t}\n> +\n> +\tif (prev && prev->end + 1 < cxlrd->res->end + 1) {\n> +\t\tfree = cxlrd->res->end + 1 - prev->end + 1;\n> +\t\tmax = max(free, max);\n> +\t}\n> +\n> +\tdev_dbg(cxlrd_dev(cxlrd), \"found %pa bytes of free space\\n\", &max);\n> +\tif (max > ctx->max_hpa) {\n> +\t\tif (ctx->cxlrd)\n> +\t\t\tput_device(cxlrd_dev(ctx->cxlrd));\n> +\t\tget_device(cxlrd_dev(cxlrd));\n> +\t\tctx->cxlrd = cxlrd;\n> +\t\tctx->max_hpa = max;\n> +\t}\n> +\treturn 0;\n> +}\n> +\n> +/**\n> + * cxl_get_hpa_freespace - find a root decoder with free capacity per constraints\n> + * @cxlmd: the mem device requiring the HPA\n> + * @interleave_ways: number of entries in @host_bridges\n> + * @flags: CXL_DECODER_F flags for selecting RAM vs PMEM, and Type2 device\n> + * @max_avail_contig: output parameter of max contiguous bytes available in the\n> + *\t\t      returned decoder\n> + *\n> + * Returns a pointer to a struct cxl_root_decoder\n> + *\n> + * The return tuple of a 'struct cxl_root_decoder' and 'bytes available given\n> + * in (@max_avail_contig))' is a point in time snapshot. If by the time the\n> + * caller goes to use this decoder and its capacity is reduced then caller needs\n> + * to loop and retry.\n> + *\n> + * The returned root decoder has an elevated reference count that needs to be\n> + * put with cxl_put_root_decoder(cxlrd).\n> + */\n> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t       int interleave_ways,\n> +\t\t\t\t\t       unsigned long flags,\n> +\t\t\t\t\t       resource_size_t *max_avail_contig)\n> +{\n> +\tstruct cxlrd_max_context ctx = {\n> +\t\t.flags = flags,\n> +\t\t.interleave_ways = interleave_ways,\n> +\t};\n> +\tstruct cxl_port *root_port;\n> +\tstruct cxl_port *endpoint;\n> +\n> +\tendpoint = cxlmd->endpoint;\n> +\tif (!endpoint) {\n> +\t\tdev_dbg(&cxlmd->dev, \"endpoint not linked to memdev\\n\");\n> +\t\treturn ERR_PTR(-ENXIO);\n> +\t}\n> +\n> +\tctx.host_bridges = &endpoint->host_bridge;\n\nMentioned earlier, interleave_ways is effectively hardcoded to 1 (unless I'm misunderstanding\nsomething). I think what you want here is to go to the CXL root and pass in the children (i.e. host bridges)?\nI'm not sure of what the fix is to get the intended behavior.\n\nIt may be worth getting rid of the interleave_ways portion of this function and\nadd it later when someone needs it. You could also explain it's hard coded to 1/unused\nin the doc comment if you know of an immediate need for it.\n\n> +\n> +\tstruct cxl_root *root __free(put_cxl_root) = find_cxl_root(endpoint);\n> +\tif (!root) {\n> +\t\tdev_dbg(&endpoint->dev, \"endpoint is not related to a root port\\n\");\n> +\t\treturn ERR_PTR(-ENXIO);\n> +\t}\n> +\n> +\troot_port = &root->port;\n> +\tscoped_guard(rwsem_read, &cxl_rwsem.region)\n> +\t\tdevice_for_each_child(&root_port->dev, &ctx, find_max_hpa);\n\nCan just use a guard() here.\n\n> +\n> +\tif (!ctx.cxlrd)\n> +\t\treturn ERR_PTR(-ENOMEM);\n> +\n> +\t*max_avail_contig = ctx.max_hpa;\n> +\treturn ctx.cxlrd;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_get_hpa_freespace, \"CXL\");\n> +\n> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd)\n> +{\n> +\tput_device(cxlrd_dev(cxlrd));\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_put_root_decoder, \"CXL\");\n> +\n>  static ssize_t size_store(struct device *dev, struct device_attribute *attr,\n>  \t\t\t  const char *buf, size_t len)\n>  {\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index 944c5d1ccceb..c7d9b2c2908f 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -706,6 +706,9 @@ struct cxl_root_decoder *to_cxl_root_decoder(struct device *dev);\n>  struct cxl_switch_decoder *to_cxl_switch_decoder(struct device *dev);\n>  struct cxl_endpoint_decoder *to_cxl_endpoint_decoder(struct device *dev);\n>  bool is_root_decoder(struct device *dev);\n> +\n> +#define cxlrd_dev(cxlrd) (&(cxlrd)->cxlsd.cxld.dev)\n> +\n>  bool is_switch_decoder(struct device *dev);\n>  bool is_endpoint_decoder(struct device *dev);\n>  struct cxl_root_decoder *cxl_root_decoder_alloc(struct cxl_port *port,\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 92880c26b2d5..834dc7e78934 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -255,4 +255,10 @@ struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n>  struct range;\n>  int cxl_get_region_range(struct cxl_region *region, struct range *range);\n>  void cxl_unregister_region(struct cxl_region *cxlr);\n> +struct cxl_port;\n> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t       int interleave_ways,\n> +\t\t\t\t\t       unsigned long flags,\n> +\t\t\t\t\t       resource_size_t *max);\n> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Check if device HDM is already committed during firmware/BIOS\n> initialization.\n> \n> A CXL region should exist if so after memdev allocation/initialization.\n> Get HPA from region and map it.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> ---\n>  drivers/net/ethernet/sfc/efx_cxl.c | 28 +++++++++++++++++++++++++++-\n>  1 file changed, 27 insertions(+), 1 deletion(-)\n> \n> diff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\n> index a77ef4783fcb..3536eccf1b2a 100644\n> --- a/drivers/net/ethernet/sfc/efx_cxl.c\n> +++ b/drivers/net/ethernet/sfc/efx_cxl.c\n> @@ -19,6 +19,7 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n>  \tstruct efx_nic *efx = &probe_data->efx;\n>  \tstruct pci_dev *pci_dev = efx->pci_dev;\n>  \tstruct efx_cxl *cxl;\n> +\tstruct range range;\n>  \tu16 dvsec;\n>  \tint rc;\n>  \n> @@ -90,13 +91,38 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n>  \t\treturn PTR_ERR(cxl->cxlmd);\n>  \t}\n>  \n> -\tprobe_data->cxl = cxl;\n> +\tcxl->cxled = cxl_get_committed_decoder(cxl->cxlmd, &cxl->efx_region);\n> +\tif (cxl->cxled) {\n> +\t\tif (!cxl->efx_region) {\n> +\t\t\tpci_err(pci_dev, \"CXL found committed decoder without a region\");\n> +\t\t\treturn -ENODEV;\n> +\t\t}\n> +\t\trc = cxl_get_region_range(cxl->efx_region, &range);\n\nMissing an empty line above.\n\n> +\t\tif (rc) {\n> +\t\t\tpci_err(pci_dev,\n> +\t\t\t\t\"CXL getting regions params from a committed decoder failed\");\n> +\t\t\treturn rc;\n> +\t\t}\n> +\n> +\t\tcxl->ctpio_cxl = ioremap(range.start, range.end - range.start + 1);\n\nMaybe use range_len() instead for the second parameter?\n\n> +\t\tif (!cxl->ctpio_cxl) {\n> +\t\t\tpci_err(pci_dev, \"CXL ioremap region (%pra) failed\", &range);\n> +\t\t\treturn -ENOMEM;\n> +\t\t}\n> +\n> +\t\tprobe_data->cxl = cxl;\n> +\t}\n>  \n>  \treturn 0;\n>  }\n>  \n>  void efx_cxl_exit(struct efx_probe_data *probe_data)\n>  {\n> +\tif (!probe_data->cxl)\n> +\t\treturn;\n> +\n> +\tiounmap(probe_data->cxl->ctpio_cxl);\n> +\tcxl_unregister_region(probe_data->cxl->efx_region);\n>  }\n>  \n>  MODULE_IMPORT_NS(\"CXL\");\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Region creation involves finding available DPA (device-physical-address)\n> capacity to map into HPA (host-physical-address) space.\n> \n> In order to support CXL Type2 devices, define an API, cxl_request_dpa(),\n> that tries to allocate the DPA memory the driver requires to operate.The\n> memory requested should not be bigger than the max available HPA obtained\n> previously with cxl_get_hpa_freespace().\n> \n> Based on https://lore.kernel.org/linux-cxl/168592158743.1948938.7622563891193802610.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> ---\n>  drivers/cxl/core/hdm.c | 84 ++++++++++++++++++++++++++++++++++++++++++\n>  drivers/cxl/cxl.h      |  1 +\n>  include/cxl/cxl.h      |  5 +++\n>  3 files changed, 90 insertions(+)\n> \n> diff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\n> index a172ce4e9b19..d60a697f12cc 100644\n> --- a/drivers/cxl/core/hdm.c\n> +++ b/drivers/cxl/core/hdm.c\n> @@ -3,6 +3,7 @@\n>  #include <linux/seq_file.h>\n>  #include <linux/device.h>\n>  #include <linux/delay.h>\n> +#include <cxl/cxl.h>\n>  \n>  #include \"cxlmem.h\"\n>  #include \"core.h\"\n> @@ -546,6 +547,12 @@ bool cxl_resource_contains_addr(const struct resource *res, const resource_size_\n>  \treturn resource_contains(res, &_addr);\n>  }\n>  \n> +/**\n> + * cxl_dpa_free - release DPA (Device Physical Address)\n> + * @cxled: endpoint decoder linked to the DPA\n> + *\n> + * Returns 0 or error.\n> + */\n>  int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n>  {\n>  \tstruct cxl_port *port = cxled_to_port(cxled);\n> @@ -572,6 +579,7 @@ int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n>  \tdevm_cxl_dpa_release(cxled);\n>  \treturn 0;\n>  }\n> +EXPORT_SYMBOL_NS_GPL(cxl_dpa_free, \"CXL\");\n>  \n>  int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n>  \t\t     enum cxl_partition_mode mode)\n> @@ -603,6 +611,82 @@ int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n>  \treturn 0;\n>  }\n>  \n> +static int find_free_decoder(struct device *dev, const void *data)\n> +{\n> +\tstruct cxl_endpoint_decoder *cxled;\n> +\tstruct cxl_port *port;\n> +\n> +\tif (!is_endpoint_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxled = to_cxl_endpoint_decoder(dev);\n> +\tport = cxled_to_port(cxled);\n> +\n> +\treturn cxled->cxld.id == (port->hdm_end + 1);\n> +}\n> +\n> +static struct cxl_endpoint_decoder *\n> +cxl_find_free_decoder(struct cxl_memdev *cxlmd)\n> +{\n> +\tstruct cxl_port *endpoint = cxlmd->endpoint;\n> +\tstruct device *dev;\n> +\n> +\tguard(rwsem_read)(&cxl_rwsem.dpa);\n> +\tdev = device_find_child(&endpoint->dev, NULL,\n> +\t\t\t\tfind_free_decoder);\n> +\tif (!dev)\n> +\t\treturn NULL;\n> +\n> +\treturn to_cxl_endpoint_decoder(dev);\n> +}\n> +\n> +/**\n> + * cxl_request_dpa - search and reserve DPA given input constraints\n> + * @cxlmd: memdev with an endpoint port with available decoders\n> + * @mode: CXL partition mode (ram vs pmem)\n> + * @alloc: dpa size required\n> + *\n> + * Returns a pointer to a 'struct cxl_endpoint_decoder' on success or\n> + * an errno encoded pointer on failure.\n> + *\n> + * Given that a region needs to allocate from limited HPA capacity it\n> + * may be the case that a device has more mappable DPA capacity than\n> + * available HPA. The expectation is that @alloc is a driver known\n> + * value based on the device capacity but which could not be fully\n> + * available due to HPA constraints.\n> + *\n> + * Returns a pinned cxl_decoder with at least @alloc bytes of capacity\n> + * reserved, or an error pointer. The caller is also expected to own the\n> + * lifetime of the memdev registration associated with the endpoint to\n> + * pin the decoder registered as well.\n> + */\n> +struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t     enum cxl_partition_mode mode,\n> +\t\t\t\t\t     resource_size_t alloc)\n> +{\n> +\tint rc;\n> +\n> +\tif (!IS_ALIGNED(alloc, SZ_256M))\n> +\t\treturn ERR_PTR(-EINVAL);\n> +\n> +\tstruct cxl_endpoint_decoder *cxled __free(put_cxled) =\n> +\t\tcxl_find_free_decoder(cxlmd);\n> +\n> +\tif (!cxled)\n> +\t\treturn ERR_PTR(-ENODEV);\n> +\n> +\trc = cxl_dpa_set_part(cxled, mode);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\trc = cxl_dpa_alloc(cxled, alloc);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n\nShould cxl_dpa_set_part() be unwound here, or does it not matter? If it doesn't matter:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> +\n> +\treturn no_free_ptr(cxled);\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_request_dpa, \"CXL\");\n> +\n>  static int __cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size)\n>  {\n>  \tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index d1b010e5e1d0..2b1f7d687a0e 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -667,6 +667,7 @@ struct cxl_root *find_cxl_root(struct cxl_port *port);\n>  \n>  DEFINE_FREE(put_cxl_root, struct cxl_root *, if (_T) put_device(&_T->port.dev))\n>  DEFINE_FREE(put_cxl_port, struct cxl_port *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n> +DEFINE_FREE(put_cxled, struct cxl_endpoint_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxld.dev))\n>  DEFINE_FREE(put_cxl_root_decoder, struct cxl_root_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxlsd.cxld.dev))\n>  DEFINE_FREE(put_cxl_region, struct cxl_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n>  \n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 783ad570a6eb..4802371db00e 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -7,6 +7,7 @@\n>  \n>  #include <linux/node.h>\n>  #include <linux/ioport.h>\n> +#include <linux/range.h>\n>  #include <cxl/mailbox.h>\n>  \n>  /**\n> @@ -276,4 +277,8 @@ struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n>  \t\t\t\t\t       unsigned long flags,\n>  \t\t\t\t\t       resource_size_t *max);\n>  void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n> +struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t     enum cxl_partition_mode mode,\n> +\t\t\t\t\t     resource_size_t alloc);\n> +int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Current code is expecting Type3 or CXL_DECODER_HOSTONLYMEM devices only.\n> Support for Type2 implies region type needs to be based on the endpoint\n> type HDM-D[B] instead.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Zhi Wang <zhiw@nvidia.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n> Reviewed-by: Davidlohr Bueso <daves@stgolabs.net>\n> ---\n>  drivers/cxl/core/region.c | 10 ++++++----\n>  1 file changed, 6 insertions(+), 4 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index bdefd088f5f1..f53b2e9fd9e6 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -2833,7 +2833,8 @@ static ssize_t create_ram_region_show(struct device *dev,\n>  }\n>  \n>  static struct cxl_region *__create_region(struct cxl_root_decoder *cxlrd,\n> -\t\t\t\t\t  enum cxl_partition_mode mode, int id)\n> +\t\t\t\t\t  enum cxl_partition_mode mode, int id,\n> +\t\t\t\t\t  enum cxl_decoder_type target_type)\n>  {\n>  \tint rc;\n>  \n> @@ -2855,7 +2856,7 @@ static struct cxl_region *__create_region(struct cxl_root_decoder *cxlrd,\n>  \t\treturn ERR_PTR(-EBUSY);\n>  \t}\n>  \n> -\treturn devm_cxl_add_region(cxlrd, id, mode, CXL_DECODER_HOSTONLYMEM);\n> +\treturn devm_cxl_add_region(cxlrd, id, mode, target_type);\n>  }\n>  \n>  static ssize_t create_region_store(struct device *dev, const char *buf,\n> @@ -2869,7 +2870,7 @@ static ssize_t create_region_store(struct device *dev, const char *buf,\n>  \tif (rc != 1)\n>  \t\treturn -EINVAL;\n>  \n> -\tcxlr = __create_region(cxlrd, mode, id);\n> +\tcxlr = __create_region(cxlrd, mode, id, CXL_DECODER_HOSTONLYMEM);\n\nI haven't read the ABI docs, but would it be worthwhile to update the documentation for this attribute\nto mention it only makes type 3 regions? I'm flip-flopping on whether it's worth the trouble but thought\nI should mention it.\n\nEither way:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n\n>  \tif (IS_ERR(cxlr))\n>  \t\treturn PTR_ERR(cxlr);\n>  \n> @@ -4036,7 +4037,8 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n>  \n>  \tdo {\n>  \t\tcxlr = __create_region(cxlrd, cxlds->part[part].mode,\n> -\t\t\t\t       atomic_read(&cxlrd->region_id));\n> +\t\t\t\t       atomic_read(&cxlrd->region_id),\n> +\t\t\t\t       cxled->cxld.target_type);\n>  \t} while (IS_ERR(cxlr) && PTR_ERR(cxlr) == -EBUSY);\n>  \n>  \tif (IS_ERR(cxlr)) {\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Region creation based on Type3 devices is triggered from user space\n> allowing memory combination through interleaving.\n> \n> In preparation for kernel driven region creation, that is Type2 drivers\n> triggering region creation backed with its advertised CXL memory, factor\n> out a common helper from the user-sysfs region setup for interleave ways.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Zhi Wang <zhiw@nvidia.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n> ---\n>  drivers/cxl/core/region.c | 43 ++++++++++++++++++++++++---------------\n>  1 file changed, 27 insertions(+), 16 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index f53b2e9fd9e6..ece1d3df7cf1 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -485,22 +485,14 @@ static ssize_t interleave_ways_show(struct device *dev,\n>  \n>  static const struct attribute_group *get_cxl_region_target_group(void);\n>  \n> -static ssize_t interleave_ways_store(struct device *dev,\n> -\t\t\t\t     struct device_attribute *attr,\n> -\t\t\t\t     const char *buf, size_t len)\n> +static int set_interleave_ways(struct cxl_region *cxlr, int val)\n\n@val should probably stay an unsigned int. You pass an unsigned int in the sysfs function, and the\nfunction was originally coded with that in mind (same with @save below). With that cleaned up:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n\n>  {\n> -\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(dev->parent);\n> +\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n>  \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n> -\tstruct cxl_region *cxlr = to_cxl_region(dev);\n>  \tstruct cxl_region_params *p = &cxlr->params;\n> -\tunsigned int val, save;\n> -\tint rc;\n> +\tint save, rc;\n>  \tu8 iw;\n>  \n> -\trc = kstrtouint(buf, 0, &val);\n> -\tif (rc)\n> -\t\treturn rc;\n> -\n>  \trc = ways_to_eiw(val, &iw);\n>  \tif (rc)\n>  \t\treturn rc;\n> @@ -515,9 +507,7 @@ static ssize_t interleave_ways_store(struct device *dev,\n>  \t\treturn -EINVAL;\n>  \t}\n>  \n> -\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n> -\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n> -\t\treturn rc;\n> +\tlockdep_assert_held_write(&cxl_rwsem.region);\n>  \n>  \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n>  \t\treturn -EBUSY;\n> @@ -525,10 +515,31 @@ static ssize_t interleave_ways_store(struct device *dev,\n>  \tsave = p->interleave_ways;\n>  \tp->interleave_ways = val;\n>  \trc = sysfs_update_group(&cxlr->dev.kobj, get_cxl_region_target_group());\n> -\tif (rc) {\n> +\tif (rc)\n>  \t\tp->interleave_ways = save;\n> +\n> +\treturn rc;\n> +}\n> +\n> +static ssize_t interleave_ways_store(struct device *dev,\n> +\t\t\t\t     struct device_attribute *attr,\n> +\t\t\t\t     const char *buf, size_t len)\n> +{\n> +\tstruct cxl_region *cxlr = to_cxl_region(dev);\n> +\tunsigned int val;\n> +\tint rc;\n> +\n> +\trc = kstrtouint(buf, 0, &val);\n> +\tif (rc)\n> +\t\treturn rc;\n> +\n> +\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n> +\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n> +\t\treturn rc;\n> +\n> +\trc = set_interleave_ways(cxlr, val);\n> +\tif (rc)\n>  \t\treturn rc;\n> -\t}\n>  \n>  \treturn len;\n>  }\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Region creation based on Type3 devices is triggered from user space\n> allowing memory combination through interleaving.\n> \n> In preparation for kernel driven region creation, that is Type2 drivers\n> triggering region creation backed with its advertised CXL memory, factor\n> out a common helper from the user-sysfs region setup forinterleave\n> granularity.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Zhi Wang <zhiw@nvidia.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n> ---\n>  drivers/cxl/core/region.c | 39 +++++++++++++++++++++++++--------------\n>  1 file changed, 25 insertions(+), 14 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index ece1d3df7cf1..63c2aeb2ee1f 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -559,21 +559,14 @@ static ssize_t interleave_granularity_show(struct device *dev,\n>  \treturn sysfs_emit(buf, \"%d\\n\", p->interleave_granularity);\n>  }\n>  \n> -static ssize_t interleave_granularity_store(struct device *dev,\n> -\t\t\t\t\t    struct device_attribute *attr,\n> -\t\t\t\t\t    const char *buf, size_t len)\n> +static int set_interleave_granularity(struct cxl_region *cxlr, int val)\n\nSame thing as last patch. Assuming it's fixed:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n>  {\n> -\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(dev->parent);\n> +\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n>  \tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n> -\tstruct cxl_region *cxlr = to_cxl_region(dev);\n>  \tstruct cxl_region_params *p = &cxlr->params;\n> -\tint rc, val;\n> +\tint rc;\n>  \tu16 ig;\n>  \n> -\trc = kstrtoint(buf, 0, &val);\n> -\tif (rc)\n> -\t\treturn rc;\n> -\n>  \trc = granularity_to_eig(val, &ig);\n>  \tif (rc)\n>  \t\treturn rc;\n> @@ -589,14 +582,32 @@ static ssize_t interleave_granularity_store(struct device *dev,\n>  \tif (cxld->interleave_ways > 1 && val != cxld->interleave_granularity)\n>  \t\treturn -EINVAL;\n>  \n> -\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n> -\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n> -\t\treturn rc;\n> -\n> +\tlockdep_assert_held_write(&cxl_rwsem.region);\n>  \tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n>  \t\treturn -EBUSY;\n>  \n>  \tp->interleave_granularity = val;\n> +\treturn 0;\n> +}\n> +\n> +static ssize_t interleave_granularity_store(struct device *dev,\n> +\t\t\t\t\t    struct device_attribute *attr,\n> +\t\t\t\t\t    const char *buf, size_t len)\n> +{\n> +\tstruct cxl_region *cxlr = to_cxl_region(dev);\n> +\tint rc, val;\n> +\n> +\trc = kstrtoint(buf, 0, &val);\n> +\tif (rc)\n> +\t\treturn rc;\n> +\n> +\tACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n> +\tif ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n> +\t\treturn rc;\n> +\n> +\trc = set_interleave_granularity(cxlr, val);\n> +\tif (rc)\n> +\t\treturn rc;\n>  \n>  \treturn len;\n>  }\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Creating a CXL region requires userspace intervention through the cxl\n> sysfs files. Type2 support should allow accelerator drivers to create\n> such cxl region from kernel code.\n> \n> Adding that functionality and integrating it with current support for\n> memory expanders.\n> \n> Based on https://lore.kernel.org/linux-cxl/168592159835.1948938.1647215579839222774.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> ---\n>  drivers/cxl/core/region.c | 131 ++++++++++++++++++++++++++++++++++++--\n>  include/cxl/cxl.h         |   3 +\n>  2 files changed, 127 insertions(+), 7 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index 63c2aeb2ee1f..293e63dfef22 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -2944,6 +2944,14 @@ cxl_find_region_by_name(struct cxl_root_decoder *cxlrd, const char *name)\n>  \treturn to_cxl_region(region_dev);\n>  }\n>  \n> +static void drop_region(struct cxl_region *cxlr)\n> +{\n> +\tstruct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n> +\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n> +\n> +\tdevm_release_action(port->uport_dev, __unregister_region, cxlr);\n> +}\n> +\n>  static ssize_t delete_region_store(struct device *dev,\n>  \t\t\t\t   struct device_attribute *attr,\n>  \t\t\t\t   const char *buf, size_t len)\n> @@ -4047,14 +4055,12 @@ static int __construct_region(struct cxl_region *cxlr,\n>  \treturn 0;\n>  }\n>  \n> -/* Establish an empty region covering the given HPA range */\n> -static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n> -\t\t\t\t\t   struct cxl_endpoint_decoder *cxled)\n> +static struct cxl_region *construct_region_begin(struct cxl_root_decoder *cxlrd,\n> +\t\t\t\t\t\t struct cxl_endpoint_decoder *cxled)\n>  {\n>  \tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n> -\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n>  \tstruct cxl_dev_state *cxlds = cxlmd->cxlds;\n> -\tint rc, part = READ_ONCE(cxled->part);\n> +\tint part = READ_ONCE(cxled->part);\n>  \tstruct cxl_region *cxlr;\n>  \n>  \tdo {\n> @@ -4063,13 +4069,26 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n>  \t\t\t\t       cxled->cxld.target_type);\n>  \t} while (IS_ERR(cxlr) && PTR_ERR(cxlr) == -EBUSY);\n>  \n> -\tif (IS_ERR(cxlr)) {\n> +\tif (IS_ERR(cxlr))\n>  \t\tdev_err(cxlmd->dev.parent,\n>  \t\t\t\"%s:%s: %s failed assign region: %ld\\n\",\n>  \t\t\tdev_name(&cxlmd->dev), dev_name(&cxled->cxld.dev),\n>  \t\t\t__func__, PTR_ERR(cxlr));\n> +\n> +\treturn cxlr;\n> +}\n> +\n> +/* Establish an empty region covering the given HPA range */\n> +static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n> +\t\t\t\t\t   struct cxl_endpoint_decoder *cxled)\n> +{\n> +\tstruct cxl_port *port = cxlrd_to_port(cxlrd);\n> +\tstruct cxl_region *cxlr;\n> +\tint rc;\n> +\n> +\tcxlr = construct_region_begin(cxlrd, cxled);\n> +\tif (IS_ERR(cxlr))\n>  \t\treturn cxlr;\n> -\t}\n>  \n>  \trc = __construct_region(cxlr, cxlrd, cxled);\n>  \tif (rc) {\n> @@ -4080,6 +4099,104 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n>  \treturn cxlr;\n>  }\n>  \n> +DEFINE_FREE(cxl_region_drop, struct cxl_region *, if (_T) drop_region(_T))\n\nThis needs to be \"if (!IS_ERR_OR_NULL(_T) drop_region(_T)\". If construct_region_begin() returns an\nerror pointer, drop_region() will be called with it as of now leading to a garbage pointer deref.\n\n> +\n> +static struct cxl_region *\n> +__construct_new_region(struct cxl_root_decoder *cxlrd,\n> +\t\t       struct cxl_endpoint_decoder **cxled, int ways)\n> +{\n> +\tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled[0]);\n> +\tstruct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n> +\tstruct cxl_region_params *p;\n> +\tresource_size_t size = 0;\n> +\tint rc, i;\n> +\n> +\tstruct cxl_region *cxlr __free(cxl_region_drop) =\n> +\t\tconstruct_region_begin(cxlrd, cxled[0]);\n> +\tif (IS_ERR(cxlr))\n> +\t\treturn cxlr;\n> +\n> +\tguard(rwsem_write)(&cxl_rwsem.region);\n> +\n> +\t/*\n> +\t * Sanity check. This should not happen with an accel driver handling\n> +\t * the region creation.\n> +\t */\n> +\tp = &cxlr->params;\n> +\tif (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE) {\n> +\t\tdev_err(cxlmd->dev.parent,\n> +\t\t\t\"%s:%s: %s  unexpected region state\\n\",\n> +\t\t\tdev_name(&cxlmd->dev), dev_name(&cxled[0]->cxld.dev),\n> +\t\t\t__func__);\n> +\t\treturn ERR_PTR(-EBUSY);\n> +\t}\n> +\n> +\trc = set_interleave_ways(cxlr, ways);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\trc = set_interleave_granularity(cxlr, cxld->interleave_granularity);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\tscoped_guard(rwsem_read, &cxl_rwsem.dpa) {\n> +\t\tfor (i = 0; i < ways; i++) {\n> +\t\t\tif (!cxled[i]->dpa_res)\n> +\t\t\t\treturn ERR_PTR(-EINVAL);\n> +\t\t\tsize += resource_size(cxled[i]->dpa_res);\n> +\t\t}\n> +\n> +\t\trc = alloc_hpa(cxlr, size);\n> +\t\tif (rc)\n> +\t\t\treturn ERR_PTR(rc);\n> +\n> +\t\tfor (i = 0; i < ways; i++) {\n> +\t\t\trc = cxl_region_attach(cxlr, cxled[i], 0);\n\nPosition parameter is hardcoded to 0. It should be set to i, right? This kind of goes back to my\nissues in patch 12/22; the interleaving functionality is there but it looks unused.\n\n> +\t\t\tif (rc)\n> +\t\t\t\treturn ERR_PTR(rc);\n> +\t\t}\n> +\t}\n> +\n> +\trc = cxl_region_decode_commit(cxlr);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\tp->state = CXL_CONFIG_COMMIT;\n> +\n> +\treturn no_free_ptr(cxlr);\n> +}\n> +\n> +/**\n> + * cxl_create_region - Establish a region given an endpoint decoder\n> + * @cxlrd: root decoder to allocate HPA\n> + * @cxled: endpoint decoders with reserved DPA capacity\n> + * @ways: interleave ways required\n> + *\n> + * Returns a fully formed region in the commit state and attached to the\n> + * cxl_region driver.\n> + */\n> +struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n> +\t\t\t\t     struct cxl_endpoint_decoder **cxled,\n> +\t\t\t\t     int ways)\n> +{\n> +\tstruct cxl_region *cxlr;\n> +\n> +\tmutex_lock(&cxlrd->range_lock);\n> +\tcxlr = __construct_new_region(cxlrd, cxled, ways);\n> +\tmutex_unlock(&cxlrd->range_lock);\n> +\tif (IS_ERR(cxlr))\n> +\t\treturn cxlr;\n> +\n> +\tif (device_attach(&cxlr->dev) <= 0) {\n> +\t\tdev_err(&cxlr->dev, \"failed to create region\\n\");\n> +\t\tdrop_region(cxlr);\n> +\t\treturn ERR_PTR(-ENODEV);\n> +\t}\n> +\n> +\treturn cxlr;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_create_region, \"CXL\");\n> +\n>  static struct cxl_region *\n>  cxl_find_region_by_range(struct cxl_root_decoder *cxlrd, struct range *hpa)\n>  {\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 4802371db00e..50acbd13bcf8 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -281,4 +281,7 @@ struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n>  \t\t\t\t\t     enum cxl_partition_mode mode,\n>  \t\t\t\t\t     resource_size_t alloc);\n>  int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n> +struct cxl_region *cxl_create_region(struct cxl_root_decoder *cxlrd,\n> +\t\t\t\t     struct cxl_endpoint_decoder **cxled,\n> +\t\t\t\t     int ways);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Differentiate CXL memory expanders (type 3) from CXL device accelerators\n> (type 2) with a new function for initializing cxl_dev_state and a macro\n> for helping accel drivers to embed cxl_dev_state inside a private\n> struct.\n> \n> Move structs to include/cxl as the size of the accel driver private\n> struct embedding cxl_dev_state needs to know the size of this struct.\n> \n> Use same new initialization with the type3 pci driver.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> ---\n>  drivers/cxl/core/mbox.c      |  12 +-\n>  drivers/cxl/core/memdev.c    |  32 +++++\n>  drivers/cxl/cxl.h            |  97 +--------------\n>  drivers/cxl/cxlmem.h         |  86 +------------\n>  drivers/cxl/pci.c            |  14 +--\n>  include/cxl/cxl.h            | 226 +++++++++++++++++++++++++++++++++++\n>  tools/testing/cxl/test/mem.c |   3 +-\n>  7 files changed, 274 insertions(+), 196 deletions(-)\n>  create mode 100644 include/cxl/cxl.h\n> \n> diff --git a/drivers/cxl/core/mbox.c b/drivers/cxl/core/mbox.c\n> index fa6dd0c94656..bee84d0101d1 100644\n> --- a/drivers/cxl/core/mbox.c\n> +++ b/drivers/cxl/core/mbox.c\n> @@ -1514,23 +1514,21 @@ int cxl_mailbox_init(struct cxl_mailbox *cxl_mbox, struct device *host)\n>  }\n>  EXPORT_SYMBOL_NS_GPL(cxl_mailbox_init, \"CXL\");\n>  \n> -struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev)\n> +struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev, u64 serial,\n> +\t\t\t\t\t\t u16 dvsec)\n>  {\n>  \tstruct cxl_memdev_state *mds;\n>  \tint rc;\n>  \n> -\tmds = devm_kzalloc(dev, sizeof(*mds), GFP_KERNEL);\n> +\tmds = devm_cxl_dev_state_create(dev, CXL_DEVTYPE_CLASSMEM, serial,\n> +\t\t\t\t\tdvsec, struct cxl_memdev_state, cxlds,\n> +\t\t\t\t\ttrue);\n>  \tif (!mds) {\n>  \t\tdev_err(dev, \"No memory available\\n\");\n>  \t\treturn ERR_PTR(-ENOMEM);\n>  \t}\n>  \n>  \tmutex_init(&mds->event.log_lock);\n> -\tmds->cxlds.dev = dev;\n> -\tmds->cxlds.reg_map.host = dev;\n> -\tmds->cxlds.cxl_mbox.host = dev;\n> -\tmds->cxlds.reg_map.resource = CXL_RESOURCE_NONE;\n> -\tmds->cxlds.type = CXL_DEVTYPE_CLASSMEM;\n>  \n>  \trc = devm_cxl_register_mce_notifier(dev, &mds->mce_notifier);\n>  \tif (rc == -EOPNOTSUPP)\n> diff --git a/drivers/cxl/core/memdev.c b/drivers/cxl/core/memdev.c\n> index af3d0cc65138..22d156f25305 100644\n> --- a/drivers/cxl/core/memdev.c\n> +++ b/drivers/cxl/core/memdev.c\n> @@ -656,6 +656,38 @@ static void detach_memdev(struct work_struct *work)\n>  \n>  static struct lock_class_key cxl_memdev_key;\n>  \n> +static void cxl_dev_state_init(struct cxl_dev_state *cxlds, struct device *dev,\n> +\t\t\t       enum cxl_devtype type, u64 serial, u16 dvsec,\n> +\t\t\t       bool has_mbox)\n> +{\n> +\t*cxlds = (struct cxl_dev_state) {\n> +\t\t.dev = dev,\n> +\t\t.type = type,\n> +\t\t.serial = serial,\n> +\t\t.cxl_dvsec = dvsec,\n> +\t\t.reg_map.host = dev,\n> +\t\t.reg_map.resource = CXL_RESOURCE_NONE,\n> +\t};\n> +\n> +\tif (has_mbox)\n> +\t\tcxlds->cxl_mbox.host = dev;\n> +}\n> +\n> +struct cxl_dev_state *_devm_cxl_dev_state_create(struct device *dev,\n> +\t\t\t\t\t\t enum cxl_devtype type,\n> +\t\t\t\t\t\t u64 serial, u16 dvsec,\n> +\t\t\t\t\t\t size_t size, bool has_mbox)\n> +{\n> +\tstruct cxl_dev_state *cxlds = devm_kzalloc(dev, size, GFP_KERNEL);\n> +\n> +\tif (!cxlds)\n> +\t\treturn NULL;\n> +\n> +\tcxl_dev_state_init(cxlds, dev, type, serial, dvsec, has_mbox);\n\nNit: Having a second function to do the init seems overkill here, especially since cxl_dev_state_init() isn't called outside this\nfunction. I'd fold it into this function instead, but I'm fine with it either way (especially if you were told otherwise before).\n\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> +\treturn cxlds;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(_devm_cxl_dev_state_create, \"CXL\");\n> +\n>  static struct cxl_memdev *cxl_memdev_alloc(struct cxl_dev_state *cxlds,\n>  \t\t\t\t\t   const struct file_operations *fops,\n>  \t\t\t\t\t   const struct cxl_memdev_attach *attach)\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index e1d47062e1d3..3eaa353e430b 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -12,6 +12,7 @@\n>  #include <linux/node.h>\n>  #include <linux/io.h>\n>  #include <linux/range.h>\n> +#include <cxl/cxl.h>\n>  \n>  extern const struct nvdimm_security_ops *cxl_security_ops;\n>  \n> @@ -201,97 +202,6 @@ static inline int ways_to_eiw(unsigned int ways, u8 *eiw)\n>  #define   CXLDEV_MBOX_BG_CMD_COMMAND_VENDOR_MASK GENMASK_ULL(63, 48)\n>  #define CXLDEV_MBOX_PAYLOAD_OFFSET 0x20\n>  \n> -/*\n> - * Using struct_group() allows for per register-block-type helper routines,\n> - * without requiring block-type agnostic code to include the prefix.\n> - */\n> -struct cxl_regs {\n> -\t/*\n> -\t * Common set of CXL Component register block base pointers\n> -\t * @hdm_decoder: CXL 2.0 8.2.5.12 CXL HDM Decoder Capability Structure\n> -\t * @ras: CXL 2.0 8.2.5.9 CXL RAS Capability Structure\n> -\t */\n> -\tstruct_group_tagged(cxl_component_regs, component,\n> -\t\tvoid __iomem *hdm_decoder;\n> -\t\tvoid __iomem *ras;\n> -\t);\n> -\t/*\n> -\t * Common set of CXL Device register block base pointers\n> -\t * @status: CXL 2.0 8.2.8.3 Device Status Registers\n> -\t * @mbox: CXL 2.0 8.2.8.4 Mailbox Registers\n> -\t * @memdev: CXL 2.0 8.2.8.5 Memory Device Registers\n> -\t */\n> -\tstruct_group_tagged(cxl_device_regs, device_regs,\n> -\t\tvoid __iomem *status, *mbox, *memdev;\n> -\t);\n> -\n> -\tstruct_group_tagged(cxl_pmu_regs, pmu_regs,\n> -\t\tvoid __iomem *pmu;\n> -\t);\n> -\n> -\t/*\n> -\t * RCH downstream port specific RAS register\n> -\t * @aer: CXL 3.0 8.2.1.1 RCH Downstream Port RCRB\n> -\t */\n> -\tstruct_group_tagged(cxl_rch_regs, rch_regs,\n> -\t\tvoid __iomem *dport_aer;\n> -\t);\n> -\n> -\t/*\n> -\t * RCD upstream port specific PCIe cap register\n> -\t * @pcie_cap: CXL 3.0 8.2.1.2 RCD Upstream Port RCRB\n> -\t */\n> -\tstruct_group_tagged(cxl_rcd_regs, rcd_regs,\n> -\t\tvoid __iomem *rcd_pcie_cap;\n> -\t);\n> -};\n> -\n> -struct cxl_reg_map {\n> -\tbool valid;\n> -\tint id;\n> -\tunsigned long offset;\n> -\tunsigned long size;\n> -};\n> -\n> -struct cxl_component_reg_map {\n> -\tstruct cxl_reg_map hdm_decoder;\n> -\tstruct cxl_reg_map ras;\n> -};\n> -\n> -struct cxl_device_reg_map {\n> -\tstruct cxl_reg_map status;\n> -\tstruct cxl_reg_map mbox;\n> -\tstruct cxl_reg_map memdev;\n> -};\n> -\n> -struct cxl_pmu_reg_map {\n> -\tstruct cxl_reg_map pmu;\n> -};\n> -\n> -/**\n> - * struct cxl_register_map - DVSEC harvested register block mapping parameters\n> - * @host: device for devm operations and logging\n> - * @base: virtual base of the register-block-BAR + @block_offset\n> - * @resource: physical resource base of the register block\n> - * @max_size: maximum mapping size to perform register search\n> - * @reg_type: see enum cxl_regloc_type\n> - * @component_map: cxl_reg_map for component registers\n> - * @device_map: cxl_reg_maps for device registers\n> - * @pmu_map: cxl_reg_maps for CXL Performance Monitoring Units\n> - */\n> -struct cxl_register_map {\n> -\tstruct device *host;\n> -\tvoid __iomem *base;\n> -\tresource_size_t resource;\n> -\tresource_size_t max_size;\n> -\tu8 reg_type;\n> -\tunion {\n> -\t\tstruct cxl_component_reg_map component_map;\n> -\t\tstruct cxl_device_reg_map device_map;\n> -\t\tstruct cxl_pmu_reg_map pmu_map;\n> -\t};\n> -};\n> -\n>  void cxl_probe_component_regs(struct device *dev, void __iomem *base,\n>  \t\t\t      struct cxl_component_reg_map *map);\n>  void cxl_probe_device_regs(struct device *dev, void __iomem *base,\n> @@ -497,11 +407,6 @@ struct cxl_region_params {\n>  \tresource_size_t cache_size;\n>  };\n>  \n> -enum cxl_partition_mode {\n> -\tCXL_PARTMODE_RAM,\n> -\tCXL_PARTMODE_PMEM,\n> -};\n> -\n>  /*\n>   * Indicate whether this region has been assembled by autodetection or\n>   * userspace assembly. Prevent endpoint decoders outside of automatic\n> diff --git a/drivers/cxl/cxlmem.h b/drivers/cxl/cxlmem.h\n> index ef202b34e5ea..281546de426e 100644\n> --- a/drivers/cxl/cxlmem.h\n> +++ b/drivers/cxl/cxlmem.h\n> @@ -113,8 +113,6 @@ int devm_cxl_dpa_reserve(struct cxl_endpoint_decoder *cxled,\n>  \t\t\t resource_size_t base, resource_size_t len,\n>  \t\t\t resource_size_t skipped);\n>  \n> -#define CXL_NR_PARTITIONS_MAX 2\n> -\n>  struct cxl_dpa_info {\n>  \tu64 size;\n>  \tstruct cxl_dpa_part_info {\n> @@ -373,87 +371,6 @@ struct cxl_security_state {\n>  \tstruct kernfs_node *sanitize_node;\n>  };\n>  \n> -/*\n> - * enum cxl_devtype - delineate type-2 from a generic type-3 device\n> - * @CXL_DEVTYPE_DEVMEM - Vendor specific CXL Type-2 device implementing HDM-D or\n> - *\t\t\t HDM-DB, no requirement that this device implements a\n> - *\t\t\t mailbox, or other memory-device-standard manageability\n> - *\t\t\t flows.\n> - * @CXL_DEVTYPE_CLASSMEM - Common class definition of a CXL Type-3 device with\n> - *\t\t\t   HDM-H and class-mandatory memory device registers\n> - */\n> -enum cxl_devtype {\n> -\tCXL_DEVTYPE_DEVMEM,\n> -\tCXL_DEVTYPE_CLASSMEM,\n> -};\n> -\n> -/**\n> - * struct cxl_dpa_perf - DPA performance property entry\n> - * @dpa_range: range for DPA address\n> - * @coord: QoS performance data (i.e. latency, bandwidth)\n> - * @cdat_coord: raw QoS performance data from CDAT\n> - * @qos_class: QoS Class cookies\n> - */\n> -struct cxl_dpa_perf {\n> -\tstruct range dpa_range;\n> -\tstruct access_coordinate coord[ACCESS_COORDINATE_MAX];\n> -\tstruct access_coordinate cdat_coord[ACCESS_COORDINATE_MAX];\n> -\tint qos_class;\n> -};\n> -\n> -/**\n> - * struct cxl_dpa_partition - DPA partition descriptor\n> - * @res: shortcut to the partition in the DPA resource tree (cxlds->dpa_res)\n> - * @perf: performance attributes of the partition from CDAT\n> - * @mode: operation mode for the DPA capacity, e.g. ram, pmem, dynamic...\n> - */\n> -struct cxl_dpa_partition {\n> -\tstruct resource res;\n> -\tstruct cxl_dpa_perf perf;\n> -\tenum cxl_partition_mode mode;\n> -};\n> -\n> -/**\n> - * struct cxl_dev_state - The driver device state\n> - *\n> - * cxl_dev_state represents the CXL driver/device state.  It provides an\n> - * interface to mailbox commands as well as some cached data about the device.\n> - * Currently only memory devices are represented.\n> - *\n> - * @dev: The device associated with this CXL state\n> - * @cxlmd: The device representing the CXL.mem capabilities of @dev\n> - * @reg_map: component and ras register mapping parameters\n> - * @regs: Parsed register blocks\n> - * @cxl_dvsec: Offset to the PCIe device DVSEC\n> - * @rcd: operating in RCD mode (CXL 3.0 9.11.8 CXL Devices Attached to an RCH)\n> - * @media_ready: Indicate whether the device media is usable\n> - * @dpa_res: Overall DPA resource tree for the device\n> - * @part: DPA partition array\n> - * @nr_partitions: Number of DPA partitions\n> - * @serial: PCIe Device Serial Number\n> - * @type: Generic Memory Class device or Vendor Specific Memory device\n> - * @cxl_mbox: CXL mailbox context\n> - * @cxlfs: CXL features context\n> - */\n> -struct cxl_dev_state {\n> -\tstruct device *dev;\n> -\tstruct cxl_memdev *cxlmd;\n> -\tstruct cxl_register_map reg_map;\n> -\tstruct cxl_regs regs;\n> -\tint cxl_dvsec;\n> -\tbool rcd;\n> -\tbool media_ready;\n> -\tstruct resource dpa_res;\n> -\tstruct cxl_dpa_partition part[CXL_NR_PARTITIONS_MAX];\n> -\tunsigned int nr_partitions;\n> -\tu64 serial;\n> -\tenum cxl_devtype type;\n> -\tstruct cxl_mailbox cxl_mbox;\n> -#ifdef CONFIG_CXL_FEATURES\n> -\tstruct cxl_features_state *cxlfs;\n> -#endif\n> -};\n> -\n>  static inline resource_size_t cxl_pmem_size(struct cxl_dev_state *cxlds)\n>  {\n>  \t/*\n> @@ -858,7 +775,8 @@ int cxl_dev_state_identify(struct cxl_memdev_state *mds);\n>  int cxl_await_media_ready(struct cxl_dev_state *cxlds);\n>  int cxl_enumerate_cmds(struct cxl_memdev_state *mds);\n>  int cxl_mem_dpa_fetch(struct cxl_memdev_state *mds, struct cxl_dpa_info *info);\n> -struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev);\n> +struct cxl_memdev_state *cxl_memdev_state_create(struct device *dev, u64 serial,\n> +\t\t\t\t\t\t u16 dvsec);\n>  void set_exclusive_cxl_commands(struct cxl_memdev_state *mds,\n>  \t\t\t\tunsigned long *cmds);\n>  void clear_exclusive_cxl_commands(struct cxl_memdev_state *mds,\n> diff --git a/drivers/cxl/pci.c b/drivers/cxl/pci.c\n> index 1cf232220873..24179cc702bf 100644\n> --- a/drivers/cxl/pci.c\n> +++ b/drivers/cxl/pci.c\n> @@ -911,25 +911,25 @@ static int cxl_pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n>  \tint rc, pmu_count;\n>  \tunsigned int i;\n>  \tbool irq_avail;\n> +\tu16 dvsec;\n>  \n>  \trc = pcim_enable_device(pdev);\n>  \tif (rc)\n>  \t\treturn rc;\n>  \tpci_set_master(pdev);\n>  \n> -\tmds = cxl_memdev_state_create(&pdev->dev);\n> +\tdvsec = pci_find_dvsec_capability(pdev, PCI_VENDOR_ID_CXL,\n> +\t\t\t\t\t  PCI_DVSEC_CXL_DEVICE);\n> +\tif (!dvsec)\n> +\t\tpci_warn(pdev, \"Device DVSEC not present, skip CXL.mem init\\n\");\n> +\n> +\tmds = cxl_memdev_state_create(&pdev->dev, pci_get_dsn(pdev), dvsec);\n>  \tif (IS_ERR(mds))\n>  \t\treturn PTR_ERR(mds);\n>  \tcxlds = &mds->cxlds;\n>  \tpci_set_drvdata(pdev, cxlds);\n>  \n>  \tcxlds->rcd = is_cxl_restricted(pdev);\n> -\tcxlds->serial = pci_get_dsn(pdev);\n> -\tcxlds->cxl_dvsec = pci_find_dvsec_capability(\n> -\t\tpdev, PCI_VENDOR_ID_CXL, PCI_DVSEC_CXL_DEVICE);\n> -\tif (!cxlds->cxl_dvsec)\n> -\t\tdev_warn(&pdev->dev,\n> -\t\t\t \"Device DVSEC not present, skip CXL.mem init\\n\");\n>  \n>  \trc = cxl_pci_setup_regs(pdev, CXL_REGLOC_RBI_MEMDEV, &map);\n>  \tif (rc)\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> new file mode 100644\n> index 000000000000..13d448686189\n> --- /dev/null\n> +++ b/include/cxl/cxl.h\n> @@ -0,0 +1,226 @@\n> +/* SPDX-License-Identifier: GPL-2.0 */\n> +/* Copyright(c) 2020 Intel Corporation. */\n> +/* Copyright(c) 2025 Advanced Micro Devices, Inc. */\n> +\n> +#ifndef __CXL_CXL_H__\n> +#define __CXL_CXL_H__\n> +\n> +#include <linux/node.h>\n> +#include <linux/ioport.h>\n> +#include <cxl/mailbox.h>\n> +\n> +/**\n> + * enum cxl_devtype - delineate type-2 from a generic type-3 device\n> + * @CXL_DEVTYPE_DEVMEM: Vendor specific CXL Type-2 device implementing HDM-D or\n> + *\t\t\t HDM-DB, no requirement that this device implements a\n> + *\t\t\t mailbox, or other memory-device-standard manageability\n> + *\t\t\t flows.\n> + * @CXL_DEVTYPE_CLASSMEM: Common class definition of a CXL Type-3 device with\n> + *\t\t\t   HDM-H and class-mandatory memory device registers\n> + */\n> +enum cxl_devtype {\n> +\tCXL_DEVTYPE_DEVMEM,\n> +\tCXL_DEVTYPE_CLASSMEM,\n> +};\n> +\n> +struct device;\n> +\n> +/*\n> + * Using struct_group() allows for per register-block-type helper routines,\n> + * without requiring block-type agnostic code to include the prefix.\n> + */\n> +struct cxl_regs {\n> +\t/*\n> +\t * Common set of CXL Component register block base pointers\n> +\t * @hdm_decoder: CXL 2.0 8.2.5.12 CXL HDM Decoder Capability Structure\n> +\t * @ras: CXL 2.0 8.2.5.9 CXL RAS Capability Structure\n> +\t */\n> +\tstruct_group_tagged(cxl_component_regs, component,\n> +\t\tvoid __iomem *hdm_decoder;\n> +\t\tvoid __iomem *ras;\n> +\t);\n> +\t/*\n> +\t * Common set of CXL Device register block base pointers\n> +\t * @status: CXL 2.0 8.2.8.3 Device Status Registers\n> +\t * @mbox: CXL 2.0 8.2.8.4 Mailbox Registers\n> +\t * @memdev: CXL 2.0 8.2.8.5 Memory Device Registers\n> +\t */\n> +\tstruct_group_tagged(cxl_device_regs, device_regs,\n> +\t\tvoid __iomem *status, *mbox, *memdev;\n> +\t);\n> +\n> +\tstruct_group_tagged(cxl_pmu_regs, pmu_regs,\n> +\t\tvoid __iomem *pmu;\n> +\t);\n> +\n> +\t/*\n> +\t * RCH downstream port specific RAS register\n> +\t * @aer: CXL 3.0 8.2.1.1 RCH Downstream Port RCRB\n> +\t */\n> +\tstruct_group_tagged(cxl_rch_regs, rch_regs,\n> +\t\tvoid __iomem *dport_aer;\n> +\t);\n> +\n> +\t/*\n> +\t * RCD upstream port specific PCIe cap register\n> +\t * @pcie_cap: CXL 3.0 8.2.1.2 RCD Upstream Port RCRB\n> +\t */\n> +\tstruct_group_tagged(cxl_rcd_regs, rcd_regs,\n> +\t\tvoid __iomem *rcd_pcie_cap;\n> +\t);\n> +};\n> +\n> +struct cxl_reg_map {\n> +\tbool valid;\n> +\tint id;\n> +\tunsigned long offset;\n> +\tunsigned long size;\n> +};\n> +\n> +struct cxl_component_reg_map {\n> +\tstruct cxl_reg_map hdm_decoder;\n> +\tstruct cxl_reg_map ras;\n> +};\n> +\n> +struct cxl_device_reg_map {\n> +\tstruct cxl_reg_map status;\n> +\tstruct cxl_reg_map mbox;\n> +\tstruct cxl_reg_map memdev;\n> +};\n> +\n> +struct cxl_pmu_reg_map {\n> +\tstruct cxl_reg_map pmu;\n> +};\n> +\n> +/**\n> + * struct cxl_register_map - DVSEC harvested register block mapping parameters\n> + * @host: device for devm operations and logging\n> + * @base: virtual base of the register-block-BAR + @block_offset\n> + * @resource: physical resource base of the register block\n> + * @max_size: maximum mapping size to perform register search\n> + * @reg_type: see enum cxl_regloc_type\n> + * @component_map: cxl_reg_map for component registers\n> + * @device_map: cxl_reg_maps for device registers\n> + * @pmu_map: cxl_reg_maps for CXL Performance Monitoring Units\n> + */\n> +struct cxl_register_map {\n> +\tstruct device *host;\n> +\tvoid __iomem *base;\n> +\tresource_size_t resource;\n> +\tresource_size_t max_size;\n> +\tu8 reg_type;\n> +\tunion {\n> +\t\tstruct cxl_component_reg_map component_map;\n> +\t\tstruct cxl_device_reg_map device_map;\n> +\t\tstruct cxl_pmu_reg_map pmu_map;\n> +\t};\n> +};\n> +\n> +/**\n> + * struct cxl_dpa_perf - DPA performance property entry\n> + * @dpa_range: range for DPA address\n> + * @coord: QoS performance data (i.e. latency, bandwidth)\n> + * @cdat_coord: raw QoS performance data from CDAT\n> + * @qos_class: QoS Class cookies\n> + */\n> +struct cxl_dpa_perf {\n> +\tstruct range dpa_range;\n> +\tstruct access_coordinate coord[ACCESS_COORDINATE_MAX];\n> +\tstruct access_coordinate cdat_coord[ACCESS_COORDINATE_MAX];\n> +\tint qos_class;\n> +};\n> +\n> +enum cxl_partition_mode {\n> +\tCXL_PARTMODE_RAM,\n> +\tCXL_PARTMODE_PMEM,\n> +};\n> +\n> +/**\n> + * struct cxl_dpa_partition - DPA partition descriptor\n> + * @res: shortcut to the partition in the DPA resource tree (cxlds->dpa_res)\n> + * @perf: performance attributes of the partition from CDAT\n> + * @mode: operation mode for the DPA capacity, e.g. ram, pmem, dynamic...\n> + */\n> +struct cxl_dpa_partition {\n> +\tstruct resource res;\n> +\tstruct cxl_dpa_perf perf;\n> +\tenum cxl_partition_mode mode;\n> +};\n> +\n> +#define CXL_NR_PARTITIONS_MAX 2\n> +\n> +/**\n> + * struct cxl_dev_state - The driver device state\n> + *\n> + * cxl_dev_state represents the CXL driver/device state.  It provides an\n> + * interface to mailbox commands as well as some cached data about the device.\n> + * Currently only memory devices are represented.\n> + *\n> + * @dev: The device associated with this CXL state\n> + * @cxlmd: The device representing the CXL.mem capabilities of @dev\n> + * @reg_map: component and ras register mapping parameters\n> + * @regs: Parsed register blocks\n> + * @cxl_dvsec: Offset to the PCIe device DVSEC\n> + * @rcd: operating in RCD mode (CXL 3.0 9.11.8 CXL Devices Attached to an RCH)\n> + * @media_ready: Indicate whether the device media is usable\n> + * @dpa_res: Overall DPA resource tree for the device\n> + * @part: DPA partition array\n> + * @nr_partitions: Number of DPA partitions\n> + * @serial: PCIe Device Serial Number\n> + * @type: Generic Memory Class device or Vendor Specific Memory device\n> + * @cxl_mbox: CXL mailbox context\n> + * @cxlfs: CXL features context\n> + */\n> +struct cxl_dev_state {\n> +\t/* public for Type2 drivers */\n> +\tstruct device *dev;\n> +\tstruct cxl_memdev *cxlmd;\n> +\n> +\t/* private for Type2 drivers */\n> +\tstruct cxl_register_map reg_map;\n> +\tstruct cxl_regs regs;\n> +\tint cxl_dvsec;\n> +\tbool rcd;\n> +\tbool media_ready;\n> +\tstruct resource dpa_res;\n> +\tstruct cxl_dpa_partition part[CXL_NR_PARTITIONS_MAX];\n> +\tunsigned int nr_partitions;\n> +\tu64 serial;\n> +\tenum cxl_devtype type;\n> +\tstruct cxl_mailbox cxl_mbox;\n> +#ifdef CONFIG_CXL_FEATURES\n> +\tstruct cxl_features_state *cxlfs;\n> +#endif\n> +};\n> +\n> +struct cxl_dev_state *_devm_cxl_dev_state_create(struct device *dev,\n> +\t\t\t\t\t\t enum cxl_devtype type,\n> +\t\t\t\t\t\t u64 serial, u16 dvsec,\n> +\t\t\t\t\t\t size_t size, bool has_mbox);\n> +\n> +/**\n> + * cxl_dev_state_create - safely create and cast a cxl dev state embedded in a\n> + * driver specific struct.\n> + *\n> + * @parent: device behind the request\n> + * @type: CXL device type\n> + * @serial: device identification\n> + * @dvsec: dvsec capability offset\n> + * @drv_struct: driver struct embedding a cxl_dev_state struct\n> + * @member: drv_struct member as cxl_dev_state\n> + * @mbox: true if mailbox supported\n> + *\n> + * Returns a pointer to the drv_struct allocated and embedding a cxl_dev_state\n> + * struct initialized.\n> + *\n> + * Introduced for Type2 driver support.\n> + */\n> +#define devm_cxl_dev_state_create(parent, type, serial, dvsec, drv_struct, member, mbox)\t\\\n> +\t({\t\t\t\t\t\t\t\t\t\t\\\n> +\t\tstatic_assert(__same_type(struct cxl_dev_state,\t\t\t\t\\\n> +\t\t\t      ((drv_struct *)NULL)->member));\t\t\t\t\\\n> +\t\tstatic_assert(offsetof(drv_struct, member) == 0);\t\t\t\\\n> +\t\t(drv_struct *)_devm_cxl_dev_state_create(parent, type, serial, dvsec,\t\\\n> +\t\t\t\t\t\t      sizeof(drv_struct), mbox);\t\\\n> +\t})\n> +#endif /* __CXL_CXL_H__ */\n> diff --git a/tools/testing/cxl/test/mem.c b/tools/testing/cxl/test/mem.c\n> index cb87e8c0e63c..79f42f4474d4 100644\n> --- a/tools/testing/cxl/test/mem.c\n> +++ b/tools/testing/cxl/test/mem.c\n> @@ -1716,7 +1716,7 @@ static int cxl_mock_mem_probe(struct platform_device *pdev)\n>  \tif (rc)\n>  \t\treturn rc;\n>  \n> -\tmds = cxl_memdev_state_create(dev);\n> +\tmds = cxl_memdev_state_create(dev, pdev->id + 1, 0);\n>  \tif (IS_ERR(mds))\n>  \t\treturn PTR_ERR(mds);\n>  \n> @@ -1732,7 +1732,6 @@ static int cxl_mock_mem_probe(struct platform_device *pdev)\n>  \tmds->event.buf = (struct cxl_get_event_payload *) mdata->event_buf;\n>  \tINIT_DELAYED_WORK(&mds->security.poll_dwork, cxl_mockmem_sanitize_work);\n>  \n> -\tcxlds->serial = pdev->id + 1;\n>  \tif (is_rcd(pdev))\n>  \t\tcxlds->rcd = true;\n>  \n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> A Type2 device configured by the BIOS can already have its HDM\n> committed. Add a cxl_get_committed_decoder() function for cheking\n> so after memdev creation. A CXL region should have been created\n> during memdev initialization, therefore a Type2 driver can ask for\n> such a region for working with the HPA. If the HDM is not committed,\n> a Type2 driver will create the region after obtaining proper HPA\n> and DPA space.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> ---\n>  drivers/cxl/core/hdm.c | 39 +++++++++++++++++++++++++++++++++++++++\n>  include/cxl/cxl.h      |  3 +++\n>  2 files changed, 42 insertions(+)\n> \n> diff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\n> index 6e516c69b2d2..a172ce4e9b19 100644\n> --- a/drivers/cxl/core/hdm.c\n> +++ b/drivers/cxl/core/hdm.c\n> @@ -686,6 +686,45 @@ int cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size)\n>  \treturn devm_add_action_or_reset(&port->dev, cxl_dpa_release, cxled);\n>  }\n>  \n> +static int find_committed_endpoint_decoder(struct device *dev, const void *data)\n> +{\n> +\tstruct cxl_endpoint_decoder *cxled;\n> +\tstruct cxl_port *port;\n> +\n> +\tif (!is_endpoint_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxled = to_cxl_endpoint_decoder(dev);\n> +\tport = cxled_to_port(cxled);\n> +\n> +\treturn cxled->cxld.id == port->hdm_end;\n\nIs this the way you're supposed to check if a decoder is committed? The doc comment for @hdm_end in\nstruct cxl_port says it's just the last allocated decoder. If allocated decoders are always committed then\nI'm fine with this, otherwise I think you'd want to a register read or something to find the commit state.\n> +}\n> +\n> +struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t\t       struct cxl_region **cxlr)\n> +{\n> +\tstruct cxl_port *endpoint = cxlmd->endpoint;\n> +\tstruct cxl_endpoint_decoder *cxled;\n> +\tstruct device *cxled_dev;\n> +\n> +\tif (!endpoint)\n> +\t\treturn NULL;\n> +\n> +\tguard(rwsem_read)(&cxl_rwsem.dpa);\n> +\tcxled_dev = device_find_child(&endpoint->dev, NULL,\n> +\t\t\t\t      find_committed_endpoint_decoder);\n> +\n> +\tif (!cxled_dev)\n> +\t\treturn NULL;\n> +\n> +\tcxled = to_cxl_endpoint_decoder(cxled_dev);\n> +\t*cxlr = cxled->cxld.region;\n> +\n> +\tput_device(cxled_dev);\n> +\treturn cxled;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_get_committed_decoder, \"CXL\");\n> +\n>  static void cxld_set_interleave(struct cxl_decoder *cxld, u32 *ctrl)\n>  {\n>  \tu16 eig;\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 6f8d365067af..928276dba952 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -249,4 +249,7 @@ int cxl_map_component_regs(const struct cxl_register_map *map,\n>  int cxl_set_capacity(struct cxl_dev_state *cxlds, u64 capacity);\n>  struct cxl_memdev *devm_cxl_add_memdev(struct cxl_dev_state *cxlds,\n>  \t\t\t\t       const struct cxl_memdev_attach *attach);\n> +struct cxl_region;\n> +struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t\t       struct cxl_region **cxlr);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> This patchset should be applied on the cxl next branch using the base\n> specified at the end of this cover letter.\n> \n> Dependencies on Dan's work has gone and also on Terry's as the only\n> patch required is now in next. The other dependency is on Smita patchset\n> but it does not exist such a dependency as that work will not avoid the\n> problem with Type2 and DAX/hmem if soft reserved memory. This needs to\n> be solved by the BIOS and Type2 UEFI driver for populating the CXL.mem\n> range as EFI_RESERVED_TYPE instead of default EFI_CONVENTIONAL_MEMORY\n> with the EFI_MEMORY_SP attribute. There exists though a dependency on\n> one Smita's patches:\n> \n> [PATCH v5 3/7] cxl/region: Skip decoder reset on detach for autodiscovered regions\n> \n> This is needed for the default behaviour with current BIOS configuration\n> where the HDM Type2 decoders will be kept unreset when driver unloads.\n> This is the main change introduced in v23: committed decoders will not\n> be reset. Previous v22 functionality supported first driver load finding\n> committed decoders but resetting them at unload and supporting\n> uncommitted decoders in next driver loads. This will be suported in\n> follow-up works.\n> \n> v23 changes:\n> \n>   patch 11: fixing minor issues and droping change in\n> \t    should_emulate_decoders (Jonathan Cameron)\n> \n>   patch13: refactoring unregister_region for safety type in Type2 API\n> \n>   sfc changes: slight modifications to error path\n> \n\nThis cover letter is really long, I'd remove the change logs for anything more\nthan 3 revisions back (assuming a v24 is needed). After that you could leave\na lore link for older revisions if you want, but it's not needed imo.\nAlso, feel free to add my Reviewed-by for anything I didn't leave a comment on\n(felt I should cut down on the mail).\n\nThanks,\nBen\n\n\n---\n\nOn 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Region creation involves finding available DPA (device-physical-address)\n> capacity to map into HPA (host-physical-address) space.\n> \n> In order to support CXL Type2 devices, define an API, cxl_request_dpa(),\n> that tries to allocate the DPA memory the driver requires to operate.The\n> memory requested should not be bigger than the max available HPA obtained\n> previously with cxl_get_hpa_freespace().\n> \n> Based on https://lore.kernel.org/linux-cxl/168592158743.1948938.7622563891193802610.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n> ---\n>  drivers/cxl/core/hdm.c | 84 ++++++++++++++++++++++++++++++++++++++++++\n>  drivers/cxl/cxl.h      |  1 +\n>  include/cxl/cxl.h      |  5 +++\n>  3 files changed, 90 insertions(+)\n> \n> diff --git a/drivers/cxl/core/hdm.c b/drivers/cxl/core/hdm.c\n> index a172ce4e9b19..d60a697f12cc 100644\n> --- a/drivers/cxl/core/hdm.c\n> +++ b/drivers/cxl/core/hdm.c\n> @@ -3,6 +3,7 @@\n>  #include <linux/seq_file.h>\n>  #include <linux/device.h>\n>  #include <linux/delay.h>\n> +#include <cxl/cxl.h>\n>  \n>  #include \"cxlmem.h\"\n>  #include \"core.h\"\n> @@ -546,6 +547,12 @@ bool cxl_resource_contains_addr(const struct resource *res, const resource_size_\n>  \treturn resource_contains(res, &_addr);\n>  }\n>  \n> +/**\n> + * cxl_dpa_free - release DPA (Device Physical Address)\n> + * @cxled: endpoint decoder linked to the DPA\n> + *\n> + * Returns 0 or error.\n> + */\n>  int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n>  {\n>  \tstruct cxl_port *port = cxled_to_port(cxled);\n> @@ -572,6 +579,7 @@ int cxl_dpa_free(struct cxl_endpoint_decoder *cxled)\n>  \tdevm_cxl_dpa_release(cxled);\n>  \treturn 0;\n>  }\n> +EXPORT_SYMBOL_NS_GPL(cxl_dpa_free, \"CXL\");\n>  \n>  int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n>  \t\t     enum cxl_partition_mode mode)\n> @@ -603,6 +611,82 @@ int cxl_dpa_set_part(struct cxl_endpoint_decoder *cxled,\n>  \treturn 0;\n>  }\n>  \n> +static int find_free_decoder(struct device *dev, const void *data)\n> +{\n> +\tstruct cxl_endpoint_decoder *cxled;\n> +\tstruct cxl_port *port;\n> +\n> +\tif (!is_endpoint_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxled = to_cxl_endpoint_decoder(dev);\n> +\tport = cxled_to_port(cxled);\n> +\n> +\treturn cxled->cxld.id == (port->hdm_end + 1);\n> +}\n> +\n> +static struct cxl_endpoint_decoder *\n> +cxl_find_free_decoder(struct cxl_memdev *cxlmd)\n> +{\n> +\tstruct cxl_port *endpoint = cxlmd->endpoint;\n> +\tstruct device *dev;\n> +\n> +\tguard(rwsem_read)(&cxl_rwsem.dpa);\n> +\tdev = device_find_child(&endpoint->dev, NULL,\n> +\t\t\t\tfind_free_decoder);\n> +\tif (!dev)\n> +\t\treturn NULL;\n> +\n> +\treturn to_cxl_endpoint_decoder(dev);\n> +}\n> +\n> +/**\n> + * cxl_request_dpa - search and reserve DPA given input constraints\n> + * @cxlmd: memdev with an endpoint port with available decoders\n> + * @mode: CXL partition mode (ram vs pmem)\n> + * @alloc: dpa size required\n> + *\n> + * Returns a pointer to a 'struct cxl_endpoint_decoder' on success or\n> + * an errno encoded pointer on failure.\n> + *\n> + * Given that a region needs to allocate from limited HPA capacity it\n> + * may be the case that a device has more mappable DPA capacity than\n> + * available HPA. The expectation is that @alloc is a driver known\n> + * value based on the device capacity but which could not be fully\n> + * available due to HPA constraints.\n> + *\n> + * Returns a pinned cxl_decoder with at least @alloc bytes of capacity\n> + * reserved, or an error pointer. The caller is also expected to own the\n> + * lifetime of the memdev registration associated with the endpoint to\n> + * pin the decoder registered as well.\n> + */\n> +struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t     enum cxl_partition_mode mode,\n> +\t\t\t\t\t     resource_size_t alloc)\n> +{\n> +\tint rc;\n> +\n> +\tif (!IS_ALIGNED(alloc, SZ_256M))\n> +\t\treturn ERR_PTR(-EINVAL);\n> +\n> +\tstruct cxl_endpoint_decoder *cxled __free(put_cxled) =\n> +\t\tcxl_find_free_decoder(cxlmd);\n> +\n> +\tif (!cxled)\n> +\t\treturn ERR_PTR(-ENODEV);\n> +\n> +\trc = cxl_dpa_set_part(cxled, mode);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n> +\n> +\trc = cxl_dpa_alloc(cxled, alloc);\n> +\tif (rc)\n> +\t\treturn ERR_PTR(rc);\n\nShould cxl_dpa_set_part() be unwound here, or does it not matter? If it doesn't matter:\nReviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n> +\n> +\treturn no_free_ptr(cxled);\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_request_dpa, \"CXL\");\n> +\n>  static int __cxl_dpa_alloc(struct cxl_endpoint_decoder *cxled, u64 size)\n>  {\n>  \tstruct cxl_memdev *cxlmd = cxled_to_memdev(cxled);\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index d1b010e5e1d0..2b1f7d687a0e 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -667,6 +667,7 @@ struct cxl_root *find_cxl_root(struct cxl_port *port);\n>  \n>  DEFINE_FREE(put_cxl_root, struct cxl_root *, if (_T) put_device(&_T->port.dev))\n>  DEFINE_FREE(put_cxl_port, struct cxl_port *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n> +DEFINE_FREE(put_cxled, struct cxl_endpoint_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxld.dev))\n>  DEFINE_FREE(put_cxl_root_decoder, struct cxl_root_decoder *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->cxlsd.cxld.dev))\n>  DEFINE_FREE(put_cxl_region, struct cxl_region *, if (!IS_ERR_OR_NULL(_T)) put_device(&_T->dev))\n>  \n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 783ad570a6eb..4802371db00e 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -7,6 +7,7 @@\n>  \n>  #include <linux/node.h>\n>  #include <linux/ioport.h>\n> +#include <linux/range.h>\n>  #include <cxl/mailbox.h>\n>  \n>  /**\n> @@ -276,4 +277,8 @@ struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n>  \t\t\t\t\t       unsigned long flags,\n>  \t\t\t\t\t       resource_size_t *max);\n>  void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n> +struct cxl_endpoint_decoder *cxl_request_dpa(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t     enum cxl_partition_mode mode,\n> +\t\t\t\t\t     resource_size_t alloc);\n> +int cxl_dpa_free(struct cxl_endpoint_decoder *cxled);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\n\n\nOn 2/19/2026 4:40 AM, Alejandro Lucero Palau wrote:\n> \n> On 2/11/26 22:11, Cheatham, Benjamin wrote:\n>> On 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n>>> From: Alejandro Lucero <alucerop@amd.com>\n>>>\n>>> Region creation based on Type3 devices is triggered from user space\n>>> allowing memory combination through interleaving.\n>>>\n>>> In preparation for kernel driven region creation, that is Type2 drivers\n>>> triggering region creation backed with its advertised CXL memory, factor\n>>> out a common helper from the user-sysfs region setup for interleave ways.\n>>>\n>>> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n>>> Reviewed-by: Zhi Wang <zhiw@nvidia.com>\n>>> Reviewed-by: Dave Jiang <dave.jiang@intel.com>\n>>> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n>>> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n>>> Reviewed-by: Alison Schofield <alison.schofield@intel.com>\n>>> ---\n>>>  drivers/cxl/core/region.c | 43 ++++++++++++++++++++++++---------------\n>>>  1 file changed, 27 insertions(+), 16 deletions(-)\n>>>\n>>> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n>>> index f53b2e9fd9e6..ece1d3df7cf1 100644\n>>> --- a/drivers/cxl/core/region.c\n>>> +++ b/drivers/cxl/core/region.c\n>>> @@ -485,22 +485,14 @@ static ssize_t interleave_ways_show(struct device *dev,\n>>>   static const struct attribute_group *get_cxl_region_target_group(void);\n>>>  -static ssize_t interleave_ways_store(struct device *dev,\n>>> - struct device_attribute *attr,\n>>> - const char *buf, size_t len)\n>>> +static int set_interleave_ways(struct cxl_region *cxlr, int val)\n>> @val should probably stay an unsigned int. You pass an unsigned int in the sysfs function, and the\n>> function was originally coded with that in mind (same with @save below).\n> \n> Good catch. I wonder if I should just change the way the value is obtained, using kstrtoint instead of kstrtouint, as those values are used for cxl_region_params fields defined as int. In other words, it seems doing that simpler than changing all the other places you mention and the structs involved. I can not see a reason for using unsigned int so I think I will follow that approach. Tell me if you think otherwise.\n> \n\nIf I had to guess unsigned int was used because a negative interleave granularity/ways makes no sense. I think your suggestion is fine though since no one\nin their right mind would give anything but a (relatively) small and positive value for these.\n\nThanks,\nBen\n\n> \n> Thank you\n> \n> \n>> With that cleaned up:\n>> Reviewed-by: Ben Cheatham <benjamin.cheatham@amd.com>\n>>\n>>>  {\n>>> - struct cxl_root_decoder *cxlrd = to_cxl_root_decoder(dev->parent);\n>>> + struct cxl_root_decoder *cxlrd = to_cxl_root_decoder(cxlr->dev.parent);\n>>>  struct cxl_decoder *cxld = &cxlrd->cxlsd.cxld;\n>>> - struct cxl_region *cxlr = to_cxl_region(dev);\n>>>  struct cxl_region_params *p = &cxlr->params;\n>>> - unsigned int val, save;\n>>> - int rc;\n>>> + int save, rc;\n>>>  u8 iw;\n>>>  - rc = kstrtouint(buf, 0, &val);\n>>> - if (rc)\n>>> - return rc;\n>>> -\n>>>  rc = ways_to_eiw(val, &iw);\n>>>  if (rc)\n>>>  return rc;\n>>> @@ -515,9 +507,7 @@ static ssize_t interleave_ways_store(struct device *dev,\n>>>  return -EINVAL;\n>>>  }\n>>>  - ACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n>>> - if ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n>>> - return rc;\n>>> + lockdep_assert_held_write(&cxl_rwsem.region);\n>>>   if (p->state >= CXL_CONFIG_INTERLEAVE_ACTIVE)\n>>>  return -EBUSY;\n>>> @@ -525,10 +515,31 @@ static ssize_t interleave_ways_store(struct device *dev,\n>>>  save = p->interleave_ways;\n>>>  p->interleave_ways = val;\n>>>  rc = sysfs_update_group(&cxlr->dev.kobj, get_cxl_region_target_group());\n>>> - if (rc) {\n>>> + if (rc)\n>>>  p->interleave_ways = save;\n>>> +\n>>> + return rc;\n>>> +}\n>>> +\n>>> +static ssize_t interleave_ways_store(struct device *dev,\n>>> + struct device_attribute *attr,\n>>> + const char *buf, size_t len)\n>>> +{\n>>> + struct cxl_region *cxlr = to_cxl_region(dev);\n>>> + unsigned int val;\n>>> + int rc;\n>>> +\n>>> + rc = kstrtouint(buf, 0, &val);\n>>> + if (rc)\n>>> + return rc;\n>>> +\n>>> + ACQUIRE(rwsem_write_kill, rwsem)(&cxl_rwsem.region);\n>>> + if ((rc = ACQUIRE_ERR(rwsem_write_kill, &rwsem)))\n>>> + return rc;\n>>> +\n>>> + rc = set_interleave_ways(cxlr, val);\n>>> + if (rc)\n>>>  return rc;\n>>> - }\n>>>   return len;\n>>>  }\n\n\n\n---\n\nOn 2/19/2026 3:58 AM, Alejandro Lucero Palau wrote:\n> \n> On 2/11/26 22:10, Cheatham, Benjamin wrote:\n>> On 2/1/2026 9:54 AM, alejandro.lucero-palau@amd.com wrote:\n>>> From: Alejandro Lucero <alucerop@amd.com>\n>>>\n>>> CXL region creation involves allocating capacity from Device Physical\n>>> Address (DPA) and assigning it to decode a given Host Physical Address\n>>> (HPA). Before determining how much DPA to allocate the amount of available\n>>> HPA must be determined. Also, not all HPA is created equal, some HPA\n>>> targets RAM, some targets PMEM, some is prepared for device-memory flows\n>>> like HDM-D and HDM-DB, and some is HDM-H (host-only).\n>>>\n>>> In order to support Type2 CXL devices, wrap all of those concerns into\n>>> an API that retrieves a root decoder (platform CXL window) that fits the\n>>> specified constraints and the capacity available for a new region.\n>>>\n>>> Add a complementary function for releasing the reference to such root\n>>> decoder.\n>>>\n>>> Based on https://lore.kernel.org/linux-cxl/168592159290.1948938.13522227102445462976.stgit@dwillia2-xfh.jf.intel.com/\n>>>\n>>> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n>>> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n>>> ---\n>>>  drivers/cxl/core/region.c | 164 ++++++++++++++++++++++++++++++++++++++\n>>>  drivers/cxl/cxl.h | 3 +\n>>>  include/cxl/cxl.h | 6 ++\n>>>  3 files changed, 173 insertions(+)\n>>>\n>>> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n>>> index 954b8fcdbac6..bdefd088f5f1 100644\n>>> --- a/drivers/cxl/core/region.c\n>>> +++ b/drivers/cxl/core/region.c\n>>> @@ -705,6 +705,170 @@ static int free_hpa(struct cxl_region *cxlr)\n>>>  return 0;\n>>>  }\n>>>  +struct cxlrd_max_context {\n>>> + struct device * const *host_bridges;\n>>> + int interleave_ways;\n>>> + unsigned long flags;\n>>> + resource_size_t max_hpa;\n>>> + struct cxl_root_decoder *cxlrd;\n>>> +};\n>>> +\n>>> +static int find_max_hpa(struct device *dev, void *data)\n>>> +{\n>>> + struct cxlrd_max_context *ctx = data;\n>>> + struct cxl_switch_decoder *cxlsd;\n>>> + struct cxl_root_decoder *cxlrd;\n>>> + struct resource *res, *prev;\n>>> + struct cxl_decoder *cxld;\n>>> + resource_size_t free = 0;\n>>> + resource_size_t max;\n>>> + int found = 0;\n>>> +\n>>> + if (!is_root_decoder(dev))\n>>> + return 0;\n>>> +\n>>> + cxlrd = to_cxl_root_decoder(dev);\n>>> + cxlsd = &cxlrd->cxlsd;\n>>> + cxld = &cxlsd->cxld;\n>>> +\n>>> + if ((cxld->flags & ctx->flags) != ctx->flags) {\n>>> + dev_dbg(dev, \"flags not matching: %08lx vs %08lx\\n\",\n>>> + cxld->flags, ctx->flags);\n>>> + return 0;\n>>> + }\n>>> +\n>>> + for (int i = 0; i < ctx->interleave_ways; i++) {\n>>> + for (int j = 0; j < ctx->interleave_ways; j++) {\n>>> + if (ctx->host_bridges[i] == cxlsd->target[j]->dport_dev) {\n>>> + found++;\n>>> + break;\n>>> + }\n>>> + }\n>>> + }\n>> This may be over complicated. I'm not quite sure how it works (I'm just slow today I guess), but I understand\n>> what the intention is based on the debug print below. My issue is that ctx->host_bridges is only set to 1 host\n>> bridge (endpoint->host_bridge) in cxl_get_hpa_freespace(), which is the only caller of this function. At that\n>> point, why have the outer loop at all? At that point, you could also simplify ctx->host_bridges to only\n>> be a struct device * const.\n>>\n>> Maybe this gets called elsewhere later on in the series? I haven't looked at the rest yet. If I'm wrong, then\n>> I'd probably add a comment saying what the cxlsd->target[] entries are supposed to be pointing at.\n> \n> \n> Hi Ben,\n> \n> \n> I do remember this one.\n> \n> \n> Dan's original patches had this support for interleaving, then I removed it as the case for Type2 and interleaving is quite unlikely, at least right now and likely in the near future. But I was told why do not support it as it was trivial to do so. FWIW, If I think only about the use case coming with the patchset, I agree with you, but because those previous discussions, I think I have to leave it.\n> \n\nI'm fine with that, but I would at least do the fix with the decoder position in 19/22 and make a note that the\ninterleave_ways parameter in cxl_get_hpa_freespace() below is currently unused (unless I'm misunderstanding\nthe endpoint->host_bridge member).\n\nThat way, the support is mostly there and just requires a small, previously noted, addition to enable. If you're\nfine with that then feel free to add my Reviewed-by after implementing in v24.\n\nThanks,\nBen\n\n> \n> Thank you\n> \n> \n>>> +\n>>> + if (found != ctx->interleave_ways) {\n>>> + dev_dbg(dev,\n>>> + \"Not enough host bridges. Found %d for %d interleave ways requested\\n\",\n>>> + found, ctx->interleave_ways);\n>>> + return 0;\n>>> + }\n>>> +\n>>> + /*\n>>> + * Walk the root decoder resource range relying on cxl_rwsem.region to\n>>> + * preclude sibling arrival/departure and find the largest free space\n>>> + * gap.\n>>> + */\n>>> + lockdep_assert_held_read(&cxl_rwsem.region);\n>>> + res = cxlrd->res->child;\n>>> +\n>>> + /* With no resource child the whole parent resource is available */\n>>> + if (!res)\n>>> + max = resource_size(cxlrd->res);\n>>> + else\n>>> + max = 0;\n>>> +\n>>> + for (prev = NULL; res; prev = res, res = res->sibling) {\n>>> + if (!prev && res->start == cxlrd->res->start &&\n>>> + res->end == cxlrd->res->end) {\n>>> + max = resource_size(cxlrd->res);\n>>> + break;\n>>> + }\n>>> + /*\n>>> + * Sanity check for preventing arithmetic problems below as a\n>>> + * resource with size 0 could imply using the end field below\n>>> + * when set to unsigned zero - 1 or all f in hex.\n>>> + */\n>>> + if (prev && !resource_size(prev))\n>>> + continue;\n>>> +\n>>> + if (!prev && res->start > cxlrd->res->start) {\n>>> + free = res->start - cxlrd->res->start;\n>>> + max = max(free, max);\n>>> + }\n>>> + if (prev && res->start > prev->end + 1) {\n>>> + free = res->start - prev->end + 1;\n>>> + max = max(free, max);\n>>> + }\n>>> + }\n>>> +\n>>> + if (prev && prev->end + 1 < cxlrd->res->end + 1) {\n>>> + free = cxlrd->res->end + 1 - prev->end + 1;\n>>> + max = max(free, max);\n>>> + }\n>>> +\n>>> + dev_dbg(cxlrd_dev(cxlrd), \"found %pa bytes of free space\\n\", &max);\n>>> + if (max > ctx->max_hpa) {\n>>> + if (ctx->cxlrd)\n>>> + put_device(cxlrd_dev(ctx->cxlrd));\n>>> + get_device(cxlrd_dev(cxlrd));\n>>> + ctx->cxlrd = cxlrd;\n>>> + ctx->max_hpa = max;\n>>> + }\n>>> + return 0;\n>>> +}\n>>> +\n>>> +/**\n>>> + * cxl_get_hpa_freespace - find a root decoder with free capacity per constraints\n>>> + * @cxlmd: the mem device requiring the HPA\n>>> + * @interleave_ways: number of entries in @host_bridges\n>>> + * @flags: CXL_DECODER_F flags for selecting RAM vs PMEM, and Type2 device\n>>> + * @max_avail_contig: output parameter of max contiguous bytes available in the\n>>> + * returned decoder\n>>> + *\n>>> + * Returns a pointer to a struct cxl_root_decoder\n>>> + *\n>>> + * The return tuple of a 'struct cxl_root_decoder' and 'bytes available given\n>>> + * in (@max_avail_contig))' is a point in time snapshot. If by the time the\n>>> + * caller goes to use this decoder and its capacity is reduced then caller needs\n>>> + * to loop and retry.\n>>> + *\n>>> + * The returned root decoder has an elevated reference count that needs to be\n>>> + * put with cxl_put_root_decoder(cxlrd).\n>>> + */\n>>> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n>>> + int interleave_ways,\n>>> + unsigned long flags,\n>>> + resource_size_t *max_avail_contig)\n>>> +{\n>>> + struct cxlrd_max_context ctx = {\n>>> + .flags = flags,\n>>> + .interleave_ways = interleave_ways,\n>>> + };\n>>> + struct cxl_port *root_port;\n>>> + struct cxl_port *endpoint;\n>>> +\n>>> + endpoint = cxlmd->endpoint;\n>>> + if (!endpoint) {\n>>> + dev_dbg(&cxlmd->dev, \"endpoint not linked to memdev\\n\");\n>>> + return ERR_PTR(-ENXIO);\n>>> + }\n>>> +\n>>> + ctx.host_bridges = &endpoint->host_bridge;\n>> Mentioned earlier, interleave_ways is effectively hardcoded to 1 (unless I'm misunderstanding\n>> something). I think what you want here is to go to the CXL root and pass in the children (i.e. host bridges)?\n>> I'm not sure of what the fix is to get the intended behavior.\n>>\n>> It may be worth getting rid of the interleave_ways portion of this function and\n>> add it later when someone needs it. You could also explain it's hard coded to 1/unused\n>> in the doc comment if you know of an immediate need for it.\n>>\n>>> +\n>>> + struct cxl_root *root __free(put_cxl_root) = find_cxl_root(endpoint);\n>>> + if (!root) {\n>>> + dev_dbg(&endpoint->dev, \"endpoint is not related to a root port\\n\");\n>>> + return ERR_PTR(-ENXIO);\n>>> + }\n>>> +\n>>> + root_port = &root->port;\n>>> + scoped_guard(rwsem_read, &cxl_rwsem.region)\n>>> + device_for_each_child(&root_port->dev, &ctx, find_max_hpa);\n>> Can just use a guard() here.\n>>\n>>> +\n>>> + if (!ctx.cxlrd)\n>>> + return ERR_PTR(-ENOMEM);\n>>> +\n>>> + *max_avail_contig = ctx.max_hpa;\n>>> + return ctx.cxlrd;\n>>> +}\n>>> +EXPORT_SYMBOL_NS_GPL(cxl_get_hpa_freespace, \"CXL\");\n>>> +\n>>> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd)\n>>> +{\n>>> + put_device(cxlrd_dev(cxlrd));\n>>> +}\n>>> +EXPORT_SYMBOL_NS_GPL(cxl_put_root_decoder, \"CXL\");\n>>> +\n>>>  static ssize_t size_store(struct device *dev, struct device_attribute *attr,\n>>>  const char *buf, size_t len)\n>>>  {\n>>> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n>>> index 944c5d1ccceb..c7d9b2c2908f 100644\n>>> --- a/drivers/cxl/cxl.h\n>>> +++ b/drivers/cxl/cxl.h\n>>> @@ -706,6 +706,9 @@ struct cxl_root_decoder *to_cxl_root_decoder(struct device *dev);\n>>>  struct cxl_switch_decoder *to_cxl_switch_decoder(struct device *dev);\n>>>  struct cxl_endpoint_decoder *to_cxl_endpoint_decoder(struct device *dev);\n>>>  bool is_root_decoder(struct device *dev);\n>>> +\n>>> +#define cxlrd_dev(cxlrd) (&(cxlrd)->cxlsd.cxld.dev)\n>>> +\n>>>  bool is_switch_decoder(struct device *dev);\n>>>  bool is_endpoint_decoder(struct device *dev);\n>>>  struct cxl_root_decoder *cxl_root_decoder_alloc(struct cxl_port *port,\n>>> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n>>> index 92880c26b2d5..834dc7e78934 100644\n>>> --- a/include/cxl/cxl.h\n>>> +++ b/include/cxl/cxl.h\n>>> @@ -255,4 +255,10 @@ struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n>>>  struct range;\n>>>  int cxl_get_region_range(struct cxl_region *region, struct range *range);\n>>>  void cxl_unregister_region(struct cxl_region *cxlr);\n>>> +struct cxl_port;\n>>> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n>>> + int interleave_ways,\n>>> + unsigned long flags,\n>>> + resource_size_t *max);\n>>> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n>>>  #endif /* __CXL_CXL_H__ */\n\n",
          "reply_to": "alejandro.lucero-palau",
          "message_date": "2026-02-11"
        }
      ],
      "analysis_source": "llm"
    },
    "2026-02-12": {
      "report_file": "2026-02-21_ollama_llama3.1-8b.html",
      "developer": "Gregory Price",
      "reviews": [
        {
          "author": "Alejandro Palau",
          "summary": "Reviewer noted that the error inside sfc should not be fatal for cxl sfc initialization and suggested fallback to another cxl initialization possibility; also raised questions about handling multiple HDMs",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Hi Ben,\n\n\nYes, I think you are right. This works in my tests and it is safe \nbecause I check the region does exist before using it. But the error \ninside sfc should then not be fatal for cxl sfc initialization and \nfallback to the other cxl initialization possibility.\n\n\nIf I add the check for the decoder state, I guess I can keep the \nfunction names. If I rely on the region being there, I should change \nthem. I will think about it.\n\n\nThis also brings the question of what is more than one hdm present. This \nis not needed in my use case and likely this is also true for other \ncoming Type2 devices, but it does also require further thinking.\n\n\nThank you!",
          "reply_to": "Cheatham, Benjamin",
          "message_date": "2026-02-12"
        }
      ],
      "analysis_source": "llm"
    },
    "2026-02-13": {
      "report_file": "2026-02-21_ollama_llama3.1-8b.html",
      "developer": "Gregory Price",
      "reviews": [
        {
          "author": "Gregory Price",
          "summary": "Reviewer Gregory Price noted that the patch introduces a potential reference leak and suggested further investigation.\n\nThe reviewer noted that in the cxl_find_free_decoder function, a device reference obtained by device_find_child is not properly released due to a missing put_device call, resulting in a permanent elevation of the endpoint decoder device refcount.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "potential reference leak",
            "leak",
            "device reference"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "This review was generated by kreview-0811365ff2. \r\n\r\nThis is not an automated email, the reviewer though this report\r\nlooked valid enough to consider discussion.\r\n\r\n----\r\n\r\nCursory browse, this does look like a legitimate reference leak.\r\n\r\n~Gregory\r\n\r\n----\n\n---\n\nDoes this leak the device reference obtained by device_find_child()\r\nin cxl_find_free_decoder()?\r\n\r\ndevice_find_child() increments the device refcount (reference A).\r\nThen cxl_dpa_alloc() -> __cxl_dpa_reserve() calls get_device() on\r\nthe same device (reference B).\r\n\r\nOn the success path, no_free_ptr() inhibits the put_cxled cleanup,\r\nso reference A is transferred to the caller.  The matching cleanup\r\nfunction cxl_dpa_free() calls __cxl_dpa_release(), which drops\r\nreference B via put_device(), but reference A is never released:\r\n\r\n    cxl_dpa_free()\r\n      -> devm_cxl_dpa_release()\r\n        -> __cxl_dpa_release()\r\n          -> put_device()     /* drops reference B only */\r\n\r\n    /* reference A from device_find_child() is still held */\r\n\r\nThe only caller in this series (sfc efx_cxl_init/efx_cxl_exit)\r\nnever calls put_device() on the returned cxled either, so the\r\nendpoint decoder device refcount remains permanently elevated.",
          "reply_to": "alejandro.lucero-palau",
          "message_date": "2026-02-13"
        },
        {
          "author": "Gregory Price",
          "summary": "reviewer noted that the cxl by accelerators patch may have ordering issues due to acquiring the per-vswap spinlock while holding the folio lock in vswap_free(), which could lead to a lock ordering violation with reclaim paths\n\nThe reviewer noted that cxl_dpa_free() is called while the endpoint decoder is still attached to the region, resulting in a -EBUSY return value without freeing the DPA. The reviewer suggested calling cxl_unregister_region() before cxl_dpa_free() and cxl_put_root_decoder() to fix the ordering issue.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "ordering question seems legit",
            "ordering issue",
            "potential bug"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "This review was generated by kreview-0811365ff2. \r\n\r\nThis is not an automated email, the reviewer though this report\r\nlooked valid enough to consider discussion.\r\n\r\n----\r\nsfc code so won't speak to the internals, but ordering question seems\r\nlegit.\r\n\r\n~Gregory\r\n----\r\n\r\nOn Sat, Feb 01, 2026, Alejandro Lucero wrote:\n\n---\n\nIn the non-committed path, cxl_dpa_free() is called while the\r\nendpoint decoder is still attached to the region.  Looking at\r\ncxl_dpa_free() in drivers/cxl/core/hdm.c:\r\n\r\n    if (cxled->cxld.region) {\r\n        dev_dbg(dev, \"decoder assigned to: %s\\n\",\r\n            dev_name(&cxled->cxld.region->dev));\r\n        return -EBUSY;\r\n    }\r\n\r\nSince cxl_unregister_region() has not run yet, cxled->cxld.region\r\nis still set, and cxl_dpa_free() returns -EBUSY without freeing\r\nthe DPA.  The return value is not checked.\r\n\r\nShould cxl_unregister_region() be called before cxl_dpa_free()\r\nand cxl_put_root_decoder() in the else branch, matching the\r\nreverse order of allocation in efx_cxl_init()?\r\n\r\nThe cover letter notes that v23 expects committed decoders as the\r\nprimary flow, and uncommitted decoder support is deferred to\r\nfollow-up work, so this else branch may not be reachable in\r\npractice today.  Still worth fixing the ordering now so it\r\ndoesn't bite when the uncommitted path is enabled later.\r\n\r\nThis issue is not fixed by the remaining commits in the series\r\n(through 10fe989f9e85).",
          "reply_to": "alejandro.lucero-palau",
          "message_date": "2026-02-13"
        },
        {
          "author": "Gregory Price",
          "summary": "Reviewer Gregory Price expressed uncertainty about the code and its purpose, stating he is unfamiliar with it but finds the question posed by the code reasonable.\n\nThe reviewer noted that the CXL path does not set nic_data->pio_write_vi_base, which causes efx_ef10_link_piobufs() to issue MC_CMD_LINK_PIOBUF commands and perform a special-case check using incorrect VI instances. The reviewer suggested that the struct field should be updated with the correct non-zero value of pio_write_vi_base.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "uncertainty",
            "lack of familiarity",
            "requested changes"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "This review was generated by kreview-0811365ff2. \r\n\r\nThis is not an automated email, the reviewer though this report\r\nlooked valid enough to consider discussion.\r\n\r\n----\r\nI am completely unfamiliar with this code, but the question it poses\r\nat least seems reasonable.\r\n\r\n~Gregory\r\n----\r\n\r\nOn Sat, Feb 01, 2026, Alejandro Lucero wrote:\n\n---\n\nThe CXL path sets nic_data->pio_write_base but does not set\r\nnic_data->pio_write_vi_base, while the legacy path does:\r\n\r\n    nic_data->pio_write_vi_base = pio_write_vi_base;\r\n\r\nSince nic_data is kzalloc'd, pio_write_vi_base stays at 0 in the CXL\r\npath.  efx_ef10_link_piobufs() then uses nic_data->pio_write_vi_base\r\nto issue MC_CMD_LINK_PIOBUF commands:\r\n\r\n    MCDI_SET_DWORD(inbuf, LINK_PIOBUF_IN_TXQ_INSTANCE,\r\n                   nic_data->pio_write_vi_base + index);\r\n\r\nand also for the special-case check:\r\n\r\n    if (tx_queue->queue == nic_data->pio_write_vi_base) {\r\n\r\nWouldn't this link PIO buffers to incorrect VI instances when using\r\nCXL, since the local variable pio_write_vi_base has the correct\r\nnon-zero value but the struct field was never updated?",
          "reply_to": "alejandro.lucero-palau",
          "message_date": "2026-02-13"
        }
      ],
      "analysis_source": "llm"
    },
    "2026-02-16": {
      "report_file": "2026-02-21_ollama_llama3.1-8b.html",
      "developer": "Gregory Price",
      "reviews": [
        {
          "author": "Alejandro Palau",
          "summary": "Reviewer Alejandro Palau noted that the patch caused a memory leak due to not dropping the reference to the cxl device when allocating another one, and suggested adding a put_device() call to fix it.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "memory leak",
            "requested changes"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "This is right, and it took a good bunch of time to debug it. Was it \ndetected by an automatic tool?\n\n\nAnyways, I had one patch for solving this which I forgot to apply to v23 \nsince the focus there was to mainly support the auto-discover region \nwhich does not go through this path:\n\n+ /* removing the reference from cxl_find_free_decoder ...\n+ * when alloc succeds another get happened\n+ */\n+\n+ put_device(&cxled->cxld.dev);\n\n\nI added that comment because it is not trivial to know if it is right to \ndo the put while you get a new reference to the device. I will apply it.\n\nThanks!",
          "reply_to": "Gregory Price",
          "message_date": "2026-02-16"
        }
      ],
      "analysis_source": "llm"
    },
    "2026-02-19": {
      "report_file": "2026-02-21_ollama_llama3.1-8b.html",
      "developer": "Gregory Price",
      "reviews": [
        {
          "author": "Alejandro Palau",
          "summary": "Reviewer Alejandro Palau expressed confusion about the original implementation of cxl support and requested changes for version 24.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "confusion",
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Hi Ben,\n\n\nI do not remember why this was done this way. Maybe some initial need \nwhich disappeared later.\n\nI can not see a reason now, so I will do so in v24.\n\n\nThank you!",
          "reply_to": "Cheatham, Benjamin",
          "message_date": "2026-02-19"
        },
        {
          "author": "Alejandro Palau",
          "summary": "Reviewer pointed out that the cxl_accel_unwind() function is not properly handling the case where the accelerator is already being reset, and suggested adding a check to ensure that the function only unwinds the accelerator if it's not currently being reset.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Sure.\n\nThanks!",
          "reply_to": "Cheatham, Benjamin",
          "message_date": "2026-02-19"
        },
        {
          "author": "Alejandro Palau",
          "summary": "Reviewer Alejandro Palau expressed concern that the code does not support interleaving for Type2 accelerators, despite being trivial to implement, due to previous discussions and potential future use cases.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "previous discussions",
            "potential future use cases"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Hi Ben,\n\n\nI do remember this one.\n\n\nDan's original patches had this support for interleaving, then I removed \nit as the case for Type2 and interleaving is quite unlikely, at least \nright now and likely in the near future. But I was told why do not \nsupport it as it was trivial to do so. FWIW, If I think only about the \nuse case coming with the patchset, I agree with you, but because those \nprevious discussions, I think I have to leave it.\n\n\nThank you",
          "reply_to": "Cheatham, Benjamin",
          "message_date": "2026-02-19"
        },
        {
          "author": "Alejandro Palau",
          "summary": "Reviewer noted that CXL initialization fails, leading to an earlier release of the modified struct, and requested further investigation.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "failure",
            "earlier release"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "No, I do not think that is necessary. The CXL initialization fails, and \nthe result is the modified struct will be released sooner or later.",
          "reply_to": "Cheatham, Benjamin",
          "message_date": "2026-02-19"
        },
        {
          "author": "Alejandro Palau",
          "summary": "Reviewer suggested replacing kstrtouint with kstrtoint to obtain values for cxl_region_params fields defined as int, considering the simplicity of this approach over modifying other places and structs.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Good catch. I wonder if I should just change the way the value is \nobtained, using kstrtoint instead of kstrtouint, as those values are \nused for cxl_region_params fields defined as int. In other words, it \nseems doing that simpler than changing all the other places you mention \nand the structs involved. I can not see a reason for using unsigned int \nso I think I will follow that approach. Tell me if you think otherwise.\n\n\nThank you",
          "reply_to": "Cheatham, Benjamin",
          "message_date": "2026-02-19"
        },
        {
          "author": "Alejandro Palau",
          "summary": "Reviewer Alejandro Palau acknowledged the issue and agreed to make the necessary corrections.",
          "sentiment": "neutral",
          "sentiment_signals": [],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "That's true. I will fix it.\n\n\nThank you!",
          "reply_to": "Cheatham, Benjamin",
          "message_date": "2026-02-19"
        },
        {
          "author": "Alejandro Palau",
          "summary": "Reviewer Alejandro Palau agreed to implement the requested change, indicating that they understand its purpose.",
          "sentiment": "neutral",
          "sentiment_signals": [],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "It makes sense. I'll do it.\n\nThanks",
          "reply_to": "Cheatham, Benjamin",
          "message_date": "2026-02-19"
        },
        {
          "author": "Cheatham, Benjamin",
          "summary": "Reviewer questioned the use of unsigned int for interleave granularity/ways, suggesting it's because negative values wouldn't make sense, but agreed that using a smaller type is reasonable.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "NEEDS_WORK"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "If I had to guess unsigned int was used because a negative interleave granularity/ways makes no sense. I think your suggestion is fine though since no one\nin their right mind would give anything but a (relatively) small and positive value for these.\n\nThanks,\nBen",
          "reply_to": "Alejandro Palau",
          "message_date": "2026-02-19"
        },
        {
          "author": "Cheatham, Benjamin",
          "summary": "Reviewer suggested modifying the patch to address a previously noted issue by adding a small addition to enable CXL support, and requested that the decoder position be fixed in a separate patch (19/22). Additionally, he pointed out that the interleave_ways parameter in cxl_get_hpa_freespace() is currently unused.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "I'm fine with that, but I would at least do the fix with the decoder position in 19/22 and make a note that the\ninterleave_ways parameter in cxl_get_hpa_freespace() below is currently unused (unless I'm misunderstanding\nthe endpoint->host_bridge member).\n\nThat way, the support is mostly there and just requires a small, previously noted, addition to enable. If you're\nfine with that then feel free to add my Reviewed-by after implementing in v24.\n\nThanks,\nBen",
          "reply_to": "Alejandro Palau",
          "message_date": "2026-02-19"
        },
        {
          "author": "Dave Jiang",
          "summary": "Gave Reviewed-by",
          "sentiment": "positive",
          "sentiment_signals": [],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "\n\nOn 2/1/26 8:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Add cxl_unregister_region() to the accelerator driver API\n> for a clean exit.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n\nReviewed-by: Dave Jiang <dave.jiang@intel.com>\n\n> ---\n>  drivers/cxl/core/region.c | 17 ++++++++++++-----\n>  include/cxl/cxl.h         |  1 +\n>  2 files changed, 13 insertions(+), 5 deletions(-)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index acf29ba3b205..954b8fcdbac6 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -2438,9 +2438,8 @@ static struct cxl_region *to_cxl_region(struct device *dev)\n>  \treturn container_of(dev, struct cxl_region, dev);\n>  }\n>  \n> -static void unregister_region(void *_cxlr)\n> +void cxl_unregister_region(struct cxl_region *cxlr)\n>  {\n> -\tstruct cxl_region *cxlr = _cxlr;\n>  \tstruct cxl_region_params *p = &cxlr->params;\n>  \tint i;\n>  \n> @@ -2457,6 +2456,14 @@ static void unregister_region(void *_cxlr)\n>  \tcxl_region_iomem_release(cxlr);\n>  \tput_device(&cxlr->dev);\n>  }\n> +EXPORT_SYMBOL_NS_GPL(cxl_unregister_region, \"CXL\");\n> +\n> +static void __unregister_region(void *_cxlr)\n> +{\n> +\tstruct cxl_region *cxlr = _cxlr;\n> +\n> +\treturn cxl_unregister_region(cxlr);\n> +}\n>  \n>  static struct lock_class_key cxl_region_key;\n>  \n> @@ -2608,7 +2615,7 @@ static struct cxl_region *devm_cxl_add_region(struct cxl_root_decoder *cxlrd,\n>  \tif (rc)\n>  \t\tgoto err;\n>  \n> -\trc = devm_add_action_or_reset(port->uport_dev, unregister_region, cxlr);\n> +\trc = devm_add_action_or_reset(port->uport_dev, __unregister_region, cxlr);\n>  \tif (rc)\n>  \t\treturn ERR_PTR(rc);\n>  \n> @@ -2762,7 +2769,7 @@ static ssize_t delete_region_store(struct device *dev,\n>  \tif (IS_ERR(cxlr))\n>  \t\treturn PTR_ERR(cxlr);\n>  \n> -\tdevm_release_action(port->uport_dev, unregister_region, cxlr);\n> +\tdevm_release_action(port->uport_dev, __unregister_region, cxlr);\n>  \tput_device(&cxlr->dev);\n>  \n>  \treturn len;\n> @@ -3878,7 +3885,7 @@ static struct cxl_region *construct_region(struct cxl_root_decoder *cxlrd,\n>  \n>  \trc = __construct_region(cxlr, cxlrd, cxled);\n>  \tif (rc) {\n> -\t\tdevm_release_action(port->uport_dev, unregister_region, cxlr);\n> +\t\tdevm_release_action(port->uport_dev, __unregister_region, cxlr);\n>  \t\treturn ERR_PTR(rc);\n>  \t}\n>  \n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 906065e0d2a6..92880c26b2d5 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -254,4 +254,5 @@ struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n>  \t\t\t\t\t\t       struct cxl_region **cxlr);\n>  struct range;\n>  int cxl_get_region_range(struct cxl_region *region, struct range *range);\n> +void cxl_unregister_region(struct cxl_region *cxlr);\n>  #endif /* __CXL_CXL_H__ */\n\n\n\n---\n\n\n\nOn 2/1/26 8:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> Check if device HDM is already committed during firmware/BIOS\n> initialization.\n> \n> A CXL region should exist if so after memdev allocation/initialization.\n> Get HPA from region and map it.\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> ---\n>  drivers/net/ethernet/sfc/efx_cxl.c | 28 +++++++++++++++++++++++++++-\n>  1 file changed, 27 insertions(+), 1 deletion(-)\n> \n> diff --git a/drivers/net/ethernet/sfc/efx_cxl.c b/drivers/net/ethernet/sfc/efx_cxl.c\n> index a77ef4783fcb..3536eccf1b2a 100644\n> --- a/drivers/net/ethernet/sfc/efx_cxl.c\n> +++ b/drivers/net/ethernet/sfc/efx_cxl.c\n> @@ -19,6 +19,7 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n>  \tstruct efx_nic *efx = &probe_data->efx;\n>  \tstruct pci_dev *pci_dev = efx->pci_dev;\n>  \tstruct efx_cxl *cxl;\n> +\tstruct range range;\n>  \tu16 dvsec;\n>  \tint rc;\n>  \n> @@ -90,13 +91,38 @@ int efx_cxl_init(struct efx_probe_data *probe_data)\n>  \t\treturn PTR_ERR(cxl->cxlmd);\n>  \t}\n>  \n> -\tprobe_data->cxl = cxl;\n> +\tcxl->cxled = cxl_get_committed_decoder(cxl->cxlmd, &cxl->efx_region);\n> +\tif (cxl->cxled) {\n\nif (!cxl->cxled)\n\treturn 0;\n\nShould save you a level of indent.\n\nDJ\n\n> +\t\tif (!cxl->efx_region) {\n> +\t\t\tpci_err(pci_dev, \"CXL found committed decoder without a region\");\n> +\t\t\treturn -ENODEV;\n> +\t\t}\n> +\t\trc = cxl_get_region_range(cxl->efx_region, &range);\n> +\t\tif (rc) {\n> +\t\t\tpci_err(pci_dev,\n> +\t\t\t\t\"CXL getting regions params from a committed decoder failed\");\n> +\t\t\treturn rc;\n> +\t\t}\n> +\n> +\t\tcxl->ctpio_cxl = ioremap(range.start, range.end - range.start + 1);\n> +\t\tif (!cxl->ctpio_cxl) {\n> +\t\t\tpci_err(pci_dev, \"CXL ioremap region (%pra) failed\", &range);\n> +\t\t\treturn -ENOMEM;\n> +\t\t}\n> +\n> +\t\tprobe_data->cxl = cxl;\n> +\t}\n>  \n>  \treturn 0;\n>  }\n>  \n>  void efx_cxl_exit(struct efx_probe_data *probe_data)\n>  {\n> +\tif (!probe_data->cxl)\n> +\t\treturn;\n> +\n> +\tiounmap(probe_data->cxl->ctpio_cxl);\n> +\tcxl_unregister_region(probe_data->cxl->efx_region);\n>  }\n>  \n>  MODULE_IMPORT_NS(\"CXL\");\n\n\n\n---\n\n\n\nOn 2/1/26 8:54 AM, alejandro.lucero-palau@amd.com wrote:\n> From: Alejandro Lucero <alucerop@amd.com>\n> \n> CXL region creation involves allocating capacity from Device Physical\n> Address (DPA) and assigning it to decode a given Host Physical Address\n> (HPA). Before determining how much DPA to allocate the amount of available\n> HPA must be determined. Also, not all HPA is created equal, some HPA\n> targets RAM, some targets PMEM, some is prepared for device-memory flows\n> like HDM-D and HDM-DB, and some is HDM-H (host-only).\n> \n> In order to support Type2 CXL devices, wrap all of those concerns into\n> an API that retrieves a root decoder (platform CXL window) that fits the\n> specified constraints and the capacity available for a new region.\n> \n> Add a complementary function for releasing the reference to such root\n> decoder.\n> \n> Based on https://lore.kernel.org/linux-cxl/168592159290.1948938.13522227102445462976.stgit@dwillia2-xfh.jf.intel.com/\n> \n> Signed-off-by: Alejandro Lucero <alucerop@amd.com>\n> Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>\n> ---\n>  drivers/cxl/core/region.c | 164 ++++++++++++++++++++++++++++++++++++++\n>  drivers/cxl/cxl.h         |   3 +\n>  include/cxl/cxl.h         |   6 ++\n>  3 files changed, 173 insertions(+)\n> \n> diff --git a/drivers/cxl/core/region.c b/drivers/cxl/core/region.c\n> index 954b8fcdbac6..bdefd088f5f1 100644\n> --- a/drivers/cxl/core/region.c\n> +++ b/drivers/cxl/core/region.c\n> @@ -705,6 +705,170 @@ static int free_hpa(struct cxl_region *cxlr)\n>  \treturn 0;\n>  }\n>  \n> +struct cxlrd_max_context {\n> +\tstruct device * const *host_bridges;\n> +\tint interleave_ways;\n> +\tunsigned long flags;\n> +\tresource_size_t max_hpa;\n> +\tstruct cxl_root_decoder *cxlrd;\n> +};\n> +\n> +static int find_max_hpa(struct device *dev, void *data)\n> +{\n> +\tstruct cxlrd_max_context *ctx = data;\n> +\tstruct cxl_switch_decoder *cxlsd;\n> +\tstruct cxl_root_decoder *cxlrd;\n> +\tstruct resource *res, *prev;\n> +\tstruct cxl_decoder *cxld;\n> +\tresource_size_t free = 0;\n> +\tresource_size_t max;\n> +\tint found = 0;\n> +\n> +\tif (!is_root_decoder(dev))\n> +\t\treturn 0;\n> +\n> +\tcxlrd = to_cxl_root_decoder(dev);\n> +\tcxlsd = &cxlrd->cxlsd;\n> +\tcxld = &cxlsd->cxld;\n> +\n> +\tif ((cxld->flags & ctx->flags) != ctx->flags) {\n> +\t\tdev_dbg(dev, \"flags not matching: %08lx vs %08lx\\n\",\n> +\t\t\tcxld->flags, ctx->flags);\n> +\t\treturn 0;\n> +\t}\n> +\n> +\tfor (int i = 0; i < ctx->interleave_ways; i++) {\n> +\t\tfor (int j = 0; j < ctx->interleave_ways; j++) {\n> +\t\t\tif (ctx->host_bridges[i] == cxlsd->target[j]->dport_dev) {\n> +\t\t\t\tfound++;\n> +\t\t\t\tbreak;\n> +\t\t\t}\n> +\t\t}\n> +\t}\n> +\n> +\tif (found != ctx->interleave_ways) {\n> +\t\tdev_dbg(dev,\n> +\t\t\t\"Not enough host bridges. Found %d for %d interleave ways requested\\n\",\n> +\t\t\tfound, ctx->interleave_ways);\n> +\t\treturn 0;\n> +\t}\n> +\n> +\t/*\n> +\t * Walk the root decoder resource range relying on cxl_rwsem.region to\n> +\t * preclude sibling arrival/departure and find the largest free space\n> +\t * gap.\n> +\t */\n> +\tlockdep_assert_held_read(&cxl_rwsem.region);\n> +\tres = cxlrd->res->child;\n> +\n> +\t/* With no resource child the whole parent resource is available */\n> +\tif (!res)\n> +\t\tmax = resource_size(cxlrd->res);\n> +\telse\n> +\t\tmax = 0;\n> +\n> +\tfor (prev = NULL; res; prev = res, res = res->sibling) {\n> +\t\tif (!prev && res->start == cxlrd->res->start &&\n> +\t\t    res->end == cxlrd->res->end) {\n> +\t\t\tmax = resource_size(cxlrd->res);\n> +\t\t\tbreak;\n> +\t\t}\n\nCan this block be pulled out of the for loop so it only needs to run once?\n\n> +\t\t/*\n> +\t\t * Sanity check for preventing arithmetic problems below as a\n> +\t\t * resource with size 0 could imply using the end field below\n> +\t\t * when set to unsigned zero - 1 or all f in hex.\n> +\t\t */\n> +\t\tif (prev && !resource_size(prev))\n> +\t\t\tcontinue;\n> +\n> +\t\tif (!prev && res->start > cxlrd->res->start) {\n> +\t\t\tfree = res->start - cxlrd->res->start;\n> +\t\t\tmax = max(free, max);\n> +\t\t}\n> +\t\tif (prev && res->start > prev->end + 1) {\n> +\t\t\tfree = res->start - prev->end + 1;\n> +\t\t\tmax = max(free, max);\n> +\t\t}\n> +\t}\n> +\n> +\tif (prev && prev->end + 1 < cxlrd->res->end + 1) {\n> +\t\tfree = cxlrd->res->end + 1 - prev->end + 1;\n> +\t\tmax = max(free, max);\n> +\t}\n> +\n> +\tdev_dbg(cxlrd_dev(cxlrd), \"found %pa bytes of free space\\n\", &max);\n> +\tif (max > ctx->max_hpa) {\n> +\t\tif (ctx->cxlrd)\n> +\t\t\tput_device(cxlrd_dev(ctx->cxlrd));\n> +\t\tget_device(cxlrd_dev(cxlrd));\n> +\t\tctx->cxlrd = cxlrd;\n> +\t\tctx->max_hpa = max;\n\nIs there any chance that ctx->cxlrd == cxlrd? Maybe you can do:\n\nif (ctx->cxlrd && ctx->cxlrd != cxlrd) {\n\tput_device(cxlrd_dev(ctx->cxlrd));\n\tget_device(cxlrd_dev(cxlrd));\n\tctx->cxlrd = cxlrd;\n}\nctx->max_hpa = max;\n\nDJ\n\n> +\t}\n> +\treturn 0;\n> +}\n> +\n> +/**\n> + * cxl_get_hpa_freespace - find a root decoder with free capacity per constraints\n> + * @cxlmd: the mem device requiring the HPA\n> + * @interleave_ways: number of entries in @host_bridges\n> + * @flags: CXL_DECODER_F flags for selecting RAM vs PMEM, and Type2 device\n> + * @max_avail_contig: output parameter of max contiguous bytes available in the\n> + *\t\t      returned decoder\n> + *\n> + * Returns a pointer to a struct cxl_root_decoder\n> + *\n> + * The return tuple of a 'struct cxl_root_decoder' and 'bytes available given\n> + * in (@max_avail_contig))' is a point in time snapshot. If by the time the\n> + * caller goes to use this decoder and its capacity is reduced then caller needs\n> + * to loop and retry.\n> + *\n> + * The returned root decoder has an elevated reference count that needs to be\n> + * put with cxl_put_root_decoder(cxlrd).\n> + */\n> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t       int interleave_ways,\n> +\t\t\t\t\t       unsigned long flags,\n> +\t\t\t\t\t       resource_size_t *max_avail_contig)\n> +{\n> +\tstruct cxlrd_max_context ctx = {\n> +\t\t.flags = flags,\n> +\t\t.interleave_ways = interleave_ways,\n> +\t};\n> +\tstruct cxl_port *root_port;\n> +\tstruct cxl_port *endpoint;\n> +\n> +\tendpoint = cxlmd->endpoint;\n> +\tif (!endpoint) {\n> +\t\tdev_dbg(&cxlmd->dev, \"endpoint not linked to memdev\\n\");\n> +\t\treturn ERR_PTR(-ENXIO);\n> +\t}\n> +\n> +\tctx.host_bridges = &endpoint->host_bridge;\n> +\n> +\tstruct cxl_root *root __free(put_cxl_root) = find_cxl_root(endpoint);\n> +\tif (!root) {\n> +\t\tdev_dbg(&endpoint->dev, \"endpoint is not related to a root port\\n\");\n> +\t\treturn ERR_PTR(-ENXIO);\n> +\t}\n> +\n> +\troot_port = &root->port;\n> +\tscoped_guard(rwsem_read, &cxl_rwsem.region)\n> +\t\tdevice_for_each_child(&root_port->dev, &ctx, find_max_hpa);\n> +\n> +\tif (!ctx.cxlrd)\n> +\t\treturn ERR_PTR(-ENOMEM);\n> +\n> +\t*max_avail_contig = ctx.max_hpa;\n> +\treturn ctx.cxlrd;\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_get_hpa_freespace, \"CXL\");\n> +\n> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd)\n> +{\n> +\tput_device(cxlrd_dev(cxlrd));\n> +}\n> +EXPORT_SYMBOL_NS_GPL(cxl_put_root_decoder, \"CXL\");\n> +\n>  static ssize_t size_store(struct device *dev, struct device_attribute *attr,\n>  \t\t\t  const char *buf, size_t len)\n>  {\n> diff --git a/drivers/cxl/cxl.h b/drivers/cxl/cxl.h\n> index 944c5d1ccceb..c7d9b2c2908f 100644\n> --- a/drivers/cxl/cxl.h\n> +++ b/drivers/cxl/cxl.h\n> @@ -706,6 +706,9 @@ struct cxl_root_decoder *to_cxl_root_decoder(struct device *dev);\n>  struct cxl_switch_decoder *to_cxl_switch_decoder(struct device *dev);\n>  struct cxl_endpoint_decoder *to_cxl_endpoint_decoder(struct device *dev);\n>  bool is_root_decoder(struct device *dev);\n> +\n> +#define cxlrd_dev(cxlrd) (&(cxlrd)->cxlsd.cxld.dev)\n> +\n>  bool is_switch_decoder(struct device *dev);\n>  bool is_endpoint_decoder(struct device *dev);\n>  struct cxl_root_decoder *cxl_root_decoder_alloc(struct cxl_port *port,\n> diff --git a/include/cxl/cxl.h b/include/cxl/cxl.h\n> index 92880c26b2d5..834dc7e78934 100644\n> --- a/include/cxl/cxl.h\n> +++ b/include/cxl/cxl.h\n> @@ -255,4 +255,10 @@ struct cxl_endpoint_decoder *cxl_get_committed_decoder(struct cxl_memdev *cxlmd,\n>  struct range;\n>  int cxl_get_region_range(struct cxl_region *region, struct range *range);\n>  void cxl_unregister_region(struct cxl_region *cxlr);\n> +struct cxl_port;\n> +struct cxl_root_decoder *cxl_get_hpa_freespace(struct cxl_memdev *cxlmd,\n> +\t\t\t\t\t       int interleave_ways,\n> +\t\t\t\t\t       unsigned long flags,\n> +\t\t\t\t\t       resource_size_t *max);\n> +void cxl_put_root_decoder(struct cxl_root_decoder *cxlrd);\n>  #endif /* __CXL_CXL_H__ */\n\n",
          "reply_to": "alejandro.lucero-palau",
          "message_date": "2026-02-19"
        },
        {
          "author": "Dave Jiang",
          "summary": "Reviewer Dave Jiang suggested simplifying the cxl code by saving an indentation level, and provided a specific line to modify",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "suggested improvement",
            "code optimization"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "if (!cxl->cxled)\n\treturn 0;\n\nShould save you a level of indent.\n\nDJ",
          "reply_to": "alejandro.lucero-palau",
          "message_date": "2026-02-19"
        }
      ],
      "analysis_source": "llm"
    },
    "2026-02-20": {
      "report_file": "2026-02-21_ollama_llama3.1-8b.html",
      "developer": "Gregory Price",
      "reviews": [
        {
          "author": "Alejandro Palau",
          "summary": "Reviewer noted that the patchset should handle both module exit paths, specifically supporting cases where cxl is initialized before or after the driver is loaded.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Hi Gregory,\n\n\nYes, it makes sense and pointing out to those changes introduced in v22 \nand mainly in v23.\n\nI'll fix it.\n\n\nRegarding the below comment, which if I am not wrong comes from kreview, \nI think the patchset needs to support both cases and therefore the code \nneeds to deal with both module exit paths.\n\n\nThank you",
          "reply_to": "Gregory Price",
          "message_date": "2026-02-20"
        },
        {
          "author": "Alejandro Palau",
          "summary": "The reviewer noted that since they've only tested with one Virtual Interface (VI), they haven't encountered an issue, but emphasized that it's essential to fix the problem.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "issue",
            "needs to be fixed"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Yes, and again, it makes sense. We have only tried with one VI, so that \nexplains why we have not suffered the issue. But it needs to be fixed.\n\n\nThanks!",
          "reply_to": "Gregory Price",
          "message_date": "2026-02-20"
        },
        {
          "author": "Alejandro Palau",
          "summary": "reviewer noted that the current implementation lacks an 'else' branch for when CXL is not enabled, and requested that subsequent patches address this omission",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "missing else branch"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Yes, but subsequent patches add the else branch ...\n\n\nThanks",
          "reply_to": "Dave Jiang",
          "message_date": "2026-02-20"
        },
        {
          "author": "Dave Jiang",
          "summary": "Reviewer Dave Jiang noted that the ctx->cxlrd pointer might not be updated correctly, suggesting a check to ensure it matches cxlrd before updating it, and proposed code to handle this case",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Is there any chance that ctx->cxlrd == cxlrd? Maybe you can do:\n\nif (ctx->cxlrd && ctx->cxlrd != cxlrd) {\n\tput_device(cxlrd_dev(ctx->cxlrd));\n\tget_device(cxlrd_dev(cxlrd));\n\tctx->cxlrd = cxlrd;\n}\nctx->max_hpa = max;\n\nDJ",
          "reply_to": "alejandro.lucero-palau",
          "message_date": "2026-02-20"
        },
        {
          "author": "Gregory Price",
          "summary": "Reviewer Gregory Price noted that during testing, he observed double-releases when aggressively loading and unloading certain drivers, which was fixed by adding a function to properly unregister regions in cxl_destroy_region()",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "fixes a bug",
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "kreview suggested you probably want this:\n\nvoid cxl_destroy_region(struct cxl_region *cxlr)\n{\n\tstruct cxl_port *port = cxlrd_to_port(cxlr->cxlrd);\n\n\tdevm_release_action(port->uport_dev, __unregister_region, cxlr);\n}\nEXPORT_SYMBOL_NS_GPL(cxl_destroy_region, \"CXL\");\n\n\nDuring testing I experienced some double-releases when doing aggressive\nloads and unloads of some drivers.  This was one of the fixes.\n\n~Gregory",
          "reply_to": "alejandro.lucero-palau",
          "message_date": "2026-02-20"
        }
      ],
      "analysis_source": "llm"
    },
    "2026-02-21": {
      "report_file": "2026-02-21_ollama_llama3.1-8b.html",
      "developer": "Gregory Price",
      "reviews": [],
      "analysis_source": "llm-per-reviewer"
    }
  }
}