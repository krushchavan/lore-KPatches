<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Review Comments: [PATCH v1 11/11] io_uring/cmd: set selected buffer index in __io_uring_cmd_done()</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto,
                         "Helvetica Neue", Arial, sans-serif;
            background: #f5f5f5;
            color: #333;
            line-height: 1.6;
            padding: 20px;
            max-width: 900px;
            margin: 0 auto;
        }
        .home-link { margin-bottom: 12px; display: block; }
        .home-link a { color: #0366d6; text-decoration: none; font-size: 0.9em; }
        .home-link a:hover { text-decoration: underline; }

        h1 { font-size: 1.3em; margin-bottom: 2px; color: #1a1a1a; line-height: 1.3; }

        .lore-link { font-size: 0.85em; margin: 4px 0 6px; display: block; }
        .lore-link a { color: #0366d6; text-decoration: none; }
        .lore-link a:hover { text-decoration: underline; }

        .date-range {
            font-size: 0.8em;
            color: #888;
            margin-bottom: 16px;
        }
        .date-range a { color: #0366d6; text-decoration: none; }
        .date-range a:hover { text-decoration: underline; }

        /* thread-node scroll margin so the card isn't clipped at the top */
        .thread-node { scroll-margin-top: 8px; }

        /* ── Patch summary ──────────────────────────────────────────── */
        .patch-summary-block {
            background: #fff;
            border-radius: 8px;
            padding: 12px 16px;
            margin-bottom: 20px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            border-left: 3px solid #4a90d9;
        }
        .patch-summary-label {
            font-size: 0.72em;
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 0.06em;
            color: #4a90d9;
            margin-bottom: 4px;
        }
        .patch-summary-text {
            font-size: 0.88em;
            color: #444;
            line-height: 1.55;
        }

        /* ── Thread tree ────────────────────────────────────────────── */
        .thread-tree {
            display: flex;
            flex-direction: column;
            gap: 6px;
        }

        /* Depth indentation via left border */
        .thread-node { position: relative; }
        .thread-children {
            margin-left: 20px;
            padding-left: 12px;
            border-left: 2px solid #e0e0e0;
            margin-top: 6px;
            display: flex;
            flex-direction: column;
            gap: 6px;
        }

        /* ── Review comment card ────────────────────────────────────── */
        .review-comment {
            background: #fff;
            border-radius: 6px;
            padding: 10px 14px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            font-size: 0.88em;
        }
        .review-comment-header {
            display: flex;
            flex-wrap: wrap;
            align-items: center;
            gap: 6px;
            margin-bottom: 5px;
        }
        .review-author {
            font-weight: 700;
            color: #1a1a1a;
            font-size: 0.95em;
        }

        /* Date chip — links back to the daily report */
        .date-chip {
            font-size: 0.75em;
            color: #777;
            background: #f0f0f0;
            border-radius: 10px;
            padding: 1px 7px;
            text-decoration: none;
            white-space: nowrap;
        }
        a.date-chip:hover { background: #e0e8f5; color: #0366d6; }

        .badge {
            display: inline-block;
            padding: 1px 8px;
            border-radius: 10px;
            font-size: 0.75em;
            font-weight: 600;
        }
        .inline-review-badge {
            display: inline-block;
            padding: 0 6px;
            border-radius: 8px;
            font-size: 0.78em;
            font-weight: 500;
            background: #e3f2fd;
            color: #1565c0;
        }
        .review-tag-badge {
            display: inline-block;
            padding: 0 6px;
            border-radius: 8px;
            font-size: 0.78em;
            font-weight: 500;
            background: #e8f5e9;
            color: #2e7d32;
        }
        .analysis-source-badge {
            display: inline-block;
            padding: 1px 7px;
            border-radius: 10px;
            font-size: 0.72em;
            font-weight: 600;
            border: 1px solid rgba(0,0,0,0.1);
        }

        .review-comment-text {
            color: #444;
            line-height: 1.55;
            margin-bottom: 4px;
        }
        .review-comment-signals {
            margin-top: 3px;
            font-size: 0.85em;
            color: #aaa;
            font-style: italic;
        }

        /* ── Collapsible raw body ───────────────────────────────────── */
        .raw-body-toggle {
            margin-top: 5px;
            font-size: 0.85em;
        }
        .raw-body-toggle summary {
            cursor: pointer;
            color: #888;
            padding: 2px 0;
            font-weight: 500;
            font-size: 0.9em;
            list-style: none;
        }
        .raw-body-toggle summary::-webkit-details-marker { display: none; }
        .raw-body-toggle summary::before { content: "▶ "; font-size: 0.7em; }
        .raw-body-toggle[open] summary::before { content: "▼ "; }
        .raw-body-toggle summary:hover { color: #555; }
        .raw-body-text {
            white-space: pre-wrap;
            font-size: 0.95em;
            background: #f8f8f8;
            padding: 8px 10px;
            border-radius: 4px;
            max-height: 360px;
            overflow-y: auto;
            margin-top: 4px;
            line-height: 1.5;
            color: #444;
            border: 1px solid #e8e8e8;
        }

        .no-reviews {
            color: #aaa;
            font-size: 0.85em;
            font-style: italic;
            padding: 8px 0;
        }

        footer {
            text-align: center;
            color: #bbb;
            font-size: 0.78em;
            margin-top: 36px;
            padding: 16px;
        }
    </style>
</head>
<body>
    <div class="home-link"><a href="../">&larr; Back to reports</a></div>
    <h1>[PATCH v1 11/11] io_uring/cmd: set selected buffer index in __io_uring_cmd_done()</h1>
    <div class="lore-link"><a href="https://lore.kernel.org/all/20260210002852.1394504-12-joannelkoong@gmail.com/" target="_blank">View on lore.kernel.org &rarr;</a></div>
    <div class="date-range">Active on: <a href="#2026-02-20">2026-02-20</a> &bull; <a href="#2026-02-18">2026-02-18</a> &bull; <a href="#2026-02-13">2026-02-13</a> &bull; <a href="#2026-02-12">2026-02-12</a> &bull; <a href="#2026-02-11">2026-02-11</a> &bull; <a href="#2026-02-10">2026-02-10</a> &bull; <a href="#2026-02-09">2026-02-09</a></div>
    <div class="patch-summary-block"><div class="patch-summary-label">Patch summary</div><div class="patch-summary-text">This patch sets the selected buffer index in the __io_uring_cmd_done() function. This is part of a larger series that introduces kernel-managed buffer rings, where the kernel allocates and manages buffers on behalf of applications using io_uring. The goal is to simplify buffer management for applications by having the kernel handle it internally.</div></div>
    <div class="thread-tree">
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Pavel Begunkov</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-10">2026-02-10</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Pavel Begunkov suggested an alternative solution for kernel-managed buffer rings, proposing to internally reuse regions for allocations and mmap&#x27;ing.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On 2/13/26 15:31, Pavel Begunkov wrote:
&gt; On 2/13/26 07:27, Christoph Hellwig wrote:
&gt;&gt; On Thu, Feb 12, 2026 at 09:29:31AM -0800, Joanne Koong wrote:
&gt;&gt;&gt;&gt;&gt; I&#x27;m arguing exactly against this. For my use case I need a setup
&gt;&gt;&gt;&gt;&gt; where the kernel controls the allocation fully and guarantees user
&gt;&gt;&gt;&gt;&gt; processes can only read the memory but never write to it. I&#x27;d love
&gt;&gt;&gt;
&gt;&gt;&gt; By &quot;control the allocation fully&quot; do you mean for your use case, the
&gt;&gt;&gt; allocation/setup isn&#x27;t triggered by userspace but is initiated by the
&gt;&gt;&gt; kernel (eg user never explicitly registers any kbuf ring, the kernel
&gt;&gt;&gt; just uses the kbuf ring data structure internally and users can read
&gt;&gt;&gt; the buffer contents)? If userspace initiates the setup of the kbuf
&gt;&gt;&gt; ring, going through IORING_REGISTER_MEM_REGION would be semantically
&gt;&gt;&gt; the same, except the buffer allocation by the kernel now happens
&gt;&gt;&gt; before the ring is created and then later populated into the ring.
&gt;&gt;&gt; userspace would still need to make an mmap call to the region and the
&gt;&gt;&gt; kernel could enforce that as read-only. But if userspace doesn&#x27;t
&gt;&gt;&gt; initiate the setup, then going through IORING_REGISTER_MEM_REGION gets
&gt;&gt;&gt; uglier.
&gt;&gt;
&gt;&gt; The idea is that the application tells the kernel that it wants to use
&gt;&gt; a fixed buffer pool for reads. Right now the application does this
&gt;&gt; using io_uring_register_buffers(). The problem with that is that
&gt;&gt; io_uring_register_buffers ends up just doing a pin of the memory,
&gt;&gt; but the application or, in case of shared memory, someone else could
&gt;&gt; still modify the memory. If the underlying file system or storage
&gt;&gt; device needs verify checksums, or worse rebuild data from parity
&gt;&gt; (or uncompress), it needs to ensure that the memory it is operating
&gt;&gt; on can&#x27;t be modified by someone else.
&gt;&gt;
&gt;&gt; So I&#x27;ve been thinking of a version of io_uring_register_buffers where
&gt;&gt; the buffers are not provided by the application, but instead by the
&gt;&gt; kernel and mapped into the application address space read-only for
&gt;&gt; a while, and I thought I could implement this on top of your series,
&gt;&gt; but I have to admit I haven&#x27;t really looked into the details all
&gt;&gt; that much.
&gt; 
&gt; There is nothing about registered buffers in this series. And even
&gt; if you try to reuse buffer allocation out of it, it&#x27;ll come with
&gt; a circular buffer you&#x27;ll have no need for. And I&#x27;m pretty much
&gt; arguing about separating those for io_uring.

FWIW, the easiest solution is to internally reuse regions for
allocations and mmap()&#x27;ing and wrap it into a registered buffer.
It just need to make vmap&#x27;ing optional as it won&#x27;t be needed.

-- 
Pavel Begunkov

</pre>
</details>
<div class="review-comment-signals">Signals: alternative solution, no clear approval or objection</div>
</div>
</div>
<div class="thread-node depth-0" id="2026-02-18">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-18">2026-02-18</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The reviewer, Joanne Koong (author), raised no specific technical concerns or objections to the patch. The patch is a straightforward implementation of setting the selected buffer index in __io_uring_cmd_done().</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Refactor the logic in io_register_pbuf_ring() into generic helpers:
- io_copy_and_validate_buf_reg(): Copy out user arg and validate user
  arg and buffer registration parameters
- io_alloc_new_buffer_list(): Allocate and initialize a new buffer
  list for the given buffer group ID
- io_setup_pbuf_ring(): Sets up the physical buffer ring region and
  handles memory mapping for provided buffer rings

This is a preparatory change for upcoming kernel-managed buffer ring
support which will need to reuse some of these helpers.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 io_uring/kbuf.c | 129 +++++++++++++++++++++++++++++++-----------------
 1 file changed, 85 insertions(+), 44 deletions(-)

diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index 67d4fe576473..850b836f32ee 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -596,55 +596,73 @@ int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags)
 	return IOU_COMPLETE;
 }
 
-int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
+static int io_copy_and_validate_buf_reg(const void __user *arg,
+					struct io_uring_buf_reg *reg,
+					unsigned int permitted_flags)
 {
-	struct io_uring_buf_reg reg;
-	struct io_buffer_list *bl;
-	struct io_uring_region_desc rd;
-	struct io_uring_buf_ring *br;
-	unsigned long mmap_offset;
-	unsigned long ring_size;
-	int ret;
-
-	lockdep_assert_held(&amp;ctx-&gt;uring_lock);
-
-	if (copy_from_user(&amp;reg, arg, sizeof(reg)))
+	if (copy_from_user(reg, arg, sizeof(*reg)))
 		return -EFAULT;
-	if (!mem_is_zero(reg.resv, sizeof(reg.resv)))
+
+	if (!mem_is_zero(reg-&gt;resv, sizeof(reg-&gt;resv)))
 		return -EINVAL;
-	if (reg.flags &amp; ~(IOU_PBUF_RING_MMAP | IOU_PBUF_RING_INC))
+	if (reg-&gt;flags &amp; ~permitted_flags)
 		return -EINVAL;
-	if (!is_power_of_2(reg.ring_entries))
+	if (!is_power_of_2(reg-&gt;ring_entries))
 		return -EINVAL;
 	/* cannot disambiguate full vs empty due to head/tail size */
-	if (reg.ring_entries &gt;= 65536)
+	if (reg-&gt;ring_entries &gt;= 65536)
 		return -EINVAL;
+	return 0;
+}
 
-	bl = io_buffer_get_list(ctx, reg.bgid);
-	if (bl) {
+static struct io_buffer_list *
+io_alloc_new_buffer_list(struct io_ring_ctx *ctx,
+			 const struct io_uring_buf_reg *reg)
+{
+	struct io_buffer_list *list;
+
+	list = io_buffer_get_list(ctx, reg-&gt;bgid);
+	if (list) {
 		/* if mapped buffer ring OR classic exists, don&#x27;t allow */
-		if (bl-&gt;flags &amp; IOBL_BUF_RING || !list_empty(&amp;bl-&gt;buf_list))
-			return -EEXIST;
-		io_destroy_bl(ctx, bl);
+		if (list-&gt;flags &amp; IOBL_BUF_RING || !list_empty(&amp;list-&gt;buf_list))
+			return ERR_PTR(-EEXIST);
+		io_destroy_bl(ctx, list);
 	}
 
-	bl = kzalloc(sizeof(*bl), GFP_KERNEL_ACCOUNT);
-	if (!bl)
-		return -ENOMEM;
+	list = kzalloc(sizeof(*list), GFP_KERNEL_ACCOUNT);
+	if (!list)
+		return ERR_PTR(-ENOMEM);
+
+	list-&gt;nr_entries = reg-&gt;ring_entries;
+	list-&gt;mask = reg-&gt;ring_entries - 1;
+	list-&gt;flags = IOBL_BUF_RING;
+
+	return list;
+}
+
+static int io_setup_pbuf_ring(struct io_ring_ctx *ctx,
+			      const struct io_uring_buf_reg *reg,
+			      struct io_buffer_list *bl)
+{
+	struct io_uring_region_desc rd;
+	unsigned long mmap_offset;
+	unsigned long ring_size;
+	int ret;
 
-	mmap_offset = (unsigned long)reg.bgid &lt;&lt; IORING_OFF_PBUF_SHIFT;
-	ring_size = flex_array_size(br, bufs, reg.ring_entries);
+	mmap_offset = (unsigned long)reg-&gt;bgid &lt;&lt; IORING_OFF_PBUF_SHIFT;
+	ring_size = flex_array_size(bl-&gt;buf_ring, bufs, reg-&gt;ring_entries);
 
 	memset(&amp;rd, 0, sizeof(rd));
 	rd.size = PAGE_ALIGN(ring_size);
-	if (!(reg.flags &amp; IOU_PBUF_RING_MMAP)) {
-		rd.user_addr = reg.ring_addr;
+	if (!(reg-&gt;flags &amp; IOU_PBUF_RING_MMAP)) {
+		rd.user_addr = reg-&gt;ring_addr;
 		rd.flags |= IORING_MEM_REGION_TYPE_USER;
 	}
+
 	ret = io_create_region(ctx, &amp;bl-&gt;region, &amp;rd, mmap_offset);
 	if (ret)
-		goto fail;
-	br = io_region_get_ptr(&amp;bl-&gt;region);
+		return ret;
+	bl-&gt;buf_ring = io_region_get_ptr(&amp;bl-&gt;region);
 
 #ifdef SHM_COLOUR
 	/*
@@ -656,25 +674,48 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
 	 * should use IOU_PBUF_RING_MMAP instead, and liburing will handle
 	 * this transparently.
 	 */
-	if (!(reg.flags &amp; IOU_PBUF_RING_MMAP) &amp;&amp;
-	    ((reg.ring_addr | (unsigned long)br) &amp; (SHM_COLOUR - 1))) {
-		ret = -EINVAL;
-		goto fail;
+	if (!(reg-&gt;flags &amp; IOU_PBUF_RING_MMAP) &amp;&amp;
+	    ((reg-&gt;ring_addr | (unsigned long)bl-&gt;buf_ring) &amp;
+	     (SHM_COLOUR - 1))) {
+		io_free_region(ctx-&gt;user, &amp;bl-&gt;region);
+		return -EINVAL;
 	}
 #endif
 
-	bl-&gt;nr_entries = reg.ring_entries;
-	bl-&gt;mask = reg.ring_entries - 1;
-	bl-&gt;flags |= IOBL_BUF_RING;
-	bl-&gt;buf_ring = br;
-	if (reg.flags &amp; IOU_PBUF_RING_INC)
+	if (reg-&gt;flags &amp; IOU_PBUF_RING_INC)
 		bl-&gt;flags |= IOBL_INC;
+
+	return 0;
+}
+
+int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
+{
+	unsigned int permitted_flags;
+	struct io_uring_buf_reg reg;
+	struct io_buffer_list *bl;
+	int ret;
+
+	lockdep_assert_held(&amp;ctx-&gt;uring_lock);
+
+	permitted_flags = IOU_PBUF_RING_MMAP | IOU_PBUF_RING_INC;
+	ret = io_copy_and_validate_buf_reg(arg, &amp;reg, permitted_flags);
+	if (ret)
+		return ret;
+
+	bl = io_alloc_new_buffer_list(ctx, &amp;reg);
+	if (IS_ERR(bl))
+		return PTR_ERR(bl);
+
+	ret = io_setup_pbuf_ring(ctx, &amp;reg, bl);
+	if (ret) {
+		kfree(bl);
+		return ret;
+	}
+
 	ret = io_buffer_add_list(ctx, bl, reg.bgid);
-	if (!ret)
-		return 0;
-fail:
-	io_free_region(ctx-&gt;user, &amp;bl-&gt;region);
-	kfree(bl);
+	if (ret)
+		io_put_bl(ctx, bl);
+
 	return ret;
 }
 
-- 
2.47.3



---

Use the more generic name io_unregister_buf_ring() as this function will
be used for unregistering both provided buffer rings and kernel-managed
buffer rings.

This is a preparatory change for upcoming kernel-managed buffer ring
support.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 io_uring/kbuf.c     | 2 +-
 io_uring/kbuf.h     | 2 +-
 io_uring/register.c | 2 +-
 3 files changed, 3 insertions(+), 3 deletions(-)

diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index 850b836f32ee..aa9b70b72db4 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -719,7 +719,7 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
 	return ret;
 }
 
-int io_unregister_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
+int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg)
 {
 	struct io_uring_buf_reg reg;
 	struct io_buffer_list *bl;
diff --git a/io_uring/kbuf.h b/io_uring/kbuf.h
index bf15e26520d3..40b44f4fdb15 100644
--- a/io_uring/kbuf.h
+++ b/io_uring/kbuf.h
@@ -74,7 +74,7 @@ int io_provide_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe
 int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags);
 
 int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);
-int io_unregister_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);
+int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg);
 int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg);
 
 bool io_kbuf_recycle_legacy(struct io_kiocb *req, unsigned issue_flags);
diff --git a/io_uring/register.c b/io_uring/register.c
index 594b1f2ce875..0882cb34f851 100644
--- a/io_uring/register.c
+++ b/io_uring/register.c
@@ -841,7 +841,7 @@ static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,
 		ret = -EINVAL;
 		if (!arg || nr_args != 1)
 			break;
-		ret = io_unregister_pbuf_ring(ctx, arg);
+		ret = io_unregister_buf_ring(ctx, arg);
 		break;
 	case IORING_REGISTER_SYNC_CANCEL:
 		ret = -EINVAL;
-- 
2.47.3



---

Add support for kernel-managed buffer rings (kmbuf rings), which allow
the kernel to allocate and manage the backing buffers for a buffer
ring, rather than requiring the application to provide and manage them.

This introduces two new registration opcodes:
- IORING_REGISTER_KMBUF_RING: Register a kernel-managed buffer ring
- IORING_UNREGISTER_KMBUF_RING: Unregister a kernel-managed buffer ring

The existing io_uring_buf_reg structure is extended with a union to
support both application-provided buffer rings (pbuf) and kernel-managed
buffer rings (kmbuf):
- For pbuf rings: ring_addr specifies the user-provided ring address
- For kmbuf rings: buf_size specifies the size of each buffer. buf_size
  must be non-zero and page-aligned.

The implementation follows the same pattern as pbuf ring registration,
reusing the validation and buffer list allocation helpers introduced in
earlier refactoring. The IOBL_KERNEL_MANAGED flag marks buffer lists as
kernel-managed for appropriate handling in the I/O path.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 include/uapi/linux/io_uring.h |  15 ++++-
 io_uring/kbuf.c               |  81 ++++++++++++++++++++++++-
 io_uring/kbuf.h               |   7 ++-
 io_uring/memmap.c             | 111 ++++++++++++++++++++++++++++++++++
 io_uring/memmap.h             |   4 ++
 io_uring/register.c           |   7 +++
 6 files changed, 219 insertions(+), 6 deletions(-)

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index fc473af6feb4..a0889c1744bd 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -715,6 +715,10 @@ enum io_uring_register_op {
 	/* register bpf filtering programs */
 	IORING_REGISTER_BPF_FILTER		= 37,
 
+	/* register/unregister kernel-managed ring buffer group */
+	IORING_REGISTER_KMBUF_RING		= 38,
+	IORING_UNREGISTER_KMBUF_RING		= 39,
+
 	/* this goes last */
 	IORING_REGISTER_LAST,
 
@@ -891,9 +895,16 @@ enum io_uring_register_pbuf_ring_flags {
 	IOU_PBUF_RING_INC	= 2,
 };
 
-/* argument for IORING_(UN)REGISTER_PBUF_RING */
+/* argument for IORING_(UN)REGISTER_PBUF_RING and
+ * IORING_(UN)REGISTER_KMBUF_RING
+ */
 struct io_uring_buf_reg {
-	__u64	ring_addr;
+	union {
+		/* used for pbuf rings */
+		__u64	ring_addr;
+		/* used for kmbuf rings */
+		__u32   buf_size;
+	};
 	__u32	ring_entries;
 	__u16	bgid;
 	__u16	flags;
diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index aa9b70b72db4..9bc36451d083 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -427,10 +427,13 @@ static int io_remove_buffers_legacy(struct io_ring_ctx *ctx,
 
 static void io_put_bl(struct io_ring_ctx *ctx, struct io_buffer_list *bl)
 {
-	if (bl-&gt;flags &amp; IOBL_BUF_RING)
+	if (bl-&gt;flags &amp; IOBL_BUF_RING) {
 		io_free_region(ctx-&gt;user, &amp;bl-&gt;region);
-	else
+		if (bl-&gt;flags &amp; IOBL_KERNEL_MANAGED)
+			kfree(bl-&gt;buf_ring);
+	} else {
 		io_remove_buffers_legacy(ctx, bl, -1U);
+	}
 
 	kfree(bl);
 }
@@ -779,3 +782,77 @@ struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,
 		return NULL;
 	return &amp;bl-&gt;region;
 }
+
+static int io_setup_kmbuf_ring(struct io_ring_ctx *ctx,
+			       struct io_buffer_list *bl,
+			       struct io_uring_buf_reg *reg)
+{
+	struct io_uring_buf_ring *ring;
+	unsigned long ring_size;
+	void *buf_region;
+	unsigned int i;
+	int ret;
+
+	/* allocate pages for the ring structure */
+	ring_size = flex_array_size(ring, bufs, bl-&gt;nr_entries);
+	ring = kzalloc(ring_size, GFP_KERNEL_ACCOUNT);
+	if (!ring)
+		return -ENOMEM;
+
+	ret = io_create_region_multi_buf(ctx, &amp;bl-&gt;region, bl-&gt;nr_entries,
+					 reg-&gt;buf_size);
+	if (ret) {
+		kfree(ring);
+		return ret;
+	}
+
+	/* initialize ring buf entries to point to the buffers */
+	buf_region = bl-&gt;region.ptr;
+	for (i = 0; i &lt; bl-&gt;nr_entries; i++) {
+		struct io_uring_buf *buf = &amp;ring-&gt;bufs[i];
+
+		buf-&gt;addr = (u64)(uintptr_t)buf_region;
+		buf-&gt;len = reg-&gt;buf_size;
+		buf-&gt;bid = i;
+
+		buf_region += reg-&gt;buf_size;
+	}
+	ring-&gt;tail = bl-&gt;nr_entries;
+
+	bl-&gt;buf_ring = ring;
+	bl-&gt;flags |= IOBL_KERNEL_MANAGED;
+
+	return 0;
+}
+
+int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
+{
+	struct io_uring_buf_reg reg;
+	struct io_buffer_list *bl;
+	int ret;
+
+	lockdep_assert_held(&amp;ctx-&gt;uring_lock);
+
+	ret = io_copy_and_validate_buf_reg(arg, &amp;reg, 0);
+	if (ret)
+		return ret;
+
+	if (!reg.buf_size || !PAGE_ALIGNED(reg.buf_size))
+		return -EINVAL;
+
+	bl = io_alloc_new_buffer_list(ctx, &amp;reg);
+	if (IS_ERR(bl))
+		return PTR_ERR(bl);
+
+	ret = io_setup_kmbuf_ring(ctx, bl, &amp;reg);
+	if (ret) {
+		kfree(bl);
+		return ret;
+	}
+
+	ret = io_buffer_add_list(ctx, bl, reg.bgid);
+	if (ret)
+		io_put_bl(ctx, bl);
+
+	return ret;
+}
diff --git a/io_uring/kbuf.h b/io_uring/kbuf.h
index 40b44f4fdb15..62c80a1ebf03 100644
--- a/io_uring/kbuf.h
+++ b/io_uring/kbuf.h
@@ -7,9 +7,11 @@
 
 enum {
 	/* ring mapped provided buffers */
-	IOBL_BUF_RING	= 1,
+	IOBL_BUF_RING		= 1,
 	/* buffers are consumed incrementally rather than always fully */
-	IOBL_INC	= 2,
+	IOBL_INC		= 2,
+	/* buffers are kernel managed */
+	IOBL_KERNEL_MANAGED	= 4,
 };
 
 struct io_buffer_list {
@@ -74,6 +76,7 @@ int io_provide_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe
 int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags);
 
 int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);
+int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg);
 int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg);
 int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg);
 
diff --git a/io_uring/memmap.c b/io_uring/memmap.c
index 89f56609e50a..8d37e93c0433 100644
--- a/io_uring/memmap.c
+++ b/io_uring/memmap.c
@@ -15,6 +15,28 @@
 #include &quot;rsrc.h&quot;
 #include &quot;zcrx.h&quot;
 
+static void release_multi_buf_pages(struct page **pages, unsigned long nr_pages)
+{
+	struct page *page;
+	unsigned int nr, i = 0;
+
+	while (nr_pages) {
+		page = pages[i];
+
+		if (!page || WARN_ON_ONCE(page != compound_head(page)))
+			return;
+
+		nr = compound_nr(page);
+		put_page(page);
+
+		if (WARN_ON_ONCE(nr &gt; nr_pages))
+			return;
+
+		i += nr;
+		nr_pages -= nr;
+	}
+}
+
 static bool io_mem_alloc_compound(struct page **pages, int nr_pages,
 				  size_t size, gfp_t gfp)
 {
@@ -86,6 +108,8 @@ enum {
 	IO_REGION_F_USER_PROVIDED		= 2,
 	/* only the first page in the array is ref&#x27;ed */
 	IO_REGION_F_SINGLE_REF			= 4,
+	/* pages in the array belong to multiple discrete allocations */
+	IO_REGION_F_MULTI_BUF			= 8,
 };
 
 void io_free_region(struct user_struct *user, struct io_mapped_region *mr)
@@ -98,6 +122,8 @@ void io_free_region(struct user_struct *user, struct io_mapped_region *mr)
 
 		if (mr-&gt;flags &amp; IO_REGION_F_USER_PROVIDED)
 			unpin_user_pages(mr-&gt;pages, nr_refs);
+		else if (mr-&gt;flags &amp; IO_REGION_F_MULTI_BUF)
+			release_multi_buf_pages(mr-&gt;pages, nr_refs);
 		else
 			release_pages(mr-&gt;pages, nr_refs);
 
@@ -149,6 +175,54 @@ static int io_region_pin_pages(struct io_mapped_region *mr,
 	return 0;
 }
 
+static int io_region_allocate_pages_multi_buf(struct io_mapped_region *mr,
+					      unsigned int nr_bufs,
+					      unsigned int buf_size)
+{
+	gfp_t gfp = GFP_USER | __GFP_ACCOUNT | __GFP_ZERO | __GFP_NOWARN;
+	struct page **pages, **cur_pages;
+	unsigned int nr_allocated;
+	unsigned int buf_pages;
+	unsigned int i;
+
+	if (!PAGE_ALIGNED(buf_size))
+		return -EINVAL;
+
+	buf_pages = buf_size &gt;&gt; PAGE_SHIFT;
+
+	pages = kvmalloc_array(mr-&gt;nr_pages, sizeof(*pages), gfp);
+	if (!pages)
+		return -ENOMEM;
+
+	cur_pages = pages;
+
+	for (i = 0; i &lt; nr_bufs; i++) {
+		if (io_mem_alloc_compound(cur_pages, buf_pages, buf_size,
+					  gfp)) {
+			cur_pages += buf_pages;
+			continue;
+		}
+
+		nr_allocated = alloc_pages_bulk_node(gfp, NUMA_NO_NODE,
+						     buf_pages, cur_pages);
+		if (nr_allocated != buf_pages) {
+			unsigned int total =
+				(cur_pages - pages) + nr_allocated;
+
+			release_multi_buf_pages(pages, total);
+			kvfree(pages);
+			return -ENOMEM;
+		}
+
+		cur_pages += buf_pages;
+	}
+
+	mr-&gt;flags |= IO_REGION_F_MULTI_BUF;
+	mr-&gt;pages = pages;
+
+	return 0;
+}
+
 static int io_region_allocate_pages(struct io_mapped_region *mr,
 				    struct io_uring_region_desc *reg,
 				    unsigned long mmap_offset)
@@ -181,6 +255,43 @@ static int io_region_allocate_pages(struct io_mapped_region *mr,
 	return 0;
 }
 
+int io_create_region_multi_buf(struct io_ring_ctx *ctx,
+			       struct io_mapped_region *mr,
+			       unsigned int nr_bufs, unsigned int buf_size)
+{
+	unsigned int nr_pages;
+	int ret;
+
+	if (WARN_ON_ONCE(mr-&gt;pages || mr-&gt;ptr || mr-&gt;nr_pages))
+		return -EFAULT;
+
+	if (WARN_ON_ONCE(!nr_bufs || !buf_size || !PAGE_ALIGNED(buf_size)))
+		return -EINVAL;
+
+	if (check_mul_overflow(buf_size &gt;&gt; PAGE_SHIFT, nr_bufs, &amp;nr_pages))
+		return -EINVAL;
+
+	if (ctx-&gt;user) {
+		ret = __io_account_mem(ctx-&gt;user, nr_pages);
+		if (ret)
+			return ret;
+	}
+	mr-&gt;nr_pages = nr_pages;
+
+	ret = io_region_allocate_pages_multi_buf(mr, nr_bufs, buf_size);
+	if (ret)
+		goto out_free;
+
+	ret = io_region_init_ptr(mr);
+	if (ret)
+		goto out_free;
+
+	return 0;
+out_free:
+	io_free_region(ctx-&gt;user, mr);
+	return ret;
+}
+
 int io_create_region(struct io_ring_ctx *ctx, struct io_mapped_region *mr,
 		     struct io_uring_region_desc *reg,
 		     unsigned long mmap_offset)
diff --git a/io_uring/memmap.h b/io_uring/memmap.h
index f4cfbb6b9a1f..3aa1167462ae 100644
--- a/io_uring/memmap.h
+++ b/io_uring/memmap.h
@@ -22,6 +22,10 @@ int io_create_region(struct io_ring_ctx *ctx, struct io_mapped_region *mr,
 		     struct io_uring_region_desc *reg,
 		     unsigned long mmap_offset);
 
+int io_create_region_multi_buf(struct io_ring_ctx *ctx,
+			       struct io_mapped_region *mr,
+			       unsigned int nr_bufs, unsigned int buf_size);
+
 static inline void *io_region_get_ptr(struct io_mapped_region *mr)
 {
 	return mr-&gt;ptr;
diff --git a/io_uring/register.c b/io_uring/register.c
index 0882cb34f851..2db8daaf8fde 100644
--- a/io_uring/register.c
+++ b/io_uring/register.c
@@ -837,7 +837,14 @@ static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,
 			break;
 		ret = io_register_pbuf_ring(ctx, arg);
 		break;
+	case IORING_REGISTER_KMBUF_RING:
+		ret = -EINVAL;
+		if (!arg || nr_args != 1)
+			break;
+		ret = io_register_kmbuf_ring(ctx, arg);
+		break;
 	case IORING_UNREGISTER_PBUF_RING:
+	case IORING_UNREGISTER_KMBUF_RING:
 		ret = -EINVAL;
 		if (!arg || nr_args != 1)
 			break;
-- 
2.47.3



---

Add support for mmapping kernel-managed buffer rings (kmbuf) to
userspace, allowing applications to access the kernel-allocated buffers.

Similar to application-provided buffer rings (pbuf), kmbuf rings use the
buffer group ID encoded in the mmap offset to identify which buffer ring
to map. The implementation follows the same pattern as pbuf rings.

New mmap offset constants are introduced:
  - IORING_OFF_KMBUF_RING (0x88000000): Base offset for kmbuf mappings
  - IORING_OFF_KMBUF_SHIFT (16): Shift value to encode buffer group ID

The mmap offset encodes the bgid shifted by IORING_OFF_KMBUF_SHIFT.
The io_buf_get_region() helper retrieves the appropriate region.

This allows userspace to mmap the kernel-allocated buffer region and
access the buffers directly.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 include/uapi/linux/io_uring.h |  2 ++
 io_uring/kbuf.c               | 11 +++++++++--
 io_uring/kbuf.h               |  5 +++--
 io_uring/memmap.c             |  5 ++++-
 4 files changed, 18 insertions(+), 5 deletions(-)

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index a0889c1744bd..42a2812c9922 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -545,6 +545,8 @@ struct io_uring_cqe {
 #define IORING_OFF_SQES			0x10000000ULL
 #define IORING_OFF_PBUF_RING		0x80000000ULL
 #define IORING_OFF_PBUF_SHIFT		16
+#define IORING_OFF_KMBUF_RING		0x88000000ULL
+#define IORING_OFF_KMBUF_SHIFT		16
 #define IORING_OFF_MMAP_MASK		0xf8000000ULL
 
 /*
diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index 9bc36451d083..ccf5b213087b 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -770,16 +770,23 @@ int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg)
 	return 0;
 }
 
-struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,
-					    unsigned int bgid)
+struct io_mapped_region *io_buf_get_region(struct io_ring_ctx *ctx,
+					   unsigned int bgid,
+					   bool kernel_managed)
 {
 	struct io_buffer_list *bl;
+	bool is_kernel_managed;
 
 	lockdep_assert_held(&amp;ctx-&gt;mmap_lock);
 
 	bl = xa_load(&amp;ctx-&gt;io_bl_xa, bgid);
 	if (!bl || !(bl-&gt;flags &amp; IOBL_BUF_RING))
 		return NULL;
+
+	is_kernel_managed = !!(bl-&gt;flags &amp; IOBL_KERNEL_MANAGED);
+	if (is_kernel_managed != kernel_managed)
+		return NULL;
+
 	return &amp;bl-&gt;region;
 }
 
diff --git a/io_uring/kbuf.h b/io_uring/kbuf.h
index 62c80a1ebf03..11d165888b8e 100644
--- a/io_uring/kbuf.h
+++ b/io_uring/kbuf.h
@@ -88,8 +88,9 @@ unsigned int __io_put_kbufs(struct io_kiocb *req, struct io_buffer_list *bl,
 bool io_kbuf_commit(struct io_kiocb *req,
 		    struct io_buffer_list *bl, int len, int nr);
 
-struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,
-					    unsigned int bgid);
+struct io_mapped_region *io_buf_get_region(struct io_ring_ctx *ctx,
+					   unsigned int bgid,
+					   bool kernel_managed);
 
 static inline bool io_kbuf_recycle_ring(struct io_kiocb *req,
 					struct io_buffer_list *bl)
diff --git a/io_uring/memmap.c b/io_uring/memmap.c
index 8d37e93c0433..916315122323 100644
--- a/io_uring/memmap.c
+++ b/io_uring/memmap.c
@@ -356,7 +356,10 @@ static struct io_mapped_region *io_mmap_get_region(struct io_ring_ctx *ctx,
 		return &amp;ctx-&gt;sq_region;
 	case IORING_OFF_PBUF_RING:
 		id = (offset &amp; ~IORING_OFF_MMAP_MASK) &gt;&gt; IORING_OFF_PBUF_SHIFT;
-		return io_pbuf_get_region(ctx, id);
+		return io_buf_get_region(ctx, id, false);
+	case IORING_OFF_KMBUF_RING:
+		id = (offset &amp; ~IORING_OFF_MMAP_MASK) &gt;&gt; IORING_OFF_KMBUF_SHIFT;
+		return io_buf_get_region(ctx, id, true);
 	case IORING_MAP_OFF_PARAM_REGION:
 		return &amp;ctx-&gt;param_region;
 	case IORING_MAP_OFF_ZCRX_REGION:
-- 
2.47.3



---

Allow kernel-managed buffers to be selected. This requires modifying the
io_br_sel struct to separate the fields for address and val, since a
kernel address cannot be distinguished from a negative val when error
checking.

Auto-commit any selected kernel-managed buffer.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 include/linux/io_uring_types.h |  8 ++++----
 io_uring/kbuf.c                | 16 ++++++++++++----
 2 files changed, 16 insertions(+), 8 deletions(-)

diff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h
index 3e4a82a6f817..36cc2e0346d9 100644
--- a/include/linux/io_uring_types.h
+++ b/include/linux/io_uring_types.h
@@ -93,13 +93,13 @@ struct io_mapped_region {
  */
 struct io_br_sel {
 	struct io_buffer_list *buf_list;
-	/*
-	 * Some selection parts return the user address, others return an error.
-	 */
 	union {
+		/* for classic/ring provided buffers */
 		void __user *addr;
-		ssize_t val;
+		/* for kernel-managed buffers */
+		void *kaddr;
 	};
+	ssize_t val;
 };
 
 
diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index ccf5b213087b..1e8395270227 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -155,7 +155,8 @@ static int io_provided_buffers_select(struct io_kiocb *req, size_t *len,
 	return 1;
 }
 
-static bool io_should_commit(struct io_kiocb *req, unsigned int issue_flags)
+static bool io_should_commit(struct io_kiocb *req, struct io_buffer_list *bl,
+			     unsigned int issue_flags)
 {
 	/*
 	* If we came in unlocked, we have no choice but to consume the
@@ -170,7 +171,11 @@ static bool io_should_commit(struct io_kiocb *req, unsigned int issue_flags)
 	if (issue_flags &amp; IO_URING_F_UNLOCKED)
 		return true;
 
-	/* uring_cmd commits kbuf upfront, no need to auto-commit */
+	/* kernel-managed buffers are auto-committed */
+	if (bl-&gt;flags &amp; IOBL_KERNEL_MANAGED)
+		return true;
+
+	/* multishot uring_cmd commits kbuf upfront, no need to auto-commit */
 	if (!io_file_can_poll(req) &amp;&amp; req-&gt;opcode != IORING_OP_URING_CMD)
 		return true;
 	return false;
@@ -200,9 +205,12 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,
 	req-&gt;flags |= REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT;
 	req-&gt;buf_index = READ_ONCE(buf-&gt;bid);
 	sel.buf_list = bl;
-	sel.addr = u64_to_user_ptr(READ_ONCE(buf-&gt;addr));
+	if (bl-&gt;flags &amp; IOBL_KERNEL_MANAGED)
+		sel.kaddr = (void *)(uintptr_t)READ_ONCE(buf-&gt;addr);
+	else
+		sel.addr = u64_to_user_ptr(READ_ONCE(buf-&gt;addr));
 
-	if (io_should_commit(req, issue_flags)) {
+	if (io_should_commit(req, bl, issue_flags)) {
 		io_kbuf_commit(req, sel.buf_list, *len, 1);
 		sel.buf_list = NULL;
 	}
-- 
2.47.3



---

Add kernel APIs to pin and unpin buffer rings, preventing userspace from
unregistering a buffer ring while it is pinned by the kernel.

This provides a mechanism for kernel subsystems to safely access buffer
ring contents while ensuring the buffer ring remains valid. A pinned
buffer ring cannot be unregistered until explicitly unpinned. On the
userspace side, trying to unregister a pinned buffer will return -EBUSY.

This is a preparatory change for upcoming fuse usage of kernel-managed
buffer rings. It is necessary for fuse to pin the buffer ring because
fuse may need to select a buffer in atomic contexts, which it can only
do so by using the underlying buffer list pointer.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 include/linux/io_uring/cmd.h | 17 +++++++++++++
 io_uring/kbuf.c              | 48 ++++++++++++++++++++++++++++++++++++
 io_uring/kbuf.h              |  5 ++++
 3 files changed, 70 insertions(+)

diff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h
index 375fd048c4cb..702b1903e6ee 100644
--- a/include/linux/io_uring/cmd.h
+++ b/include/linux/io_uring/cmd.h
@@ -84,6 +84,10 @@ struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,
 bool io_uring_mshot_cmd_post_cqe(struct io_uring_cmd *ioucmd,
 				 struct io_br_sel *sel, unsigned int issue_flags);
 
+int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,
+			  unsigned issue_flags, struct io_buffer_list **bl);
+int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,
+			    unsigned issue_flags);
 #else
 static inline int
 io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,
@@ -126,6 +130,19 @@ static inline bool io_uring_mshot_cmd_post_cqe(struct io_uring_cmd *ioucmd,
 {
 	return true;
 }
+static inline int io_uring_buf_ring_pin(struct io_uring_cmd *cmd,
+					unsigned buf_group,
+					unsigned issue_flags,
+					struct io_buffer_list **bl)
+{
+	return -EOPNOTSUPP;
+}
+static inline int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd,
+					  unsigned buf_group,
+					  unsigned issue_flags)
+{
+	return -EOPNOTSUPP;
+}
 #endif
 
 static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)
diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index 1e8395270227..dee1764ed19f 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -9,6 +9,7 @@
 #include &lt;linux/poll.h&gt;
 #include &lt;linux/vmalloc.h&gt;
 #include &lt;linux/io_uring.h&gt;
+#include &lt;linux/io_uring/cmd.h&gt;
 
 #include &lt;uapi/linux/io_uring.h&gt;
 
@@ -237,6 +238,51 @@ struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,
 	return sel;
 }
 
+int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,
+			  unsigned issue_flags, struct io_buffer_list **bl)
+{
+	struct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)-&gt;ctx;
+	struct io_buffer_list *buffer_list;
+	int ret = -EINVAL;
+
+	io_ring_submit_lock(ctx, issue_flags);
+
+	buffer_list = io_buffer_get_list(ctx, buf_group);
+	if (buffer_list &amp;&amp; (buffer_list-&gt;flags &amp; IOBL_BUF_RING)) {
+		if (unlikely(buffer_list-&gt;flags &amp; IOBL_PINNED)) {
+			ret = -EALREADY;
+		} else {
+			buffer_list-&gt;flags |= IOBL_PINNED;
+			ret = 0;
+			*bl = buffer_list;
+		}
+	}
+
+	io_ring_submit_unlock(ctx, issue_flags);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(io_uring_buf_ring_pin);
+
+int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,
+		       unsigned issue_flags)
+{
+	struct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)-&gt;ctx;
+	struct io_buffer_list *bl;
+	int ret = -EINVAL;
+
+	io_ring_submit_lock(ctx, issue_flags);
+
+	bl = io_buffer_get_list(ctx, buf_group);
+	if (bl &amp;&amp; (bl-&gt;flags &amp; IOBL_BUF_RING) &amp;&amp; (bl-&gt;flags &amp; IOBL_PINNED)) {
+		bl-&gt;flags &amp;= ~IOBL_PINNED;
+		ret = 0;
+	}
+
+	io_ring_submit_unlock(ctx, issue_flags);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(io_uring_buf_ring_unpin);
+
 /* cap it at a reasonable 256, will be one page even for 4K */
 #define PEEK_MAX_IMPORT		256
 
@@ -747,6 +793,8 @@ int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg)
 		return -ENOENT;
 	if (!(bl-&gt;flags &amp; IOBL_BUF_RING))
 		return -EINVAL;
+	if (bl-&gt;flags &amp; IOBL_PINNED)
+		return -EBUSY;
 
 	scoped_guard(mutex, &amp;ctx-&gt;mmap_lock)
 		xa_erase(&amp;ctx-&gt;io_bl_xa, bl-&gt;bgid);
diff --git a/io_uring/kbuf.h b/io_uring/kbuf.h
index 11d165888b8e..781630c2cc10 100644
--- a/io_uring/kbuf.h
+++ b/io_uring/kbuf.h
@@ -12,6 +12,11 @@ enum {
 	IOBL_INC		= 2,
 	/* buffers are kernel managed */
 	IOBL_KERNEL_MANAGED	= 4,
+	/*
+	 * buffer ring is pinned and cannot be unregistered by userspace until
+	 * it has been unpinned
+	 */
+	IOBL_PINNED		= 8,
 };
 
 struct io_buffer_list {
-- 
2.47.3



---

Add an interface for buffers to be recycled back into a kernel-managed
buffer ring.

This is a preparatory patch for fuse over io-uring.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 include/linux/io_uring/cmd.h | 11 +++++++++
 io_uring/kbuf.c              | 44 ++++++++++++++++++++++++++++++++++++
 2 files changed, 55 insertions(+)

diff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h
index 702b1903e6ee..a488e945f883 100644
--- a/include/linux/io_uring/cmd.h
+++ b/include/linux/io_uring/cmd.h
@@ -88,6 +88,10 @@ int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,
 			  unsigned issue_flags, struct io_buffer_list **bl);
 int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,
 			    unsigned issue_flags);
+
+int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,
+			   u64 addr, unsigned int len, unsigned int bid,
+			   unsigned int issue_flags);
 #else
 static inline int
 io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,
@@ -143,6 +147,13 @@ static inline int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd,
 {
 	return -EOPNOTSUPP;
 }
+static inline int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd,
+					 unsigned int buf_group, u64 addr,
+					 unsigned int len, unsigned int bid,
+					 unsigned int issue_flags)
+{
+	return -EOPNOTSUPP;
+}
 #endif
 
 static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)
diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index dee1764ed19f..17b6178be4ce 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -102,6 +102,50 @@ void io_kbuf_drop_legacy(struct io_kiocb *req)
 	req-&gt;kbuf = NULL;
 }
 
+int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,
+			   u64 addr, unsigned int len, unsigned int bid,
+			   unsigned int issue_flags)
+{
+	struct io_kiocb *req = cmd_to_io_kiocb(cmd);
+	struct io_ring_ctx *ctx = req-&gt;ctx;
+	struct io_uring_buf_ring *br;
+	struct io_uring_buf *buf;
+	struct io_buffer_list *bl;
+	int ret = -EINVAL;
+
+	if (WARN_ON_ONCE(req-&gt;flags &amp; REQ_F_BUFFERS_COMMIT))
+		return ret;
+
+	io_ring_submit_lock(ctx, issue_flags);
+
+	bl = io_buffer_get_list(ctx, buf_group);
+
+	if (!bl || WARN_ON_ONCE(!(bl-&gt;flags &amp; IOBL_BUF_RING)) ||
+	    WARN_ON_ONCE(!(bl-&gt;flags &amp; IOBL_KERNEL_MANAGED)))
+		goto done;
+
+	br = bl-&gt;buf_ring;
+
+	if (WARN_ON_ONCE((br-&gt;tail - bl-&gt;head) &gt;= bl-&gt;nr_entries))
+		goto done;
+
+	buf = &amp;br-&gt;bufs[(br-&gt;tail) &amp; bl-&gt;mask];
+
+	buf-&gt;addr = addr;
+	buf-&gt;len = len;
+	buf-&gt;bid = bid;
+
+	req-&gt;flags &amp;= ~REQ_F_BUFFER_RING;
+
+	br-&gt;tail++;
+	ret = 0;
+
+done:
+	io_ring_submit_unlock(ctx, issue_flags);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(io_uring_kmbuf_recycle);
+
 bool io_kbuf_recycle_legacy(struct io_kiocb *req, unsigned issue_flags)
 {
 	struct io_ring_ctx *ctx = req-&gt;ctx;
-- 
2.47.3



---

io_uring_is_kmbuf_ring() returns true if there is a kernel-managed
buffer ring at the specified buffer group.

This is a preparatory patch for upcoming fuse kernel-managed buffer
support, which needs to ensure the buffer ring registered by the server
is a kernel-managed buffer ring.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 include/linux/io_uring/cmd.h |  9 +++++++++
 io_uring/kbuf.c              | 20 ++++++++++++++++++++
 2 files changed, 29 insertions(+)

diff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h
index a488e945f883..04a937f6f4d3 100644
--- a/include/linux/io_uring/cmd.h
+++ b/include/linux/io_uring/cmd.h
@@ -92,6 +92,9 @@ int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,
 int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,
 			   u64 addr, unsigned int len, unsigned int bid,
 			   unsigned int issue_flags);
+
+bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,
+			    unsigned int issue_flags);
 #else
 static inline int
 io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,
@@ -154,6 +157,12 @@ static inline int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd,
 {
 	return -EOPNOTSUPP;
 }
+static inline bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd,
+					  unsigned int buf_group,
+					  unsigned int issue_flags)
+{
+	return false;
+}
 #endif
 
 static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)
diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index 17b6178be4ce..797cc2f0a5e9 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -963,3 +963,23 @@ int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
 
 	return ret;
 }
+
+bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,
+			    unsigned int issue_flags)
+{
+	struct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)-&gt;ctx;
+	struct io_buffer_list *bl;
+	bool is_kmbuf_ring = false;
+
+	io_ring_submit_lock(ctx, issue_flags);
+
+	bl = io_buffer_get_list(ctx, buf_group);
+	if (likely(bl) &amp;&amp; (bl-&gt;flags &amp; IOBL_KERNEL_MANAGED)) {
+		WARN_ON_ONCE(!(bl-&gt;flags &amp; IOBL_BUF_RING));
+		is_kmbuf_ring = true;
+	}
+
+	io_ring_submit_unlock(ctx, issue_flags);
+	return is_kmbuf_ring;
+}
+EXPORT_SYMBOL_GPL(io_uring_is_kmbuf_ring);
-- 
2.47.3



---

Export io_ring_buffer_select() so that it may be used by callers who
pass in a pinned bufring without needing to grab the io_uring mutex.

This is a preparatory patch that will be needed by fuse io-uring, which
will need to select a buffer from a kernel-managed bufring while the
uring mutex may already be held by in-progress commits, and may need to
select a buffer in atomic contexts.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 include/linux/io_uring/cmd.h | 14 ++++++++++++++
 io_uring/kbuf.c              |  7 ++++---
 2 files changed, 18 insertions(+), 3 deletions(-)

diff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h
index 04a937f6f4d3..d4b5943bdeb1 100644
--- a/include/linux/io_uring/cmd.h
+++ b/include/linux/io_uring/cmd.h
@@ -95,6 +95,10 @@ int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,
 
 bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,
 			    unsigned int issue_flags);
+
+struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,
+				       struct io_buffer_list *bl,
+				       unsigned int issue_flags);
 #else
 static inline int
 io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,
@@ -163,6 +167,16 @@ static inline bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd,
 {
 	return false;
 }
+static inline struct io_br_sel io_ring_buffer_select(struct io_kiocb *req,
+						     size_t *len,
+						     struct io_buffer_list *bl,
+						     unsigned int issue_flags)
+{
+	struct io_br_sel sel = {
+		.val = -EOPNOTSUPP,
+	};
+	return sel;
+}
 #endif
 
 static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)
diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index 797cc2f0a5e9..9a93f10d3214 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -226,9 +226,9 @@ static bool io_should_commit(struct io_kiocb *req, struct io_buffer_list *bl,
 	return false;
 }
 
-static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,
-					      struct io_buffer_list *bl,
-					      unsigned int issue_flags)
+struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,
+				       struct io_buffer_list *bl,
+				       unsigned int issue_flags)
 {
 	struct io_uring_buf_ring *br = bl-&gt;buf_ring;
 	__u16 tail, head = bl-&gt;head;
@@ -261,6 +261,7 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,
 	}
 	return sel;
 }
+EXPORT_SYMBOL_GPL(io_ring_buffer_select);
 
 struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,
 				  unsigned buf_group, unsigned int issue_flags)
-- 
2.47.3



---

Return the id of the selected buffer in io_buffer_select(). This is
needed for kernel-managed buffer rings to later recycle the selected
buffer.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 include/linux/io_uring/cmd.h   | 2 +-
 include/linux/io_uring_types.h | 2 ++
 io_uring/kbuf.c                | 7 +++++--
 3 files changed, 8 insertions(+), 3 deletions(-)

diff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h
index d4b5943bdeb1..94df2bdebe77 100644
--- a/include/linux/io_uring/cmd.h
+++ b/include/linux/io_uring/cmd.h
@@ -71,7 +71,7 @@ void io_uring_cmd_issue_blocking(struct io_uring_cmd *ioucmd);
 
 /*
  * Select a buffer from the provided buffer group for multishot uring_cmd.
- * Returns the selected buffer address and size.
+ * Returns the selected buffer address, size, and id.
  */
 struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,
 					    unsigned buf_group, size_t *len,
diff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h
index 36cc2e0346d9..5a56bb341337 100644
--- a/include/linux/io_uring_types.h
+++ b/include/linux/io_uring_types.h
@@ -100,6 +100,8 @@ struct io_br_sel {
 		void *kaddr;
 	};
 	ssize_t val;
+	/* id of the selected buffer */
+	unsigned buf_id;
 };
 
 
diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index 9a93f10d3214..24c1e34ea23e 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -250,6 +250,7 @@ struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,
 	req-&gt;flags |= REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT;
 	req-&gt;buf_index = READ_ONCE(buf-&gt;bid);
 	sel.buf_list = bl;
+	sel.buf_id = req-&gt;buf_index;
 	if (bl-&gt;flags &amp; IOBL_KERNEL_MANAGED)
 		sel.kaddr = (void *)(uintptr_t)READ_ONCE(buf-&gt;addr);
 	else
@@ -274,10 +275,12 @@ struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,
 
 	bl = io_buffer_get_list(ctx, buf_group);
 	if (likely(bl)) {
-		if (bl-&gt;flags &amp; IOBL_BUF_RING)
+		if (bl-&gt;flags &amp; IOBL_BUF_RING) {
 			sel = io_ring_buffer_select(req, len, bl, issue_flags);
-		else
+		} else {
 			sel.addr = io_provided_buffer_select(req, len, bl);
+			sel.buf_id = req-&gt;buf_index;
+		}
 	}
 	io_ring_submit_unlock(req-&gt;ctx, issue_flags);
 	return sel;
-- 
2.47.3



---

When uring_cmd operations select a buffer, the completion queue entry
should indicate which buffer was selected.

Set IORING_CQE_F_BUFFER on the completed entry and encode the buffer
index if a buffer was selected.

This will be needed for fuse, which needs to relay to userspace which
selected buffer contains the data.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 io_uring/uring_cmd.c | 6 +++++-
 1 file changed, 5 insertions(+), 1 deletion(-)

diff --git a/io_uring/uring_cmd.c b/io_uring/uring_cmd.c
index ee7b49f47cb5..6d38df1a812d 100644
--- a/io_uring/uring_cmd.c
+++ b/io_uring/uring_cmd.c
@@ -151,6 +151,7 @@ void __io_uring_cmd_done(struct io_uring_cmd *ioucmd, s32 ret, u64 res2,
 		       unsigned issue_flags, bool is_cqe32)
 {
 	struct io_kiocb *req = cmd_to_io_kiocb(ioucmd);
+	u32 cflags = 0;
 
 	if (WARN_ON_ONCE(req-&gt;flags &amp; REQ_F_APOLL_MULTISHOT))
 		return;
@@ -160,7 +161,10 @@ void __io_uring_cmd_done(struct io_uring_cmd *ioucmd, s32 ret, u64 res2,
 	if (ret &lt; 0)
 		req_set_fail(req);
 
-	io_req_set_res(req, ret, 0);
+	if (req-&gt;flags &amp; (REQ_F_BUFFER_SELECTED | REQ_F_BUFFER_RING))
+		cflags |= IORING_CQE_F_BUFFER |
+			(req-&gt;buf_index &lt;&lt; IORING_CQE_BUFFER_SHIFT);
+	io_req_set_res(req, ret, cflags);
 	if (is_cqe32) {
 		if (req-&gt;ctx-&gt;flags &amp; IORING_SETUP_CQE_MIXED)
 			req-&gt;cqe.flags |= IORING_CQE_F_32;
-- 
2.47.3

</pre>
</details>
<div class="review-comment-signals">Signals: no explicit approval or disapproval, patch description and code changes are clear</div>
</div>
<div class="thread-children">
<div class="thread-node depth-1" id="2026-02-09">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Jens Axboe</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-09">2026-02-09</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Jens Axboe raised several minor issues and suggestions for improvement in the patch series, including a potential issue with int promotion, a simpler way to handle buffer selection, and readability improvements.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On 2/9/26 5:28 PM, Joanne Koong wrote:
&gt; +int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,
&gt; +			   u64 addr, unsigned int len, unsigned int bid,
&gt; +			   unsigned int issue_flags)
&gt; +{
&gt; +	struct io_kiocb *req = cmd_to_io_kiocb(cmd);
&gt; +	struct io_ring_ctx *ctx = req-&gt;ctx;
&gt; +	struct io_uring_buf_ring *br;
&gt; +	struct io_uring_buf *buf;
&gt; +	struct io_buffer_list *bl;
&gt; +	int ret = -EINVAL;
&gt; +
&gt; +	if (WARN_ON_ONCE(req-&gt;flags &amp; REQ_F_BUFFERS_COMMIT))
&gt; +		return ret;
&gt; +
&gt; +	io_ring_submit_lock(ctx, issue_flags);
&gt; +
&gt; +	bl = io_buffer_get_list(ctx, buf_group);
&gt; +
&gt; +	if (!bl || WARN_ON_ONCE(!(bl-&gt;flags &amp; IOBL_BUF_RING)) ||
&gt; +	    WARN_ON_ONCE(!(bl-&gt;flags &amp; IOBL_KERNEL_MANAGED)))
&gt; +		goto done;
&gt; +
&gt; +	br = bl-&gt;buf_ring;
&gt; +
&gt; +	if (WARN_ON_ONCE((br-&gt;tail - bl-&gt;head) &gt;= bl-&gt;nr_entries))
&gt; +		goto done;

I think you want:

	if (WARN_ON_ONCE((__u16)(br-&gt;tail - bl-&gt;head) &gt;= bl-&gt;nr_entries))

here to avoid int promotion from messing this up if tail has wrapped.

In general, across the patches for the WARN_ON_ONCE(), it&#x27;s not a huge
issue to have a litter of them for now. Hopefully we can prune some of
these down the line, however.

-- 
Jens Axboe


---

On 2/9/26 5:28 PM, Joanne Koong wrote:
&gt; Return the id of the selected buffer in io_buffer_select(). This is
&gt; needed for kernel-managed buffer rings to later recycle the selected
&gt; buffer.
&gt; 
&gt; Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
&gt; ---
&gt;  include/linux/io_uring/cmd.h   | 2 +-
&gt;  include/linux/io_uring_types.h | 2 ++
&gt;  io_uring/kbuf.c                | 7 +++++--
&gt;  3 files changed, 8 insertions(+), 3 deletions(-)
&gt; 
&gt; diff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h
&gt; index d4b5943bdeb1..94df2bdebe77 100644
&gt; --- a/include/linux/io_uring/cmd.h
&gt; +++ b/include/linux/io_uring/cmd.h
&gt; @@ -71,7 +71,7 @@ void io_uring_cmd_issue_blocking(struct io_uring_cmd *ioucmd);
&gt;  
&gt;  /*
&gt;   * Select a buffer from the provided buffer group for multishot uring_cmd.
&gt; - * Returns the selected buffer address and size.
&gt; + * Returns the selected buffer address, size, and id.
&gt;   */
&gt;  struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,
&gt;  					    unsigned buf_group, size_t *len,
&gt; diff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h
&gt; index 36cc2e0346d9..5a56bb341337 100644
&gt; --- a/include/linux/io_uring_types.h
&gt; +++ b/include/linux/io_uring_types.h
&gt; @@ -100,6 +100,8 @@ struct io_br_sel {
&gt;  		void *kaddr;
&gt;  	};
&gt;  	ssize_t val;
&gt; +	/* id of the selected buffer */
&gt; +	unsigned buf_id;
&gt;  };

I&#x27;m probably missing something here, but why can&#x27;t the caller just use
req-&gt;buf_index for this?

-- 
Jens Axboe


---

On 2/9/26 5:28 PM, Joanne Koong wrote:
&gt; Currently, io_uring buffer rings require the application to allocate and
&gt; manage the backing buffers. This series introduces kernel-managed buffer
&gt; rings, where the kernel allocates and manages the buffers on behalf of
&gt; the application.
&gt; 
&gt; This is split out from the fuse over io_uring series in [1], which needs the
&gt; kernel to own and manage buffers shared between the fuse server and the
&gt; kernel.
&gt; 
&gt; This series is on top of the for-next branch in Jens&#x27; io-uring tree. The
&gt; corresponding liburing changes are in [2] and will be submitted after the
&gt; changes in this patchset are accepted.

Generally looks pretty good - for context, do you have a branch with
these patches and the users on top too? Makes it a bit easier for cross
referencing, as some of these really do need an exposed user to make a
good judgement on the helpers.

I know there&#x27;s the older series, but I&#x27;m assuming the latter patches
changed somewhat too, and it&#x27;d be nicer to look at a current set rather
than go back to the older ones.

-- 
Jens Axboe


---

On 2/9/26 5:28 PM, Joanne Koong wrote:
&gt; Add support for mmapping kernel-managed buffer rings (kmbuf) to
&gt; userspace, allowing applications to access the kernel-allocated buffers.
&gt; 
&gt; Similar to application-provided buffer rings (pbuf), kmbuf rings use the
&gt; buffer group ID encoded in the mmap offset to identify which buffer ring
&gt; to map. The implementation follows the same pattern as pbuf rings.
&gt; 
&gt; New mmap offset constants are introduced:
&gt;   - IORING_OFF_KMBUF_RING (0x88000000): Base offset for kmbuf mappings
&gt;   - IORING_OFF_KMBUF_SHIFT (16): Shift value to encode buffer group ID
&gt; 
&gt; The mmap offset encodes the bgid shifted by IORING_OFF_KMBUF_SHIFT.
&gt; The io_buf_get_region() helper retrieves the appropriate region.
&gt; 
&gt; This allows userspace to mmap the kernel-allocated buffer region and
&gt; access the buffers directly.
&gt; 
&gt; Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
&gt; ---
&gt;  include/uapi/linux/io_uring.h |  2 ++
&gt;  io_uring/kbuf.c               | 11 +++++++++--
&gt;  io_uring/kbuf.h               |  5 +++--
&gt;  io_uring/memmap.c             |  5 ++++-
&gt;  4 files changed, 18 insertions(+), 5 deletions(-)
&gt; 
&gt; diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
&gt; index a0889c1744bd..42a2812c9922 100644
&gt; --- a/include/uapi/linux/io_uring.h
&gt; +++ b/include/uapi/linux/io_uring.h
&gt; @@ -545,6 +545,8 @@ struct io_uring_cqe {
&gt;  #define IORING_OFF_SQES			0x10000000ULL
&gt;  #define IORING_OFF_PBUF_RING		0x80000000ULL
&gt;  #define IORING_OFF_PBUF_SHIFT		16
&gt; +#define IORING_OFF_KMBUF_RING		0x88000000ULL
&gt; +#define IORING_OFF_KMBUF_SHIFT		16
&gt;  #define IORING_OFF_MMAP_MASK		0xf8000000ULL
&gt;  
&gt;  /*
&gt; diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
&gt; index 9bc36451d083..ccf5b213087b 100644
&gt; --- a/io_uring/kbuf.c
&gt; +++ b/io_uring/kbuf.c
&gt; @@ -770,16 +770,23 @@ int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg)
&gt;  	return 0;
&gt;  }
&gt;  
&gt; -struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,
&gt; -					    unsigned int bgid)
&gt; +struct io_mapped_region *io_buf_get_region(struct io_ring_ctx *ctx,
&gt; +					   unsigned int bgid,
&gt; +					   bool kernel_managed)
&gt;  {
&gt;  	struct io_buffer_list *bl;
&gt; +	bool is_kernel_managed;
&gt;  
&gt;  	lockdep_assert_held(&amp;ctx-&gt;mmap_lock);
&gt;  
&gt;  	bl = xa_load(&amp;ctx-&gt;io_bl_xa, bgid);
&gt;  	if (!bl || !(bl-&gt;flags &amp; IOBL_BUF_RING))
&gt;  		return NULL;
&gt; +
&gt; +	is_kernel_managed = !!(bl-&gt;flags &amp; IOBL_KERNEL_MANAGED);
&gt; +	if (is_kernel_managed != kernel_managed)
&gt; +		return NULL;
&gt; +
&gt;  	return &amp;bl-&gt;region;
&gt;  }

For this, I think just add another helper - leave io_pbuf_get_region()
and add a bl-&gt;flags &amp; IOBL_KERNEL_MANAGED error check in there, and
add a io_kbuf_get_region() or similar and have a !(bl-&gt;flags &amp;
IOBL_KERNEL_MANAGED) error check in that one.

That&#x27;s easier to read, and there&#x27;s little reason to avoid duplicating
the xa_load() part.

Minor nit, but imho it&#x27;s more readable that way.

-- 
Jens Axboe


---

On 2/9/26 5:28 PM, Joanne Koong wrote:
&gt; +int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,
&gt; +			  unsigned issue_flags, struct io_buffer_list **bl)
&gt; +{
&gt; +	struct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)-&gt;ctx;
&gt; +	struct io_buffer_list *buffer_list;
&gt; +	int ret = -EINVAL;

Probably use the usual struct io_buffer_list *bl here and either use an
ERR_PTR return, or rename the passed on **bl to **blret or something.

&gt; +int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,
&gt; +		       unsigned issue_flags)
&gt; +{
&gt; +	struct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)-&gt;ctx;
&gt; +	struct io_buffer_list *bl;
&gt; +	int ret = -EINVAL;
&gt; +
&gt; +	io_ring_submit_lock(ctx, issue_flags);
&gt; +
&gt; +	bl = io_buffer_get_list(ctx, buf_group);
&gt; +	if (bl &amp;&amp; (bl-&gt;flags &amp; IOBL_BUF_RING) &amp;&amp; (bl-&gt;flags &amp; IOBL_PINNED)) {

Usually done as:

	if ((bl-&gt;flags &amp; (IOBL_BUF_RING|IOBL_PINNED)) == (IOBL_BUF_RING|IOBL_PINNED))

and maybe then just have an earlier

	if (!bl)
		goto err;

&gt; +		bl-&gt;flags &amp;= ~IOBL_PINNED;
&gt; +		ret = 0;
&gt; +	}
err:
&gt; +	io_ring_submit_unlock(ctx, issue_flags);
&gt; +	return ret;
&gt; +}

to avoid making it way too long. For io_uring, it&#x27;s fine to exceed 80
chars where it makes sense.

-- 
Jens Axboe
</pre>
</details>
<div class="review-comment-signals">Signals: minor issues, suggestions</div>
</div>
<div class="thread-children">
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Caleb Mateos</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-10">2026-02-10</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Caleb Mateos pointed out that the patch&#x27;s optimization for buffer ring pinning/unpinning is unnecessary, as modern compilers will perform it automatically.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Mon, Feb 9, 2026 at 5:07 PM Jens Axboe &lt;axboe@kernel.dk&gt; wrote:
&gt;
&gt; On 2/9/26 5:28 PM, Joanne Koong wrote:
&gt; &gt; +int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,
&gt; &gt; +                       unsigned issue_flags, struct io_buffer_list **bl)
&gt; &gt; +{
&gt; &gt; +     struct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)-&gt;ctx;
&gt; &gt; +     struct io_buffer_list *buffer_list;
&gt; &gt; +     int ret = -EINVAL;
&gt;
&gt; Probably use the usual struct io_buffer_list *bl here and either use an
&gt; ERR_PTR return, or rename the passed on **bl to **blret or something.
&gt;
&gt; &gt; +int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,
&gt; &gt; +                    unsigned issue_flags)
&gt; &gt; +{
&gt; &gt; +     struct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)-&gt;ctx;
&gt; &gt; +     struct io_buffer_list *bl;
&gt; &gt; +     int ret = -EINVAL;
&gt; &gt; +
&gt; &gt; +     io_ring_submit_lock(ctx, issue_flags);
&gt; &gt; +
&gt; &gt; +     bl = io_buffer_get_list(ctx, buf_group);
&gt; &gt; +     if (bl &amp;&amp; (bl-&gt;flags &amp; IOBL_BUF_RING) &amp;&amp; (bl-&gt;flags &amp; IOBL_PINNED)) {
&gt;
&gt; Usually done as:
&gt;
&gt;         if ((bl-&gt;flags &amp; (IOBL_BUF_RING|IOBL_PINNED)) == (IOBL_BUF_RING|IOBL_PINNED))

FWIW, modern compilers will perform this optimization automatically.
They&#x27;ll even optimize it further to !(~bl-&gt;flags &amp;
(IOBL_BUF_RING|IOBL_PINNED)): https://godbolt.org/z/xGoP4TfhP

Best,
Caleb

&gt;
&gt; and maybe then just have an earlier
&gt;
&gt;         if (!bl)
&gt;                 goto err;
&gt;
&gt; &gt; +             bl-&gt;flags &amp;= ~IOBL_PINNED;
&gt; &gt; +             ret = 0;
&gt; &gt; +     }
&gt; err:
&gt; &gt; +     io_ring_submit_unlock(ctx, issue_flags);
&gt; &gt; +     return ret;
&gt; &gt; +}
&gt;
&gt; to avoid making it way too long. For io_uring, it&#x27;s fine to exceed 80
&gt; chars where it makes sense.
&gt;
&gt; --
&gt; Jens Axboe
</pre>
</details>
<div class="review-comment-signals">Signals: optimization, compiler</div>
</div>
<div class="thread-children">
<div class="thread-node depth-3">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Jens Axboe</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-09">2026-02-09</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Jens Axboe suggested reordering code in __io_uring_cmd_done() for readability</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On 2/10/26 10:57 AM, Caleb Sander Mateos wrote:
&gt; On Mon, Feb 9, 2026 at 5:07?PM Jens Axboe &lt;axboe@kernel.dk&gt; wrote:
&gt;&gt;
&gt;&gt; On 2/9/26 5:28 PM, Joanne Koong wrote:
&gt;&gt;&gt; +int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,
&gt;&gt;&gt; +                       unsigned issue_flags, struct io_buffer_list **bl)
&gt;&gt;&gt; +{
&gt;&gt;&gt; +     struct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)-&gt;ctx;
&gt;&gt;&gt; +     struct io_buffer_list *buffer_list;
&gt;&gt;&gt; +     int ret = -EINVAL;
&gt;&gt;
&gt;&gt; Probably use the usual struct io_buffer_list *bl here and either use an
&gt;&gt; ERR_PTR return, or rename the passed on **bl to **blret or something.
&gt;&gt;
&gt;&gt;&gt; +int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,
&gt;&gt;&gt; +                    unsigned issue_flags)
&gt;&gt;&gt; +{
&gt;&gt;&gt; +     struct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)-&gt;ctx;
&gt;&gt;&gt; +     struct io_buffer_list *bl;
&gt;&gt;&gt; +     int ret = -EINVAL;
&gt;&gt;&gt; +
&gt;&gt;&gt; +     io_ring_submit_lock(ctx, issue_flags);
&gt;&gt;&gt; +
&gt;&gt;&gt; +     bl = io_buffer_get_list(ctx, buf_group);
&gt;&gt;&gt; +     if (bl &amp;&amp; (bl-&gt;flags &amp; IOBL_BUF_RING) &amp;&amp; (bl-&gt;flags &amp; IOBL_PINNED)) {
&gt;&gt;
&gt;&gt; Usually done as:
&gt;&gt;
&gt;&gt;         if ((bl-&gt;flags &amp; (IOBL_BUF_RING|IOBL_PINNED)) == (IOBL_BUF_RING|IOBL_PINNED))
&gt; 
&gt; FWIW, modern compilers will perform this optimization automatically.
&gt; They&#x27;ll even optimize it further to !(~bl-&gt;flags &amp;
&gt; (IOBL_BUF_RING|IOBL_PINNED)): https://godbolt.org/z/xGoP4TfhP

Sure, it&#x27;s not about that, it&#x27;s more about the common way of doing it,
which makes it easier to read for people. FWIW, your example is easier
to read too than the original.

-- 
Jens Axboe
</pre>
</details>
<div class="review-comment-signals">Signals: readability suggestion</div>
</div>
</div>
<div class="thread-node depth-3">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Jens Axboe</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-18">2026-02-18</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Jens Axboe suggested a coding style change to make the code more readable</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On 2/10/26 10:57 AM, Caleb Sander Mateos wrote:
&gt; On Mon, Feb 9, 2026 at 5:07?PM Jens Axboe &lt;axboe@kernel.dk&gt; wrote:
&gt;&gt;
&gt;&gt; On 2/9/26 5:28 PM, Joanne Koong wrote:
&gt;&gt;&gt; +int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,
&gt;&gt;&gt; +                       unsigned issue_flags, struct io_buffer_list **bl)
&gt;&gt;&gt; +{
&gt;&gt;&gt; +     struct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)-&gt;ctx;
&gt;&gt;&gt; +     struct io_buffer_list *buffer_list;
&gt;&gt;&gt; +     int ret = -EINVAL;
&gt;&gt;
&gt;&gt; Probably use the usual struct io_buffer_list *bl here and either use an
&gt;&gt; ERR_PTR return, or rename the passed on **bl to **blret or something.
&gt;&gt;
&gt;&gt;&gt; +int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,
&gt;&gt;&gt; +                    unsigned issue_flags)
&gt;&gt;&gt; +{
&gt;&gt;&gt; +     struct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)-&gt;ctx;
&gt;&gt;&gt; +     struct io_buffer_list *bl;
&gt;&gt;&gt; +     int ret = -EINVAL;
&gt;&gt;&gt; +
&gt;&gt;&gt; +     io_ring_submit_lock(ctx, issue_flags);
&gt;&gt;&gt; +
&gt;&gt;&gt; +     bl = io_buffer_get_list(ctx, buf_group);
&gt;&gt;&gt; +     if (bl &amp;&amp; (bl-&gt;flags &amp; IOBL_BUF_RING) &amp;&amp; (bl-&gt;flags &amp; IOBL_PINNED)) {
&gt;&gt;
&gt;&gt; Usually done as:
&gt;&gt;
&gt;&gt;         if ((bl-&gt;flags &amp; (IOBL_BUF_RING|IOBL_PINNED)) == (IOBL_BUF_RING|IOBL_PINNED))
&gt; 
&gt; FWIW, modern compilers will perform this optimization automatically.
&gt; They&#x27;ll even optimize it further to !(~bl-&gt;flags &amp;
&gt; (IOBL_BUF_RING|IOBL_PINNED)): https://godbolt.org/z/xGoP4TfhP

Sure, it&#x27;s not about that, it&#x27;s more about the common way of doing it,
which makes it easier to read for people. FWIW, your example is easier
to read too than the original.

-- 
Jens Axboe
</pre>
</details>
<div class="review-comment-signals">Signals: coding style, readability</div>
</div>
</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Caleb Mateos</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-18">2026-02-18</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Caleb Mateos pointed out that the optimization in __io_uring_cmd_done() is unnecessary, as modern compilers can perform it automatically.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Mon, Feb 9, 2026 at 5:07 PM Jens Axboe &lt;axboe@kernel.dk&gt; wrote:
&gt;
&gt; On 2/9/26 5:28 PM, Joanne Koong wrote:
&gt; &gt; +int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,
&gt; &gt; +                       unsigned issue_flags, struct io_buffer_list **bl)
&gt; &gt; +{
&gt; &gt; +     struct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)-&gt;ctx;
&gt; &gt; +     struct io_buffer_list *buffer_list;
&gt; &gt; +     int ret = -EINVAL;
&gt;
&gt; Probably use the usual struct io_buffer_list *bl here and either use an
&gt; ERR_PTR return, or rename the passed on **bl to **blret or something.
&gt;
&gt; &gt; +int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,
&gt; &gt; +                    unsigned issue_flags)
&gt; &gt; +{
&gt; &gt; +     struct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)-&gt;ctx;
&gt; &gt; +     struct io_buffer_list *bl;
&gt; &gt; +     int ret = -EINVAL;
&gt; &gt; +
&gt; &gt; +     io_ring_submit_lock(ctx, issue_flags);
&gt; &gt; +
&gt; &gt; +     bl = io_buffer_get_list(ctx, buf_group);
&gt; &gt; +     if (bl &amp;&amp; (bl-&gt;flags &amp; IOBL_BUF_RING) &amp;&amp; (bl-&gt;flags &amp; IOBL_PINNED)) {
&gt;
&gt; Usually done as:
&gt;
&gt;         if ((bl-&gt;flags &amp; (IOBL_BUF_RING|IOBL_PINNED)) == (IOBL_BUF_RING|IOBL_PINNED))

FWIW, modern compilers will perform this optimization automatically.
They&#x27;ll even optimize it further to !(~bl-&gt;flags &amp;
(IOBL_BUF_RING|IOBL_PINNED)): https://godbolt.org/z/xGoP4TfhP

Best,
Caleb

&gt;
&gt; and maybe then just have an earlier
&gt;
&gt;         if (!bl)
&gt;                 goto err;
&gt;
&gt; &gt; +             bl-&gt;flags &amp;= ~IOBL_PINNED;
&gt; &gt; +             ret = 0;
&gt; &gt; +     }
&gt; err:
&gt; &gt; +     io_ring_submit_unlock(ctx, issue_flags);
&gt; &gt; +     return ret;
&gt; &gt; +}
&gt;
&gt; to avoid making it way too long. For io_uring, it&#x27;s fine to exceed 80
&gt; chars where it makes sense.
&gt;
&gt; --
&gt; Jens Axboe
</pre>
</details>
<div class="review-comment-signals">Signals: optimization, compiler</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-18">2026-02-18</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Joanne Koong suggested an alternative approach to accessing the buffer index, proposing a helper function or returning the buf id as part of the io_br_sel struct.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Mon, Feb 9, 2026 at 4:53 PM Jens Axboe &lt;axboe@kernel.dk&gt; wrote:
&gt;
&gt; On 2/9/26 5:28 PM, Joanne Koong wrote:
&gt; &gt; Return the id of the selected buffer in io_buffer_select(). This is
&gt; &gt; needed for kernel-managed buffer rings to later recycle the selected
&gt; &gt; buffer.
&gt; &gt;
&gt; &gt; Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
&gt; &gt; ---
&gt; &gt;  include/linux/io_uring/cmd.h   | 2 +-
&gt; &gt;  include/linux/io_uring_types.h | 2 ++
&gt; &gt;  io_uring/kbuf.c                | 7 +++++--
&gt; &gt;  3 files changed, 8 insertions(+), 3 deletions(-)
&gt; &gt;
&gt; &gt; diff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h
&gt; &gt; index d4b5943bdeb1..94df2bdebe77 100644
&gt; &gt; --- a/include/linux/io_uring/cmd.h
&gt; &gt; +++ b/include/linux/io_uring/cmd.h
&gt; &gt; @@ -71,7 +71,7 @@ void io_uring_cmd_issue_blocking(struct io_uring_cmd *ioucmd);
&gt; &gt;
&gt; &gt;  /*
&gt; &gt;   * Select a buffer from the provided buffer group for multishot uring_cmd.
&gt; &gt; - * Returns the selected buffer address and size.
&gt; &gt; + * Returns the selected buffer address, size, and id.
&gt; &gt;   */
&gt; &gt;  struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,
&gt; &gt;                                           unsigned buf_group, size_t *len,
&gt; &gt; diff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h
&gt; &gt; index 36cc2e0346d9..5a56bb341337 100644
&gt; &gt; --- a/include/linux/io_uring_types.h
&gt; &gt; +++ b/include/linux/io_uring_types.h
&gt; &gt; @@ -100,6 +100,8 @@ struct io_br_sel {
&gt; &gt;               void *kaddr;
&gt; &gt;       };
&gt; &gt;       ssize_t val;
&gt; &gt; +     /* id of the selected buffer */
&gt; &gt; +     unsigned buf_id;
&gt; &gt;  };
&gt;
&gt; I&#x27;m probably missing something here, but why can&#x27;t the caller just use
&gt; req-&gt;buf_index for this?

The caller can, but from the caller side they only have access to the
cmd so they would need to do something like

struct io_kiocb *req = cmd_to_iocb_kiocb(ent-&gt;cmd);
buf_id = req-&gt;buf_index;

which may be kind of ugly with looking inside io-uring internals.
Maybe a helper here would be nicer, something like
io_uring_cmd_buf_id() or io_uring_req_buf_id(). It seemed cleaner to
me to just return the buf id as part of the io_br_sel struct, but I&#x27;m
happy to do it another way if you have a preference.

Thanks,
Joanne

&gt;
&gt; --
&gt; Jens Axboe


---

On Mon, Feb 9, 2026 at 4:55 PM Jens Axboe &lt;axboe@kernel.dk&gt; wrote:
&gt;
&gt; On 2/9/26 5:28 PM, Joanne Koong wrote:
&gt; &gt; Currently, io_uring buffer rings require the application to allocate and
&gt; &gt; manage the backing buffers. This series introduces kernel-managed buffer
&gt; &gt; rings, where the kernel allocates and manages the buffers on behalf of
&gt; &gt; the application.
&gt; &gt;
&gt; &gt; This is split out from the fuse over io_uring series in [1], which needs the
&gt; &gt; kernel to own and manage buffers shared between the fuse server and the
&gt; &gt; kernel.
&gt; &gt;
&gt; &gt; This series is on top of the for-next branch in Jens&#x27; io-uring tree. The
&gt; &gt; corresponding liburing changes are in [2] and will be submitted after the
&gt; &gt; changes in this patchset are accepted.
&gt;
&gt; Generally looks pretty good - for context, do you have a branch with
&gt; these patches and the users on top too? Makes it a bit easier for cross
&gt; referencing, as some of these really do need an exposed user to make a
&gt; good judgement on the helpers.

Thanks for reviewing the patches. The branch containing the userside
changes on top of these patches is in [1]. I&#x27;ll make the changes you
pointed out in your other comments as part of v2. Once the discussion
with Pavel is resolved / figured out with the changes he wants for v2,
I&#x27;ll submit v2.

Thanks,
Joanne

[1] https://github.com/joannekoong/linux/commits/fuse_zero_copy/

&gt;
&gt; I know there&#x27;s the older series, but I&#x27;m assuming the latter patches
&gt; changed somewhat too, and it&#x27;d be nicer to look at a current set rather
&gt; than go back to the older ones.
&gt;
&gt; --
&gt; Jens Axboe
</pre>
</details>
<div class="review-comment-signals">Signals: requested changes, open to suggestions</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-20_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-20">2026-02-20</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Joanne Koong suggested an alternative way to get the buffer ID from the io_kiocb structure, proposing a helper function or returning it as part of the io_br_sel struct.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Mon, Feb 9, 2026 at 4:53 PM Jens Axboe &lt;axboe@kernel.dk&gt; wrote:
&gt;
&gt; On 2/9/26 5:28 PM, Joanne Koong wrote:
&gt; &gt; Return the id of the selected buffer in io_buffer_select(). This is
&gt; &gt; needed for kernel-managed buffer rings to later recycle the selected
&gt; &gt; buffer.
&gt; &gt;
&gt; &gt; Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
&gt; &gt; ---
&gt; &gt;  include/linux/io_uring/cmd.h   | 2 +-
&gt; &gt;  include/linux/io_uring_types.h | 2 ++
&gt; &gt;  io_uring/kbuf.c                | 7 +++++--
&gt; &gt;  3 files changed, 8 insertions(+), 3 deletions(-)
&gt; &gt;
&gt; &gt; diff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h
&gt; &gt; index d4b5943bdeb1..94df2bdebe77 100644
&gt; &gt; --- a/include/linux/io_uring/cmd.h
&gt; &gt; +++ b/include/linux/io_uring/cmd.h
&gt; &gt; @@ -71,7 +71,7 @@ void io_uring_cmd_issue_blocking(struct io_uring_cmd *ioucmd);
&gt; &gt;
&gt; &gt;  /*
&gt; &gt;   * Select a buffer from the provided buffer group for multishot uring_cmd.
&gt; &gt; - * Returns the selected buffer address and size.
&gt; &gt; + * Returns the selected buffer address, size, and id.
&gt; &gt;   */
&gt; &gt;  struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,
&gt; &gt;                                           unsigned buf_group, size_t *len,
&gt; &gt; diff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h
&gt; &gt; index 36cc2e0346d9..5a56bb341337 100644
&gt; &gt; --- a/include/linux/io_uring_types.h
&gt; &gt; +++ b/include/linux/io_uring_types.h
&gt; &gt; @@ -100,6 +100,8 @@ struct io_br_sel {
&gt; &gt;               void *kaddr;
&gt; &gt;       };
&gt; &gt;       ssize_t val;
&gt; &gt; +     /* id of the selected buffer */
&gt; &gt; +     unsigned buf_id;
&gt; &gt;  };
&gt;
&gt; I&#x27;m probably missing something here, but why can&#x27;t the caller just use
&gt; req-&gt;buf_index for this?

The caller can, but from the caller side they only have access to the
cmd so they would need to do something like

struct io_kiocb *req = cmd_to_iocb_kiocb(ent-&gt;cmd);
buf_id = req-&gt;buf_index;

which may be kind of ugly with looking inside io-uring internals.
Maybe a helper here would be nicer, something like
io_uring_cmd_buf_id() or io_uring_req_buf_id(). It seemed cleaner to
me to just return the buf id as part of the io_br_sel struct, but I&#x27;m
happy to do it another way if you have a preference.

Thanks,
Joanne

&gt;
&gt; --
&gt; Jens Axboe


---

On Mon, Feb 9, 2026 at 4:55 PM Jens Axboe &lt;axboe@kernel.dk&gt; wrote:
&gt;
&gt; On 2/9/26 5:28 PM, Joanne Koong wrote:
&gt; &gt; Currently, io_uring buffer rings require the application to allocate and
&gt; &gt; manage the backing buffers. This series introduces kernel-managed buffer
&gt; &gt; rings, where the kernel allocates and manages the buffers on behalf of
&gt; &gt; the application.
&gt; &gt;
&gt; &gt; This is split out from the fuse over io_uring series in [1], which needs the
&gt; &gt; kernel to own and manage buffers shared between the fuse server and the
&gt; &gt; kernel.
&gt; &gt;
&gt; &gt; This series is on top of the for-next branch in Jens&#x27; io-uring tree. The
&gt; &gt; corresponding liburing changes are in [2] and will be submitted after the
&gt; &gt; changes in this patchset are accepted.
&gt;
&gt; Generally looks pretty good - for context, do you have a branch with
&gt; these patches and the users on top too? Makes it a bit easier for cross
&gt; referencing, as some of these really do need an exposed user to make a
&gt; good judgement on the helpers.

Thanks for reviewing the patches. The branch containing the userside
changes on top of these patches is in [1]. I&#x27;ll make the changes you
pointed out in your other comments as part of v2. Once the discussion
with Pavel is resolved / figured out with the changes he wants for v2,
I&#x27;ll submit v2.

Thanks,
Joanne

[1] https://github.com/joannekoong/linux/commits/fuse_zero_copy/

&gt;
&gt; I know there&#x27;s the older series, but I&#x27;m assuming the latter patches
&gt; changed somewhat too, and it&#x27;d be nicer to look at a current set rather
&gt; than go back to the older ones.
&gt;
&gt; --
&gt; Jens Axboe
</pre>
</details>
<div class="review-comment-signals">Signals: requested changes</div>
</div>
</div>
</div>
</div>
<div class="thread-node depth-1" id="2026-02-10">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Pavel Begunkov</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-10">2026-02-10</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Pavel Begunkov raised concerns about the kernel-managed buffer rings, suggesting that they should not be responsible for allocating buffers and instead require users to register a memory region of appropriate size. He also questioned whether extending buffer rings is the right approach.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On 2/10/26 00:28, Joanne Koong wrote:
&gt; Add support for kernel-managed buffer rings (kmbuf rings), which allow
&gt; the kernel to allocate and manage the backing buffers for a buffer
&gt; ring, rather than requiring the application to provide and manage them.
&gt; 
&gt; This introduces two new registration opcodes:
&gt; - IORING_REGISTER_KMBUF_RING: Register a kernel-managed buffer ring
&gt; - IORING_UNREGISTER_KMBUF_RING: Unregister a kernel-managed buffer ring
&gt; 
&gt; The existing io_uring_buf_reg structure is extended with a union to
&gt; support both application-provided buffer rings (pbuf) and kernel-managed
&gt; buffer rings (kmbuf):
&gt; - For pbuf rings: ring_addr specifies the user-provided ring address
&gt; - For kmbuf rings: buf_size specifies the size of each buffer. buf_size
&gt;    must be non-zero and page-aligned.
&gt; 
&gt; The implementation follows the same pattern as pbuf ring registration,
&gt; reusing the validation and buffer list allocation helpers introduced in
&gt; earlier refactoring. The IOBL_KERNEL_MANAGED flag marks buffer lists as
&gt; kernel-managed for appropriate handling in the I/O path.
&gt; 
&gt; Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
&gt; ---
&gt;   include/uapi/linux/io_uring.h |  15 ++++-
&gt;   io_uring/kbuf.c               |  81 ++++++++++++++++++++++++-
&gt;   io_uring/kbuf.h               |   7 ++-
&gt;   io_uring/memmap.c             | 111 ++++++++++++++++++++++++++++++++++
&gt;   io_uring/memmap.h             |   4 ++
&gt;   io_uring/register.c           |   7 +++
&gt;   6 files changed, 219 insertions(+), 6 deletions(-)
&gt; 
&gt; diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
&gt; index fc473af6feb4..a0889c1744bd 100644
&gt; --- a/include/uapi/linux/io_uring.h
&gt; +++ b/include/uapi/linux/io_uring.h
&gt; @@ -715,6 +715,10 @@ enum io_uring_register_op {
&gt;   	/* register bpf filtering programs */
&gt;   	IORING_REGISTER_BPF_FILTER		= 37,
&gt;   
&gt; +	/* register/unregister kernel-managed ring buffer group */
&gt; +	IORING_REGISTER_KMBUF_RING		= 38,
&gt; +	IORING_UNREGISTER_KMBUF_RING		= 39,
&gt; +
&gt;   	/* this goes last */
&gt;   	IORING_REGISTER_LAST,
&gt;   
&gt; @@ -891,9 +895,16 @@ enum io_uring_register_pbuf_ring_flags {
&gt;   	IOU_PBUF_RING_INC	= 2,
&gt;   };
&gt;   
&gt; -/* argument for IORING_(UN)REGISTER_PBUF_RING */
&gt; +/* argument for IORING_(UN)REGISTER_PBUF_RING and
&gt; + * IORING_(UN)REGISTER_KMBUF_RING
&gt; + */
&gt;   struct io_uring_buf_reg {
&gt; -	__u64	ring_addr;
&gt; +	union {
&gt; +		/* used for pbuf rings */
&gt; +		__u64	ring_addr;
&gt; +		/* used for kmbuf rings */
&gt; +		__u32   buf_size;

If you&#x27;re creating a region, there should be no reason why it
can&#x27;t work with user passed memory. You&#x27;re fencing yourself off
optimisations that are already there like huge pages.

&gt; +	};
&gt;   	__u32	ring_entries;
&gt;   	__u16	bgid;
&gt;   	__u16	flags;
&gt; diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
&gt; index aa9b70b72db4..9bc36451d083 100644
&gt; --- a/io_uring/kbuf.c
&gt; +++ b/io_uring/kbuf.c
...
&gt; +static int io_setup_kmbuf_ring(struct io_ring_ctx *ctx,
&gt; +			       struct io_buffer_list *bl,
&gt; +			       struct io_uring_buf_reg *reg)
&gt; +{
&gt; +	struct io_uring_buf_ring *ring;
&gt; +	unsigned long ring_size;
&gt; +	void *buf_region;
&gt; +	unsigned int i;
&gt; +	int ret;
&gt; +
&gt; +	/* allocate pages for the ring structure */
&gt; +	ring_size = flex_array_size(ring, bufs, bl-&gt;nr_entries);
&gt; +	ring = kzalloc(ring_size, GFP_KERNEL_ACCOUNT);
&gt; +	if (!ring)
&gt; +		return -ENOMEM;
&gt; +
&gt; +	ret = io_create_region_multi_buf(ctx, &amp;bl-&gt;region, bl-&gt;nr_entries,
&gt; +					 reg-&gt;buf_size);

Please use io_create_region(), the new function does nothing new
and only violates abstractions.

Provided buffer rings with kernel addresses could be an interesting
abstraction, but why is it also responsible for allocating buffers?
What I&#x27;d do:

1. Strip buffer allocation from IORING_REGISTER_KMBUF_RING.
2. Replace *_REGISTER_KMBUF_RING with *_REGISTER_PBUF_RING + a new flag.
    Or maybe don&#x27;t expose it to the user at all and create it from
    fuse via internal API.
3. Require the user to register a memory region of appropriate size,
    see IORING_REGISTER_MEM_REGION, ctx-&gt;param_region. Make fuse
    populating the buffer ring using the memory region.

I wanted to make regions shareable anyway (need it for other purposes),
I can toss patches for that tomorrow.

A separate question is whether extending buffer rings is the right
approach as it seems like you&#x27;re only using it for fuse requests and
not for passing buffers to normal requests, but I don&#x27;t see the
big picture here.

&gt; +	if (ret) {
&gt; +		kfree(ring);
&gt; +		return ret;
&gt; +	}
&gt; +
&gt; +	/* initialize ring buf entries to point to the buffers */
&gt; +	buf_region = bl-&gt;region.ptr;

io_region_get_ptr()

&gt; +	for (i = 0; i &lt; bl-&gt;nr_entries; i++) {
&gt; +		struct io_uring_buf *buf = &amp;ring-&gt;bufs[i];
&gt; +
&gt; +		buf-&gt;addr = (u64)(uintptr_t)buf_region;
&gt; +		buf-&gt;len = reg-&gt;buf_size;
&gt; +		buf-&gt;bid = i;
&gt; +
&gt; +		buf_region += reg-&gt;buf_size;
&gt; +	}
&gt; +	ring-&gt;tail = bl-&gt;nr_entries;
&gt; +
&gt; +	bl-&gt;buf_ring = ring;
&gt; +	bl-&gt;flags |= IOBL_KERNEL_MANAGED;
&gt; +
&gt; +	return 0;
&gt; +}
&gt; +
&gt; +int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
&gt; +{
&gt; +	struct io_uring_buf_reg reg;
&gt; +	struct io_buffer_list *bl;
&gt; +	int ret;
&gt; +
&gt; +	lockdep_assert_held(&amp;ctx-&gt;uring_lock);
&gt; +
&gt; +	ret = io_copy_and_validate_buf_reg(arg, &amp;reg, 0);
&gt; +	if (ret)
&gt; +		return ret;
&gt; +
&gt; +	if (!reg.buf_size || !PAGE_ALIGNED(reg.buf_size))

With io_create_region_multi_buf() gone, you shouldn&#x27;t need
to align every buffer, that could be a lot of wasted memory
(thinking about 64KB pages).

&gt; +		return -EINVAL;
&gt; +
&gt; +	bl = io_alloc_new_buffer_list(ctx, &amp;reg);
&gt; +	if (IS_ERR(bl))
&gt; +		return PTR_ERR(bl);
&gt; +
&gt; +	ret = io_setup_kmbuf_ring(ctx, bl, &amp;reg);
&gt; +	if (ret) {
&gt; +		kfree(bl);
&gt; +		return ret;
&gt; +	}
&gt; +
&gt; +	ret = io_buffer_add_list(ctx, bl, reg.bgid);
&gt; +	if (ret)
&gt; +		io_put_bl(ctx, bl);
&gt; +
&gt; +	return ret;

-- 
Pavel Begunkov



---

On 2/10/26 19:39, Joanne Koong wrote:
&gt; On Tue, Feb 10, 2026 at 8:34\u202fAM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
...
&gt;&gt;&gt; -/* argument for IORING_(UN)REGISTER_PBUF_RING */
&gt;&gt;&gt; +/* argument for IORING_(UN)REGISTER_PBUF_RING and
&gt;&gt;&gt; + * IORING_(UN)REGISTER_KMBUF_RING
&gt;&gt;&gt; + */
&gt;&gt;&gt;    struct io_uring_buf_reg {
&gt;&gt;&gt; -     __u64   ring_addr;
&gt;&gt;&gt; +     union {
&gt;&gt;&gt; +             /* used for pbuf rings */
&gt;&gt;&gt; +             __u64   ring_addr;
&gt;&gt;&gt; +             /* used for kmbuf rings */
&gt;&gt;&gt; +             __u32   buf_size;
&gt;&gt;
&gt;&gt; If you&#x27;re creating a region, there should be no reason why it
&gt;&gt; can&#x27;t work with user passed memory. You&#x27;re fencing yourself off
&gt;&gt; optimisations that are already there like huge pages.
&gt; 
&gt; Are there any optimizations with user-allocated buffers that wouldn&#x27;t
&gt; be possible with kernel-allocated buffers? For huge pages, can&#x27;t the
&gt; kernel do this as well (eg I see in io_mem_alloc_compound(), it calls
&gt; into alloc_pages() with order &gt; 0)?

Yes, there is handful of differences. To name one, 1MB allocation won&#x27;t
get you a PMD mappable huge page, while user space can allocate 2MB,
register the first 1MB and reuse the rest for other purposes.

&gt;&gt;&gt; +     };
&gt;&gt;&gt;        __u32   ring_entries;
&gt;&gt;&gt;        __u16   bgid;
&gt;&gt;&gt;        __u16   flags;
&gt;&gt;&gt; diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
&gt;&gt;&gt; index aa9b70b72db4..9bc36451d083 100644
&gt;&gt;&gt; --- a/io_uring/kbuf.c
&gt;&gt;&gt; +++ b/io_uring/kbuf.c
&gt;&gt; ...
&gt;&gt;&gt; +static int io_setup_kmbuf_ring(struct io_ring_ctx *ctx,
&gt;&gt;&gt; +                            struct io_buffer_list *bl,
&gt;&gt;&gt; +                            struct io_uring_buf_reg *reg)
&gt;&gt;&gt; +{
&gt;&gt;&gt; +     struct io_uring_buf_ring *ring;
&gt;&gt;&gt; +     unsigned long ring_size;
&gt;&gt;&gt; +     void *buf_region;
&gt;&gt;&gt; +     unsigned int i;
&gt;&gt;&gt; +     int ret;
&gt;&gt;&gt; +
&gt;&gt;&gt; +     /* allocate pages for the ring structure */
&gt;&gt;&gt; +     ring_size = flex_array_size(ring, bufs, bl-&gt;nr_entries);
&gt;&gt;&gt; +     ring = kzalloc(ring_size, GFP_KERNEL_ACCOUNT);
&gt;&gt;&gt; +     if (!ring)
&gt;&gt;&gt; +             return -ENOMEM;
&gt;&gt;&gt; +
&gt;&gt;&gt; +     ret = io_create_region_multi_buf(ctx, &amp;bl-&gt;region, bl-&gt;nr_entries,
&gt;&gt;&gt; +                                      reg-&gt;buf_size);
&gt;&gt;
&gt;&gt; Please use io_create_region(), the new function does nothing new
&gt;&gt; and only violates abstractions.
&gt; 
&gt; There&#x27;s separate checks needed between io_create_region() and
&gt; io_create_region_multi_buf() (eg IORING_MEM_REGION_TYPE_USER flag

If io_create_region() is too strict, let&#x27;s discuss that in
examples if there are any, but it&#x27;s likely not a good idea changing
that. If it&#x27;s too lax, filter arguments in the caller. IOW, don&#x27;t
pass IORING_MEM_REGION_TYPE_USER if it&#x27;s not used.

&gt; checking) and different allocation calls (eg
&gt; io_region_allocate_pages() vs io_region_allocate_pages_multi_buf()).

I saw that and saying that all memmap.c changes can get dropped.
You&#x27;re using it as one big virtually contig kernel memory range then
chunked into buffers, and that&#x27;s pretty much what you&#x27;re getting with
normal io_create_region(). I get that you only need it to be
contiguous within a single buffer, but that&#x27;s not what you&#x27;re doing,
and it&#x27;ll be only worse than default io_create_region() e.g.
effectively disabling any usefulness of io_mem_alloc_compound(),
and ultimately you don&#x27;t need to care.

Regions shouldn&#x27;t know anything about your buffers, how it&#x27;s
subdivided after, etc.

&gt; Maybe I&#x27;m misinterpreting your comment (or the code), but I&#x27;m not
&gt; seeing how this can just use io_create_region().

struct io_uring_region_desc rd = {};
total_size = nr_bufs * buf_size;
rd.size = PAGE_ALIGN(total_size);
io_create_region(&amp;region, &amp;rd);

Add something like this for user provided memory:

if (use_user_memory) {
	rd.user_addr = uaddr;
	rd.flags |= IORING_MEM_REGION_TYPE_USER;
}


&gt;&gt; Provided buffer rings with kernel addresses could be an interesting
&gt;&gt; abstraction, but why is it also responsible for allocating buffers?
&gt; 
&gt; Conceptually, I think it makes the interface and lifecycle management
&gt; simpler/cleaner. With registering it from userspace, imo there&#x27;s
&gt; additional complications with no tangible benefits, eg it&#x27;s not
&gt; guaranteed that the memory regions registered for the buffers are the
&gt; same size, with allocating it from the kernel-side we can guarantee
&gt; that the pages are allocated physically contiguously, userspace setup
&gt; with user-allocated buffers is less straightforward, etc. In general,
&gt; I&#x27;m just not really seeing what advantages there are in allocating the
&gt; buffers from userspace. Could you elaborate on that part more?

I don&#x27;t think I follow. I&#x27;m saying that it might be interesting
to separate rings from how and with what they&#x27;re populated on the
kernel API level, but the fuse kernel module can do the population
and get exactly same layout as you currently have:

int fuse_create_ring(size_t region_offset /* user space argument */) {
	struct io_mapped_region *mr = get_mem_region(ctx);
	// that can take full control of the ring
	ring = grab_empty_ring(io_uring_ctx);

	size = nr_bufs * buf_size;
	if (region_offset + size &gt; get_size(mr)) // + other validation
		return error;

	buf = mr_get_ptr(mr) + offset;
	for (i = 0; i &lt; nr_bufs; i++) {
		ring_push_buffer(ring, buf, buf_size);
		buf += buf_size;
	}
}

fuse might not care, but with empty rings other users will get a
channel they can use to do IO (e.g. read requests) using their
kernel addresses in the future. 	

&gt;&gt; What I&#x27;d do:
&gt;&gt;
&gt;&gt; 1. Strip buffer allocation from IORING_REGISTER_KMBUF_RING.
&gt;&gt; 2. Replace *_REGISTER_KMBUF_RING with *_REGISTER_PBUF_RING + a new flag.
&gt;&gt;      Or maybe don&#x27;t expose it to the user at all and create it from
&gt;&gt;      fuse via internal API.
&gt; 
&gt; If kmbuf rings are squashed into pbuf rings, then pbuf rings will need
&gt; to support pinning. In fuse, there are some contexts where you can&#x27;t

It&#x27;d change uapi but not internals, you already piggy back it
on pbuf implementation and differentiate with a flag.

It could basically be:

if (flags &amp; IOU_PBUF_RING_KM)
	bl-&gt;flags |= IOBL_KERNEL_MANAGED;

Pinning can be gated on that flag as well. Pretty likely uapi
and internals will be a bit cleaner, but that&#x27;s not a huge deal,
just don&#x27;t see why would you roll out a separate set of uapi
([un]register, offsets, etc.) when essentially it can be treated
as the same thing.

&gt; grab the uring mutex because you&#x27;re running in atomic context and this
&gt; can be encountered while recycling the buffer. I originally had a
&gt; patch adding pinning to pbuf rings (to mitigate the overhead of
&gt; registered buffers lookups) 

IIRC, you was pinning the registered buffer table and not provided
buffer rings? Which would indeed be a bad idea. Thinking about it,
fwiw, instead of creating multiple registered buffers and trying to
lock the entire table, you could&#x27;ve kept all memory in one larger
registered buffer and pinned only it. It&#x27;s already refcounted, so
shouldn&#x27;t have been much of a problem.

&gt; but dropped it when Jens and Caleb didn&#x27;t
&gt; like the idea. But for kmbuf rings, pinning will be necessary for
&gt; fuse.
&gt; 
&gt;&gt; 3. Require the user to register a memory region of appropriate size,
&gt;&gt;      see IORING_REGISTER_MEM_REGION, ctx-&gt;param_region. Make fuse
&gt;&gt;      populating the buffer ring using the memory region.

To explain why, I don&#x27;t think that creating many small regions
is a good direction going forward. In case of kernel allocation,
it&#x27;s extra mmap()s, extra user space management, and wasted space.
For user provided memory it&#x27;s over-accounting and extra memory
footprint. It&#x27;ll also give you better lifecycle guarantees, i.e.
you won&#x27;t be able to free buffers while there are requests for the
context. I&#x27;m not so sure about ring bound memory, let&#x27;s say I have
my suspicions, and you&#x27;d need to be extra careful about buffer
lifetimes even after a fuse instance dies.

&gt;&gt; I wanted to make regions shareable anyway (need it for other purposes),
&gt;&gt; I can toss patches for that tomorrow.
&gt;&gt;
&gt;&gt; A separate question is whether extending buffer rings is the right
&gt;&gt; approach as it seems like you&#x27;re only using it for fuse requests and
&gt;&gt; not for passing buffers to normal requests, but I don&#x27;t see the
&gt; 
&gt; What are &#x27;normal requests&#x27;? For fuse&#x27;s use case, there are only fuse requests.

Any kind of read/recv/etc. that can use provided buffers. It&#x27;s
where kernel memory filled rings would shine, as you&#x27;d be able
to use them together without changing any opcode specific code.
I.e. not changes in read request implementation, only kbuf.c

-- 
Pavel Begunkov



---

On 2/11/26 22:06, Joanne Koong wrote:
&gt; On Wed, Feb 11, 2026 at 4:01\u202fAM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt;&gt; On 2/10/26 19:39, Joanne Koong wrote:
&gt;&gt;&gt; On Tue, Feb 10, 2026 at 8:34\u202fAM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
...
&gt;&gt;&gt; checking) and different allocation calls (eg
&gt;&gt;&gt; io_region_allocate_pages() vs io_region_allocate_pages_multi_buf()).
&gt;&gt;
&gt;&gt; I saw that and saying that all memmap.c changes can get dropped.
&gt;&gt; You&#x27;re using it as one big virtually contig kernel memory range then
&gt;&gt; chunked into buffers, and that&#x27;s pretty much what you&#x27;re getting with
&gt;&gt; normal io_create_region(). I get that you only need it to be
&gt;&gt; contiguous within a single buffer, but that&#x27;s not what you&#x27;re doing,
&gt;&gt; and it&#x27;ll be only worse than default io_create_region() e.g.
&gt;&gt; effectively disabling any usefulness of io_mem_alloc_compound(),
&gt;&gt; and ultimately you don&#x27;t need to care.
&gt; 
&gt; When I originally implemented it, I had it use
&gt; io_region_allocate_pages() but this fails because it&#x27;s allocating way
&gt; too much memory at once. For fuse&#x27;s use case, each buffer is usually
&gt; at least 1 MB if not more. Allocating the memory one buffer a time in
&gt; io_region_allocate_pages_multi_buf() bypasses the allocation errors I
&gt; was seeing. That&#x27;s the main reason I don&#x27;t think this can just use
&gt; io_create_region().

Let&#x27;s fix that then. For now, just work it around by wrapping
into a loop.

Btw, I thought you&#x27;re going to use it for metadata like some
fuse headers and payloads would be zero copied by installing
it as registered buffers.

...
&gt;&gt;&gt;&gt; Provided buffer rings with kernel addresses could be an interesting
&gt;&gt;&gt;&gt; abstraction, but why is it also responsible for allocating buffers?
&gt;&gt;&gt;
&gt;&gt;&gt; Conceptually, I think it makes the interface and lifecycle management
&gt;&gt;&gt; simpler/cleaner. With registering it from userspace, imo there&#x27;s
&gt;&gt;&gt; additional complications with no tangible benefits, eg it&#x27;s not
&gt;&gt;&gt; guaranteed that the memory regions registered for the buffers are the
&gt;&gt;&gt; same size, with allocating it from the kernel-side we can guarantee
&gt;&gt;&gt; that the pages are allocated physically contiguously, userspace setup
&gt;&gt;&gt; with user-allocated buffers is less straightforward, etc. In general,
&gt;&gt;&gt; I&#x27;m just not really seeing what advantages there are in allocating the
&gt;&gt;&gt; buffers from userspace. Could you elaborate on that part more?
&gt;&gt;
&gt;&gt; I don&#x27;t think I follow. I&#x27;m saying that it might be interesting
&gt;&gt; to separate rings from how and with what they&#x27;re populated on the
&gt;&gt; kernel API level, but the fuse kernel module can do the population
&gt; 
&gt; Oh okay, from your first message I (and I think christoph too) thought
&gt; what you were saying is that the user should be responsible for
&gt; allocating the buffers with complete ownership over them, and then
&gt; just pass those allocated to the kernel to use. But what you&#x27;re saying
&gt; is that just use a different way for getting the kernel to allocate
&gt; the buffers (eg through the IORING_REGISTER_MEM_REGION interface). Am
&gt; I reading this correctly?

The main point is disentangling memory allocation from ring
creation in the io_uring uapi, and moving ring population
into fuse instead of doing it at creation. And it&#x27;ll still be
populated by the kernel (fuse), user space doesn&#x27;t have access
to the ring. IORING_REGISTER_MEM_REGION is just the easiest way
to achieve that without any extra uapi.

...
&gt;&gt; Pinning can be gated on that flag as well. Pretty likely uapi
&gt;&gt; and internals will be a bit cleaner, but that&#x27;s not a huge deal,
&gt;&gt; just don&#x27;t see why would you roll out a separate set of uapi
&gt;&gt; ([un]register, offsets, etc.) when essentially it can be treated
&gt;&gt; as the same thing.
&gt; 
&gt; imo, it looked cleaner as a separate api because it has different
&gt; expectations and behaviors and squashing kmbuf into the pbuf api makes
&gt; the pbuf api needlessly more complex. Though I guess from the

It appeared to me that they&#x27;re different because of special
region path and embedded buffer allocations, and otherwise
differences would be minimal. But if you think it&#x27;s still
better to be made as a separate opcode, I&#x27;m not opposing it,
go for it.

&gt; userspace pov, liburing could have a wrapper that takes care of
&gt; setting up the pbuf details for kernel-managed pbufs. But in my head,
&gt; having pbufs vs. kmbufs makes it clearer what each one does vs regular
&gt; pbufs vs. pbufs that are kernel-managed.
&gt; 
&gt; Especially with now having kmbufs go through the ioring mem region
&gt; interface, it makes things more confusing imo if they&#x27;re combined, eg
&gt; pbufs that are kernel-managed are created empty and then populated
&gt; from the kernel side by whatever subsystem is using them. Right now
&gt; there&#x27;s only one mem region supported per ring, but in the future if
&gt; there&#x27;s the possibility that multiple mem regions can be registered

That shouldn&#x27;t be a problem

&gt; (eg if userspace doesn&#x27;t know upfront what mem region length they&#x27;ll
&gt; need), then we should also probably add in a region id param for the
&gt; registration arg, which if kmbuf rings go through the pbuf ring
&gt; registration api, is not possible to do.

Not having patches using the functionality is inconvenient. How
fuse looks up the buffer ring from io_uring? I could imagine you
have some control path io-uring command:

case FUSE_CMD_BIND_BUFFER_RING:
	return bind_queue(params);

Then you can pass all necessary parameters to it, pseudo code:

struct fuse_bind_kmbuf_ring_params {
	region_id;
	buf_ring_id;
	...
};

bind_queue(cmd, struct fuse_bind_kmbuf_ring_params *p)
{
	region = io_uring_get_region(cmd, p-&gt;region_id);
	// get exclusive access:
	buf_ring = io_uring_get_buf_ring(cmd, p-&gt;buf_ring_id);

	if (!validate_buf_ring(buf_ring))
		return NOTSUPPORTED;

	io_uring_pin(buf_ring);
	fuse_populate_buf_ring(buf_ring, region, ...);
}

Does that match expectations? I don&#x27;t think you even need
the ring part exposed as an io_uring uapi, tbh, as it
stays completely in fuse and doesn&#x27;t meaningfully interact
with the rest of io_uring.

...
&gt;&gt;&gt;&gt; 3. Require the user to register a memory region of appropriate size,
&gt;&gt;&gt;&gt;       see IORING_REGISTER_MEM_REGION, ctx-&gt;param_region. Make fuse
&gt;&gt;&gt;&gt;       populating the buffer ring using the memory region.
&gt;&gt;
&gt;&gt; To explain why, I don&#x27;t think that creating many small regions
&gt;&gt; is a good direction going forward. In case of kernel allocation,
&gt;&gt; it&#x27;s extra mmap()s, extra user space management, and wasted space.
&gt; 
&gt; To clarify, is this in reply to why the individual buffers shouldn&#x27;t
&gt; be allocated separately by the kernel?

That was about an argument for using IORING_REGISTER_MEM_REGION
instead a separate region. And it&#x27;s separate from whether
buffers should be bound to the ring.

&gt; I added a comment about this above in the discussion about
&gt; io_region_allocate_pages_multi_buf(), and if the memory allocation
&gt; issue I was seeing is bypassable and the region can be allocated all
&gt; at once, I&#x27;m happy to make that change. With having the allocation be
&gt; separate buffers though, I&#x27;m not sure I agree that there are extra
&gt; mmaps / userspace management. All the pages across the buffers are
&gt; vmapped together and the userspace just needs to do 1 mmap call for
&gt; them. On the userspace side, I don&#x27;t think there&#x27;s more management
&gt; since the mmapped address represents the range across all the buffers.
&gt; I&#x27;m not seeing how there&#x27;s wasted space either since the only

I shouldn&#x27;t affect you much since you have such large buffers,
but imagine the total allocation size is not being pow2, and
the kernel allocating it as a single folio. E.g. 3 buffers,
0.5 MB each, total = 1.5MB, and the kernel allocates a 2MB
huge page.

-- 
Pavel Begunkov



---

On 2/12/26 17:29, Joanne Koong wrote:
&gt; On Thu, Feb 12, 2026 at 2:52\u202fAM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt;&gt;
&gt;&gt; On 2/12/26 10:07, Christoph Hellwig wrote:
&gt;&gt;&gt; On Wed, Feb 11, 2026 at 02:06:18PM -0800, Joanne Koong wrote:
&gt;&gt;&gt;&gt;&gt; I don&#x27;t think I follow. I&#x27;m saying that it might be interesting
&gt;&gt;&gt;&gt;&gt; to separate rings from how and with what they&#x27;re populated on the
&gt;&gt;&gt;&gt;&gt; kernel API level, but the fuse kernel module can do the population
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Oh okay, from your first message I (and I think christoph too) thought
&gt;&gt;&gt;&gt; what you were saying is that the user should be responsible for
&gt;&gt;&gt;&gt; allocating the buffers with complete ownership over them, and then
&gt;&gt;&gt;&gt; just pass those allocated to the kernel to use. But what you&#x27;re saying
&gt;&gt;&gt;&gt; is that just use a different way for getting the kernel to allocate
&gt;&gt;&gt;&gt; the buffers (eg through the IORING_REGISTER_MEM_REGION interface). Am
&gt;&gt;&gt;&gt; I reading this correctly?
&gt;&gt;&gt;
&gt;&gt;&gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt;&gt;&gt; where the kernel controls the allocation fully and guarantees user
&gt;&gt;&gt; processes can only read the memory but never write to it.  I&#x27;d love
&gt; 
&gt; By &quot;control the allocation fully&quot; do you mean for your use case, the
&gt; allocation/setup isn&#x27;t triggered by userspace but is initiated by the
&gt; kernel (eg user never explicitly registers any kbuf ring, the kernel
&gt; just uses the kbuf ring data structure internally and users can read
&gt; the buffer contents)? If userspace initiates the setup of the kbuf
&gt; ring, going through IORING_REGISTER_MEM_REGION would be semantically
&gt; the same, except the buffer allocation by the kernel now happens
&gt; before the ring is created and then later populated into the ring.
&gt; userspace would still need to make an mmap call to the region and the
&gt; kernel could enforce that as read-only. But if userspace doesn&#x27;t
&gt; initiate the setup, then going through IORING_REGISTER_MEM_REGION gets
&gt; uglier.
&gt; 
&gt;&gt;&gt; to be able to piggy back than onto your work.
&gt;&gt;
&gt;&gt; IORING_REGISTER_MEM_REGION supports both types of allocations. It can
&gt;&gt; have a new registration flag for read-only, and then you either make
&gt;&gt; the bounce avoidance optional or reject binding fuse to unsupported
&gt;&gt; setups during init. Any arguments against that? I need to go over
&gt;&gt; Joanne&#x27;s reply, but I don&#x27;t see any contradiction in principal with
&gt;&gt; your use case.
&gt; 
&gt; So i guess the flow would have to be:
&gt; a) user calls io_uring_register_region(&amp;ring, &amp;mem_region_reg) with
&gt; mem_region_reg.region_uptr&#x27;s size field set to the total buffer size
&gt; (and mem_region_reg.flags read-only bit set if needed)
&gt;       kernel allocates region
&gt; b) user calls mmap() to get the address of the region. If read-only
&gt; bit was set, it gets a read-only address
&gt; c) user calls io_uring_register_buf_ring(&amp;ring, &amp;buf_reg, flags) with
&gt; buf_reg.flags |= IOU_PBUF_RING_KERNEL_MANAGED
&gt;       kernel creates an empty kernel-managed ring. None of the buffers
&gt; are populated
&gt; d) user tells X subsystem to populate the ring starting from offset Z
&gt; in the registered mem region
&gt; e) on the kernel side, the subsystem populates the ring starting from
&gt; offset Z, filling it up using the buf_size and ring_entries values
&gt; that the user registered the ring with in c)
&gt; 
&gt; To be completely honest, the more I look at this the more this feels
&gt; like overkill / over-engineered to me. I get that now the user can do
&gt; the PMD optimization, but does that actually lead to noticeable
&gt; performance benefits? It seems especially confusing with them going

No, it&#x27;s mainly about not keeping payload buffers and rings in the same
object from the io_uring uapi perspective.

1. If it&#x27;s an io_uring uapi, it shouldn&#x27;t be fuse specific or with
a bunch of use case specific expectations attached. Why does it
require all buffers to be uniform in size? Why does it require
the ring size to match the number of buffers? Why does it require
buffers to be allocated by io_uring in the first place? Maybe some
subsystem got memory from somewhere else and wants to do use it
with io_uring. Why does it need to know the total size at creation,
and what would you do if you want to add more memory at runtime
while using the same ring?

2. If it&#x27;s meant to be fuse specific and _not_ used with other requests
like recv/read/etc., then what&#x27;s the point of having it as an io_uring
uapi? Which also adds additional trouble like the once you&#x27;re solving
with pinning.

If it&#x27;s supposed to be used with other requests, then buffers and
rings will have different in-kernel lifetime expectations imposed
by io_uring, so having them together won&#x27;t even help with
management.

I have a strong opinion about the memmap.c change. For the
rest, if you believe it&#x27;s fine, just send it out and let Jens
decide.

&gt; through the same pbuf ring interface but having totally different
&gt; expectations.

It&#x27;s predicated on separating buffers from rings, see above,
and assuming that I&#x27;m not sure what expectations are different
apart from one being in-kernel with kernel addresses and the
other user visible with user addresses.

-- 
Pavel Begunkov



---

On 2/13/26 22:04, Joanne Koong wrote:
&gt; On Fri, Feb 13, 2026 at 4:41\u202fAM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
...
&gt;&gt; Fuse is doing both adding (kernel) buffers to the ring and consuming
&gt;&gt; them. At which point it&#x27;s not clear:
&gt;&gt;
&gt;&gt; 1. Why it even needs io_uring provided buffer rings, it can be all
&gt;&gt;      contained in fuse. Maybe it&#x27;s trying to reuse pbuf ring code as
&gt;&gt;      basically an internal memory allocator, but then why expose buffer
&gt;&gt;      rings as an io_uring uapi instead of keeping it internally.
&gt;&gt;
&gt;&gt;      That&#x27;s also why I mentioned whether those buffers are supposed to
&gt;&gt;      be used with other types of io_uring requests like recv, etc.
&gt; 
&gt; On the userspace/server side, it uses the buffers for other io-uring
&gt; operations (eg reading or writing the contents from/to a
&gt; locally-backed file).

Oops, typo. I was asking whether the buffer rings (not buffers) are
supposed to be used with other requests. E.g. submitting a
IORING_OP_RECV with IOSQE_BUFFER_SELECT set and the bgid specifying
your kernel-managed buffer ring.

&gt;&gt; 2. Why making io_uring to allocate payload memory. The answer to which
&gt;&gt;      is probably to reuse the region api with mmap and so on. And why
&gt;&gt;      payload buffers are inseparably created together with the ring
&gt; 
&gt; My main motivation for this is simplicity. I see (and thanks for
&gt; explaining) that using a registered mem region allows the use of some
&gt; optimizations (the only one I know of right now is the PMD one you
&gt; mentioned but maybe there&#x27;s more I&#x27;m missing) that could be useful for
&gt; some workloads, but I don&#x27;t think (and this could just be my lack of
&gt; understanding of what more optimizations there are) most use cases of
&gt; kmbufs benefit from those optimizations, so to me it feels like we&#x27;re
&gt; adding non-trivial complexity for no noticeable benefit.

There are two separate arguments. The first is about not making buffers
inseparable from buffer rings in the io_uring user API. Whether it&#x27;s
IORING_REGISTER_MEM_REGION or something else is not that important.
I have no objection if it&#x27;s a part of fuse instead though, e.g. if
fuse binds two objects together when you register it with fuse, or even
if fuse create a buffer ring internally (assuming it doesn&#x27;t indirectly
leak into io_uring uapi).

And the second was about optionally allowing user memory for buffer
creation as you&#x27;re reusing the region abstraction. You can find pros
and cons for both modes, and funnily enough, SQ/CQ were first kernel
allocated and then people asked for backing it by user memory, and IIRC
it was in the reverse order for pbuf rings.

Implementing this is trivial as well, you just need to pass an argument
while creating a region. All new region users use struct
io_uring_region_desc for uapi and forward it to io_create_region()
without caring if it&#x27;s user or kernel allocated memory.

&gt; I feel like we get the best of both worlds by letting users have both:
&gt; the simple kernel-managed pbuf where the kernel allocates the buffers
&gt; and the buffers are tied to the lifecycle of the ring, and the more
&gt; advanced kernel-managed pbuf where buffers are tied to a registered
&gt; memory region that the subsystem is responsible for later populating
&gt; the ring with.
&gt; 
&gt;&gt;      and via a new io_uring uapi.
&gt; 
&gt; imo it felt cleaner to have a new uapi for it because kmbufs and pbufs

The stress is on why it&#x27;s an _io_uring_ API. It doesn&#x27;t matter to me
whether it&#x27;s a separate opcode or not. Currently, buffer rings don&#x27;t give
you anything that can&#x27;t be pure fuse, and it might be simpler to have
it implemented in fuse than binding to some io_uring object. Or it could
create buffer rings internally to reuse code but it doesn&#x27;t become an
io_uring uapi but rather implementation detail. And that predicates on
whether km rings are intended to be used with other / non-fuse requests.

&gt; have different expectations and behaviors (eg pbufs only work with
&gt; user-provided buffers and requires userspace to populate the ring
&gt; before using it, whereas for kmbufs the kernel allocates the buffers
&gt; and populates it for you; pbufs require userspace to recycle back the
&gt; buffer, whereas for kmbufs the kernel is the one in control of
&gt; recycling) and from the user pov it seemed confusing to have kmbufs as
&gt; part of the pbuf ring uapi, instead of separating it out as a
&gt; different type of ringbuffer with a different expectation and

I believe the source of disagreement is that you&#x27;re thinking
about how it&#x27;s going to look like for fuse specifically, and I
believe you that it&#x27;ll be nicer for the fuse use case. However,
on the other hand it&#x27;s an io_uring uapi, and if it is an io_uring
uapi, we need reusable blocks that are not specific to particular
users.

If it km rings has to stay an io_uring uapi, I guess a middle
ground would be to allow registering km rings together with memory,
but make it a pure region without a notion of a buffer, and let
fuse to chunk it. Later, we can make payload memory allocation
optional.

&gt; behavior. I was trying to make the point that combining the interface
&gt; if we go with IORING_MEM_REGION gets even more confusing because now
&gt; pbufs that are kernel-managed are also empty at initialization and
&gt; only can point to areas inside a registered mem region and the
&gt; responsibility of populating it is now on whatever subsystem is using
&gt; it.

Right, intentionally so, because otherwise it&#x27;s a fuse uapi that
pretends to be a generic io_uring uapi but it&#x27;s not because of
all assumptions in different places.

&gt; I still have this opinion but I also think in general, you likely know
&gt; better than I do what kind of io-uring uapi is best for io-uring&#x27;s
&gt; users. For v2 I&#x27;ll have kmbufs go through the pbuf uapi.
&gt; 
&gt;&gt;
&gt;&gt;      And yes, I believe in the current form it&#x27;s inflexible, it requires
&gt;&gt;      a new io_uring uapi. It requires the number of buffers to match
&gt;&gt;      the number of ring entries, which are related but not the same
&gt; 
&gt; I&#x27;m not really seeing what the purpose of having a ring entry with no
&gt; buffer associated with it is. In the existing code for non-kernel
&gt; managed pbuf rings, there&#x27;s the same tie between reg-&gt;ring_entries
&gt; being used as the marker for how many buffers the ring supports. But

Not really, it tells the buffer ring depth but says nothing about
how much memory user space allocated and how it&#x27;s pushed. It&#x27;s a
reasonable default but they could be different. For example, if you
expect adding more memory at runtime, you might create the buffer
ring a bit larger. Or when server processing takes a while and you
can&#x27;t recycle until it finishes, you might have more buffers than
you need ring entries. Or you might might decide to split buffers
and as you mentioned incremental consumption, which is an entire
separate topic because it doesn&#x27;t do de-fragmentation and you&#x27;d
need to have it in fuse, just like user space does with pbufs.

&gt; if the number of buffers should be different than the number of ring
&gt; entries, this can be easily fixed by passing in the number of buffers
&gt; from the uapi for kernel-managed pbuf rings.

My entire point is that we&#x27;re making lots of assumptions for io_uring
uapi, and if it&#x27;s moved to fuse because it knows better what it
needs, it should be a win.

IOW, it sounds better if instead of passing the number of buffers to
io_uring, you just ask it to create a large chunk of memory, and then
fuse chunks it up and puts into the ring.

&gt;&gt;      thing. You can&#x27;t easily add more memory as it&#x27;s bound to the ring
&gt;&gt;      object. The buffer memory won&#x27;t even have same lifetime as the
&gt; 
&gt; To play devil&#x27;s advocate, we also can&#x27;t easily add more memory to the
&gt; mem region once it&#x27;s been registered. I think there&#x27;s also a worse
&gt; penalty where the user needs to know upfront how much memory to
&gt; allocate for the mem region for the lifetime of the ring, which imo
&gt; may be hard to do (eg if a kernel-managed buf ring only needs to be
&gt; registered for some code paths and not others, the mem region
&gt; registration would still have to allocate the memory a potential kbuf
&gt; ring would use).

I agree, and you&#x27;d need something new in either case to add more
memory, and it doesn&#x27;t need to be IORING_REGISTER_MEM_REGION
specifically.

&gt;&gt;      ring object -- allow using that km buffer ring with recv requests
&gt;&gt;      and highly likely I&#x27;ll most likely give you a way to crash the
&gt;&gt;      kernel.
&gt; 
&gt; I&#x27;m a bit confused by this part. The buffer memory does have the same
&gt; lifetime as the ring object, no? The buffers only get freed when the
&gt; ring itself is freed.

Unregistering a buffer ring doesn&#x27;t guarantee that there are no
inflight requests that are still using buffers that came out of
the buffer ring. The fuse driver can wait/terminate its requests
before unregisteration, but allow userspace issued IORING_OP_RECV
to use this km buffer ring, and you&#x27;ll need to somehow synchronise
with all other io_uring requests.

-- 
Pavel Begunkov



---

On 2/18/26 21:43, Joanne Koong wrote:
&gt; On Wed, Feb 18, 2026 at 4:36\u202fAM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt;&gt;
&gt;&gt; On 2/13/26 22:04, Joanne Koong wrote:
&gt;&gt;&gt; On Fri, Feb 13, 2026 at 4:41\u202fAM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt;&gt; ...
&gt;&gt;&gt;&gt; Fuse is doing both adding (kernel) buffers to the ring and consuming
&gt;&gt;&gt;&gt; them. At which point it&#x27;s not clear:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; 1. Why it even needs io_uring provided buffer rings, it can be all
&gt;&gt;&gt;&gt;       contained in fuse. Maybe it&#x27;s trying to reuse pbuf ring code as
&gt;&gt;&gt;&gt;       basically an internal memory allocator, but then why expose buffer
&gt;&gt;&gt;&gt;       rings as an io_uring uapi instead of keeping it internally.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;       That&#x27;s also why I mentioned whether those buffers are supposed to
&gt;&gt;&gt;&gt;       be used with other types of io_uring requests like recv, etc.
&gt;&gt;&gt;
&gt;&gt;&gt; On the userspace/server side, it uses the buffers for other io-uring
&gt;&gt;&gt; operations (eg reading or writing the contents from/to a
&gt;&gt;&gt; locally-backed file).
&gt;&gt;
&gt; 
&gt; Sorry, I submitted v2 last night thinking the conversation on this
&gt; thread had died. After reading through your reply, I&#x27;ll modify v2.

No worries at all, and sorry I&#x27;m a bit slow to reply

&gt;&gt; Oops, typo. I was asking whether the buffer rings (not buffers) are
&gt;&gt; supposed to be used with other requests. E.g. submitting a
&gt;&gt; IORING_OP_RECV with IOSQE_BUFFER_SELECT set and the bgid specifying
&gt;&gt; your kernel-managed buffer ring.
&gt; 
&gt; Yes the buffer rings are intended to be used with other io-uring
&gt; requests. The ideal scenario is that the user can then do the
&gt; equivalent of IORING_OP_READ/WRITE_FIXED operations on the
&gt; kernel-managed buffers and avoid the per-i/o page pinning overhead
&gt; costs.

You mention OP_READ_FIXED and below agreed not exposing km rings
an io_uring uapi, which makes me believe we&#x27;re still talking about
different things.

Correct me if I&#x27;m wrong. Currently, only fuse cmds use the buffer
ring itself, I&#x27;m not talking about buffer, i.e. fuse cmds consume
entries from the ring (!!! that&#x27;s the part I&#x27;m interested in), then
process them and tell the server &quot;this offset in the region has user
data to process or should be populated with data&quot;.

Naturally, the server should be able to use the buffers to issue
some I/O and process it in other ways, whether it&#x27;s a normal
OP_READ to which you pass the user space address (you can since
it&#x27;s mmap()&#x27;ed by the server) or something else is important but
a separate question than the one I&#x27;m trying to understand.

So I&#x27;m asking whether you expect that a server or other user space
program should be able to issue a READ_OP_RECV, READ_OP_READ or any
other similar request, which would consume buffers/entries from the
km ring without any fuse kernel code involved? Do you have some
use case for that in mind?

Understanding that is the key in deciding whether km rings should
be exposed as io_uring uapi or not, regardless of where buffers
to populate the ring come from.

...
&gt; With it going through a mem region, I don&#x27;t think it should even go
&gt; through the &quot;pbuf ring&quot; interface then if it&#x27;s not going to specify
&gt; the number of entries and buffer sizes upfront, if support is added
&gt; for io-uring normal requests (eg IORING_OP_READ/WRITE) to use the
&gt; backing pages from a memory region and if we&#x27;re able to guarantee that
&gt; the registered memory region will never be able to be unregistered by
&gt; the user. I think if we repurpose the
&gt; 
&gt; union {
&gt;    __u64 addr; /* pointer to buffer or iovecs */
&gt;    __u64 splice_off_in;
&gt; };
&gt; 
&gt; fields in the struct io_uring_sqe to
&gt; 
&gt; union {
&gt;    __u64 addr; /* pointer to buffer or iovecs */
&gt;    __u64 splice_off_in;
&gt;    __u64 offset; /* offset into registered mem region */
&gt; };
&gt; 
&gt; and add some IOSQE_ flag to indicate it should find the pages from the
&gt; registered mem region, then that should work for normal requests.
&gt; Where on the kernel side, it looks up the associated pages stored in
&gt; the io_mapped_region&#x27;s pages array for the offset passed in.

So you already can do all that using the mmap()&#x27;ed region user
pointer, and you just want it to be more efficient, right?
For that let&#x27;s just reuse registered buffers, we don&#x27;t need a
new mechanism that needs to be propagated to all request types.
And registered buffer are already optimised for I/O in a bunch
of ways. And as a bonus, it&#x27;ll be similar to the zero-copy
internally registered buffers if you still plan to add them.

The simplest way to do that is to create a registered buffer out
of the mmap&#x27;ed region pointer. Pseudo code:

// mmap&#x27;ed if it&#x27;s kernel allocated.
{region_ptr, region_size} = create_region();

struct iovec iov;
iov.iov_base = region_ptr;
iov.iov_len = region_size;
io_uring_register_buffers(ring, &amp;iov, 1);

// later instead of this:
ptr = region_ptr + off;
io_uring_prep_read(sqe, fd, ptr, ...);

// you use registered buffers as usual:
io_uring_prep_read_fixed(sqe, fd, off, regbuf_idx, ...);


IIRC the registration would fail because it doesn&#x27;t allow file
backed pages, but it should be fine if we know it&#x27;s io_uring
region memory, so that would need to be patched.

There might be a bunch of other ways you can do that like
create a kernel allocated registered buffer like what Cristoph
wants, and then register it as a region. Or allow creating
registered buffers out of a region. etc.

I wanted to unify registered buffers and regions internally
at some point, but then drifted away from active io_uring core
infrastructure development, so I guess that could&#x27;ve been useful.

&gt; Right now there&#x27;s only a uapi to register a memory region and none to
&gt; unregister one. Is it guaranteed that io-uring will never add
&gt; something in the future that will let userspace unregister the memory
&gt; region or at least unregister it while it&#x27;s being used (eg if we add
&gt; future refcounting to it to track active uses of it)?

Let&#x27;s talk about it when it&#x27;s needed or something changes, but if
you do registered buffers instead as per above, they&#x27;ll be holding
page references and or have to pin the region in some other way.

&gt; If so, then end-to-end, with it going through the mem region, it would
&gt; be something like:
&gt; * user creates a mem region for the io-uring
&gt; * user mmaps the mem region

FWIW, we should just add a liburing helper, so that fuse server
doesn&#x27;t need to deal with mmap&#x27;ing.

&gt; * user passes in offset into region, length of each buffer, and number
&gt; of entries in the ring to the subsystem
&gt; * subsystem creates a locally managed bufring and adds buffers to that
&gt; ring from the mem region

That&#x27;s sounds clean to me _if_ it allows you to achieve all
(fast path) optimisations you want to have. I hope it does?

-- 
Pavel Begunkov

</pre>
</details>
<div class="review-comment-signals">Signals: requested changes, technical concerns</div>
</div>
<div class="thread-children">
<div class="thread-node depth-2" id="2026-02-11">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Christoph Hellwig</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-11">2026-02-11</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Christoph Hellwig raised concerns about the patch&#x27;s handling of buffer rings, specifically asking for clarification on &#x27;pbuf&#x27; and questioning the purpose of IORING_REGISTER_MEM_REGION.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Tue, Feb 10, 2026 at 04:34:47PM +0000, Pavel Begunkov wrote:
&gt; &gt; +	union {
&gt; &gt; +		/* used for pbuf rings */
&gt; &gt; +		__u64	ring_addr;
&gt; &gt; +		/* used for kmbuf rings */
&gt; &gt; +		__u32   buf_size;
&gt; 
&gt; If you&#x27;re creating a region, there should be no reason why it
&gt; can&#x27;t work with user passed memory. You&#x27;re fencing yourself off
&gt; optimisations that are already there like huge pages.

Any pages mapped to userspace can be allocated in the kernel as well.

And I really do like this design, because it means we can have a
buffer ring that is only mapped read-only into userspace.  That way
we can still do zero-copy raids if the device requires stable pages
for checksumming or raid.  I was going to implement this as soon
as this series lands upstream.



---

On Thu, Feb 12, 2026 at 10:44:44AM +0000, Pavel Begunkov wrote:
&gt; &gt; 
&gt; &gt; Any pages mapped to userspace can be allocated in the kernel as well.
&gt; 
&gt; pow2 round ups will waste memory. 1MB allocations will never
&gt; become 2MB huge pages. And there is a separate question of
&gt; 1GB huge pages. The user can be smarter about all placement
&gt; decisions.

Sure.  But if the application cares that much about TLB pressure
I&#x27;d just round up to nice multtiple of PTE levels.

&gt; 
&gt; &gt; And I really do like this design, because it means we can have a
&gt; &gt; buffer ring that is only mapped read-only into userspace.  That way
&gt; &gt; we can still do zero-copy raids if the device requires stable pages
&gt; &gt; for checksumming or raid.  I was going to implement this as soon
&gt; &gt; as this series lands upstream.
&gt; 
&gt; That&#x27;s an interesting case. To be clear, user provided memory is
&gt; an optional feature for pbuf rings / regions / etc., and I think
&gt; the io_uring uapi should leave fields for the feature. However, I
&gt; have nothing against fuse refusing to bind to buffer rings it
&gt; doesn&#x27;t like.

Can you clarify what you mean with &#x27;pbuf&#x27;?  The only fixed buffer API I
know is io_uring_register_buffers* which always takes user provided
buffers, so I have a hard time parsing what you&#x27;re saying there.  But
that might just be sign that I&#x27;m no expert in io_uring APIs, and that
web searches have degraded to the point of not being very useful
anymore.



---

On Thu, Feb 12, 2026 at 10:52:29AM +0000, Pavel Begunkov wrote:
&gt; &gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt; &gt; where the kernel controls the allocation fully and guarantees user
&gt; &gt; processes can only read the memory but never write to it.  I&#x27;d love
&gt; &gt; to be able to piggy back than onto your work.
&gt; 
&gt; IORING_REGISTER_MEM_REGION supports both types of allocations. It can
&gt; have a new registration flag for read-only, and then you either make

IORING_REGISTER_MEM_REGION seems to be all about cqs from both your
commit message and the public documentation.  I&#x27;m confused.

&gt; the bounce avoidance optional or reject binding fuse to unsupported
&gt; setups during init. Any arguments against that? I need to go over
&gt; Joanne&#x27;s reply, but I don&#x27;t see any contradiction in principal with
&gt; your use case.

My use case is not about fuse, but good old block and file system
I/O.

</pre>
</details>
<div class="review-comment-signals">Signals: clarification needed, confusion</div>
</div>
<div class="thread-children">
<div class="thread-node depth-3">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Pavel Begunkov</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-10">2026-02-10</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The reviewer Pavel Begunkov raised concerns about the usefulness of provided buffer rings for storage read/write requests, as they bind to a buffer immediately and lack control over where data lands. He suggested using IORING_MEM_REGION or a standalone registered buffer extension instead.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On 2/11/26 15:45, Christoph Hellwig wrote:
&gt; On Tue, Feb 10, 2026 at 04:34:47PM +0000, Pavel Begunkov wrote:
&gt;&gt;&gt; +	union {
&gt;&gt;&gt; +		/* used for pbuf rings */
&gt;&gt;&gt; +		__u64	ring_addr;
&gt;&gt;&gt; +		/* used for kmbuf rings */
&gt;&gt;&gt; +		__u32   buf_size;
&gt;&gt;
&gt;&gt; If you&#x27;re creating a region, there should be no reason why it
&gt;&gt; can&#x27;t work with user passed memory. You&#x27;re fencing yourself off
&gt;&gt; optimisations that are already there like huge pages.
&gt; 
&gt; Any pages mapped to userspace can be allocated in the kernel as well.

pow2 round ups will waste memory. 1MB allocations will never
become 2MB huge pages. And there is a separate question of
1GB huge pages. The user can be smarter about all placement
decisions.

&gt; And I really do like this design, because it means we can have a
&gt; buffer ring that is only mapped read-only into userspace.  That way
&gt; we can still do zero-copy raids if the device requires stable pages
&gt; for checksumming or raid.  I was going to implement this as soon
&gt; as this series lands upstream.

That&#x27;s an interesting case. To be clear, user provided memory is
an optional feature for pbuf rings / regions / etc., and I think
the io_uring uapi should leave fields for the feature. However, I
have nothing against fuse refusing to bind to buffer rings it
doesn&#x27;t like.

-- 
Pavel Begunkov



---

On 2/12/26 10:07, Christoph Hellwig wrote:
&gt; On Wed, Feb 11, 2026 at 02:06:18PM -0800, Joanne Koong wrote:
&gt;&gt;&gt; I don&#x27;t think I follow. I&#x27;m saying that it might be interesting
&gt;&gt;&gt; to separate rings from how and with what they&#x27;re populated on the
&gt;&gt;&gt; kernel API level, but the fuse kernel module can do the population
&gt;&gt;
&gt;&gt; Oh okay, from your first message I (and I think christoph too) thought
&gt;&gt; what you were saying is that the user should be responsible for
&gt;&gt; allocating the buffers with complete ownership over them, and then
&gt;&gt; just pass those allocated to the kernel to use. But what you&#x27;re saying
&gt;&gt; is that just use a different way for getting the kernel to allocate
&gt;&gt; the buffers (eg through the IORING_REGISTER_MEM_REGION interface). Am
&gt;&gt; I reading this correctly?
&gt; 
&gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt; where the kernel controls the allocation fully and guarantees user
&gt; processes can only read the memory but never write to it.  I&#x27;d love
&gt; to be able to piggy back than onto your work.

IORING_REGISTER_MEM_REGION supports both types of allocations. It can
have a new registration flag for read-only, and then you either make
the bounce avoidance optional or reject binding fuse to unsupported
setups during init. Any arguments against that? I need to go over
Joanne&#x27;s reply, but I don&#x27;t see any contradiction in principal with
your use case.

-- 
Pavel Begunkov



---

On 2/13/26 07:18, Christoph Hellwig wrote:
&gt; On Thu, Feb 12, 2026 at 10:44:44AM +0000, Pavel Begunkov wrote:
&gt;&gt;&gt;
&gt;&gt;&gt; Any pages mapped to userspace can be allocated in the kernel as well.
&gt;&gt;
&gt;&gt; pow2 round ups will waste memory. 1MB allocations will never
&gt;&gt; become 2MB huge pages. And there is a separate question of
&gt;&gt; 1GB huge pages. The user can be smarter about all placement
&gt;&gt; decisions.
&gt; 
&gt; Sure.  But if the application cares that much about TLB pressure
&gt; I&#x27;d just round up to nice multtiple of PTE levels.
&gt; 
&gt;&gt;
&gt;&gt;&gt; And I really do like this design, because it means we can have a
&gt;&gt;&gt; buffer ring that is only mapped read-only into userspace.  That way
&gt;&gt;&gt; we can still do zero-copy raids if the device requires stable pages
&gt;&gt;&gt; for checksumming or raid.  I was going to implement this as soon
&gt;&gt;&gt; as this series lands upstream.
&gt;&gt;
&gt;&gt; That&#x27;s an interesting case. To be clear, user provided memory is
&gt;&gt; an optional feature for pbuf rings / regions / etc., and I think
&gt;&gt; the io_uring uapi should leave fields for the feature. However, I
&gt;&gt; have nothing against fuse refusing to bind to buffer rings it
&gt;&gt; doesn&#x27;t like.
&gt; 
&gt; Can you clarify what you mean with &#x27;pbuf&#x27;?  The only fixed buffer API I
&gt; know is io_uring_register_buffers* which always takes user provided
&gt; buffers, so I have a hard time parsing what you&#x27;re saying there.  But
&gt; that might just be sign that I&#x27;m no expert in io_uring APIs, and that
&gt; web searches have degraded to the point of not being very useful
&gt; anymore.

Registered, aka fixed, buffers are the ones you pass to
IORING_OP_[READ,WRITE]_FIXED and some other requests. It&#x27;s normally
created by io_uring_register_buffers*() / IORING_REGISTER_BUFFERS*
with user memory, but there are special cases when it&#x27;s installed
internally by other kernel components, e.g. ublk.
This series has nothing to do with them, and relevant parts of
the discussion here don&#x27;t mention them either.

Provided buffer rings, a.k.a pbuf rings, IORING_REGISTER_PBUF_RING
is a kernel-user shared ring. The entries are user buffers
{uaddr, size}. The user space adds entries, the kernel (io_uring
requests) consumes them and issues I/O using the user addresses.
E.g. you can issue a IORING_OP_RECV request (+IOSQE_BUFFER_SELECT)
and it&#x27;ll grab a buffer from the ring instead of using sqe-&gt;addr.

pbuf rings, IORING_REGISTER_MEM_REGION, completion/submission
queues and all other kernel-user rings/etc. are internally based
on so called regions. All of them support both user allocated
memory and kernel allocations + mmap.

This series essentially creates provided buffer rings, where
1. the ring now contains kernel addresses
2. the ring itself is in-kernel only and not shared with user space
3. it also allocates kernel buffers (as a region), populates the ring
    with them, and allows mapping the buffers into the user space.

Fuse is doing both adding (kernel) buffers to the ring and consuming
them. At which point it&#x27;s not clear:

1. Why it even needs io_uring provided buffer rings, it can be all
    contained in fuse. Maybe it&#x27;s trying to reuse pbuf ring code as
    basically an internal memory allocator, but then why expose buffer
    rings as an io_uring uapi instead of keeping it internally.

    That&#x27;s also why I mentioned whether those buffers are supposed to
    be used with other types of io_uring requests like recv, etc.

2. Why making io_uring to allocate payload memory. The answer to which
    is probably to reuse the region api with mmap and so on. And why
    payload buffers are inseparably created together with the ring
    and via a new io_uring uapi.

    And yes, I believe in the current form it&#x27;s inflexible, it requires
    a new io_uring uapi. It requires the number of buffers to match
    the number of ring entries, which are related but not the same
    thing. You can&#x27;t easily add more memory as it&#x27;s bound to the ring
    object. The buffer memory won&#x27;t even have same lifetime as the
    ring object -- allow using that km buffer ring with recv requests
    and highly likely I&#x27;ll most likely give you a way to crash the
    kernel.

But hey, I&#x27;m tired. I don&#x27;t have any beef here and am only trying
to make it a bit cleaner and flexible for fuse in the first place
without even questioning the I/O path. If everyone believes
everything is right, just ask Jens to merge it.

-- 
Pavel Begunkov



---

On 2/13/26 07:21, Christoph Hellwig wrote:
&gt; On Thu, Feb 12, 2026 at 10:52:29AM +0000, Pavel Begunkov wrote:
&gt;&gt;&gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt;&gt;&gt; where the kernel controls the allocation fully and guarantees user
&gt;&gt;&gt; processes can only read the memory but never write to it.  I&#x27;d love
&gt;&gt;&gt; to be able to piggy back than onto your work.
&gt;&gt;
&gt;&gt; IORING_REGISTER_MEM_REGION supports both types of allocations. It can
&gt;&gt; have a new registration flag for read-only, and then you either make
&gt; 
&gt; IORING_REGISTER_MEM_REGION seems to be all about cqs from both your
&gt; commit message and the public documentation.  I&#x27;m confused.

Think of it as an area of memory for kernel-user communication. Used
for syscall parameters passing to avoid copy_from_user, but I added
it for a bunch of use cases. We&#x27;ll hopefully get support at some
point for passing request arguments like struct iovec. BPF patches
use it for communication. I need to respin patches placing SQ/CQ onto
it (avoid some memory waste).

Tbh, I never meant it nor io_uring regions to be used for huge
payload buffers, but this series already uses regions for that.


&gt;&gt; the bounce avoidance optional or reject binding fuse to unsupported
&gt;&gt; setups during init. Any arguments against that? I need to go over
&gt;&gt; Joanne&#x27;s reply, but I don&#x27;t see any contradiction in principal with
&gt;&gt; your use case.
&gt; 
&gt; My use case is not about fuse, but good old block and file system
&gt; I/O.

Then I&#x27;m confused. Take a look at the other reply, this series is
about buffer rings with kernel memory, it can&#x27;t work without a kernel
component returning buffers into the ring, and io_uring doesn&#x27;t do
that. But maybe you&#x27;re thinking about adding some more elaborate API.

IIUC, Joanne also wants to add support for fuse installing registered
buffers, which would allow zero-copy, but those got split out of
this series.

-- 
Pavel Begunkov



---

On 2/13/26 07:27, Christoph Hellwig wrote:
&gt; On Thu, Feb 12, 2026 at 09:29:31AM -0800, Joanne Koong wrote:
&gt;&gt;&gt;&gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt;&gt;&gt;&gt; where the kernel controls the allocation fully and guarantees user
&gt;&gt;&gt;&gt; processes can only read the memory but never write to it.  I&#x27;d love
&gt;&gt;
&gt;&gt; By &quot;control the allocation fully&quot; do you mean for your use case, the
&gt;&gt; allocation/setup isn&#x27;t triggered by userspace but is initiated by the
&gt;&gt; kernel (eg user never explicitly registers any kbuf ring, the kernel
&gt;&gt; just uses the kbuf ring data structure internally and users can read
&gt;&gt; the buffer contents)? If userspace initiates the setup of the kbuf
&gt;&gt; ring, going through IORING_REGISTER_MEM_REGION would be semantically
&gt;&gt; the same, except the buffer allocation by the kernel now happens
&gt;&gt; before the ring is created and then later populated into the ring.
&gt;&gt; userspace would still need to make an mmap call to the region and the
&gt;&gt; kernel could enforce that as read-only. But if userspace doesn&#x27;t
&gt;&gt; initiate the setup, then going through IORING_REGISTER_MEM_REGION gets
&gt;&gt; uglier.
&gt; 
&gt; The idea is that the application tells the kernel that it wants to use
&gt; a fixed buffer pool for reads.  Right now the application does this
&gt; using io_uring_register_buffers().  The problem with that is that
&gt; io_uring_register_buffers ends up just doing a pin of the memory,
&gt; but the application or, in case of shared memory, someone else could
&gt; still modify the memory.  If the underlying file system or storage
&gt; device needs verify checksums, or worse rebuild data from parity
&gt; (or uncompress), it needs to ensure that the memory it is operating
&gt; on can&#x27;t be modified by someone else.
&gt; 
&gt; So I&#x27;ve been thinking of a version of io_uring_register_buffers where
&gt; the buffers are not provided by the application, but instead by the
&gt; kernel and mapped into the application address space read-only for
&gt; a while, and I thought I could implement this on top of your series,
&gt; but I have to admit I haven&#x27;t really looked into the details all
&gt; that much.

There is nothing about registered buffers in this series. And even
if you try to reuse buffer allocation out of it, it&#x27;ll come with
a circular buffer you&#x27;ll have no need for. And I&#x27;m pretty much
arguing about separating those for io_uring.

-- 
Pavel Begunkov



---

On 2/17/26 05:38, Christoph Hellwig wrote:
&gt; On Fri, Feb 13, 2026 at 11:14:03AM -0800, Joanne Koong wrote:
&gt;&gt; I think we have the exact same use case, except your buffers need to
&gt;&gt; be read-only. I think your use case benefits from the same memory wins
&gt;&gt; we&#x27;ll get with incremental buffer consumption, which is the primary
&gt;&gt; reason fuse is using a bufring instead of fixed buffers.
&gt; 
&gt; Yeah.

Provided buffer rings are not useful for storage read/write requests
because they bind to a buffer right away, that&#x27;s in contrast to some
recv request, where io_uring will first poll the socket to confirm
the data is there, and only then take a buffer from the buffer ring
and copy into it. With storage rw it makes more sense to specify
the buffer directly gain control over where exactly data lands
IOW, instead of the usual &quot;read data into a given pointer&quot; request
semantics like what read(2) gives you, buffer rings are rather
&quot;read data somewhere and return a pointer to where you placed it&quot;.

Another problem is that someone needs to return buffers back into
the buffer ring, and it&#x27;s a kernel private ring. For this patchset
it&#x27;s assumed the fuse driver is going to be doing that, but there
is no one for normal rw requests.

&gt;&gt; I think you can and it&#x27;ll be very easy to do so. All that would be
&gt;&gt; needed is to pass in a read-only flag from the userspace side when it
&gt;&gt; registers the bufring, and then when userspace makes the mmap call to
&gt;&gt; the bufring, the kernel checks if that read-only flag is set on the
&gt;&gt; bufring and if so returns a read-only mapping.
&gt; 
&gt; Yes, tat&#x27;s what I though.  But Pavel seems to disagree?

Yes. You only need buffers, and it&#x27;ll be better to base on sth that
gives you buffers/memory without extra semantics, i.e.
IORING_MEM_REGION. Or it can be a standalone registered buffer
extension, likely reusing regions internally. That might even yield
a finer API.

-- 
Pavel Begunkov

</pre>
</details>
<div class="review-comment-signals">Signals: requested changes, alternative solutions</div>
</div>
</div>
<div class="thread-node depth-3">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Pavel Begunkov</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-12">2026-02-12</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The reviewer raised concerns about the usability of kernel-managed buffer rings for storage read/write requests, citing issues with buffer binding and return. They suggested using IORING_MEM_REGION or a standalone registered buffer extension instead.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On 2/11/26 15:45, Christoph Hellwig wrote:
&gt; On Tue, Feb 10, 2026 at 04:34:47PM +0000, Pavel Begunkov wrote:
&gt;&gt;&gt; +	union {
&gt;&gt;&gt; +		/* used for pbuf rings */
&gt;&gt;&gt; +		__u64	ring_addr;
&gt;&gt;&gt; +		/* used for kmbuf rings */
&gt;&gt;&gt; +		__u32   buf_size;
&gt;&gt;
&gt;&gt; If you&#x27;re creating a region, there should be no reason why it
&gt;&gt; can&#x27;t work with user passed memory. You&#x27;re fencing yourself off
&gt;&gt; optimisations that are already there like huge pages.
&gt; 
&gt; Any pages mapped to userspace can be allocated in the kernel as well.

pow2 round ups will waste memory. 1MB allocations will never
become 2MB huge pages. And there is a separate question of
1GB huge pages. The user can be smarter about all placement
decisions.

&gt; And I really do like this design, because it means we can have a
&gt; buffer ring that is only mapped read-only into userspace.  That way
&gt; we can still do zero-copy raids if the device requires stable pages
&gt; for checksumming or raid.  I was going to implement this as soon
&gt; as this series lands upstream.

That&#x27;s an interesting case. To be clear, user provided memory is
an optional feature for pbuf rings / regions / etc., and I think
the io_uring uapi should leave fields for the feature. However, I
have nothing against fuse refusing to bind to buffer rings it
doesn&#x27;t like.

-- 
Pavel Begunkov



---

On 2/12/26 10:07, Christoph Hellwig wrote:
&gt; On Wed, Feb 11, 2026 at 02:06:18PM -0800, Joanne Koong wrote:
&gt;&gt;&gt; I don&#x27;t think I follow. I&#x27;m saying that it might be interesting
&gt;&gt;&gt; to separate rings from how and with what they&#x27;re populated on the
&gt;&gt;&gt; kernel API level, but the fuse kernel module can do the population
&gt;&gt;
&gt;&gt; Oh okay, from your first message I (and I think christoph too) thought
&gt;&gt; what you were saying is that the user should be responsible for
&gt;&gt; allocating the buffers with complete ownership over them, and then
&gt;&gt; just pass those allocated to the kernel to use. But what you&#x27;re saying
&gt;&gt; is that just use a different way for getting the kernel to allocate
&gt;&gt; the buffers (eg through the IORING_REGISTER_MEM_REGION interface). Am
&gt;&gt; I reading this correctly?
&gt; 
&gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt; where the kernel controls the allocation fully and guarantees user
&gt; processes can only read the memory but never write to it.  I&#x27;d love
&gt; to be able to piggy back than onto your work.

IORING_REGISTER_MEM_REGION supports both types of allocations. It can
have a new registration flag for read-only, and then you either make
the bounce avoidance optional or reject binding fuse to unsupported
setups during init. Any arguments against that? I need to go over
Joanne&#x27;s reply, but I don&#x27;t see any contradiction in principal with
your use case.

-- 
Pavel Begunkov



---

On 2/13/26 07:18, Christoph Hellwig wrote:
&gt; On Thu, Feb 12, 2026 at 10:44:44AM +0000, Pavel Begunkov wrote:
&gt;&gt;&gt;
&gt;&gt;&gt; Any pages mapped to userspace can be allocated in the kernel as well.
&gt;&gt;
&gt;&gt; pow2 round ups will waste memory. 1MB allocations will never
&gt;&gt; become 2MB huge pages. And there is a separate question of
&gt;&gt; 1GB huge pages. The user can be smarter about all placement
&gt;&gt; decisions.
&gt; 
&gt; Sure.  But if the application cares that much about TLB pressure
&gt; I&#x27;d just round up to nice multtiple of PTE levels.
&gt; 
&gt;&gt;
&gt;&gt;&gt; And I really do like this design, because it means we can have a
&gt;&gt;&gt; buffer ring that is only mapped read-only into userspace.  That way
&gt;&gt;&gt; we can still do zero-copy raids if the device requires stable pages
&gt;&gt;&gt; for checksumming or raid.  I was going to implement this as soon
&gt;&gt;&gt; as this series lands upstream.
&gt;&gt;
&gt;&gt; That&#x27;s an interesting case. To be clear, user provided memory is
&gt;&gt; an optional feature for pbuf rings / regions / etc., and I think
&gt;&gt; the io_uring uapi should leave fields for the feature. However, I
&gt;&gt; have nothing against fuse refusing to bind to buffer rings it
&gt;&gt; doesn&#x27;t like.
&gt; 
&gt; Can you clarify what you mean with &#x27;pbuf&#x27;?  The only fixed buffer API I
&gt; know is io_uring_register_buffers* which always takes user provided
&gt; buffers, so I have a hard time parsing what you&#x27;re saying there.  But
&gt; that might just be sign that I&#x27;m no expert in io_uring APIs, and that
&gt; web searches have degraded to the point of not being very useful
&gt; anymore.

Registered, aka fixed, buffers are the ones you pass to
IORING_OP_[READ,WRITE]_FIXED and some other requests. It&#x27;s normally
created by io_uring_register_buffers*() / IORING_REGISTER_BUFFERS*
with user memory, but there are special cases when it&#x27;s installed
internally by other kernel components, e.g. ublk.
This series has nothing to do with them, and relevant parts of
the discussion here don&#x27;t mention them either.

Provided buffer rings, a.k.a pbuf rings, IORING_REGISTER_PBUF_RING
is a kernel-user shared ring. The entries are user buffers
{uaddr, size}. The user space adds entries, the kernel (io_uring
requests) consumes them and issues I/O using the user addresses.
E.g. you can issue a IORING_OP_RECV request (+IOSQE_BUFFER_SELECT)
and it&#x27;ll grab a buffer from the ring instead of using sqe-&gt;addr.

pbuf rings, IORING_REGISTER_MEM_REGION, completion/submission
queues and all other kernel-user rings/etc. are internally based
on so called regions. All of them support both user allocated
memory and kernel allocations + mmap.

This series essentially creates provided buffer rings, where
1. the ring now contains kernel addresses
2. the ring itself is in-kernel only and not shared with user space
3. it also allocates kernel buffers (as a region), populates the ring
    with them, and allows mapping the buffers into the user space.

Fuse is doing both adding (kernel) buffers to the ring and consuming
them. At which point it&#x27;s not clear:

1. Why it even needs io_uring provided buffer rings, it can be all
    contained in fuse. Maybe it&#x27;s trying to reuse pbuf ring code as
    basically an internal memory allocator, but then why expose buffer
    rings as an io_uring uapi instead of keeping it internally.

    That&#x27;s also why I mentioned whether those buffers are supposed to
    be used with other types of io_uring requests like recv, etc.

2. Why making io_uring to allocate payload memory. The answer to which
    is probably to reuse the region api with mmap and so on. And why
    payload buffers are inseparably created together with the ring
    and via a new io_uring uapi.

    And yes, I believe in the current form it&#x27;s inflexible, it requires
    a new io_uring uapi. It requires the number of buffers to match
    the number of ring entries, which are related but not the same
    thing. You can&#x27;t easily add more memory as it&#x27;s bound to the ring
    object. The buffer memory won&#x27;t even have same lifetime as the
    ring object -- allow using that km buffer ring with recv requests
    and highly likely I&#x27;ll most likely give you a way to crash the
    kernel.

But hey, I&#x27;m tired. I don&#x27;t have any beef here and am only trying
to make it a bit cleaner and flexible for fuse in the first place
without even questioning the I/O path. If everyone believes
everything is right, just ask Jens to merge it.

-- 
Pavel Begunkov



---

On 2/13/26 07:21, Christoph Hellwig wrote:
&gt; On Thu, Feb 12, 2026 at 10:52:29AM +0000, Pavel Begunkov wrote:
&gt;&gt;&gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt;&gt;&gt; where the kernel controls the allocation fully and guarantees user
&gt;&gt;&gt; processes can only read the memory but never write to it.  I&#x27;d love
&gt;&gt;&gt; to be able to piggy back than onto your work.
&gt;&gt;
&gt;&gt; IORING_REGISTER_MEM_REGION supports both types of allocations. It can
&gt;&gt; have a new registration flag for read-only, and then you either make
&gt; 
&gt; IORING_REGISTER_MEM_REGION seems to be all about cqs from both your
&gt; commit message and the public documentation.  I&#x27;m confused.

Think of it as an area of memory for kernel-user communication. Used
for syscall parameters passing to avoid copy_from_user, but I added
it for a bunch of use cases. We&#x27;ll hopefully get support at some
point for passing request arguments like struct iovec. BPF patches
use it for communication. I need to respin patches placing SQ/CQ onto
it (avoid some memory waste).

Tbh, I never meant it nor io_uring regions to be used for huge
payload buffers, but this series already uses regions for that.


&gt;&gt; the bounce avoidance optional or reject binding fuse to unsupported
&gt;&gt; setups during init. Any arguments against that? I need to go over
&gt;&gt; Joanne&#x27;s reply, but I don&#x27;t see any contradiction in principal with
&gt;&gt; your use case.
&gt; 
&gt; My use case is not about fuse, but good old block and file system
&gt; I/O.

Then I&#x27;m confused. Take a look at the other reply, this series is
about buffer rings with kernel memory, it can&#x27;t work without a kernel
component returning buffers into the ring, and io_uring doesn&#x27;t do
that. But maybe you&#x27;re thinking about adding some more elaborate API.

IIUC, Joanne also wants to add support for fuse installing registered
buffers, which would allow zero-copy, but those got split out of
this series.

-- 
Pavel Begunkov



---

On 2/13/26 07:27, Christoph Hellwig wrote:
&gt; On Thu, Feb 12, 2026 at 09:29:31AM -0800, Joanne Koong wrote:
&gt;&gt;&gt;&gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt;&gt;&gt;&gt; where the kernel controls the allocation fully and guarantees user
&gt;&gt;&gt;&gt; processes can only read the memory but never write to it.  I&#x27;d love
&gt;&gt;
&gt;&gt; By &quot;control the allocation fully&quot; do you mean for your use case, the
&gt;&gt; allocation/setup isn&#x27;t triggered by userspace but is initiated by the
&gt;&gt; kernel (eg user never explicitly registers any kbuf ring, the kernel
&gt;&gt; just uses the kbuf ring data structure internally and users can read
&gt;&gt; the buffer contents)? If userspace initiates the setup of the kbuf
&gt;&gt; ring, going through IORING_REGISTER_MEM_REGION would be semantically
&gt;&gt; the same, except the buffer allocation by the kernel now happens
&gt;&gt; before the ring is created and then later populated into the ring.
&gt;&gt; userspace would still need to make an mmap call to the region and the
&gt;&gt; kernel could enforce that as read-only. But if userspace doesn&#x27;t
&gt;&gt; initiate the setup, then going through IORING_REGISTER_MEM_REGION gets
&gt;&gt; uglier.
&gt; 
&gt; The idea is that the application tells the kernel that it wants to use
&gt; a fixed buffer pool for reads.  Right now the application does this
&gt; using io_uring_register_buffers().  The problem with that is that
&gt; io_uring_register_buffers ends up just doing a pin of the memory,
&gt; but the application or, in case of shared memory, someone else could
&gt; still modify the memory.  If the underlying file system or storage
&gt; device needs verify checksums, or worse rebuild data from parity
&gt; (or uncompress), it needs to ensure that the memory it is operating
&gt; on can&#x27;t be modified by someone else.
&gt; 
&gt; So I&#x27;ve been thinking of a version of io_uring_register_buffers where
&gt; the buffers are not provided by the application, but instead by the
&gt; kernel and mapped into the application address space read-only for
&gt; a while, and I thought I could implement this on top of your series,
&gt; but I have to admit I haven&#x27;t really looked into the details all
&gt; that much.

There is nothing about registered buffers in this series. And even
if you try to reuse buffer allocation out of it, it&#x27;ll come with
a circular buffer you&#x27;ll have no need for. And I&#x27;m pretty much
arguing about separating those for io_uring.

-- 
Pavel Begunkov



---

On 2/17/26 05:38, Christoph Hellwig wrote:
&gt; On Fri, Feb 13, 2026 at 11:14:03AM -0800, Joanne Koong wrote:
&gt;&gt; I think we have the exact same use case, except your buffers need to
&gt;&gt; be read-only. I think your use case benefits from the same memory wins
&gt;&gt; we&#x27;ll get with incremental buffer consumption, which is the primary
&gt;&gt; reason fuse is using a bufring instead of fixed buffers.
&gt; 
&gt; Yeah.

Provided buffer rings are not useful for storage read/write requests
because they bind to a buffer right away, that&#x27;s in contrast to some
recv request, where io_uring will first poll the socket to confirm
the data is there, and only then take a buffer from the buffer ring
and copy into it. With storage rw it makes more sense to specify
the buffer directly gain control over where exactly data lands
IOW, instead of the usual &quot;read data into a given pointer&quot; request
semantics like what read(2) gives you, buffer rings are rather
&quot;read data somewhere and return a pointer to where you placed it&quot;.

Another problem is that someone needs to return buffers back into
the buffer ring, and it&#x27;s a kernel private ring. For this patchset
it&#x27;s assumed the fuse driver is going to be doing that, but there
is no one for normal rw requests.

&gt;&gt; I think you can and it&#x27;ll be very easy to do so. All that would be
&gt;&gt; needed is to pass in a read-only flag from the userspace side when it
&gt;&gt; registers the bufring, and then when userspace makes the mmap call to
&gt;&gt; the bufring, the kernel checks if that read-only flag is set on the
&gt;&gt; bufring and if so returns a read-only mapping.
&gt; 
&gt; Yes, tat&#x27;s what I though.  But Pavel seems to disagree?

Yes. You only need buffers, and it&#x27;ll be better to base on sth that
gives you buffers/memory without extra semantics, i.e.
IORING_MEM_REGION. Or it can be a standalone registered buffer
extension, likely reusing regions internally. That might even yield
a finer API.

-- 
Pavel Begunkov

</pre>
</details>
<div class="review-comment-signals">Signals: requested changes</div>
</div>
</div>
<div class="thread-node depth-3">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Pavel Begunkov</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-18">2026-02-18</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The reviewer Pavel Begunkov raised concerns about the usability and design of buffer rings in io_uring, specifically questioning their usefulness for storage read/write requests and suggesting that they should be based on IORING_MEM_REGION or a standalone registered buffer extension.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On 2/11/26 15:45, Christoph Hellwig wrote:
&gt; On Tue, Feb 10, 2026 at 04:34:47PM +0000, Pavel Begunkov wrote:
&gt;&gt;&gt; +	union {
&gt;&gt;&gt; +		/* used for pbuf rings */
&gt;&gt;&gt; +		__u64	ring_addr;
&gt;&gt;&gt; +		/* used for kmbuf rings */
&gt;&gt;&gt; +		__u32   buf_size;
&gt;&gt;
&gt;&gt; If you&#x27;re creating a region, there should be no reason why it
&gt;&gt; can&#x27;t work with user passed memory. You&#x27;re fencing yourself off
&gt;&gt; optimisations that are already there like huge pages.
&gt; 
&gt; Any pages mapped to userspace can be allocated in the kernel as well.

pow2 round ups will waste memory. 1MB allocations will never
become 2MB huge pages. And there is a separate question of
1GB huge pages. The user can be smarter about all placement
decisions.

&gt; And I really do like this design, because it means we can have a
&gt; buffer ring that is only mapped read-only into userspace.  That way
&gt; we can still do zero-copy raids if the device requires stable pages
&gt; for checksumming or raid.  I was going to implement this as soon
&gt; as this series lands upstream.

That&#x27;s an interesting case. To be clear, user provided memory is
an optional feature for pbuf rings / regions / etc., and I think
the io_uring uapi should leave fields for the feature. However, I
have nothing against fuse refusing to bind to buffer rings it
doesn&#x27;t like.

-- 
Pavel Begunkov



---

On 2/12/26 10:07, Christoph Hellwig wrote:
&gt; On Wed, Feb 11, 2026 at 02:06:18PM -0800, Joanne Koong wrote:
&gt;&gt;&gt; I don&#x27;t think I follow. I&#x27;m saying that it might be interesting
&gt;&gt;&gt; to separate rings from how and with what they&#x27;re populated on the
&gt;&gt;&gt; kernel API level, but the fuse kernel module can do the population
&gt;&gt;
&gt;&gt; Oh okay, from your first message I (and I think christoph too) thought
&gt;&gt; what you were saying is that the user should be responsible for
&gt;&gt; allocating the buffers with complete ownership over them, and then
&gt;&gt; just pass those allocated to the kernel to use. But what you&#x27;re saying
&gt;&gt; is that just use a different way for getting the kernel to allocate
&gt;&gt; the buffers (eg through the IORING_REGISTER_MEM_REGION interface). Am
&gt;&gt; I reading this correctly?
&gt; 
&gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt; where the kernel controls the allocation fully and guarantees user
&gt; processes can only read the memory but never write to it.  I&#x27;d love
&gt; to be able to piggy back than onto your work.

IORING_REGISTER_MEM_REGION supports both types of allocations. It can
have a new registration flag for read-only, and then you either make
the bounce avoidance optional or reject binding fuse to unsupported
setups during init. Any arguments against that? I need to go over
Joanne&#x27;s reply, but I don&#x27;t see any contradiction in principal with
your use case.

-- 
Pavel Begunkov



---

On 2/13/26 07:18, Christoph Hellwig wrote:
&gt; On Thu, Feb 12, 2026 at 10:44:44AM +0000, Pavel Begunkov wrote:
&gt;&gt;&gt;
&gt;&gt;&gt; Any pages mapped to userspace can be allocated in the kernel as well.
&gt;&gt;
&gt;&gt; pow2 round ups will waste memory. 1MB allocations will never
&gt;&gt; become 2MB huge pages. And there is a separate question of
&gt;&gt; 1GB huge pages. The user can be smarter about all placement
&gt;&gt; decisions.
&gt; 
&gt; Sure.  But if the application cares that much about TLB pressure
&gt; I&#x27;d just round up to nice multtiple of PTE levels.
&gt; 
&gt;&gt;
&gt;&gt;&gt; And I really do like this design, because it means we can have a
&gt;&gt;&gt; buffer ring that is only mapped read-only into userspace.  That way
&gt;&gt;&gt; we can still do zero-copy raids if the device requires stable pages
&gt;&gt;&gt; for checksumming or raid.  I was going to implement this as soon
&gt;&gt;&gt; as this series lands upstream.
&gt;&gt;
&gt;&gt; That&#x27;s an interesting case. To be clear, user provided memory is
&gt;&gt; an optional feature for pbuf rings / regions / etc., and I think
&gt;&gt; the io_uring uapi should leave fields for the feature. However, I
&gt;&gt; have nothing against fuse refusing to bind to buffer rings it
&gt;&gt; doesn&#x27;t like.
&gt; 
&gt; Can you clarify what you mean with &#x27;pbuf&#x27;?  The only fixed buffer API I
&gt; know is io_uring_register_buffers* which always takes user provided
&gt; buffers, so I have a hard time parsing what you&#x27;re saying there.  But
&gt; that might just be sign that I&#x27;m no expert in io_uring APIs, and that
&gt; web searches have degraded to the point of not being very useful
&gt; anymore.

Registered, aka fixed, buffers are the ones you pass to
IORING_OP_[READ,WRITE]_FIXED and some other requests. It&#x27;s normally
created by io_uring_register_buffers*() / IORING_REGISTER_BUFFERS*
with user memory, but there are special cases when it&#x27;s installed
internally by other kernel components, e.g. ublk.
This series has nothing to do with them, and relevant parts of
the discussion here don&#x27;t mention them either.

Provided buffer rings, a.k.a pbuf rings, IORING_REGISTER_PBUF_RING
is a kernel-user shared ring. The entries are user buffers
{uaddr, size}. The user space adds entries, the kernel (io_uring
requests) consumes them and issues I/O using the user addresses.
E.g. you can issue a IORING_OP_RECV request (+IOSQE_BUFFER_SELECT)
and it&#x27;ll grab a buffer from the ring instead of using sqe-&gt;addr.

pbuf rings, IORING_REGISTER_MEM_REGION, completion/submission
queues and all other kernel-user rings/etc. are internally based
on so called regions. All of them support both user allocated
memory and kernel allocations + mmap.

This series essentially creates provided buffer rings, where
1. the ring now contains kernel addresses
2. the ring itself is in-kernel only and not shared with user space
3. it also allocates kernel buffers (as a region), populates the ring
    with them, and allows mapping the buffers into the user space.

Fuse is doing both adding (kernel) buffers to the ring and consuming
them. At which point it&#x27;s not clear:

1. Why it even needs io_uring provided buffer rings, it can be all
    contained in fuse. Maybe it&#x27;s trying to reuse pbuf ring code as
    basically an internal memory allocator, but then why expose buffer
    rings as an io_uring uapi instead of keeping it internally.

    That&#x27;s also why I mentioned whether those buffers are supposed to
    be used with other types of io_uring requests like recv, etc.

2. Why making io_uring to allocate payload memory. The answer to which
    is probably to reuse the region api with mmap and so on. And why
    payload buffers are inseparably created together with the ring
    and via a new io_uring uapi.

    And yes, I believe in the current form it&#x27;s inflexible, it requires
    a new io_uring uapi. It requires the number of buffers to match
    the number of ring entries, which are related but not the same
    thing. You can&#x27;t easily add more memory as it&#x27;s bound to the ring
    object. The buffer memory won&#x27;t even have same lifetime as the
    ring object -- allow using that km buffer ring with recv requests
    and highly likely I&#x27;ll most likely give you a way to crash the
    kernel.

But hey, I&#x27;m tired. I don&#x27;t have any beef here and am only trying
to make it a bit cleaner and flexible for fuse in the first place
without even questioning the I/O path. If everyone believes
everything is right, just ask Jens to merge it.

-- 
Pavel Begunkov



---

On 2/13/26 07:21, Christoph Hellwig wrote:
&gt; On Thu, Feb 12, 2026 at 10:52:29AM +0000, Pavel Begunkov wrote:
&gt;&gt;&gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt;&gt;&gt; where the kernel controls the allocation fully and guarantees user
&gt;&gt;&gt; processes can only read the memory but never write to it.  I&#x27;d love
&gt;&gt;&gt; to be able to piggy back than onto your work.
&gt;&gt;
&gt;&gt; IORING_REGISTER_MEM_REGION supports both types of allocations. It can
&gt;&gt; have a new registration flag for read-only, and then you either make
&gt; 
&gt; IORING_REGISTER_MEM_REGION seems to be all about cqs from both your
&gt; commit message and the public documentation.  I&#x27;m confused.

Think of it as an area of memory for kernel-user communication. Used
for syscall parameters passing to avoid copy_from_user, but I added
it for a bunch of use cases. We&#x27;ll hopefully get support at some
point for passing request arguments like struct iovec. BPF patches
use it for communication. I need to respin patches placing SQ/CQ onto
it (avoid some memory waste).

Tbh, I never meant it nor io_uring regions to be used for huge
payload buffers, but this series already uses regions for that.


&gt;&gt; the bounce avoidance optional or reject binding fuse to unsupported
&gt;&gt; setups during init. Any arguments against that? I need to go over
&gt;&gt; Joanne&#x27;s reply, but I don&#x27;t see any contradiction in principal with
&gt;&gt; your use case.
&gt; 
&gt; My use case is not about fuse, but good old block and file system
&gt; I/O.

Then I&#x27;m confused. Take a look at the other reply, this series is
about buffer rings with kernel memory, it can&#x27;t work without a kernel
component returning buffers into the ring, and io_uring doesn&#x27;t do
that. But maybe you&#x27;re thinking about adding some more elaborate API.

IIUC, Joanne also wants to add support for fuse installing registered
buffers, which would allow zero-copy, but those got split out of
this series.

-- 
Pavel Begunkov



---

On 2/13/26 07:27, Christoph Hellwig wrote:
&gt; On Thu, Feb 12, 2026 at 09:29:31AM -0800, Joanne Koong wrote:
&gt;&gt;&gt;&gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt;&gt;&gt;&gt; where the kernel controls the allocation fully and guarantees user
&gt;&gt;&gt;&gt; processes can only read the memory but never write to it.  I&#x27;d love
&gt;&gt;
&gt;&gt; By &quot;control the allocation fully&quot; do you mean for your use case, the
&gt;&gt; allocation/setup isn&#x27;t triggered by userspace but is initiated by the
&gt;&gt; kernel (eg user never explicitly registers any kbuf ring, the kernel
&gt;&gt; just uses the kbuf ring data structure internally and users can read
&gt;&gt; the buffer contents)? If userspace initiates the setup of the kbuf
&gt;&gt; ring, going through IORING_REGISTER_MEM_REGION would be semantically
&gt;&gt; the same, except the buffer allocation by the kernel now happens
&gt;&gt; before the ring is created and then later populated into the ring.
&gt;&gt; userspace would still need to make an mmap call to the region and the
&gt;&gt; kernel could enforce that as read-only. But if userspace doesn&#x27;t
&gt;&gt; initiate the setup, then going through IORING_REGISTER_MEM_REGION gets
&gt;&gt; uglier.
&gt; 
&gt; The idea is that the application tells the kernel that it wants to use
&gt; a fixed buffer pool for reads.  Right now the application does this
&gt; using io_uring_register_buffers().  The problem with that is that
&gt; io_uring_register_buffers ends up just doing a pin of the memory,
&gt; but the application or, in case of shared memory, someone else could
&gt; still modify the memory.  If the underlying file system or storage
&gt; device needs verify checksums, or worse rebuild data from parity
&gt; (or uncompress), it needs to ensure that the memory it is operating
&gt; on can&#x27;t be modified by someone else.
&gt; 
&gt; So I&#x27;ve been thinking of a version of io_uring_register_buffers where
&gt; the buffers are not provided by the application, but instead by the
&gt; kernel and mapped into the application address space read-only for
&gt; a while, and I thought I could implement this on top of your series,
&gt; but I have to admit I haven&#x27;t really looked into the details all
&gt; that much.

There is nothing about registered buffers in this series. And even
if you try to reuse buffer allocation out of it, it&#x27;ll come with
a circular buffer you&#x27;ll have no need for. And I&#x27;m pretty much
arguing about separating those for io_uring.

-- 
Pavel Begunkov



---

On 2/17/26 05:38, Christoph Hellwig wrote:
&gt; On Fri, Feb 13, 2026 at 11:14:03AM -0800, Joanne Koong wrote:
&gt;&gt; I think we have the exact same use case, except your buffers need to
&gt;&gt; be read-only. I think your use case benefits from the same memory wins
&gt;&gt; we&#x27;ll get with incremental buffer consumption, which is the primary
&gt;&gt; reason fuse is using a bufring instead of fixed buffers.
&gt; 
&gt; Yeah.

Provided buffer rings are not useful for storage read/write requests
because they bind to a buffer right away, that&#x27;s in contrast to some
recv request, where io_uring will first poll the socket to confirm
the data is there, and only then take a buffer from the buffer ring
and copy into it. With storage rw it makes more sense to specify
the buffer directly gain control over where exactly data lands
IOW, instead of the usual &quot;read data into a given pointer&quot; request
semantics like what read(2) gives you, buffer rings are rather
&quot;read data somewhere and return a pointer to where you placed it&quot;.

Another problem is that someone needs to return buffers back into
the buffer ring, and it&#x27;s a kernel private ring. For this patchset
it&#x27;s assumed the fuse driver is going to be doing that, but there
is no one for normal rw requests.

&gt;&gt; I think you can and it&#x27;ll be very easy to do so. All that would be
&gt;&gt; needed is to pass in a read-only flag from the userspace side when it
&gt;&gt; registers the bufring, and then when userspace makes the mmap call to
&gt;&gt; the bufring, the kernel checks if that read-only flag is set on the
&gt;&gt; bufring and if so returns a read-only mapping.
&gt; 
&gt; Yes, tat&#x27;s what I though.  But Pavel seems to disagree?

Yes. You only need buffers, and it&#x27;ll be better to base on sth that
gives you buffers/memory without extra semantics, i.e.
IORING_MEM_REGION. Or it can be a standalone registered buffer
extension, likely reusing regions internally. That might even yield
a finer API.

-- 
Pavel Begunkov

</pre>
</details>
<div class="review-comment-signals">Signals: requested changes, design concerns</div>
</div>
</div>
<div class="thread-node depth-3">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-18">2026-02-18</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Joanne Koong suggested modifying the io_uring_register_buf_ring() API to support read-only buffers, and proposed a solution involving the use of IORING_MEM_REGION and mmap.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Thu, Feb 12, 2026 at 11:27 PM Christoph Hellwig &lt;hch@infradead.org&gt; wrote:
&gt;
&gt; On Thu, Feb 12, 2026 at 09:29:31AM -0800, Joanne Koong wrote:
&gt; &gt; &gt; &gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt; &gt; &gt; &gt; where the kernel controls the allocation fully and guarantees user
&gt; &gt; &gt; &gt; processes can only read the memory but never write to it.  I&#x27;d love
&gt; &gt;
&gt; &gt; By &quot;control the allocation fully&quot; do you mean for your use case, the
&gt; &gt; allocation/setup isn&#x27;t triggered by userspace but is initiated by the
&gt; &gt; kernel (eg user never explicitly registers any kbuf ring, the kernel
&gt; &gt; just uses the kbuf ring data structure internally and users can read
&gt; &gt; the buffer contents)? If userspace initiates the setup of the kbuf
&gt; &gt; ring, going through IORING_REGISTER_MEM_REGION would be semantically
&gt; &gt; the same, except the buffer allocation by the kernel now happens
&gt; &gt; before the ring is created and then later populated into the ring.
&gt; &gt; userspace would still need to make an mmap call to the region and the
&gt; &gt; kernel could enforce that as read-only. But if userspace doesn&#x27;t
&gt; &gt; initiate the setup, then going through IORING_REGISTER_MEM_REGION gets
&gt; &gt; uglier.
&gt;
&gt; The idea is that the application tells the kernel that it wants to use
&gt; a fixed buffer pool for reads.  Right now the application does this
&gt; using io_uring_register_buffers().  The problem with that is that
&gt; io_uring_register_buffers ends up just doing a pin of the memory,
&gt; but the application or, in case of shared memory, someone else could
&gt; still modify the memory.  If the underlying file system or storage
&gt; device needs verify checksums, or worse rebuild data from parity
&gt; (or uncompress), it needs to ensure that the memory it is operating
&gt; on can&#x27;t be modified by someone else.

(resending because I hit reply instead of reply-all)

I think we have the exact same use case, except your buffers need to
be read-only. I think your use case benefits from the same memory wins
we&#x27;ll get with incremental buffer consumption, which is the primary
reason fuse is using a bufring instead of fixed buffers.

&gt;
&gt; So I&#x27;ve been thinking of a version of io_uring_register_buffers where
&gt; the buffers are not provided by the application, but instead by the
&gt; kernel and mapped into the application address space read-only for
&gt; a while, and I thought I could implement this on top of your series,
&gt; but I have to admit I haven&#x27;t really looked into the details all
&gt; that much.

I think you can and it&#x27;ll be very easy to do so. All that would be
needed is to pass in a read-only flag from the userspace side when it
registers the bufring, and then when userspace makes the mmap call to
the bufring, the kernel checks if that read-only flag is set on the
bufring and if so returns a read-only mapping. I&#x27;m happy to add that
patch to this series if that would make things easier for you. The
io_uring_register_buffers() api registers fixed buffers (which have to
be user-allocated memory) so you would need to go through the
io_uring_register_buf_ring() api once kmbufs are squashed into the
pbuf interface.

With going through IORING_MEM_REGION, this would work for your use
case as well. The user would have to register the mem region with
io_uring_register_region() and pass in a read-only flag, and then the
kernel will allocate the memory region. Then userspace would mmap the
memory region and on the kernel side, it would set the mapping to be
read-only. When the kmbufring then gets registered, the buffers in it
will be empty. The filesystem will then have to populate the buffers
in it from the mem region that was previously registered.

Thanks,
Joanne

&gt;
&gt; &gt;
&gt; &gt; To be completely honest, the more I look at this the more this feels
&gt; &gt; like overkill / over-engineered to me. I get that now the user can do
&gt; &gt; the PMD optimization, but does that actually lead to noticeable
&gt; &gt; performance benefits? It seems especially confusing with them going
&gt; &gt; through the same pbuf ring interface but having totally different
&gt; &gt; expectations.
&gt;
&gt; Yes.  The PMD mapping also is not that relevant.  Both AMD (implicit)
&gt; and ARM (explicit) have optimizations for contiguous PTEs that are
&gt; almost as valuable.
&gt;
&gt; &gt; What about adding a straightforward kmbuf ring that goes through the
&gt; &gt; pbuf interface (eg the design in this patchset) and then in the future
&gt; &gt; adding an interface for pbuf rings (both kernel-managed and
&gt; &gt; non-kernel-managed) to go through IORING_REGISTERED_MEM_REGIONS if
&gt; &gt; users end up needing/wanting to have their rings populated that way?
&gt;
&gt; That feels much simpler to me as well.
&gt;
</pre>
</details>
<div class="review-comment-signals">Signals: requested changes, proposed modification</div>
</div>
</div>
<div class="thread-node depth-3">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-20_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-20">2026-02-20</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Joanne Koong suggested modifying the io_uring_register_buf_ring() API to support read-only buffers, which would benefit Christoph&#x27;s use case.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Thu, Feb 12, 2026 at 11:27 PM Christoph Hellwig &lt;hch@infradead.org&gt; wrote:
&gt;
&gt; On Thu, Feb 12, 2026 at 09:29:31AM -0800, Joanne Koong wrote:
&gt; &gt; &gt; &gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt; &gt; &gt; &gt; where the kernel controls the allocation fully and guarantees user
&gt; &gt; &gt; &gt; processes can only read the memory but never write to it.  I&#x27;d love
&gt; &gt;
&gt; &gt; By &quot;control the allocation fully&quot; do you mean for your use case, the
&gt; &gt; allocation/setup isn&#x27;t triggered by userspace but is initiated by the
&gt; &gt; kernel (eg user never explicitly registers any kbuf ring, the kernel
&gt; &gt; just uses the kbuf ring data structure internally and users can read
&gt; &gt; the buffer contents)? If userspace initiates the setup of the kbuf
&gt; &gt; ring, going through IORING_REGISTER_MEM_REGION would be semantically
&gt; &gt; the same, except the buffer allocation by the kernel now happens
&gt; &gt; before the ring is created and then later populated into the ring.
&gt; &gt; userspace would still need to make an mmap call to the region and the
&gt; &gt; kernel could enforce that as read-only. But if userspace doesn&#x27;t
&gt; &gt; initiate the setup, then going through IORING_REGISTER_MEM_REGION gets
&gt; &gt; uglier.
&gt;
&gt; The idea is that the application tells the kernel that it wants to use
&gt; a fixed buffer pool for reads.  Right now the application does this
&gt; using io_uring_register_buffers().  The problem with that is that
&gt; io_uring_register_buffers ends up just doing a pin of the memory,
&gt; but the application or, in case of shared memory, someone else could
&gt; still modify the memory.  If the underlying file system or storage
&gt; device needs verify checksums, or worse rebuild data from parity
&gt; (or uncompress), it needs to ensure that the memory it is operating
&gt; on can&#x27;t be modified by someone else.

(resending because I hit reply instead of reply-all)

I think we have the exact same use case, except your buffers need to
be read-only. I think your use case benefits from the same memory wins
we&#x27;ll get with incremental buffer consumption, which is the primary
reason fuse is using a bufring instead of fixed buffers.

&gt;
&gt; So I&#x27;ve been thinking of a version of io_uring_register_buffers where
&gt; the buffers are not provided by the application, but instead by the
&gt; kernel and mapped into the application address space read-only for
&gt; a while, and I thought I could implement this on top of your series,
&gt; but I have to admit I haven&#x27;t really looked into the details all
&gt; that much.

I think you can and it&#x27;ll be very easy to do so. All that would be
needed is to pass in a read-only flag from the userspace side when it
registers the bufring, and then when userspace makes the mmap call to
the bufring, the kernel checks if that read-only flag is set on the
bufring and if so returns a read-only mapping. I&#x27;m happy to add that
patch to this series if that would make things easier for you. The
io_uring_register_buffers() api registers fixed buffers (which have to
be user-allocated memory) so you would need to go through the
io_uring_register_buf_ring() api once kmbufs are squashed into the
pbuf interface.

With going through IORING_MEM_REGION, this would work for your use
case as well. The user would have to register the mem region with
io_uring_register_region() and pass in a read-only flag, and then the
kernel will allocate the memory region. Then userspace would mmap the
memory region and on the kernel side, it would set the mapping to be
read-only. When the kmbufring then gets registered, the buffers in it
will be empty. The filesystem will then have to populate the buffers
in it from the mem region that was previously registered.

Thanks,
Joanne

&gt;
&gt; &gt;
&gt; &gt; To be completely honest, the more I look at this the more this feels
&gt; &gt; like overkill / over-engineered to me. I get that now the user can do
&gt; &gt; the PMD optimization, but does that actually lead to noticeable
&gt; &gt; performance benefits? It seems especially confusing with them going
&gt; &gt; through the same pbuf ring interface but having totally different
&gt; &gt; expectations.
&gt;
&gt; Yes.  The PMD mapping also is not that relevant.  Both AMD (implicit)
&gt; and ARM (explicit) have optimizations for contiguous PTEs that are
&gt; almost as valuable.
&gt;
&gt; &gt; What about adding a straightforward kmbuf ring that goes through the
&gt; &gt; pbuf interface (eg the design in this patchset) and then in the future
&gt; &gt; adding an interface for pbuf rings (both kernel-managed and
&gt; &gt; non-kernel-managed) to go through IORING_REGISTERED_MEM_REGIONS if
&gt; &gt; users end up needing/wanting to have their rings populated that way?
&gt;
&gt; That feels much simpler to me as well.
&gt;
</pre>
</details>
<div class="review-comment-signals">Signals: requested changes, modification needed</div>
</div>
</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-18">2026-02-18</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Joanne Koong raised concerns about the benefits of kernel-managed buffer rings, questioning whether user-allocated buffers have any advantages and pointing out potential complications in interface and lifecycle management.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Tue, Feb 10, 2026 at 8:34 AM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt;
&gt; On 2/10/26 00:28, Joanne Koong wrote:
&gt; &gt; Add support for kernel-managed buffer rings (kmbuf rings), which allow
&gt; &gt; the kernel to allocate and manage the backing buffers for a buffer
&gt; &gt; ring, rather than requiring the application to provide and manage them.
&gt; &gt;
&gt; &gt; This introduces two new registration opcodes:
&gt; &gt; - IORING_REGISTER_KMBUF_RING: Register a kernel-managed buffer ring
&gt; &gt; - IORING_UNREGISTER_KMBUF_RING: Unregister a kernel-managed buffer ring
&gt; &gt;
&gt; &gt; The existing io_uring_buf_reg structure is extended with a union to
&gt; &gt; support both application-provided buffer rings (pbuf) and kernel-managed
&gt; &gt; buffer rings (kmbuf):
&gt; &gt; - For pbuf rings: ring_addr specifies the user-provided ring address
&gt; &gt; - For kmbuf rings: buf_size specifies the size of each buffer. buf_size
&gt; &gt;    must be non-zero and page-aligned.
&gt; &gt;
&gt; &gt; The implementation follows the same pattern as pbuf ring registration,
&gt; &gt; reusing the validation and buffer list allocation helpers introduced in
&gt; &gt; earlier refactoring. The IOBL_KERNEL_MANAGED flag marks buffer lists as
&gt; &gt; kernel-managed for appropriate handling in the I/O path.
&gt; &gt;
&gt; &gt; Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
&gt; &gt; ---
&gt; &gt;   include/uapi/linux/io_uring.h |  15 ++++-
&gt; &gt;   io_uring/kbuf.c               |  81 ++++++++++++++++++++++++-
&gt; &gt;   io_uring/kbuf.h               |   7 ++-
&gt; &gt;   io_uring/memmap.c             | 111 ++++++++++++++++++++++++++++++++++
&gt; &gt;   io_uring/memmap.h             |   4 ++
&gt; &gt;   io_uring/register.c           |   7 +++
&gt; &gt;   6 files changed, 219 insertions(+), 6 deletions(-)
&gt; &gt;
&gt; &gt; diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
&gt; &gt; index fc473af6feb4..a0889c1744bd 100644
&gt; &gt; --- a/include/uapi/linux/io_uring.h
&gt; &gt; +++ b/include/uapi/linux/io_uring.h
&gt; &gt; @@ -715,6 +715,10 @@ enum io_uring_register_op {
&gt; &gt;       /* register bpf filtering programs */
&gt; &gt;       IORING_REGISTER_BPF_FILTER              = 37,
&gt; &gt;
&gt; &gt; +     /* register/unregister kernel-managed ring buffer group */
&gt; &gt; +     IORING_REGISTER_KMBUF_RING              = 38,
&gt; &gt; +     IORING_UNREGISTER_KMBUF_RING            = 39,
&gt; &gt; +
&gt; &gt;       /* this goes last */
&gt; &gt;       IORING_REGISTER_LAST,
&gt; &gt;
&gt; &gt; @@ -891,9 +895,16 @@ enum io_uring_register_pbuf_ring_flags {
&gt; &gt;       IOU_PBUF_RING_INC       = 2,
&gt; &gt;   };
&gt; &gt;
&gt; &gt; -/* argument for IORING_(UN)REGISTER_PBUF_RING */
&gt; &gt; +/* argument for IORING_(UN)REGISTER_PBUF_RING and
&gt; &gt; + * IORING_(UN)REGISTER_KMBUF_RING
&gt; &gt; + */
&gt; &gt;   struct io_uring_buf_reg {
&gt; &gt; -     __u64   ring_addr;
&gt; &gt; +     union {
&gt; &gt; +             /* used for pbuf rings */
&gt; &gt; +             __u64   ring_addr;
&gt; &gt; +             /* used for kmbuf rings */
&gt; &gt; +             __u32   buf_size;
&gt;
&gt; If you&#x27;re creating a region, there should be no reason why it
&gt; can&#x27;t work with user passed memory. You&#x27;re fencing yourself off
&gt; optimisations that are already there like huge pages.

Are there any optimizations with user-allocated buffers that wouldn&#x27;t
be possible with kernel-allocated buffers? For huge pages, can&#x27;t the
kernel do this as well (eg I see in io_mem_alloc_compound(), it calls
into alloc_pages() with order &gt; 0)?

&gt;
&gt; &gt; +     };
&gt; &gt;       __u32   ring_entries;
&gt; &gt;       __u16   bgid;
&gt; &gt;       __u16   flags;
&gt; &gt; diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
&gt; &gt; index aa9b70b72db4..9bc36451d083 100644
&gt; &gt; --- a/io_uring/kbuf.c
&gt; &gt; +++ b/io_uring/kbuf.c
&gt; ...
&gt; &gt; +static int io_setup_kmbuf_ring(struct io_ring_ctx *ctx,
&gt; &gt; +                            struct io_buffer_list *bl,
&gt; &gt; +                            struct io_uring_buf_reg *reg)
&gt; &gt; +{
&gt; &gt; +     struct io_uring_buf_ring *ring;
&gt; &gt; +     unsigned long ring_size;
&gt; &gt; +     void *buf_region;
&gt; &gt; +     unsigned int i;
&gt; &gt; +     int ret;
&gt; &gt; +
&gt; &gt; +     /* allocate pages for the ring structure */
&gt; &gt; +     ring_size = flex_array_size(ring, bufs, bl-&gt;nr_entries);
&gt; &gt; +     ring = kzalloc(ring_size, GFP_KERNEL_ACCOUNT);
&gt; &gt; +     if (!ring)
&gt; &gt; +             return -ENOMEM;
&gt; &gt; +
&gt; &gt; +     ret = io_create_region_multi_buf(ctx, &amp;bl-&gt;region, bl-&gt;nr_entries,
&gt; &gt; +                                      reg-&gt;buf_size);
&gt;
&gt; Please use io_create_region(), the new function does nothing new
&gt; and only violates abstractions.

There&#x27;s separate checks needed between io_create_region() and
io_create_region_multi_buf() (eg IORING_MEM_REGION_TYPE_USER flag
checking) and different allocation calls (eg
io_region_allocate_pages() vs io_region_allocate_pages_multi_buf()).
Maybe I&#x27;m misinterpreting your comment (or the code), but I&#x27;m not
seeing how this can just use io_create_region().

&gt;
&gt; Provided buffer rings with kernel addresses could be an interesting
&gt; abstraction, but why is it also responsible for allocating buffers?

Conceptually, I think it makes the interface and lifecycle management
simpler/cleaner. With registering it from userspace, imo there&#x27;s
additional complications with no tangible benefits, eg it&#x27;s not
guaranteed that the memory regions registered for the buffers are the
same size, with allocating it from the kernel-side we can guarantee
that the pages are allocated physically contiguously, userspace setup
with user-allocated buffers is less straightforward, etc. In general,
I&#x27;m just not really seeing what advantages there are in allocating the
buffers from userspace. Could you elaborate on that part more?

&gt; What I&#x27;d do:
&gt;
&gt; 1. Strip buffer allocation from IORING_REGISTER_KMBUF_RING.
&gt; 2. Replace *_REGISTER_KMBUF_RING with *_REGISTER_PBUF_RING + a new flag.
&gt;     Or maybe don&#x27;t expose it to the user at all and create it from
&gt;     fuse via internal API.

If kmbuf rings are squashed into pbuf rings, then pbuf rings will need
to support pinning. In fuse, there are some contexts where you can&#x27;t
grab the uring mutex because you&#x27;re running in atomic context and this
can be encountered while recycling the buffer. I originally had a
patch adding pinning to pbuf rings (to mitigate the overhead of
registered buffers lookups) but dropped it when Jens and Caleb didn&#x27;t
like the idea. But for kmbuf rings, pinning will be necessary for
fuse.

&gt; 3. Require the user to register a memory region of appropriate size,
&gt;     see IORING_REGISTER_MEM_REGION, ctx-&gt;param_region. Make fuse
&gt;     populating the buffer ring using the memory region.
&gt;
&gt; I wanted to make regions shareable anyway (need it for other purposes),
&gt; I can toss patches for that tomorrow.
&gt;
&gt; A separate question is whether extending buffer rings is the right
&gt; approach as it seems like you&#x27;re only using it for fuse requests and
&gt; not for passing buffers to normal requests, but I don&#x27;t see the

What are &#x27;normal requests&#x27;? For fuse&#x27;s use case, there are only fuse requests.

Thanks,
Joanne

&gt; big picture here.
&gt;
&gt; &gt; +     if (ret) {
&gt; &gt; +             kfree(ring);
&gt; &gt; +             return ret;
&gt; &gt; +     }
&gt; &gt; +
&gt; &gt; +     /* initialize ring buf entries to point to the buffers */
&gt; &gt; +     buf_region = bl-&gt;region.ptr;
&gt;
&gt; io_region_get_ptr()
&gt;
&gt; &gt; +     for (i = 0; i &lt; bl-&gt;nr_entries; i++) {
&gt; &gt; +             struct io_uring_buf *buf = &amp;ring-&gt;bufs[i];
&gt; &gt; +
&gt; &gt; +             buf-&gt;addr = (u64)(uintptr_t)buf_region;
&gt; &gt; +             buf-&gt;len = reg-&gt;buf_size;
&gt; &gt; +             buf-&gt;bid = i;
&gt; &gt; +
&gt; &gt; +             buf_region += reg-&gt;buf_size;
&gt; &gt; +     }
&gt; &gt; +     ring-&gt;tail = bl-&gt;nr_entries;
&gt; &gt; +
&gt; &gt; +     bl-&gt;buf_ring = ring;
&gt; &gt; +     bl-&gt;flags |= IOBL_KERNEL_MANAGED;
&gt; &gt; +
&gt; &gt; +     return 0;
&gt; &gt; +}
&gt; &gt; +
&gt; &gt; +int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
&gt; &gt; +{
&gt; &gt; +     struct io_uring_buf_reg reg;
&gt; &gt; +     struct io_buffer_list *bl;
&gt; &gt; +     int ret;
&gt; &gt; +
&gt; &gt; +     lockdep_assert_held(&amp;ctx-&gt;uring_lock);
&gt; &gt; +
&gt; &gt; +     ret = io_copy_and_validate_buf_reg(arg, &amp;reg, 0);
&gt; &gt; +     if (ret)
&gt; &gt; +             return ret;
&gt; &gt; +
&gt; &gt; +     if (!reg.buf_size || !PAGE_ALIGNED(reg.buf_size))
&gt;
&gt; With io_create_region_multi_buf() gone, you shouldn&#x27;t need
&gt; to align every buffer, that could be a lot of wasted memory
&gt; (thinking about 64KB pages).
&gt;
&gt; &gt; +             return -EINVAL;
&gt; &gt; +
&gt; &gt; +     bl = io_alloc_new_buffer_list(ctx, &amp;reg);
&gt; &gt; +     if (IS_ERR(bl))
&gt; &gt; +             return PTR_ERR(bl);
&gt; &gt; +
&gt; &gt; +     ret = io_setup_kmbuf_ring(ctx, bl, &amp;reg);
&gt; &gt; +     if (ret) {
&gt; &gt; +             kfree(bl);
&gt; &gt; +             return ret;
&gt; &gt; +     }
&gt; &gt; +
&gt; &gt; +     ret = io_buffer_add_list(ctx, bl, reg.bgid);
&gt; &gt; +     if (ret)
&gt; &gt; +             io_put_bl(ctx, bl);
&gt; &gt; +
&gt; &gt; +     return ret;
&gt;
&gt; --
&gt; Pavel Begunkov
&gt;


---

On Wed, Feb 11, 2026 at 4:01 AM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt;
&gt; On 2/10/26 19:39, Joanne Koong wrote:
&gt; &gt; On Tue, Feb 10, 2026 at 8:34 AM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt; ...
&gt; &gt;&gt;&gt; -/* argument for IORING_(UN)REGISTER_PBUF_RING */
&gt; &gt;&gt;&gt; +/* argument for IORING_(UN)REGISTER_PBUF_RING and
&gt; &gt;&gt;&gt; + * IORING_(UN)REGISTER_KMBUF_RING
&gt; &gt;&gt;&gt; + */
&gt; &gt;&gt;&gt;    struct io_uring_buf_reg {
&gt; &gt;&gt;&gt; -     __u64   ring_addr;
&gt; &gt;&gt;&gt; +     union {
&gt; &gt;&gt;&gt; +             /* used for pbuf rings */
&gt; &gt;&gt;&gt; +             __u64   ring_addr;
&gt; &gt;&gt;&gt; +             /* used for kmbuf rings */
&gt; &gt;&gt;&gt; +             __u32   buf_size;
&gt; &gt;&gt;
&gt; &gt;&gt; If you&#x27;re creating a region, there should be no reason why it
&gt; &gt;&gt; can&#x27;t work with user passed memory. You&#x27;re fencing yourself off
&gt; &gt;&gt; optimisations that are already there like huge pages.
&gt; &gt;
&gt; &gt; Are there any optimizations with user-allocated buffers that wouldn&#x27;t
&gt; &gt; be possible with kernel-allocated buffers? For huge pages, can&#x27;t the
&gt; &gt; kernel do this as well (eg I see in io_mem_alloc_compound(), it calls
&gt; &gt; into alloc_pages() with order &gt; 0)?
&gt;
&gt; Yes, there is handful of differences. To name one, 1MB allocation won&#x27;t
&gt; get you a PMD mappable huge page, while user space can allocate 2MB,
&gt; register the first 1MB and reuse the rest for other purposes.
&gt;
&gt; &gt;&gt;&gt; +     };
&gt; &gt;&gt;&gt;        __u32   ring_entries;
&gt; &gt;&gt;&gt;        __u16   bgid;
&gt; &gt;&gt;&gt;        __u16   flags;
&gt; &gt;&gt;&gt; diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
&gt; &gt;&gt;&gt; index aa9b70b72db4..9bc36451d083 100644
&gt; &gt;&gt;&gt; --- a/io_uring/kbuf.c
&gt; &gt;&gt;&gt; +++ b/io_uring/kbuf.c
&gt; &gt;&gt; ...
&gt; &gt;&gt;&gt; +static int io_setup_kmbuf_ring(struct io_ring_ctx *ctx,
&gt; &gt;&gt;&gt; +                            struct io_buffer_list *bl,
&gt; &gt;&gt;&gt; +                            struct io_uring_buf_reg *reg)
&gt; &gt;&gt;&gt; +{
&gt; &gt;&gt;&gt; +     struct io_uring_buf_ring *ring;
&gt; &gt;&gt;&gt; +     unsigned long ring_size;
&gt; &gt;&gt;&gt; +     void *buf_region;
&gt; &gt;&gt;&gt; +     unsigned int i;
&gt; &gt;&gt;&gt; +     int ret;
&gt; &gt;&gt;&gt; +
&gt; &gt;&gt;&gt; +     /* allocate pages for the ring structure */
&gt; &gt;&gt;&gt; +     ring_size = flex_array_size(ring, bufs, bl-&gt;nr_entries);
&gt; &gt;&gt;&gt; +     ring = kzalloc(ring_size, GFP_KERNEL_ACCOUNT);
&gt; &gt;&gt;&gt; +     if (!ring)
&gt; &gt;&gt;&gt; +             return -ENOMEM;
&gt; &gt;&gt;&gt; +
&gt; &gt;&gt;&gt; +     ret = io_create_region_multi_buf(ctx, &amp;bl-&gt;region, bl-&gt;nr_entries,
&gt; &gt;&gt;&gt; +                                      reg-&gt;buf_size);
&gt; &gt;&gt;
&gt; &gt;&gt; Please use io_create_region(), the new function does nothing new
&gt; &gt;&gt; and only violates abstractions.
&gt; &gt;
&gt; &gt; There&#x27;s separate checks needed between io_create_region() and
&gt; &gt; io_create_region_multi_buf() (eg IORING_MEM_REGION_TYPE_USER flag
&gt;
&gt; If io_create_region() is too strict, let&#x27;s discuss that in
&gt; examples if there are any, but it&#x27;s likely not a good idea changing
&gt; that. If it&#x27;s too lax, filter arguments in the caller. IOW, don&#x27;t
&gt; pass IORING_MEM_REGION_TYPE_USER if it&#x27;s not used.
&gt;
&gt; &gt; checking) and different allocation calls (eg
&gt; &gt; io_region_allocate_pages() vs io_region_allocate_pages_multi_buf()).
&gt;
&gt; I saw that and saying that all memmap.c changes can get dropped.
&gt; You&#x27;re using it as one big virtually contig kernel memory range then
&gt; chunked into buffers, and that&#x27;s pretty much what you&#x27;re getting with
&gt; normal io_create_region(). I get that you only need it to be
&gt; contiguous within a single buffer, but that&#x27;s not what you&#x27;re doing,
&gt; and it&#x27;ll be only worse than default io_create_region() e.g.
&gt; effectively disabling any usefulness of io_mem_alloc_compound(),
&gt; and ultimately you don&#x27;t need to care.

When I originally implemented it, I had it use
io_region_allocate_pages() but this fails because it&#x27;s allocating way
too much memory at once. For fuse&#x27;s use case, each buffer is usually
at least 1 MB if not more. Allocating the memory one buffer a time in
io_region_allocate_pages_multi_buf() bypasses the allocation errors I
was seeing. That&#x27;s the main reason I don&#x27;t think this can just use
io_create_region().

&gt;
&gt; Regions shouldn&#x27;t know anything about your buffers, how it&#x27;s
&gt; subdivided after, etc.
&gt;
&gt; &gt; Maybe I&#x27;m misinterpreting your comment (or the code), but I&#x27;m not
&gt; &gt; seeing how this can just use io_create_region().
&gt;
&gt; struct io_uring_region_desc rd = {};
&gt; total_size = nr_bufs * buf_size;
&gt; rd.size = PAGE_ALIGN(total_size);
&gt; io_create_region(&amp;region, &amp;rd);
&gt;
&gt; Add something like this for user provided memory:
&gt;
&gt; if (use_user_memory) {
&gt;         rd.user_addr = uaddr;
&gt;         rd.flags |= IORING_MEM_REGION_TYPE_USER;
&gt; }
&gt;
&gt;
&gt; &gt;&gt; Provided buffer rings with kernel addresses could be an interesting
&gt; &gt;&gt; abstraction, but why is it also responsible for allocating buffers?
&gt; &gt;
&gt; &gt; Conceptually, I think it makes the interface and lifecycle management
&gt; &gt; simpler/cleaner. With registering it from userspace, imo there&#x27;s
&gt; &gt; additional complications with no tangible benefits, eg it&#x27;s not
&gt; &gt; guaranteed that the memory regions registered for the buffers are the
&gt; &gt; same size, with allocating it from the kernel-side we can guarantee
&gt; &gt; that the pages are allocated physically contiguously, userspace setup
&gt; &gt; with user-allocated buffers is less straightforward, etc. In general,
&gt; &gt; I&#x27;m just not really seeing what advantages there are in allocating the
&gt; &gt; buffers from userspace. Could you elaborate on that part more?
&gt;
&gt; I don&#x27;t think I follow. I&#x27;m saying that it might be interesting
&gt; to separate rings from how and with what they&#x27;re populated on the
&gt; kernel API level, but the fuse kernel module can do the population

Oh okay, from your first message I (and I think christoph too) thought
what you were saying is that the user should be responsible for
allocating the buffers with complete ownership over them, and then
just pass those allocated to the kernel to use. But what you&#x27;re saying
is that just use a different way for getting the kernel to allocate
the buffers (eg through the IORING_REGISTER_MEM_REGION interface). Am
I reading this correctly?

&gt; and get exactly same layout as you currently have:
&gt;
&gt; int fuse_create_ring(size_t region_offset /* user space argument */) {
&gt;         struct io_mapped_region *mr = get_mem_region(ctx);
&gt;         // that can take full control of the ring
&gt;         ring = grab_empty_ring(io_uring_ctx);
&gt;
&gt;         size = nr_bufs * buf_size;
&gt;         if (region_offset + size &gt; get_size(mr)) // + other validation
&gt;                 return error;
&gt;
&gt;         buf = mr_get_ptr(mr) + offset;
&gt;         for (i = 0; i &lt; nr_bufs; i++) {
&gt;                 ring_push_buffer(ring, buf, buf_size);
&gt;                 buf += buf_size;
&gt;         }
&gt; }
&gt;
&gt; fuse might not care, but with empty rings other users will get a
&gt; channel they can use to do IO (e.g. read requests) using their
&gt; kernel addresses in the future.
&gt;
&gt; &gt;&gt; What I&#x27;d do:
&gt; &gt;&gt;
&gt; &gt;&gt; 1. Strip buffer allocation from IORING_REGISTER_KMBUF_RING.
&gt; &gt;&gt; 2. Replace *_REGISTER_KMBUF_RING with *_REGISTER_PBUF_RING + a new flag.
&gt; &gt;&gt;      Or maybe don&#x27;t expose it to the user at all and create it from
&gt; &gt;&gt;      fuse via internal API.
&gt; &gt;
&gt; &gt; If kmbuf rings are squashed into pbuf rings, then pbuf rings will need
&gt; &gt; to support pinning. In fuse, there are some contexts where you can&#x27;t
&gt;
&gt; It&#x27;d change uapi but not internals, you already piggy back it
&gt; on pbuf implementation and differentiate with a flag.
&gt;
&gt; It could basically be:
&gt;
&gt; if (flags &amp; IOU_PBUF_RING_KM)
&gt;         bl-&gt;flags |= IOBL_KERNEL_MANAGED;
&gt;
&gt; Pinning can be gated on that flag as well. Pretty likely uapi
&gt; and internals will be a bit cleaner, but that&#x27;s not a huge deal,
&gt; just don&#x27;t see why would you roll out a separate set of uapi
&gt; ([un]register, offsets, etc.) when essentially it can be treated
&gt; as the same thing.

imo, it looked cleaner as a separate api because it has different
expectations and behaviors and squashing kmbuf into the pbuf api makes
the pbuf api needlessly more complex. Though I guess from the
userspace pov, liburing could have a wrapper that takes care of
setting up the pbuf details for kernel-managed pbufs. But in my head,
having pbufs vs. kmbufs makes it clearer what each one does vs regular
pbufs vs. pbufs that are kernel-managed.

Especially with now having kmbufs go through the ioring mem region
interface, it makes things more confusing imo if they&#x27;re combined, eg
pbufs that are kernel-managed are created empty and then populated
from the kernel side by whatever subsystem is using them. Right now
there&#x27;s only one mem region supported per ring, but in the future if
there&#x27;s the possibility that multiple mem regions can be registered
(eg if userspace doesn&#x27;t know upfront what mem region length they&#x27;ll
need), then we should also probably add in a region id param for the
registration arg, which if kmbuf rings go through the pbuf ring
registration api, is not possible to do.

But I&#x27;m happy to combine the interfaces and go with your suggestion.
I&#x27;ll make this change for v2 unless someone else objects.

&gt;
&gt; &gt; grab the uring mutex because you&#x27;re running in atomic context and this
&gt; &gt; can be encountered while recycling the buffer. I originally had a
&gt; &gt; patch adding pinning to pbuf rings (to mitigate the overhead of
&gt; &gt; registered buffers lookups)
&gt;
&gt; IIRC, you was pinning the registered buffer table and not provided

Yeah, you&#x27;re right I misremembered and the objections / patch I
dropped was pinning the registered buffer table, not the pbuf ring

&gt; buffer rings? Which would indeed be a bad idea. Thinking about it,
&gt; fwiw, instead of creating multiple registered buffers and trying to
&gt; lock the entire table, you could&#x27;ve kept all memory in one larger
&gt; registered buffer and pinned only it. It&#x27;s already refcounted, so
&gt; shouldn&#x27;t have been much of a problem.

Hmm, I&#x27;m not sure this idea would work for sparse buffers populated by
the kernel, unless those are automatically pinned too but then from
the user POV for unregistration they&#x27;d need to unregister buffers
individually instead of just calling IORING_UNREGISTER_BUFFERS but it
might be annoying for them to now need to know which buffers are
pinned vs not. When i benchmarked the fuse code with vs without pinned
registered buffers, it didn&#x27;t seem to make much of a difference
performance-wise thankfully, so I just dropped it.

&gt;
&gt; &gt; but dropped it when Jens and Caleb didn&#x27;t
&gt; &gt; like the idea. But for kmbuf rings, pinning will be necessary for
&gt; &gt; fuse.
&gt; &gt;
&gt; &gt;&gt; 3. Require the user to register a memory region of appropriate size,
&gt; &gt;&gt;      see IORING_REGISTER_MEM_REGION, ctx-&gt;param_region. Make fuse
&gt; &gt;&gt;      populating the buffer ring using the memory region.
&gt;
&gt; To explain why, I don&#x27;t think that creating many small regions
&gt; is a good direction going forward. In case of kernel allocation,
&gt; it&#x27;s extra mmap()s, extra user space management, and wasted space.

To clarify, is this in reply to why the individual buffers shouldn&#x27;t
be allocated separately by the kernel?
I added a comment about this above in the discussion about
io_region_allocate_pages_multi_buf(), and if the memory allocation
issue I was seeing is bypassable and the region can be allocated all
at once, I&#x27;m happy to make that change. With having the allocation be
separate buffers though, I&#x27;m not sure I agree that there are extra
mmaps / userspace management. All the pages across the buffers are
vmapped together and the userspace just needs to do 1 mmap call for
them. On the userspace side, I don&#x27;t think there&#x27;s more management
since the mmapped address represents the range across all the buffers.
I&#x27;m not seeing how there&#x27;s wasted space either since the only
requirement is that the buffer size is page aligned. I think also
there&#x27;s a higher chance of the entire buffer region being physically
contiguous if each buffer is allocated separately vs. all the buffers
are allocated as 1 region. I don&#x27;t feel strongly about this either way
and I&#x27;m happy to allocate the entire region at once if that&#x27;s
possible.

&gt; For user provided memory it&#x27;s over-accounting and extra memory
&gt; footprint. It&#x27;ll also give you better lifecycle guarantees, i.e.

Just out of curiosity, could you elaborate on the over-accounting and
extra memory footprint? I was under the impression it would be the
same since the accounting gets adjusted by the total bytes allocated?
For the extra memory footprint, is the extra footprint from the
metadata to describe each buffer region, or are you referring to
something else?

&gt; you won&#x27;t be able to free buffers while there are requests for the
&gt; context. I&#x27;m not so sure about ring bound memory, let&#x27;s say I have
&gt; my suspicions, and you&#x27;d need to be extra careful about buffer
&gt; lifetimes even after a fuse instance dies.
&gt;
&gt; &gt;&gt; I wanted to make regions shareable anyway (need it for other purposes),
&gt; &gt;&gt; I can toss patches for that tomorrow.
&gt; &gt;&gt;
&gt; &gt;&gt; A separate question is whether extending buffer rings is the right
&gt; &gt;&gt; approach as it seems like you&#x27;re only using it for fuse requests and
&gt; &gt;&gt; not for passing buffers to normal requests, but I don&#x27;t see the
&gt; &gt;
&gt; &gt; What are &#x27;normal requests&#x27;? For fuse&#x27;s use case, there are only fuse requests.
&gt;
&gt; Any kind of read/recv/etc. that can use provided buffers. It&#x27;s
&gt; where kernel memory filled rings would shine, as you&#x27;d be able
&gt; to use them together without changing any opcode specific code.
&gt; I.e. not changes in read request implementation, only kbuf.c
&gt;

Thanks for your input on the series. To iterate / sum up, these are
changes for v2 I&#x27;ll be making:
- api-wise from userspace/liburing: get rid of KMBUF_RING api
interface and have users go through PBUF_RING api instead with a flag
indicating the ring is kernel-managed
- have kernel buffer allocation go through IORING_REGISTER_MEM_REGION
instead, which means when the pbuf ring is created and the
kernel-managed flag is set, the ring will be empty. The memory region
will need to be registered before the mmap call to the ring fd.
- add apis for subsystems to populate a kernel-managed buffer ring
with addresses from the registered mem region

Does this align with your understanding of the conversation as well or
is there anything I&#x27;m missing?

And Christoph, do these changes for v2 work for your use case as well?

Thanks,
Joanne
&gt; --
&gt; Pavel Begunkov
&gt;


---

On Thu, Feb 12, 2026 at 2:52 AM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt;
&gt; On 2/12/26 10:07, Christoph Hellwig wrote:
&gt; &gt; On Wed, Feb 11, 2026 at 02:06:18PM -0800, Joanne Koong wrote:
&gt; &gt;&gt;&gt; I don&#x27;t think I follow. I&#x27;m saying that it might be interesting
&gt; &gt;&gt;&gt; to separate rings from how and with what they&#x27;re populated on the
&gt; &gt;&gt;&gt; kernel API level, but the fuse kernel module can do the population
&gt; &gt;&gt;
&gt; &gt;&gt; Oh okay, from your first message I (and I think christoph too) thought
&gt; &gt;&gt; what you were saying is that the user should be responsible for
&gt; &gt;&gt; allocating the buffers with complete ownership over them, and then
&gt; &gt;&gt; just pass those allocated to the kernel to use. But what you&#x27;re saying
&gt; &gt;&gt; is that just use a different way for getting the kernel to allocate
&gt; &gt;&gt; the buffers (eg through the IORING_REGISTER_MEM_REGION interface). Am
&gt; &gt;&gt; I reading this correctly?
&gt; &gt;
&gt; &gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt; &gt; where the kernel controls the allocation fully and guarantees user
&gt; &gt; processes can only read the memory but never write to it.  I&#x27;d love

By &quot;control the allocation fully&quot; do you mean for your use case, the
allocation/setup isn&#x27;t triggered by userspace but is initiated by the
kernel (eg user never explicitly registers any kbuf ring, the kernel
just uses the kbuf ring data structure internally and users can read
the buffer contents)? If userspace initiates the setup of the kbuf
ring, going through IORING_REGISTER_MEM_REGION would be semantically
the same, except the buffer allocation by the kernel now happens
before the ring is created and then later populated into the ring.
userspace would still need to make an mmap call to the region and the
kernel could enforce that as read-only. But if userspace doesn&#x27;t
initiate the setup, then going through IORING_REGISTER_MEM_REGION gets
uglier.

&gt; &gt; to be able to piggy back than onto your work.
&gt;
&gt; IORING_REGISTER_MEM_REGION supports both types of allocations. It can
&gt; have a new registration flag for read-only, and then you either make
&gt; the bounce avoidance optional or reject binding fuse to unsupported
&gt; setups during init. Any arguments against that? I need to go over
&gt; Joanne&#x27;s reply, but I don&#x27;t see any contradiction in principal with
&gt; your use case.

So i guess the flow would have to be:
a) user calls io_uring_register_region(&amp;ring, &amp;mem_region_reg) with
mem_region_reg.region_uptr&#x27;s size field set to the total buffer size
(and mem_region_reg.flags read-only bit set if needed)
     kernel allocates region
b) user calls mmap() to get the address of the region. If read-only
bit was set, it gets a read-only address
c) user calls io_uring_register_buf_ring(&amp;ring, &amp;buf_reg, flags) with
buf_reg.flags |= IOU_PBUF_RING_KERNEL_MANAGED
     kernel creates an empty kernel-managed ring. None of the buffers
are populated
d) user tells X subsystem to populate the ring starting from offset Z
in the registered mem region
e) on the kernel side, the subsystem populates the ring starting from
offset Z, filling it up using the buf_size and ring_entries values
that the user registered the ring with in c)

To be completely honest, the more I look at this the more this feels
like overkill / over-engineered to me. I get that now the user can do
the PMD optimization, but does that actually lead to noticeable
performance benefits? It seems especially confusing with them going
through the same pbuf ring interface but having totally different
expectations.

What about adding a straightforward kmbuf ring that goes through the
pbuf interface (eg the design in this patchset) and then in the future
adding an interface for pbuf rings (both kernel-managed and
non-kernel-managed) to go through IORING_REGISTERED_MEM_REGIONS if
users end up needing/wanting to have their rings populated that way?

Thanks,
Joanne

&gt;
&gt; --
&gt; Pavel Begunkov
&gt;


---

On Fri, Feb 13, 2026 at 7:31 AM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt;
&gt; On 2/13/26 07:27, Christoph Hellwig wrote:
&gt; &gt; On Thu, Feb 12, 2026 at 09:29:31AM -0800, Joanne Koong wrote:
&gt; &gt;&gt;&gt;&gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt; &gt;&gt;&gt;&gt; where the kernel controls the allocation fully and guarantees user
&gt; &gt;&gt;&gt;&gt; processes can only read the memory but never write to it.  I&#x27;d love
&gt; &gt;&gt;
&gt; &gt;&gt; By &quot;control the allocation fully&quot; do you mean for your use case, the
&gt; &gt;&gt; allocation/setup isn&#x27;t triggered by userspace but is initiated by the
&gt; &gt;&gt; kernel (eg user never explicitly registers any kbuf ring, the kernel
&gt; &gt;&gt; just uses the kbuf ring data structure internally and users can read
&gt; &gt;&gt; the buffer contents)? If userspace initiates the setup of the kbuf
&gt; &gt;&gt; ring, going through IORING_REGISTER_MEM_REGION would be semantically
&gt; &gt;&gt; the same, except the buffer allocation by the kernel now happens
&gt; &gt;&gt; before the ring is created and then later populated into the ring.
&gt; &gt;&gt; userspace would still need to make an mmap call to the region and the
&gt; &gt;&gt; kernel could enforce that as read-only. But if userspace doesn&#x27;t
&gt; &gt;&gt; initiate the setup, then going through IORING_REGISTER_MEM_REGION gets
&gt; &gt;&gt; uglier.
&gt; &gt;
&gt; &gt; The idea is that the application tells the kernel that it wants to use
&gt; &gt; a fixed buffer pool for reads.  Right now the application does this
&gt; &gt; using io_uring_register_buffers().  The problem with that is that
&gt; &gt; io_uring_register_buffers ends up just doing a pin of the memory,
&gt; &gt; but the application or, in case of shared memory, someone else could
&gt; &gt; still modify the memory.  If the underlying file system or storage
&gt; &gt; device needs verify checksums, or worse rebuild data from parity
&gt; &gt; (or uncompress), it needs to ensure that the memory it is operating
&gt; &gt; on can&#x27;t be modified by someone else.
&gt; &gt;
&gt; &gt; So I&#x27;ve been thinking of a version of io_uring_register_buffers where
&gt; &gt; the buffers are not provided by the application, but instead by the
&gt; &gt; kernel and mapped into the application address space read-only for
&gt; &gt; a while, and I thought I could implement this on top of your series,
&gt; &gt; but I have to admit I haven&#x27;t really looked into the details all
&gt; &gt; that much.
&gt;
&gt; There is nothing about registered buffers in this series. And even
&gt; if you try to reuse buffer allocation out of it, it&#x27;ll come with
&gt; a circular buffer you&#x27;ll have no need for. And I&#x27;m pretty much

I think the circular buffer will be useful for Christoph&#x27;s use case in
the same way it&#x27;ll be useful for fuse&#x27;s. The read payload could be
differently sized across requests, so it&#x27;s a lot of wasted space to
have to allocate a buffer large enough to support the max-size request
per entry in the io_ring. With using a circular buffer, buffers have a
way to be shared across entries, which means we can significantly
reduce how much memory needs to be allocated.

Thanks,
Joanne

&gt; arguing about separating those for io_uring.
&gt;
&gt; --
&gt; Pavel Begunkov
&gt;


---

On Fri, Feb 13, 2026 at 4:41 AM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt;
&gt; On 2/13/26 07:18, Christoph Hellwig wrote:
&gt; &gt; On Thu, Feb 12, 2026 at 10:44:44AM +0000, Pavel Begunkov wrote:
&gt; &gt;&gt;&gt;
&gt; &gt; Can you clarify what you mean with &#x27;pbuf&#x27;?  The only fixed buffer API I
&gt; &gt; know is io_uring_register_buffers* which always takes user provided
&gt; &gt; buffers, so I have a hard time parsing what you&#x27;re saying there.  But
&gt; &gt; that might just be sign that I&#x27;m no expert in io_uring APIs, and that
&gt; &gt; web searches have degraded to the point of not being very useful
&gt; &gt; anymore.
&gt;
&gt; Registered, aka fixed, buffers are the ones you pass to
&gt; IORING_OP_[READ,WRITE]_FIXED and some other requests. It&#x27;s normally
&gt; created by io_uring_register_buffers*() / IORING_REGISTER_BUFFERS*
&gt; with user memory, but there are special cases when it&#x27;s installed
&gt; internally by other kernel components, e.g. ublk.
&gt; This series has nothing to do with them, and relevant parts of
&gt; the discussion here don&#x27;t mention them either.
&gt;
&gt; Provided buffer rings, a.k.a pbuf rings, IORING_REGISTER_PBUF_RING
&gt; is a kernel-user shared ring. The entries are user buffers
&gt; {uaddr, size}. The user space adds entries, the kernel (io_uring
&gt; requests) consumes them and issues I/O using the user addresses.
&gt; E.g. you can issue a IORING_OP_RECV request (+IOSQE_BUFFER_SELECT)
&gt; and it&#x27;ll grab a buffer from the ring instead of using sqe-&gt;addr.
&gt;
&gt; pbuf rings, IORING_REGISTER_MEM_REGION, completion/submission
&gt; queues and all other kernel-user rings/etc. are internally based
&gt; on so called regions. All of them support both user allocated
&gt; memory and kernel allocations + mmap.
&gt;
&gt; This series essentially creates provided buffer rings, where
&gt; 1. the ring now contains kernel addresses
&gt; 2. the ring itself is in-kernel only and not shared with user space
&gt; 3. it also allocates kernel buffers (as a region), populates the ring
&gt;     with them, and allows mapping the buffers into the user space.

The most important part and the whole reason fuse needs the buffer
ring to be kernel-managed is because the kernel needs to control when
buffers get recycled back into the ring. For fuse&#x27;s use case, the
buffer is used for passing data between the kernel and the server. We
can&#x27;t have the server recycle the buffer because the server writes
back data to the kernel in that buffer when it submits the sqe. After
fuse receives the sqe and reads the reply from the server, it then
needs to recycle that buffer back into the ring so it can be reused
for a future cqe (eg sending a future request).

&gt;
&gt; Fuse is doing both adding (kernel) buffers to the ring and consuming
&gt; them. At which point it&#x27;s not clear:
&gt;
&gt; 1. Why it even needs io_uring provided buffer rings, it can be all
&gt;     contained in fuse. Maybe it&#x27;s trying to reuse pbuf ring code as
&gt;     basically an internal memory allocator, but then why expose buffer
&gt;     rings as an io_uring uapi instead of keeping it internally.
&gt;
&gt;     That&#x27;s also why I mentioned whether those buffers are supposed to
&gt;     be used with other types of io_uring requests like recv, etc.

On the userspace/server side, it uses the buffers for other io-uring
operations (eg reading or writing the contents from/to a
locally-backed file).

&gt;
&gt; 2. Why making io_uring to allocate payload memory. The answer to which
&gt;     is probably to reuse the region api with mmap and so on. And why
&gt;     payload buffers are inseparably created together with the ring

My main motivation for this is simplicity. I see (and thanks for
explaining) that using a registered mem region allows the use of some
optimizations (the only one I know of right now is the PMD one you
mentioned but maybe there&#x27;s more I&#x27;m missing) that could be useful for
some workloads, but I don&#x27;t think (and this could just be my lack of
understanding of what more optimizations there are) most use cases of
kmbufs benefit from those optimizations, so to me it feels like we&#x27;re
adding non-trivial complexity for no noticeable benefit.

I feel like we get the best of both worlds by letting users have both:
the simple kernel-managed pbuf where the kernel allocates the buffers
and the buffers are tied to the lifecycle of the ring, and the more
advanced kernel-managed pbuf where buffers are tied to a registered
memory region that the subsystem is responsible for later populating
the ring with.

&gt;     and via a new io_uring uapi.

imo it felt cleaner to have a new uapi for it because kmbufs and pbufs
have different expectations and behaviors (eg pbufs only work with
user-provided buffers and requires userspace to populate the ring
before using it, whereas for kmbufs the kernel allocates the buffers
and populates it for you; pbufs require userspace to recycle back the
buffer, whereas for kmbufs the kernel is the one in control of
recycling) and from the user pov it seemed confusing to have kmbufs as
part of the pbuf ring uapi, instead of separating it out as a
different type of ringbuffer with a different expectation and
behavior. I was trying to make the point that combining the interface
if we go with IORING_MEM_REGION gets even more confusing because now
pbufs that are kernel-managed are also empty at initialization and
only can point to areas inside a registered mem region and the
responsibility of populating it is now on whatever subsystem is using
it.

I still have this opinion but I also think in general, you likely know
better than I do what kind of io-uring uapi is best for io-uring&#x27;s
users. For v2 I&#x27;ll have kmbufs go through the pbuf uapi.

&gt;
&gt;     And yes, I believe in the current form it&#x27;s inflexible, it requires
&gt;     a new io_uring uapi. It requires the number of buffers to match
&gt;     the number of ring entries, which are related but not the same

I&#x27;m not really seeing what the purpose of having a ring entry with no
buffer associated with it is. In the existing code for non-kernel
managed pbuf rings, there&#x27;s the same tie between reg-&gt;ring_entries
being used as the marker for how many buffers the ring supports. But
if the number of buffers should be different than the number of ring
entries, this can be easily fixed by passing in the number of buffers
from the uapi for kernel-managed pbuf rings.

&gt;     thing. You can&#x27;t easily add more memory as it&#x27;s bound to the ring
&gt;     object. The buffer memory won&#x27;t even have same lifetime as the

To play devil&#x27;s advocate, we also can&#x27;t easily add more memory to the
mem region once it&#x27;s been registered. I think there&#x27;s also a worse
penalty where the user needs to know upfront how much memory to
allocate for the mem region for the lifetime of the ring, which imo
may be hard to do (eg if a kernel-managed buf ring only needs to be
registered for some code paths and not others, the mem region
registration would still have to allocate the memory a potential kbuf
ring would use).

&gt;     ring object -- allow using that km buffer ring with recv requests
&gt;     and highly likely I&#x27;ll most likely give you a way to crash the
&gt;     kernel.

I&#x27;m a bit confused by this part. The buffer memory does have the same
lifetime as the ring object, no? The buffers only get freed when the
ring itself is freed.

&gt;
&gt; But hey, I&#x27;m tired. I don&#x27;t have any beef here and am only trying
&gt; to make it a bit cleaner and flexible for fuse in the first place
&gt; without even questioning the I/O path. If everyone believes

I appreciate you looking at this and giving your feedback and insight.
Thank you for doing so. I don&#x27;t want to merge in something you&#x27;re
unhappy with.

Are you open to having support for both a simple kernel-managed pbuf
interface and later on if/when the need arises, a kernel-managed pbuf
interface that goes through a registered memory region? If the answer
is no, then I&#x27;ll make the change to have kmbufs go through the
registered memory region.

Thanks,
Joanne

&gt; everything is right, just ask Jens to merge it.
&gt;
&gt; --
&gt; Pavel Begunkov
&gt;


---

On Wed, Feb 18, 2026 at 4:36 AM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt;
&gt; On 2/13/26 22:04, Joanne Koong wrote:
&gt; &gt; On Fri, Feb 13, 2026 at 4:41 AM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt; ...
&gt; &gt;&gt; Fuse is doing both adding (kernel) buffers to the ring and consuming
&gt; &gt;&gt; them. At which point it&#x27;s not clear:
&gt; &gt;&gt;
&gt; &gt;&gt; 1. Why it even needs io_uring provided buffer rings, it can be all
&gt; &gt;&gt;      contained in fuse. Maybe it&#x27;s trying to reuse pbuf ring code as
&gt; &gt;&gt;      basically an internal memory allocator, but then why expose buffer
&gt; &gt;&gt;      rings as an io_uring uapi instead of keeping it internally.
&gt; &gt;&gt;
&gt; &gt;&gt;      That&#x27;s also why I mentioned whether those buffers are supposed to
&gt; &gt;&gt;      be used with other types of io_uring requests like recv, etc.
&gt; &gt;
&gt; &gt; On the userspace/server side, it uses the buffers for other io-uring
&gt; &gt; operations (eg reading or writing the contents from/to a
&gt; &gt; locally-backed file).
&gt;

Sorry, I submitted v2 last night thinking the conversation on this
thread had died. After reading through your reply, I&#x27;ll modify v2.

&gt; Oops, typo. I was asking whether the buffer rings (not buffers) are
&gt; supposed to be used with other requests. E.g. submitting a
&gt; IORING_OP_RECV with IOSQE_BUFFER_SELECT set and the bgid specifying
&gt; your kernel-managed buffer ring.

Yes the buffer rings are intended to be used with other io-uring
requests. The ideal scenario is that the user can then do the
equivalent of IORING_OP_READ/WRITE_FIXED operations on the
kernel-managed buffers and avoid the per-i/o page pinning overhead
costs.

&gt;
&gt; &gt;&gt; 2. Why making io_uring to allocate payload memory. The answer to which
&gt; &gt;&gt;      is probably to reuse the region api with mmap and so on. And why
&gt; &gt;&gt;      payload buffers are inseparably created together with the ring
&gt; &gt;
&gt; &gt; My main motivation for this is simplicity. I see (and thanks for
&gt; &gt; explaining) that using a registered mem region allows the use of some
&gt; &gt; optimizations (the only one I know of right now is the PMD one you
&gt; &gt; mentioned but maybe there&#x27;s more I&#x27;m missing) that could be useful for
&gt; &gt; some workloads, but I don&#x27;t think (and this could just be my lack of
&gt; &gt; understanding of what more optimizations there are) most use cases of
&gt; &gt; kmbufs benefit from those optimizations, so to me it feels like we&#x27;re
&gt; &gt; adding non-trivial complexity for no noticeable benefit.
&gt;
&gt; There are two separate arguments. The first is about not making buffers
&gt; inseparable from buffer rings in the io_uring user API. Whether it&#x27;s
&gt; IORING_REGISTER_MEM_REGION or something else is not that important.
&gt; I have no objection if it&#x27;s a part of fuse instead though, e.g. if
&gt; fuse binds two objects together when you register it with fuse, or even
&gt; if fuse create a buffer ring internally (assuming it doesn&#x27;t indirectly
&gt; leak into io_uring uapi).
&gt;
&gt; And the second was about optionally allowing user memory for buffer
&gt; creation as you&#x27;re reusing the region abstraction. You can find pros
&gt; and cons for both modes, and funnily enough, SQ/CQ were first kernel
&gt; allocated and then people asked for backing it by user memory, and IIRC
&gt; it was in the reverse order for pbuf rings.
&gt;
&gt; Implementing this is trivial as well, you just need to pass an argument
&gt; while creating a region. All new region users use struct
&gt; io_uring_region_desc for uapi and forward it to io_create_region()
&gt; without caring if it&#x27;s user or kernel allocated memory.
&gt;
&gt; &gt; I feel like we get the best of both worlds by letting users have both:
&gt; &gt; the simple kernel-managed pbuf where the kernel allocates the buffers
&gt; &gt; and the buffers are tied to the lifecycle of the ring, and the more
&gt; &gt; advanced kernel-managed pbuf where buffers are tied to a registered
&gt; &gt; memory region that the subsystem is responsible for later populating
&gt; &gt; the ring with.
&gt; &gt;
&gt; &gt;&gt;      and via a new io_uring uapi.
&gt; &gt;
&gt; &gt; imo it felt cleaner to have a new uapi for it because kmbufs and pbufs
&gt;
&gt; The stress is on why it&#x27;s an _io_uring_ API. It doesn&#x27;t matter to me
&gt; whether it&#x27;s a separate opcode or not. Currently, buffer rings don&#x27;t give
&gt; you anything that can&#x27;t be pure fuse, and it might be simpler to have
&gt; it implemented in fuse than binding to some io_uring object. Or it could
&gt; create buffer rings internally to reuse code but it doesn&#x27;t become an
&gt; io_uring uapi but rather implementation detail. And that predicates on
&gt; whether km rings are intended to be used with other / non-fuse requests.
&gt;
&gt; &gt; have different expectations and behaviors (eg pbufs only work with
&gt; &gt; user-provided buffers and requires userspace to populate the ring
&gt; &gt; before using it, whereas for kmbufs the kernel allocates the buffers
&gt; &gt; and populates it for you; pbufs require userspace to recycle back the
&gt; &gt; buffer, whereas for kmbufs the kernel is the one in control of
&gt; &gt; recycling) and from the user pov it seemed confusing to have kmbufs as
&gt; &gt; part of the pbuf ring uapi, instead of separating it out as a
&gt; &gt; different type of ringbuffer with a different expectation and
&gt;
&gt; I believe the source of disagreement is that you&#x27;re thinking
&gt; about how it&#x27;s going to look like for fuse specifically, and I
&gt; believe you that it&#x27;ll be nicer for the fuse use case. However,
&gt; on the other hand it&#x27;s an io_uring uapi, and if it is an io_uring
&gt; uapi, we need reusable blocks that are not specific to particular
&gt; users.

I agree 100%. The api we add should be what&#x27;s best for io-uring, not fuse.

For the majority of use cases, it seemed to me that having the buffers
separated from the buffer rings didn&#x27;t yield perceptible benefits but
added complexity and more restrictions like having to statically know
up front how big the mem region needs to be across the lifetime of the
io-uring for anything the io-uring might use the mem region for. It
seems more generically useful as a concept to have the buffers owned
by the ring and tied to the lifetime of the ring. I like how with this
design everything is self-contained and multiple subsystems can use it
without having to reimplement functionality locally in the subsystem.
On the other hand, I see your point about how it might be something
users want in the future if they want complete control over which
parts of the mem region get used as the backing buffers to do stuff
like PMD optimizations.

I think this is a matter of opinion/preference and I think in general
for anything io-uring related, yours should take precedence.

With it going through a mem region, I don&#x27;t think it should even go
through the &quot;pbuf ring&quot; interface then if it&#x27;s not going to specify
the number of entries and buffer sizes upfront, if support is added
for io-uring normal requests (eg IORING_OP_READ/WRITE) to use the
backing pages from a memory region and if we&#x27;re able to guarantee that
the registered memory region will never be able to be unregistered by
the user. I think if we repurpose the

union {
  __u64 addr; /* pointer to buffer or iovecs */
  __u64 splice_off_in;
};

fields in the struct io_uring_sqe to

union {
  __u64 addr; /* pointer to buffer or iovecs */
  __u64 splice_off_in;
  __u64 offset; /* offset into registered mem region */
};

and add some IOSQE_ flag to indicate it should find the pages from the
registered mem region, then that should work for normal requests.
Where on the kernel side, it looks up the associated pages stored in
the io_mapped_region&#x27;s pages array for the offset passed in.

Right now there&#x27;s only a uapi to register a memory region and none to
unregister one. Is it guaranteed that io-uring will never add
something in the future that will let userspace unregister the memory
region or at least unregister it while it&#x27;s being used (eg if we add
future refcounting to it to track active uses of it)?

If so, then end-to-end, with it going through the mem region, it would
be something like:
* user creates a mem region for the io-uring
* user mmaps the mem region
* user passes in offset into region, length of each buffer, and number
of entries in the ring to the subsystem
* subsystem creates a locally managed bufring and adds buffers to that
ring from the mem region
* on the cqe side, it sends the buffer id of the registered mem region
through the same &quot;IORING_CQE_F_BUFFER |  (buf_id &lt;&lt;
IORING_CQE_BUFFER_SHIFT)&quot; mechanism

Does this design match what you had in mind / prefer?

I think the above works for Christoph&#x27;s use case too (as his and my
use case are the same) but if not, please let me know.

Thanks,
Joanne
</pre>
</details>
<div class="review-comment-signals">Signals: requested clarification, pointed out technical complexities</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Christoph Hellwig</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-18">2026-02-18</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Christoph Hellwig raised concerns about the patch&#x27;s implementation of kernel-managed buffer rings, specifically asking for clarification on TLB pressure handling and questioning the use of &#x27;pbuf&#x27; terminology.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Tue, Feb 10, 2026 at 04:34:47PM +0000, Pavel Begunkov wrote:
&gt; &gt; +	union {
&gt; &gt; +		/* used for pbuf rings */
&gt; &gt; +		__u64	ring_addr;
&gt; &gt; +		/* used for kmbuf rings */
&gt; &gt; +		__u32   buf_size;
&gt; 
&gt; If you&#x27;re creating a region, there should be no reason why it
&gt; can&#x27;t work with user passed memory. You&#x27;re fencing yourself off
&gt; optimisations that are already there like huge pages.

Any pages mapped to userspace can be allocated in the kernel as well.

And I really do like this design, because it means we can have a
buffer ring that is only mapped read-only into userspace.  That way
we can still do zero-copy raids if the device requires stable pages
for checksumming or raid.  I was going to implement this as soon
as this series lands upstream.



---

On Thu, Feb 12, 2026 at 10:44:44AM +0000, Pavel Begunkov wrote:
&gt; &gt; 
&gt; &gt; Any pages mapped to userspace can be allocated in the kernel as well.
&gt; 
&gt; pow2 round ups will waste memory. 1MB allocations will never
&gt; become 2MB huge pages. And there is a separate question of
&gt; 1GB huge pages. The user can be smarter about all placement
&gt; decisions.

Sure.  But if the application cares that much about TLB pressure
I&#x27;d just round up to nice multtiple of PTE levels.

&gt; 
&gt; &gt; And I really do like this design, because it means we can have a
&gt; &gt; buffer ring that is only mapped read-only into userspace.  That way
&gt; &gt; we can still do zero-copy raids if the device requires stable pages
&gt; &gt; for checksumming or raid.  I was going to implement this as soon
&gt; &gt; as this series lands upstream.
&gt; 
&gt; That&#x27;s an interesting case. To be clear, user provided memory is
&gt; an optional feature for pbuf rings / regions / etc., and I think
&gt; the io_uring uapi should leave fields for the feature. However, I
&gt; have nothing against fuse refusing to bind to buffer rings it
&gt; doesn&#x27;t like.

Can you clarify what you mean with &#x27;pbuf&#x27;?  The only fixed buffer API I
know is io_uring_register_buffers* which always takes user provided
buffers, so I have a hard time parsing what you&#x27;re saying there.  But
that might just be sign that I&#x27;m no expert in io_uring APIs, and that
web searches have degraded to the point of not being very useful
anymore.



---

On Thu, Feb 12, 2026 at 10:52:29AM +0000, Pavel Begunkov wrote:
&gt; &gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt; &gt; where the kernel controls the allocation fully and guarantees user
&gt; &gt; processes can only read the memory but never write to it.  I&#x27;d love
&gt; &gt; to be able to piggy back than onto your work.
&gt; 
&gt; IORING_REGISTER_MEM_REGION supports both types of allocations. It can
&gt; have a new registration flag for read-only, and then you either make

IORING_REGISTER_MEM_REGION seems to be all about cqs from both your
commit message and the public documentation.  I&#x27;m confused.

&gt; the bounce avoidance optional or reject binding fuse to unsupported
&gt; setups during init. Any arguments against that? I need to go over
&gt; Joanne&#x27;s reply, but I don&#x27;t see any contradiction in principal with
&gt; your use case.

My use case is not about fuse, but good old block and file system
I/O.

</pre>
</details>
<div class="review-comment-signals">Signals: requested changes, clarification needed</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-20_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-20">2026-02-20</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Joanne Koong raised concerns about the advantages of kernel-managed buffer rings, specifically questioning the benefits of user-allocated buffers and suggesting that kernel-allocated buffers provide simpler interface and lifecycle management.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Tue, Feb 10, 2026 at 8:34 AM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt;
&gt; On 2/10/26 00:28, Joanne Koong wrote:
&gt; &gt; Add support for kernel-managed buffer rings (kmbuf rings), which allow
&gt; &gt; the kernel to allocate and manage the backing buffers for a buffer
&gt; &gt; ring, rather than requiring the application to provide and manage them.
&gt; &gt;
&gt; &gt; This introduces two new registration opcodes:
&gt; &gt; - IORING_REGISTER_KMBUF_RING: Register a kernel-managed buffer ring
&gt; &gt; - IORING_UNREGISTER_KMBUF_RING: Unregister a kernel-managed buffer ring
&gt; &gt;
&gt; &gt; The existing io_uring_buf_reg structure is extended with a union to
&gt; &gt; support both application-provided buffer rings (pbuf) and kernel-managed
&gt; &gt; buffer rings (kmbuf):
&gt; &gt; - For pbuf rings: ring_addr specifies the user-provided ring address
&gt; &gt; - For kmbuf rings: buf_size specifies the size of each buffer. buf_size
&gt; &gt;    must be non-zero and page-aligned.
&gt; &gt;
&gt; &gt; The implementation follows the same pattern as pbuf ring registration,
&gt; &gt; reusing the validation and buffer list allocation helpers introduced in
&gt; &gt; earlier refactoring. The IOBL_KERNEL_MANAGED flag marks buffer lists as
&gt; &gt; kernel-managed for appropriate handling in the I/O path.
&gt; &gt;
&gt; &gt; Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
&gt; &gt; ---
&gt; &gt;   include/uapi/linux/io_uring.h |  15 ++++-
&gt; &gt;   io_uring/kbuf.c               |  81 ++++++++++++++++++++++++-
&gt; &gt;   io_uring/kbuf.h               |   7 ++-
&gt; &gt;   io_uring/memmap.c             | 111 ++++++++++++++++++++++++++++++++++
&gt; &gt;   io_uring/memmap.h             |   4 ++
&gt; &gt;   io_uring/register.c           |   7 +++
&gt; &gt;   6 files changed, 219 insertions(+), 6 deletions(-)
&gt; &gt;
&gt; &gt; diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
&gt; &gt; index fc473af6feb4..a0889c1744bd 100644
&gt; &gt; --- a/include/uapi/linux/io_uring.h
&gt; &gt; +++ b/include/uapi/linux/io_uring.h
&gt; &gt; @@ -715,6 +715,10 @@ enum io_uring_register_op {
&gt; &gt;       /* register bpf filtering programs */
&gt; &gt;       IORING_REGISTER_BPF_FILTER              = 37,
&gt; &gt;
&gt; &gt; +     /* register/unregister kernel-managed ring buffer group */
&gt; &gt; +     IORING_REGISTER_KMBUF_RING              = 38,
&gt; &gt; +     IORING_UNREGISTER_KMBUF_RING            = 39,
&gt; &gt; +
&gt; &gt;       /* this goes last */
&gt; &gt;       IORING_REGISTER_LAST,
&gt; &gt;
&gt; &gt; @@ -891,9 +895,16 @@ enum io_uring_register_pbuf_ring_flags {
&gt; &gt;       IOU_PBUF_RING_INC       = 2,
&gt; &gt;   };
&gt; &gt;
&gt; &gt; -/* argument for IORING_(UN)REGISTER_PBUF_RING */
&gt; &gt; +/* argument for IORING_(UN)REGISTER_PBUF_RING and
&gt; &gt; + * IORING_(UN)REGISTER_KMBUF_RING
&gt; &gt; + */
&gt; &gt;   struct io_uring_buf_reg {
&gt; &gt; -     __u64   ring_addr;
&gt; &gt; +     union {
&gt; &gt; +             /* used for pbuf rings */
&gt; &gt; +             __u64   ring_addr;
&gt; &gt; +             /* used for kmbuf rings */
&gt; &gt; +             __u32   buf_size;
&gt;
&gt; If you&#x27;re creating a region, there should be no reason why it
&gt; can&#x27;t work with user passed memory. You&#x27;re fencing yourself off
&gt; optimisations that are already there like huge pages.

Are there any optimizations with user-allocated buffers that wouldn&#x27;t
be possible with kernel-allocated buffers? For huge pages, can&#x27;t the
kernel do this as well (eg I see in io_mem_alloc_compound(), it calls
into alloc_pages() with order &gt; 0)?

&gt;
&gt; &gt; +     };
&gt; &gt;       __u32   ring_entries;
&gt; &gt;       __u16   bgid;
&gt; &gt;       __u16   flags;
&gt; &gt; diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
&gt; &gt; index aa9b70b72db4..9bc36451d083 100644
&gt; &gt; --- a/io_uring/kbuf.c
&gt; &gt; +++ b/io_uring/kbuf.c
&gt; ...
&gt; &gt; +static int io_setup_kmbuf_ring(struct io_ring_ctx *ctx,
&gt; &gt; +                            struct io_buffer_list *bl,
&gt; &gt; +                            struct io_uring_buf_reg *reg)
&gt; &gt; +{
&gt; &gt; +     struct io_uring_buf_ring *ring;
&gt; &gt; +     unsigned long ring_size;
&gt; &gt; +     void *buf_region;
&gt; &gt; +     unsigned int i;
&gt; &gt; +     int ret;
&gt; &gt; +
&gt; &gt; +     /* allocate pages for the ring structure */
&gt; &gt; +     ring_size = flex_array_size(ring, bufs, bl-&gt;nr_entries);
&gt; &gt; +     ring = kzalloc(ring_size, GFP_KERNEL_ACCOUNT);
&gt; &gt; +     if (!ring)
&gt; &gt; +             return -ENOMEM;
&gt; &gt; +
&gt; &gt; +     ret = io_create_region_multi_buf(ctx, &amp;bl-&gt;region, bl-&gt;nr_entries,
&gt; &gt; +                                      reg-&gt;buf_size);
&gt;
&gt; Please use io_create_region(), the new function does nothing new
&gt; and only violates abstractions.

There&#x27;s separate checks needed between io_create_region() and
io_create_region_multi_buf() (eg IORING_MEM_REGION_TYPE_USER flag
checking) and different allocation calls (eg
io_region_allocate_pages() vs io_region_allocate_pages_multi_buf()).
Maybe I&#x27;m misinterpreting your comment (or the code), but I&#x27;m not
seeing how this can just use io_create_region().

&gt;
&gt; Provided buffer rings with kernel addresses could be an interesting
&gt; abstraction, but why is it also responsible for allocating buffers?

Conceptually, I think it makes the interface and lifecycle management
simpler/cleaner. With registering it from userspace, imo there&#x27;s
additional complications with no tangible benefits, eg it&#x27;s not
guaranteed that the memory regions registered for the buffers are the
same size, with allocating it from the kernel-side we can guarantee
that the pages are allocated physically contiguously, userspace setup
with user-allocated buffers is less straightforward, etc. In general,
I&#x27;m just not really seeing what advantages there are in allocating the
buffers from userspace. Could you elaborate on that part more?

&gt; What I&#x27;d do:
&gt;
&gt; 1. Strip buffer allocation from IORING_REGISTER_KMBUF_RING.
&gt; 2. Replace *_REGISTER_KMBUF_RING with *_REGISTER_PBUF_RING + a new flag.
&gt;     Or maybe don&#x27;t expose it to the user at all and create it from
&gt;     fuse via internal API.

If kmbuf rings are squashed into pbuf rings, then pbuf rings will need
to support pinning. In fuse, there are some contexts where you can&#x27;t
grab the uring mutex because you&#x27;re running in atomic context and this
can be encountered while recycling the buffer. I originally had a
patch adding pinning to pbuf rings (to mitigate the overhead of
registered buffers lookups) but dropped it when Jens and Caleb didn&#x27;t
like the idea. But for kmbuf rings, pinning will be necessary for
fuse.

&gt; 3. Require the user to register a memory region of appropriate size,
&gt;     see IORING_REGISTER_MEM_REGION, ctx-&gt;param_region. Make fuse
&gt;     populating the buffer ring using the memory region.
&gt;
&gt; I wanted to make regions shareable anyway (need it for other purposes),
&gt; I can toss patches for that tomorrow.
&gt;
&gt; A separate question is whether extending buffer rings is the right
&gt; approach as it seems like you&#x27;re only using it for fuse requests and
&gt; not for passing buffers to normal requests, but I don&#x27;t see the

What are &#x27;normal requests&#x27;? For fuse&#x27;s use case, there are only fuse requests.

Thanks,
Joanne

&gt; big picture here.
&gt;
&gt; &gt; +     if (ret) {
&gt; &gt; +             kfree(ring);
&gt; &gt; +             return ret;
&gt; &gt; +     }
&gt; &gt; +
&gt; &gt; +     /* initialize ring buf entries to point to the buffers */
&gt; &gt; +     buf_region = bl-&gt;region.ptr;
&gt;
&gt; io_region_get_ptr()
&gt;
&gt; &gt; +     for (i = 0; i &lt; bl-&gt;nr_entries; i++) {
&gt; &gt; +             struct io_uring_buf *buf = &amp;ring-&gt;bufs[i];
&gt; &gt; +
&gt; &gt; +             buf-&gt;addr = (u64)(uintptr_t)buf_region;
&gt; &gt; +             buf-&gt;len = reg-&gt;buf_size;
&gt; &gt; +             buf-&gt;bid = i;
&gt; &gt; +
&gt; &gt; +             buf_region += reg-&gt;buf_size;
&gt; &gt; +     }
&gt; &gt; +     ring-&gt;tail = bl-&gt;nr_entries;
&gt; &gt; +
&gt; &gt; +     bl-&gt;buf_ring = ring;
&gt; &gt; +     bl-&gt;flags |= IOBL_KERNEL_MANAGED;
&gt; &gt; +
&gt; &gt; +     return 0;
&gt; &gt; +}
&gt; &gt; +
&gt; &gt; +int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
&gt; &gt; +{
&gt; &gt; +     struct io_uring_buf_reg reg;
&gt; &gt; +     struct io_buffer_list *bl;
&gt; &gt; +     int ret;
&gt; &gt; +
&gt; &gt; +     lockdep_assert_held(&amp;ctx-&gt;uring_lock);
&gt; &gt; +
&gt; &gt; +     ret = io_copy_and_validate_buf_reg(arg, &amp;reg, 0);
&gt; &gt; +     if (ret)
&gt; &gt; +             return ret;
&gt; &gt; +
&gt; &gt; +     if (!reg.buf_size || !PAGE_ALIGNED(reg.buf_size))
&gt;
&gt; With io_create_region_multi_buf() gone, you shouldn&#x27;t need
&gt; to align every buffer, that could be a lot of wasted memory
&gt; (thinking about 64KB pages).
&gt;
&gt; &gt; +             return -EINVAL;
&gt; &gt; +
&gt; &gt; +     bl = io_alloc_new_buffer_list(ctx, &amp;reg);
&gt; &gt; +     if (IS_ERR(bl))
&gt; &gt; +             return PTR_ERR(bl);
&gt; &gt; +
&gt; &gt; +     ret = io_setup_kmbuf_ring(ctx, bl, &amp;reg);
&gt; &gt; +     if (ret) {
&gt; &gt; +             kfree(bl);
&gt; &gt; +             return ret;
&gt; &gt; +     }
&gt; &gt; +
&gt; &gt; +     ret = io_buffer_add_list(ctx, bl, reg.bgid);
&gt; &gt; +     if (ret)
&gt; &gt; +             io_put_bl(ctx, bl);
&gt; &gt; +
&gt; &gt; +     return ret;
&gt;
&gt; --
&gt; Pavel Begunkov
&gt;


---

On Wed, Feb 11, 2026 at 4:01 AM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt;
&gt; On 2/10/26 19:39, Joanne Koong wrote:
&gt; &gt; On Tue, Feb 10, 2026 at 8:34 AM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt; ...
&gt; &gt;&gt;&gt; -/* argument for IORING_(UN)REGISTER_PBUF_RING */
&gt; &gt;&gt;&gt; +/* argument for IORING_(UN)REGISTER_PBUF_RING and
&gt; &gt;&gt;&gt; + * IORING_(UN)REGISTER_KMBUF_RING
&gt; &gt;&gt;&gt; + */
&gt; &gt;&gt;&gt;    struct io_uring_buf_reg {
&gt; &gt;&gt;&gt; -     __u64   ring_addr;
&gt; &gt;&gt;&gt; +     union {
&gt; &gt;&gt;&gt; +             /* used for pbuf rings */
&gt; &gt;&gt;&gt; +             __u64   ring_addr;
&gt; &gt;&gt;&gt; +             /* used for kmbuf rings */
&gt; &gt;&gt;&gt; +             __u32   buf_size;
&gt; &gt;&gt;
&gt; &gt;&gt; If you&#x27;re creating a region, there should be no reason why it
&gt; &gt;&gt; can&#x27;t work with user passed memory. You&#x27;re fencing yourself off
&gt; &gt;&gt; optimisations that are already there like huge pages.
&gt; &gt;
&gt; &gt; Are there any optimizations with user-allocated buffers that wouldn&#x27;t
&gt; &gt; be possible with kernel-allocated buffers? For huge pages, can&#x27;t the
&gt; &gt; kernel do this as well (eg I see in io_mem_alloc_compound(), it calls
&gt; &gt; into alloc_pages() with order &gt; 0)?
&gt;
&gt; Yes, there is handful of differences. To name one, 1MB allocation won&#x27;t
&gt; get you a PMD mappable huge page, while user space can allocate 2MB,
&gt; register the first 1MB and reuse the rest for other purposes.
&gt;
&gt; &gt;&gt;&gt; +     };
&gt; &gt;&gt;&gt;        __u32   ring_entries;
&gt; &gt;&gt;&gt;        __u16   bgid;
&gt; &gt;&gt;&gt;        __u16   flags;
&gt; &gt;&gt;&gt; diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
&gt; &gt;&gt;&gt; index aa9b70b72db4..9bc36451d083 100644
&gt; &gt;&gt;&gt; --- a/io_uring/kbuf.c
&gt; &gt;&gt;&gt; +++ b/io_uring/kbuf.c
&gt; &gt;&gt; ...
&gt; &gt;&gt;&gt; +static int io_setup_kmbuf_ring(struct io_ring_ctx *ctx,
&gt; &gt;&gt;&gt; +                            struct io_buffer_list *bl,
&gt; &gt;&gt;&gt; +                            struct io_uring_buf_reg *reg)
&gt; &gt;&gt;&gt; +{
&gt; &gt;&gt;&gt; +     struct io_uring_buf_ring *ring;
&gt; &gt;&gt;&gt; +     unsigned long ring_size;
&gt; &gt;&gt;&gt; +     void *buf_region;
&gt; &gt;&gt;&gt; +     unsigned int i;
&gt; &gt;&gt;&gt; +     int ret;
&gt; &gt;&gt;&gt; +
&gt; &gt;&gt;&gt; +     /* allocate pages for the ring structure */
&gt; &gt;&gt;&gt; +     ring_size = flex_array_size(ring, bufs, bl-&gt;nr_entries);
&gt; &gt;&gt;&gt; +     ring = kzalloc(ring_size, GFP_KERNEL_ACCOUNT);
&gt; &gt;&gt;&gt; +     if (!ring)
&gt; &gt;&gt;&gt; +             return -ENOMEM;
&gt; &gt;&gt;&gt; +
&gt; &gt;&gt;&gt; +     ret = io_create_region_multi_buf(ctx, &amp;bl-&gt;region, bl-&gt;nr_entries,
&gt; &gt;&gt;&gt; +                                      reg-&gt;buf_size);
&gt; &gt;&gt;
&gt; &gt;&gt; Please use io_create_region(), the new function does nothing new
&gt; &gt;&gt; and only violates abstractions.
&gt; &gt;
&gt; &gt; There&#x27;s separate checks needed between io_create_region() and
&gt; &gt; io_create_region_multi_buf() (eg IORING_MEM_REGION_TYPE_USER flag
&gt;
&gt; If io_create_region() is too strict, let&#x27;s discuss that in
&gt; examples if there are any, but it&#x27;s likely not a good idea changing
&gt; that. If it&#x27;s too lax, filter arguments in the caller. IOW, don&#x27;t
&gt; pass IORING_MEM_REGION_TYPE_USER if it&#x27;s not used.
&gt;
&gt; &gt; checking) and different allocation calls (eg
&gt; &gt; io_region_allocate_pages() vs io_region_allocate_pages_multi_buf()).
&gt;
&gt; I saw that and saying that all memmap.c changes can get dropped.
&gt; You&#x27;re using it as one big virtually contig kernel memory range then
&gt; chunked into buffers, and that&#x27;s pretty much what you&#x27;re getting with
&gt; normal io_create_region(). I get that you only need it to be
&gt; contiguous within a single buffer, but that&#x27;s not what you&#x27;re doing,
&gt; and it&#x27;ll be only worse than default io_create_region() e.g.
&gt; effectively disabling any usefulness of io_mem_alloc_compound(),
&gt; and ultimately you don&#x27;t need to care.

When I originally implemented it, I had it use
io_region_allocate_pages() but this fails because it&#x27;s allocating way
too much memory at once. For fuse&#x27;s use case, each buffer is usually
at least 1 MB if not more. Allocating the memory one buffer a time in
io_region_allocate_pages_multi_buf() bypasses the allocation errors I
was seeing. That&#x27;s the main reason I don&#x27;t think this can just use
io_create_region().

&gt;
&gt; Regions shouldn&#x27;t know anything about your buffers, how it&#x27;s
&gt; subdivided after, etc.
&gt;
&gt; &gt; Maybe I&#x27;m misinterpreting your comment (or the code), but I&#x27;m not
&gt; &gt; seeing how this can just use io_create_region().
&gt;
&gt; struct io_uring_region_desc rd = {};
&gt; total_size = nr_bufs * buf_size;
&gt; rd.size = PAGE_ALIGN(total_size);
&gt; io_create_region(&amp;region, &amp;rd);
&gt;
&gt; Add something like this for user provided memory:
&gt;
&gt; if (use_user_memory) {
&gt;         rd.user_addr = uaddr;
&gt;         rd.flags |= IORING_MEM_REGION_TYPE_USER;
&gt; }
&gt;
&gt;
&gt; &gt;&gt; Provided buffer rings with kernel addresses could be an interesting
&gt; &gt;&gt; abstraction, but why is it also responsible for allocating buffers?
&gt; &gt;
&gt; &gt; Conceptually, I think it makes the interface and lifecycle management
&gt; &gt; simpler/cleaner. With registering it from userspace, imo there&#x27;s
&gt; &gt; additional complications with no tangible benefits, eg it&#x27;s not
&gt; &gt; guaranteed that the memory regions registered for the buffers are the
&gt; &gt; same size, with allocating it from the kernel-side we can guarantee
&gt; &gt; that the pages are allocated physically contiguously, userspace setup
&gt; &gt; with user-allocated buffers is less straightforward, etc. In general,
&gt; &gt; I&#x27;m just not really seeing what advantages there are in allocating the
&gt; &gt; buffers from userspace. Could you elaborate on that part more?
&gt;
&gt; I don&#x27;t think I follow. I&#x27;m saying that it might be interesting
&gt; to separate rings from how and with what they&#x27;re populated on the
&gt; kernel API level, but the fuse kernel module can do the population

Oh okay, from your first message I (and I think christoph too) thought
what you were saying is that the user should be responsible for
allocating the buffers with complete ownership over them, and then
just pass those allocated to the kernel to use. But what you&#x27;re saying
is that just use a different way for getting the kernel to allocate
the buffers (eg through the IORING_REGISTER_MEM_REGION interface). Am
I reading this correctly?

&gt; and get exactly same layout as you currently have:
&gt;
&gt; int fuse_create_ring(size_t region_offset /* user space argument */) {
&gt;         struct io_mapped_region *mr = get_mem_region(ctx);
&gt;         // that can take full control of the ring
&gt;         ring = grab_empty_ring(io_uring_ctx);
&gt;
&gt;         size = nr_bufs * buf_size;
&gt;         if (region_offset + size &gt; get_size(mr)) // + other validation
&gt;                 return error;
&gt;
&gt;         buf = mr_get_ptr(mr) + offset;
&gt;         for (i = 0; i &lt; nr_bufs; i++) {
&gt;                 ring_push_buffer(ring, buf, buf_size);
&gt;                 buf += buf_size;
&gt;         }
&gt; }
&gt;
&gt; fuse might not care, but with empty rings other users will get a
&gt; channel they can use to do IO (e.g. read requests) using their
&gt; kernel addresses in the future.
&gt;
&gt; &gt;&gt; What I&#x27;d do:
&gt; &gt;&gt;
&gt; &gt;&gt; 1. Strip buffer allocation from IORING_REGISTER_KMBUF_RING.
&gt; &gt;&gt; 2. Replace *_REGISTER_KMBUF_RING with *_REGISTER_PBUF_RING + a new flag.
&gt; &gt;&gt;      Or maybe don&#x27;t expose it to the user at all and create it from
&gt; &gt;&gt;      fuse via internal API.
&gt; &gt;
&gt; &gt; If kmbuf rings are squashed into pbuf rings, then pbuf rings will need
&gt; &gt; to support pinning. In fuse, there are some contexts where you can&#x27;t
&gt;
&gt; It&#x27;d change uapi but not internals, you already piggy back it
&gt; on pbuf implementation and differentiate with a flag.
&gt;
&gt; It could basically be:
&gt;
&gt; if (flags &amp; IOU_PBUF_RING_KM)
&gt;         bl-&gt;flags |= IOBL_KERNEL_MANAGED;
&gt;
&gt; Pinning can be gated on that flag as well. Pretty likely uapi
&gt; and internals will be a bit cleaner, but that&#x27;s not a huge deal,
&gt; just don&#x27;t see why would you roll out a separate set of uapi
&gt; ([un]register, offsets, etc.) when essentially it can be treated
&gt; as the same thing.

imo, it looked cleaner as a separate api because it has different
expectations and behaviors and squashing kmbuf into the pbuf api makes
the pbuf api needlessly more complex. Though I guess from the
userspace pov, liburing could have a wrapper that takes care of
setting up the pbuf details for kernel-managed pbufs. But in my head,
having pbufs vs. kmbufs makes it clearer what each one does vs regular
pbufs vs. pbufs that are kernel-managed.

Especially with now having kmbufs go through the ioring mem region
interface, it makes things more confusing imo if they&#x27;re combined, eg
pbufs that are kernel-managed are created empty and then populated
from the kernel side by whatever subsystem is using them. Right now
there&#x27;s only one mem region supported per ring, but in the future if
there&#x27;s the possibility that multiple mem regions can be registered
(eg if userspace doesn&#x27;t know upfront what mem region length they&#x27;ll
need), then we should also probably add in a region id param for the
registration arg, which if kmbuf rings go through the pbuf ring
registration api, is not possible to do.

But I&#x27;m happy to combine the interfaces and go with your suggestion.
I&#x27;ll make this change for v2 unless someone else objects.

&gt;
&gt; &gt; grab the uring mutex because you&#x27;re running in atomic context and this
&gt; &gt; can be encountered while recycling the buffer. I originally had a
&gt; &gt; patch adding pinning to pbuf rings (to mitigate the overhead of
&gt; &gt; registered buffers lookups)
&gt;
&gt; IIRC, you was pinning the registered buffer table and not provided

Yeah, you&#x27;re right I misremembered and the objections / patch I
dropped was pinning the registered buffer table, not the pbuf ring

&gt; buffer rings? Which would indeed be a bad idea. Thinking about it,
&gt; fwiw, instead of creating multiple registered buffers and trying to
&gt; lock the entire table, you could&#x27;ve kept all memory in one larger
&gt; registered buffer and pinned only it. It&#x27;s already refcounted, so
&gt; shouldn&#x27;t have been much of a problem.

Hmm, I&#x27;m not sure this idea would work for sparse buffers populated by
the kernel, unless those are automatically pinned too but then from
the user POV for unregistration they&#x27;d need to unregister buffers
individually instead of just calling IORING_UNREGISTER_BUFFERS but it
might be annoying for them to now need to know which buffers are
pinned vs not. When i benchmarked the fuse code with vs without pinned
registered buffers, it didn&#x27;t seem to make much of a difference
performance-wise thankfully, so I just dropped it.

&gt;
&gt; &gt; but dropped it when Jens and Caleb didn&#x27;t
&gt; &gt; like the idea. But for kmbuf rings, pinning will be necessary for
&gt; &gt; fuse.
&gt; &gt;
&gt; &gt;&gt; 3. Require the user to register a memory region of appropriate size,
&gt; &gt;&gt;      see IORING_REGISTER_MEM_REGION, ctx-&gt;param_region. Make fuse
&gt; &gt;&gt;      populating the buffer ring using the memory region.
&gt;
&gt; To explain why, I don&#x27;t think that creating many small regions
&gt; is a good direction going forward. In case of kernel allocation,
&gt; it&#x27;s extra mmap()s, extra user space management, and wasted space.

To clarify, is this in reply to why the individual buffers shouldn&#x27;t
be allocated separately by the kernel?
I added a comment about this above in the discussion about
io_region_allocate_pages_multi_buf(), and if the memory allocation
issue I was seeing is bypassable and the region can be allocated all
at once, I&#x27;m happy to make that change. With having the allocation be
separate buffers though, I&#x27;m not sure I agree that there are extra
mmaps / userspace management. All the pages across the buffers are
vmapped together and the userspace just needs to do 1 mmap call for
them. On the userspace side, I don&#x27;t think there&#x27;s more management
since the mmapped address represents the range across all the buffers.
I&#x27;m not seeing how there&#x27;s wasted space either since the only
requirement is that the buffer size is page aligned. I think also
there&#x27;s a higher chance of the entire buffer region being physically
contiguous if each buffer is allocated separately vs. all the buffers
are allocated as 1 region. I don&#x27;t feel strongly about this either way
and I&#x27;m happy to allocate the entire region at once if that&#x27;s
possible.

&gt; For user provided memory it&#x27;s over-accounting and extra memory
&gt; footprint. It&#x27;ll also give you better lifecycle guarantees, i.e.

Just out of curiosity, could you elaborate on the over-accounting and
extra memory footprint? I was under the impression it would be the
same since the accounting gets adjusted by the total bytes allocated?
For the extra memory footprint, is the extra footprint from the
metadata to describe each buffer region, or are you referring to
something else?

&gt; you won&#x27;t be able to free buffers while there are requests for the
&gt; context. I&#x27;m not so sure about ring bound memory, let&#x27;s say I have
&gt; my suspicions, and you&#x27;d need to be extra careful about buffer
&gt; lifetimes even after a fuse instance dies.
&gt;
&gt; &gt;&gt; I wanted to make regions shareable anyway (need it for other purposes),
&gt; &gt;&gt; I can toss patches for that tomorrow.
&gt; &gt;&gt;
&gt; &gt;&gt; A separate question is whether extending buffer rings is the right
&gt; &gt;&gt; approach as it seems like you&#x27;re only using it for fuse requests and
&gt; &gt;&gt; not for passing buffers to normal requests, but I don&#x27;t see the
&gt; &gt;
&gt; &gt; What are &#x27;normal requests&#x27;? For fuse&#x27;s use case, there are only fuse requests.
&gt;
&gt; Any kind of read/recv/etc. that can use provided buffers. It&#x27;s
&gt; where kernel memory filled rings would shine, as you&#x27;d be able
&gt; to use them together without changing any opcode specific code.
&gt; I.e. not changes in read request implementation, only kbuf.c
&gt;

Thanks for your input on the series. To iterate / sum up, these are
changes for v2 I&#x27;ll be making:
- api-wise from userspace/liburing: get rid of KMBUF_RING api
interface and have users go through PBUF_RING api instead with a flag
indicating the ring is kernel-managed
- have kernel buffer allocation go through IORING_REGISTER_MEM_REGION
instead, which means when the pbuf ring is created and the
kernel-managed flag is set, the ring will be empty. The memory region
will need to be registered before the mmap call to the ring fd.
- add apis for subsystems to populate a kernel-managed buffer ring
with addresses from the registered mem region

Does this align with your understanding of the conversation as well or
is there anything I&#x27;m missing?

And Christoph, do these changes for v2 work for your use case as well?

Thanks,
Joanne
&gt; --
&gt; Pavel Begunkov
&gt;


---

On Thu, Feb 12, 2026 at 2:52 AM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt;
&gt; On 2/12/26 10:07, Christoph Hellwig wrote:
&gt; &gt; On Wed, Feb 11, 2026 at 02:06:18PM -0800, Joanne Koong wrote:
&gt; &gt;&gt;&gt; I don&#x27;t think I follow. I&#x27;m saying that it might be interesting
&gt; &gt;&gt;&gt; to separate rings from how and with what they&#x27;re populated on the
&gt; &gt;&gt;&gt; kernel API level, but the fuse kernel module can do the population
&gt; &gt;&gt;
&gt; &gt;&gt; Oh okay, from your first message I (and I think christoph too) thought
&gt; &gt;&gt; what you were saying is that the user should be responsible for
&gt; &gt;&gt; allocating the buffers with complete ownership over them, and then
&gt; &gt;&gt; just pass those allocated to the kernel to use. But what you&#x27;re saying
&gt; &gt;&gt; is that just use a different way for getting the kernel to allocate
&gt; &gt;&gt; the buffers (eg through the IORING_REGISTER_MEM_REGION interface). Am
&gt; &gt;&gt; I reading this correctly?
&gt; &gt;
&gt; &gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt; &gt; where the kernel controls the allocation fully and guarantees user
&gt; &gt; processes can only read the memory but never write to it.  I&#x27;d love

By &quot;control the allocation fully&quot; do you mean for your use case, the
allocation/setup isn&#x27;t triggered by userspace but is initiated by the
kernel (eg user never explicitly registers any kbuf ring, the kernel
just uses the kbuf ring data structure internally and users can read
the buffer contents)? If userspace initiates the setup of the kbuf
ring, going through IORING_REGISTER_MEM_REGION would be semantically
the same, except the buffer allocation by the kernel now happens
before the ring is created and then later populated into the ring.
userspace would still need to make an mmap call to the region and the
kernel could enforce that as read-only. But if userspace doesn&#x27;t
initiate the setup, then going through IORING_REGISTER_MEM_REGION gets
uglier.

&gt; &gt; to be able to piggy back than onto your work.
&gt;
&gt; IORING_REGISTER_MEM_REGION supports both types of allocations. It can
&gt; have a new registration flag for read-only, and then you either make
&gt; the bounce avoidance optional or reject binding fuse to unsupported
&gt; setups during init. Any arguments against that? I need to go over
&gt; Joanne&#x27;s reply, but I don&#x27;t see any contradiction in principal with
&gt; your use case.

So i guess the flow would have to be:
a) user calls io_uring_register_region(&amp;ring, &amp;mem_region_reg) with
mem_region_reg.region_uptr&#x27;s size field set to the total buffer size
(and mem_region_reg.flags read-only bit set if needed)
     kernel allocates region
b) user calls mmap() to get the address of the region. If read-only
bit was set, it gets a read-only address
c) user calls io_uring_register_buf_ring(&amp;ring, &amp;buf_reg, flags) with
buf_reg.flags |= IOU_PBUF_RING_KERNEL_MANAGED
     kernel creates an empty kernel-managed ring. None of the buffers
are populated
d) user tells X subsystem to populate the ring starting from offset Z
in the registered mem region
e) on the kernel side, the subsystem populates the ring starting from
offset Z, filling it up using the buf_size and ring_entries values
that the user registered the ring with in c)

To be completely honest, the more I look at this the more this feels
like overkill / over-engineered to me. I get that now the user can do
the PMD optimization, but does that actually lead to noticeable
performance benefits? It seems especially confusing with them going
through the same pbuf ring interface but having totally different
expectations.

What about adding a straightforward kmbuf ring that goes through the
pbuf interface (eg the design in this patchset) and then in the future
adding an interface for pbuf rings (both kernel-managed and
non-kernel-managed) to go through IORING_REGISTERED_MEM_REGIONS if
users end up needing/wanting to have their rings populated that way?

Thanks,
Joanne

&gt;
&gt; --
&gt; Pavel Begunkov
&gt;


---

On Fri, Feb 13, 2026 at 7:31 AM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt;
&gt; On 2/13/26 07:27, Christoph Hellwig wrote:
&gt; &gt; On Thu, Feb 12, 2026 at 09:29:31AM -0800, Joanne Koong wrote:
&gt; &gt;&gt;&gt;&gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt; &gt;&gt;&gt;&gt; where the kernel controls the allocation fully and guarantees user
&gt; &gt;&gt;&gt;&gt; processes can only read the memory but never write to it.  I&#x27;d love
&gt; &gt;&gt;
&gt; &gt;&gt; By &quot;control the allocation fully&quot; do you mean for your use case, the
&gt; &gt;&gt; allocation/setup isn&#x27;t triggered by userspace but is initiated by the
&gt; &gt;&gt; kernel (eg user never explicitly registers any kbuf ring, the kernel
&gt; &gt;&gt; just uses the kbuf ring data structure internally and users can read
&gt; &gt;&gt; the buffer contents)? If userspace initiates the setup of the kbuf
&gt; &gt;&gt; ring, going through IORING_REGISTER_MEM_REGION would be semantically
&gt; &gt;&gt; the same, except the buffer allocation by the kernel now happens
&gt; &gt;&gt; before the ring is created and then later populated into the ring.
&gt; &gt;&gt; userspace would still need to make an mmap call to the region and the
&gt; &gt;&gt; kernel could enforce that as read-only. But if userspace doesn&#x27;t
&gt; &gt;&gt; initiate the setup, then going through IORING_REGISTER_MEM_REGION gets
&gt; &gt;&gt; uglier.
&gt; &gt;
&gt; &gt; The idea is that the application tells the kernel that it wants to use
&gt; &gt; a fixed buffer pool for reads.  Right now the application does this
&gt; &gt; using io_uring_register_buffers().  The problem with that is that
&gt; &gt; io_uring_register_buffers ends up just doing a pin of the memory,
&gt; &gt; but the application or, in case of shared memory, someone else could
&gt; &gt; still modify the memory.  If the underlying file system or storage
&gt; &gt; device needs verify checksums, or worse rebuild data from parity
&gt; &gt; (or uncompress), it needs to ensure that the memory it is operating
&gt; &gt; on can&#x27;t be modified by someone else.
&gt; &gt;
&gt; &gt; So I&#x27;ve been thinking of a version of io_uring_register_buffers where
&gt; &gt; the buffers are not provided by the application, but instead by the
&gt; &gt; kernel and mapped into the application address space read-only for
&gt; &gt; a while, and I thought I could implement this on top of your series,
&gt; &gt; but I have to admit I haven&#x27;t really looked into the details all
&gt; &gt; that much.
&gt;
&gt; There is nothing about registered buffers in this series. And even
&gt; if you try to reuse buffer allocation out of it, it&#x27;ll come with
&gt; a circular buffer you&#x27;ll have no need for. And I&#x27;m pretty much

I think the circular buffer will be useful for Christoph&#x27;s use case in
the same way it&#x27;ll be useful for fuse&#x27;s. The read payload could be
differently sized across requests, so it&#x27;s a lot of wasted space to
have to allocate a buffer large enough to support the max-size request
per entry in the io_ring. With using a circular buffer, buffers have a
way to be shared across entries, which means we can significantly
reduce how much memory needs to be allocated.

Thanks,
Joanne

&gt; arguing about separating those for io_uring.
&gt;
&gt; --
&gt; Pavel Begunkov
&gt;


---

On Fri, Feb 13, 2026 at 4:41 AM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt;
&gt; On 2/13/26 07:18, Christoph Hellwig wrote:
&gt; &gt; On Thu, Feb 12, 2026 at 10:44:44AM +0000, Pavel Begunkov wrote:
&gt; &gt;&gt;&gt;
&gt; &gt; Can you clarify what you mean with &#x27;pbuf&#x27;?  The only fixed buffer API I
&gt; &gt; know is io_uring_register_buffers* which always takes user provided
&gt; &gt; buffers, so I have a hard time parsing what you&#x27;re saying there.  But
&gt; &gt; that might just be sign that I&#x27;m no expert in io_uring APIs, and that
&gt; &gt; web searches have degraded to the point of not being very useful
&gt; &gt; anymore.
&gt;
&gt; Registered, aka fixed, buffers are the ones you pass to
&gt; IORING_OP_[READ,WRITE]_FIXED and some other requests. It&#x27;s normally
&gt; created by io_uring_register_buffers*() / IORING_REGISTER_BUFFERS*
&gt; with user memory, but there are special cases when it&#x27;s installed
&gt; internally by other kernel components, e.g. ublk.
&gt; This series has nothing to do with them, and relevant parts of
&gt; the discussion here don&#x27;t mention them either.
&gt;
&gt; Provided buffer rings, a.k.a pbuf rings, IORING_REGISTER_PBUF_RING
&gt; is a kernel-user shared ring. The entries are user buffers
&gt; {uaddr, size}. The user space adds entries, the kernel (io_uring
&gt; requests) consumes them and issues I/O using the user addresses.
&gt; E.g. you can issue a IORING_OP_RECV request (+IOSQE_BUFFER_SELECT)
&gt; and it&#x27;ll grab a buffer from the ring instead of using sqe-&gt;addr.
&gt;
&gt; pbuf rings, IORING_REGISTER_MEM_REGION, completion/submission
&gt; queues and all other kernel-user rings/etc. are internally based
&gt; on so called regions. All of them support both user allocated
&gt; memory and kernel allocations + mmap.
&gt;
&gt; This series essentially creates provided buffer rings, where
&gt; 1. the ring now contains kernel addresses
&gt; 2. the ring itself is in-kernel only and not shared with user space
&gt; 3. it also allocates kernel buffers (as a region), populates the ring
&gt;     with them, and allows mapping the buffers into the user space.

The most important part and the whole reason fuse needs the buffer
ring to be kernel-managed is because the kernel needs to control when
buffers get recycled back into the ring. For fuse&#x27;s use case, the
buffer is used for passing data between the kernel and the server. We
can&#x27;t have the server recycle the buffer because the server writes
back data to the kernel in that buffer when it submits the sqe. After
fuse receives the sqe and reads the reply from the server, it then
needs to recycle that buffer back into the ring so it can be reused
for a future cqe (eg sending a future request).

&gt;
&gt; Fuse is doing both adding (kernel) buffers to the ring and consuming
&gt; them. At which point it&#x27;s not clear:
&gt;
&gt; 1. Why it even needs io_uring provided buffer rings, it can be all
&gt;     contained in fuse. Maybe it&#x27;s trying to reuse pbuf ring code as
&gt;     basically an internal memory allocator, but then why expose buffer
&gt;     rings as an io_uring uapi instead of keeping it internally.
&gt;
&gt;     That&#x27;s also why I mentioned whether those buffers are supposed to
&gt;     be used with other types of io_uring requests like recv, etc.

On the userspace/server side, it uses the buffers for other io-uring
operations (eg reading or writing the contents from/to a
locally-backed file).

&gt;
&gt; 2. Why making io_uring to allocate payload memory. The answer to which
&gt;     is probably to reuse the region api with mmap and so on. And why
&gt;     payload buffers are inseparably created together with the ring

My main motivation for this is simplicity. I see (and thanks for
explaining) that using a registered mem region allows the use of some
optimizations (the only one I know of right now is the PMD one you
mentioned but maybe there&#x27;s more I&#x27;m missing) that could be useful for
some workloads, but I don&#x27;t think (and this could just be my lack of
understanding of what more optimizations there are) most use cases of
kmbufs benefit from those optimizations, so to me it feels like we&#x27;re
adding non-trivial complexity for no noticeable benefit.

I feel like we get the best of both worlds by letting users have both:
the simple kernel-managed pbuf where the kernel allocates the buffers
and the buffers are tied to the lifecycle of the ring, and the more
advanced kernel-managed pbuf where buffers are tied to a registered
memory region that the subsystem is responsible for later populating
the ring with.

&gt;     and via a new io_uring uapi.

imo it felt cleaner to have a new uapi for it because kmbufs and pbufs
have different expectations and behaviors (eg pbufs only work with
user-provided buffers and requires userspace to populate the ring
before using it, whereas for kmbufs the kernel allocates the buffers
and populates it for you; pbufs require userspace to recycle back the
buffer, whereas for kmbufs the kernel is the one in control of
recycling) and from the user pov it seemed confusing to have kmbufs as
part of the pbuf ring uapi, instead of separating it out as a
different type of ringbuffer with a different expectation and
behavior. I was trying to make the point that combining the interface
if we go with IORING_MEM_REGION gets even more confusing because now
pbufs that are kernel-managed are also empty at initialization and
only can point to areas inside a registered mem region and the
responsibility of populating it is now on whatever subsystem is using
it.

I still have this opinion but I also think in general, you likely know
better than I do what kind of io-uring uapi is best for io-uring&#x27;s
users. For v2 I&#x27;ll have kmbufs go through the pbuf uapi.

&gt;
&gt;     And yes, I believe in the current form it&#x27;s inflexible, it requires
&gt;     a new io_uring uapi. It requires the number of buffers to match
&gt;     the number of ring entries, which are related but not the same

I&#x27;m not really seeing what the purpose of having a ring entry with no
buffer associated with it is. In the existing code for non-kernel
managed pbuf rings, there&#x27;s the same tie between reg-&gt;ring_entries
being used as the marker for how many buffers the ring supports. But
if the number of buffers should be different than the number of ring
entries, this can be easily fixed by passing in the number of buffers
from the uapi for kernel-managed pbuf rings.

&gt;     thing. You can&#x27;t easily add more memory as it&#x27;s bound to the ring
&gt;     object. The buffer memory won&#x27;t even have same lifetime as the

To play devil&#x27;s advocate, we also can&#x27;t easily add more memory to the
mem region once it&#x27;s been registered. I think there&#x27;s also a worse
penalty where the user needs to know upfront how much memory to
allocate for the mem region for the lifetime of the ring, which imo
may be hard to do (eg if a kernel-managed buf ring only needs to be
registered for some code paths and not others, the mem region
registration would still have to allocate the memory a potential kbuf
ring would use).

&gt;     ring object -- allow using that km buffer ring with recv requests
&gt;     and highly likely I&#x27;ll most likely give you a way to crash the
&gt;     kernel.

I&#x27;m a bit confused by this part. The buffer memory does have the same
lifetime as the ring object, no? The buffers only get freed when the
ring itself is freed.

&gt;
&gt; But hey, I&#x27;m tired. I don&#x27;t have any beef here and am only trying
&gt; to make it a bit cleaner and flexible for fuse in the first place
&gt; without even questioning the I/O path. If everyone believes

I appreciate you looking at this and giving your feedback and insight.
Thank you for doing so. I don&#x27;t want to merge in something you&#x27;re
unhappy with.

Are you open to having support for both a simple kernel-managed pbuf
interface and later on if/when the need arises, a kernel-managed pbuf
interface that goes through a registered memory region? If the answer
is no, then I&#x27;ll make the change to have kmbufs go through the
registered memory region.

Thanks,
Joanne

&gt; everything is right, just ask Jens to merge it.
&gt;
&gt; --
&gt; Pavel Begunkov
&gt;


---

On Wed, Feb 18, 2026 at 4:36 AM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt;
&gt; On 2/13/26 22:04, Joanne Koong wrote:
&gt; &gt; On Fri, Feb 13, 2026 at 4:41 AM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt; ...
&gt; &gt;&gt; Fuse is doing both adding (kernel) buffers to the ring and consuming
&gt; &gt;&gt; them. At which point it&#x27;s not clear:
&gt; &gt;&gt;
&gt; &gt;&gt; 1. Why it even needs io_uring provided buffer rings, it can be all
&gt; &gt;&gt;      contained in fuse. Maybe it&#x27;s trying to reuse pbuf ring code as
&gt; &gt;&gt;      basically an internal memory allocator, but then why expose buffer
&gt; &gt;&gt;      rings as an io_uring uapi instead of keeping it internally.
&gt; &gt;&gt;
&gt; &gt;&gt;      That&#x27;s also why I mentioned whether those buffers are supposed to
&gt; &gt;&gt;      be used with other types of io_uring requests like recv, etc.
&gt; &gt;
&gt; &gt; On the userspace/server side, it uses the buffers for other io-uring
&gt; &gt; operations (eg reading or writing the contents from/to a
&gt; &gt; locally-backed file).
&gt;

Sorry, I submitted v2 last night thinking the conversation on this
thread had died. After reading through your reply, I&#x27;ll modify v2.

&gt; Oops, typo. I was asking whether the buffer rings (not buffers) are
&gt; supposed to be used with other requests. E.g. submitting a
&gt; IORING_OP_RECV with IOSQE_BUFFER_SELECT set and the bgid specifying
&gt; your kernel-managed buffer ring.

Yes the buffer rings are intended to be used with other io-uring
requests. The ideal scenario is that the user can then do the
equivalent of IORING_OP_READ/WRITE_FIXED operations on the
kernel-managed buffers and avoid the per-i/o page pinning overhead
costs.

&gt;
&gt; &gt;&gt; 2. Why making io_uring to allocate payload memory. The answer to which
&gt; &gt;&gt;      is probably to reuse the region api with mmap and so on. And why
&gt; &gt;&gt;      payload buffers are inseparably created together with the ring
&gt; &gt;
&gt; &gt; My main motivation for this is simplicity. I see (and thanks for
&gt; &gt; explaining) that using a registered mem region allows the use of some
&gt; &gt; optimizations (the only one I know of right now is the PMD one you
&gt; &gt; mentioned but maybe there&#x27;s more I&#x27;m missing) that could be useful for
&gt; &gt; some workloads, but I don&#x27;t think (and this could just be my lack of
&gt; &gt; understanding of what more optimizations there are) most use cases of
&gt; &gt; kmbufs benefit from those optimizations, so to me it feels like we&#x27;re
&gt; &gt; adding non-trivial complexity for no noticeable benefit.
&gt;
&gt; There are two separate arguments. The first is about not making buffers
&gt; inseparable from buffer rings in the io_uring user API. Whether it&#x27;s
&gt; IORING_REGISTER_MEM_REGION or something else is not that important.
&gt; I have no objection if it&#x27;s a part of fuse instead though, e.g. if
&gt; fuse binds two objects together when you register it with fuse, or even
&gt; if fuse create a buffer ring internally (assuming it doesn&#x27;t indirectly
&gt; leak into io_uring uapi).
&gt;
&gt; And the second was about optionally allowing user memory for buffer
&gt; creation as you&#x27;re reusing the region abstraction. You can find pros
&gt; and cons for both modes, and funnily enough, SQ/CQ were first kernel
&gt; allocated and then people asked for backing it by user memory, and IIRC
&gt; it was in the reverse order for pbuf rings.
&gt;
&gt; Implementing this is trivial as well, you just need to pass an argument
&gt; while creating a region. All new region users use struct
&gt; io_uring_region_desc for uapi and forward it to io_create_region()
&gt; without caring if it&#x27;s user or kernel allocated memory.
&gt;
&gt; &gt; I feel like we get the best of both worlds by letting users have both:
&gt; &gt; the simple kernel-managed pbuf where the kernel allocates the buffers
&gt; &gt; and the buffers are tied to the lifecycle of the ring, and the more
&gt; &gt; advanced kernel-managed pbuf where buffers are tied to a registered
&gt; &gt; memory region that the subsystem is responsible for later populating
&gt; &gt; the ring with.
&gt; &gt;
&gt; &gt;&gt;      and via a new io_uring uapi.
&gt; &gt;
&gt; &gt; imo it felt cleaner to have a new uapi for it because kmbufs and pbufs
&gt;
&gt; The stress is on why it&#x27;s an _io_uring_ API. It doesn&#x27;t matter to me
&gt; whether it&#x27;s a separate opcode or not. Currently, buffer rings don&#x27;t give
&gt; you anything that can&#x27;t be pure fuse, and it might be simpler to have
&gt; it implemented in fuse than binding to some io_uring object. Or it could
&gt; create buffer rings internally to reuse code but it doesn&#x27;t become an
&gt; io_uring uapi but rather implementation detail. And that predicates on
&gt; whether km rings are intended to be used with other / non-fuse requests.
&gt;
&gt; &gt; have different expectations and behaviors (eg pbufs only work with
&gt; &gt; user-provided buffers and requires userspace to populate the ring
&gt; &gt; before using it, whereas for kmbufs the kernel allocates the buffers
&gt; &gt; and populates it for you; pbufs require userspace to recycle back the
&gt; &gt; buffer, whereas for kmbufs the kernel is the one in control of
&gt; &gt; recycling) and from the user pov it seemed confusing to have kmbufs as
&gt; &gt; part of the pbuf ring uapi, instead of separating it out as a
&gt; &gt; different type of ringbuffer with a different expectation and
&gt;
&gt; I believe the source of disagreement is that you&#x27;re thinking
&gt; about how it&#x27;s going to look like for fuse specifically, and I
&gt; believe you that it&#x27;ll be nicer for the fuse use case. However,
&gt; on the other hand it&#x27;s an io_uring uapi, and if it is an io_uring
&gt; uapi, we need reusable blocks that are not specific to particular
&gt; users.

I agree 100%. The api we add should be what&#x27;s best for io-uring, not fuse.

For the majority of use cases, it seemed to me that having the buffers
separated from the buffer rings didn&#x27;t yield perceptible benefits but
added complexity and more restrictions like having to statically know
up front how big the mem region needs to be across the lifetime of the
io-uring for anything the io-uring might use the mem region for. It
seems more generically useful as a concept to have the buffers owned
by the ring and tied to the lifetime of the ring. I like how with this
design everything is self-contained and multiple subsystems can use it
without having to reimplement functionality locally in the subsystem.
On the other hand, I see your point about how it might be something
users want in the future if they want complete control over which
parts of the mem region get used as the backing buffers to do stuff
like PMD optimizations.

I think this is a matter of opinion/preference and I think in general
for anything io-uring related, yours should take precedence.

With it going through a mem region, I don&#x27;t think it should even go
through the &quot;pbuf ring&quot; interface then if it&#x27;s not going to specify
the number of entries and buffer sizes upfront, if support is added
for io-uring normal requests (eg IORING_OP_READ/WRITE) to use the
backing pages from a memory region and if we&#x27;re able to guarantee that
the registered memory region will never be able to be unregistered by
the user. I think if we repurpose the

union {
  __u64 addr; /* pointer to buffer or iovecs */
  __u64 splice_off_in;
};

fields in the struct io_uring_sqe to

union {
  __u64 addr; /* pointer to buffer or iovecs */
  __u64 splice_off_in;
  __u64 offset; /* offset into registered mem region */
};

and add some IOSQE_ flag to indicate it should find the pages from the
registered mem region, then that should work for normal requests.
Where on the kernel side, it looks up the associated pages stored in
the io_mapped_region&#x27;s pages array for the offset passed in.

Right now there&#x27;s only a uapi to register a memory region and none to
unregister one. Is it guaranteed that io-uring will never add
something in the future that will let userspace unregister the memory
region or at least unregister it while it&#x27;s being used (eg if we add
future refcounting to it to track active uses of it)?

If so, then end-to-end, with it going through the mem region, it would
be something like:
* user creates a mem region for the io-uring
* user mmaps the mem region
* user passes in offset into region, length of each buffer, and number
of entries in the ring to the subsystem
* subsystem creates a locally managed bufring and adds buffers to that
ring from the mem region
* on the cqe side, it sends the buffer id of the registered mem region
through the same &quot;IORING_CQE_F_BUFFER |  (buf_id &lt;&lt;
IORING_CQE_BUFFER_SHIFT)&quot; mechanism

Does this design match what you had in mind / prefer?

I think the above works for Christoph&#x27;s use case too (as his and my
use case are the same) but if not, please let me know.

Thanks,
Joanne


---

On Fri, Feb 20, 2026 at 4:53 AM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt;
&gt; On 2/18/26 21:43, Joanne Koong wrote:
&gt; &gt; On Wed, Feb 18, 2026 at 4:36 AM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt; &gt;&gt;
&gt; &gt;&gt; On 2/13/26 22:04, Joanne Koong wrote:
&gt; &gt;&gt;&gt; On Fri, Feb 13, 2026 at 4:41 AM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt; &gt;&gt; ...
&gt; &gt;&gt;&gt;&gt; Fuse is doing both adding (kernel) buffers to the ring and consuming
&gt; &gt;&gt;&gt;&gt; them. At which point it&#x27;s not clear:
&gt; &gt;&gt;&gt;&gt;
&gt; &gt;&gt;&gt;&gt; 1. Why it even needs io_uring provided buffer rings, it can be all
&gt; &gt;&gt;&gt;&gt;       contained in fuse. Maybe it&#x27;s trying to reuse pbuf ring code as
&gt; &gt;&gt;&gt;&gt;       basically an internal memory allocator, but then why expose buffer
&gt; &gt;&gt;&gt;&gt;       rings as an io_uring uapi instead of keeping it internally.
&gt; &gt;&gt;&gt;&gt;
&gt; &gt;&gt;&gt;&gt;       That&#x27;s also why I mentioned whether those buffers are supposed to
&gt; &gt;&gt;&gt;&gt;       be used with other types of io_uring requests like recv, etc.
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt; On the userspace/server side, it uses the buffers for other io-uring
&gt; &gt;&gt;&gt; operations (eg reading or writing the contents from/to a
&gt; &gt;&gt;&gt; locally-backed file).
&gt; &gt;&gt;
&gt; &gt;
&gt; &gt; Sorry, I submitted v2 last night thinking the conversation on this
&gt; &gt; thread had died. After reading through your reply, I&#x27;ll modify v2.
&gt;
&gt; No worries at all, and sorry I&#x27;m a bit slow to reply
&gt;
&gt; &gt;&gt; Oops, typo. I was asking whether the buffer rings (not buffers) are
&gt; &gt;&gt; supposed to be used with other requests. E.g. submitting a
&gt; &gt;&gt; IORING_OP_RECV with IOSQE_BUFFER_SELECT set and the bgid specifying
&gt; &gt;&gt; your kernel-managed buffer ring.
&gt; &gt;
&gt; &gt; Yes the buffer rings are intended to be used with other io-uring
&gt; &gt; requests. The ideal scenario is that the user can then do the
&gt; &gt; equivalent of IORING_OP_READ/WRITE_FIXED operations on the
&gt; &gt; kernel-managed buffers and avoid the per-i/o page pinning overhead
&gt; &gt; costs.
&gt;
&gt; You mention OP_READ_FIXED and below agreed not exposing km rings
&gt; an io_uring uapi, which makes me believe we&#x27;re still talking about
&gt; different things.
&gt;
&gt; Correct me if I&#x27;m wrong. Currently, only fuse cmds use the buffer
&gt; ring itself, I&#x27;m not talking about buffer, i.e. fuse cmds consume
&gt; entries from the ring (!!! that&#x27;s the part I&#x27;m interested in), then
&gt; process them and tell the server &quot;this offset in the region has user
&gt; data to process or should be populated with data&quot;.
&gt;
&gt; Naturally, the server should be able to use the buffers to issue
&gt; some I/O and process it in other ways, whether it&#x27;s a normal
&gt; OP_READ to which you pass the user space address (you can since
&gt; it&#x27;s mmap()&#x27;ed by the server) or something else is important but
&gt; a separate question than the one I&#x27;m trying to understand.
&gt;
&gt; So I&#x27;m asking whether you expect that a server or other user space
&gt; program should be able to issue a READ_OP_RECV, READ_OP_READ or any
&gt; other similar request, which would consume buffers/entries from the
&gt; km ring without any fuse kernel code involved? Do you have some
&gt; use case for that in mind?

Thanks for clarifying your question. Yes, this would be a useful
optimization in the future for fuse servers with certain workload
characteristics (eg network-backed servers with high concurrency and
unpredictable latencies). I don&#x27;t think the concept of kmbufrings is
exclusively fuse-specific though (for example, Christoph&#x27;s use case
being a recent instance); I think other subsystems/users that&#x27;ll use
kmbuf rings would also generically find it useful to have the option
of READ_OP_RECV/READ_OP_READ operating directly on the ring.

&gt;
&gt; Understanding that is the key in deciding whether km rings should
&gt; be exposed as io_uring uapi or not, regardless of where buffers
&gt; to populate the ring come from.
&gt;
&gt; ...
&gt; &gt; With it going through a mem region, I don&#x27;t think it should even go
&gt; &gt; through the &quot;pbuf ring&quot; interface then if it&#x27;s not going to specify
&gt; &gt; the number of entries and buffer sizes upfront, if support is added
&gt; &gt; for io-uring normal requests (eg IORING_OP_READ/WRITE) to use the
&gt; &gt; backing pages from a memory region and if we&#x27;re able to guarantee that
&gt; &gt; the registered memory region will never be able to be unregistered by
&gt; &gt; the user. I think if we repurpose the
&gt; &gt;
&gt; &gt; union {
&gt; &gt;    __u64 addr; /* pointer to buffer or iovecs */
&gt; &gt;    __u64 splice_off_in;
&gt; &gt; };
&gt; &gt;
&gt; &gt; fields in the struct io_uring_sqe to
&gt; &gt;
&gt; &gt; union {
&gt; &gt;    __u64 addr; /* pointer to buffer or iovecs */
&gt; &gt;    __u64 splice_off_in;
&gt; &gt;    __u64 offset; /* offset into registered mem region */
&gt; &gt; };
&gt; &gt;
&gt; &gt; and add some IOSQE_ flag to indicate it should find the pages from the
&gt; &gt; registered mem region, then that should work for normal requests.
&gt; &gt; Where on the kernel side, it looks up the associated pages stored in
&gt; &gt; the io_mapped_region&#x27;s pages array for the offset passed in.
&gt;
&gt; So you already can do all that using the mmap()&#x27;ed region user
&gt; pointer, and you just want it to be more efficient, right?
&gt; For that let&#x27;s just reuse registered buffers, we don&#x27;t need a
&gt; new mechanism that needs to be propagated to all request types.
&gt; And registered buffer are already optimised for I/O in a bunch
&gt; of ways. And as a bonus, it&#x27;ll be similar to the zero-copy
&gt; internally registered buffers if you still plan to add them.
&gt;
&gt; The simplest way to do that is to create a registered buffer out
&gt; of the mmap&#x27;ed region pointer. Pseudo code:
&gt;
&gt; // mmap&#x27;ed if it&#x27;s kernel allocated.
&gt; {region_ptr, region_size} = create_region();
&gt;
&gt; struct iovec iov;
&gt; iov.iov_base = region_ptr;
&gt; iov.iov_len = region_size;
&gt; io_uring_register_buffers(ring, &amp;iov, 1);
&gt;
&gt; // later instead of this:
&gt; ptr = region_ptr + off;
&gt; io_uring_prep_read(sqe, fd, ptr, ...);
&gt;
&gt; // you use registered buffers as usual:
&gt; io_uring_prep_read_fixed(sqe, fd, off, regbuf_idx, ...);
&gt;

I feel like this design makes the interface more convoluted and now
muddies different concepts together by adding new complexity /
relationships between them whereas they were otherwise cleanly
isolated. Maybe I&#x27;m just not seeing/understanding the overarching
vision for why conceptually it makes sense for them to be tied
together besides as a mechanism to tell io-uring requests where to
copy from by reusing what exists for fixed buffer ids. There&#x27;s more
complexity now on the kernel side (eg having to detect if the buffer
passed in is kernel-allocated to know whether to pin the pages /
charge it against the user&#x27;s RLIMIT_MEMLOCK limit) but I&#x27;m not
understanding what we gain from it. I got the sense from your previous
comments that memory regions are the de facto way to go and should be
decoupled from other structures, so if that&#x27;s the case, why doesn&#x27;t it
make sense for io-uring to add native support for using memory regions
for io-uring requests? I feel like from the userspace side it makes
things more confusing with this extra layer of indirection that now
has to go through a fixed buffer.

&gt;
&gt; IIRC the registration would fail because it doesn&#x27;t allow file
&gt; backed pages, but it should be fine if we know it&#x27;s io_uring
&gt; region memory, so that would need to be patched.
&gt;
&gt; There might be a bunch of other ways you can do that like
&gt; create a kernel allocated registered buffer like what Cristoph
&gt; wants, and then register it as a region. Or allow creating
&gt; registered buffers out of a region. etc.
&gt;
&gt; I wanted to unify registered buffers and regions internally
&gt; at some point, but then drifted away from active io_uring core
&gt; infrastructure development, so I guess that could&#x27;ve been useful.
&gt;
&gt; &gt; Right now there&#x27;s only a uapi to register a memory region and none to
&gt; &gt; unregister one. Is it guaranteed that io-uring will never add
&gt; &gt; something in the future that will let userspace unregister the memory
&gt; &gt; region or at least unregister it while it&#x27;s being used (eg if we add
&gt; &gt; future refcounting to it to track active uses of it)?
&gt;
&gt; Let&#x27;s talk about it when it&#x27;s needed or something changes, but if
&gt; you do registered buffers instead as per above, they&#x27;ll be holding
&gt; page references and or have to pin the region in some other way.

I don&#x27;t think we can guarantee that the caller will register the
memory region as a fixed buffer (eg if it doesn&#x27;t need/want to use the
buffer for normal io-uring requests). On the kernel side, the internal
buffer entry uses the kaddr of the registered memory region buffer for
any memcpys. If it&#x27;s not guaranteed that registered memory regions
persist for the lifetime of the ring, there&#x27;ll have to be extra
overhead for every I/O (eg grab the io-uring lock, checking if the mem
region is still registered, grab a refcount to that mem region, unlock
the ring, do the memcpy to the kaddr, then grab the io-uring lock
again, decrement the refcount, and unlock). Or I guess we could add
pinning to a registered memory region.

Thanks,
Joanne
&gt;
&gt; &gt; If so, then end-to-end, with it going through the mem region, it would
&gt; &gt; be something like:
&gt; &gt; * user creates a mem region for the io-uring
&gt; &gt; * user mmaps the mem region
&gt;
&gt; FWIW, we should just add a liburing helper, so that fuse server
&gt; doesn&#x27;t need to deal with mmap&#x27;ing.
&gt;
&gt; &gt; * user passes in offset into region, length of each buffer, and number
&gt; &gt; of entries in the ring to the subsystem
&gt; &gt; * subsystem creates a locally managed bufring and adds buffers to that
&gt; &gt; ring from the mem region
&gt;
&gt; That&#x27;s sounds clean to me _if_ it allows you to achieve all
&gt; (fast path) optimisations you want to have. I hope it does?
&gt;
&gt; --
&gt; Pavel Begunkov
&gt;

</pre>
</details>
<div class="review-comment-signals">Signals: requested changes, technical concerns</div>
</div>
</div>
</div>
</div>
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Christoph Hellwig</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-11">2026-02-11</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Christoph Hellwig raised concerns about the kernel-managed buffer rings feature, specifically that it doesn&#x27;t guarantee user processes can only read memory and not write to it. He wants a setup where the kernel controls allocation fully.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Wed, Feb 11, 2026 at 02:06:18PM -0800, Joanne Koong wrote:
&gt; &gt; I don&#x27;t think I follow. I&#x27;m saying that it might be interesting
&gt; &gt; to separate rings from how and with what they&#x27;re populated on the
&gt; &gt; kernel API level, but the fuse kernel module can do the population
&gt; 
&gt; Oh okay, from your first message I (and I think christoph too) thought
&gt; what you were saying is that the user should be responsible for
&gt; allocating the buffers with complete ownership over them, and then
&gt; just pass those allocated to the kernel to use. But what you&#x27;re saying
&gt; is that just use a different way for getting the kernel to allocate
&gt; the buffers (eg through the IORING_REGISTER_MEM_REGION interface). Am
&gt; I reading this correctly?

I&#x27;m arguing exactly against this.  For my use case I need a setup
where the kernel controls the allocation fully and guarantees user
processes can only read the memory but never write to it.  I&#x27;d love
to be able to piggy back than onto your work.



---

On Thu, Feb 12, 2026 at 09:29:31AM -0800, Joanne Koong wrote:
&gt; &gt; &gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt; &gt; &gt; where the kernel controls the allocation fully and guarantees user
&gt; &gt; &gt; processes can only read the memory but never write to it.  I&#x27;d love
&gt; 
&gt; By &quot;control the allocation fully&quot; do you mean for your use case, the
&gt; allocation/setup isn&#x27;t triggered by userspace but is initiated by the
&gt; kernel (eg user never explicitly registers any kbuf ring, the kernel
&gt; just uses the kbuf ring data structure internally and users can read
&gt; the buffer contents)? If userspace initiates the setup of the kbuf
&gt; ring, going through IORING_REGISTER_MEM_REGION would be semantically
&gt; the same, except the buffer allocation by the kernel now happens
&gt; before the ring is created and then later populated into the ring.
&gt; userspace would still need to make an mmap call to the region and the
&gt; kernel could enforce that as read-only. But if userspace doesn&#x27;t
&gt; initiate the setup, then going through IORING_REGISTER_MEM_REGION gets
&gt; uglier.

The idea is that the application tells the kernel that it wants to use
a fixed buffer pool for reads.  Right now the application does this
using io_uring_register_buffers().  The problem with that is that
io_uring_register_buffers ends up just doing a pin of the memory,
but the application or, in case of shared memory, someone else could
still modify the memory.  If the underlying file system or storage
device needs verify checksums, or worse rebuild data from parity
(or uncompress), it needs to ensure that the memory it is operating
on can&#x27;t be modified by someone else.

So I&#x27;ve been thinking of a version of io_uring_register_buffers where
the buffers are not provided by the application, but instead by the
kernel and mapped into the application address space read-only for
a while, and I thought I could implement this on top of your series,
but I have to admit I haven&#x27;t really looked into the details all
that much.

&gt; 
&gt; To be completely honest, the more I look at this the more this feels
&gt; like overkill / over-engineered to me. I get that now the user can do
&gt; the PMD optimization, but does that actually lead to noticeable
&gt; performance benefits? It seems especially confusing with them going
&gt; through the same pbuf ring interface but having totally different
&gt; expectations.

Yes.  The PMD mapping also is not that relevant.  Both AMD (implicit)
and ARM (explicit) have optimizations for contiguous PTEs that are
almost as valuable.

&gt; What about adding a straightforward kmbuf ring that goes through the
&gt; pbuf interface (eg the design in this patchset) and then in the future
&gt; adding an interface for pbuf rings (both kernel-managed and
&gt; non-kernel-managed) to go through IORING_REGISTERED_MEM_REGIONS if
&gt; users end up needing/wanting to have their rings populated that way?

That feels much simpler to me as well.



---

On Fri, Feb 13, 2026 at 11:09:13AM -0800, Joanne Koong wrote:
&gt; 
&gt; I think the circular buffer will be useful for Christoph&#x27;s use case in
&gt; the same way it&#x27;ll be useful for fuse&#x27;s. The read payload could be
&gt; differently sized across requests, so it&#x27;s a lot of wasted space to
&gt; have to allocate a buffer large enough to support the max-size request
&gt; per entry in the io_ring.

Yes.

&gt; With using a circular buffer, buffers have a
&gt; way to be shared across entries, which means we can significantly
&gt; reduce how much memory needs to be allocated.

Or enable such flexible use cases at all.



---

On Fri, Feb 13, 2026 at 11:14:03AM -0800, Joanne Koong wrote:
&gt; I think we have the exact same use case, except your buffers need to
&gt; be read-only. I think your use case benefits from the same memory wins
&gt; we&#x27;ll get with incremental buffer consumption, which is the primary
&gt; reason fuse is using a bufring instead of fixed buffers.

Yeah.

&gt; I think you can and it&#x27;ll be very easy to do so. All that would be
&gt; needed is to pass in a read-only flag from the userspace side when it
&gt; registers the bufring, and then when userspace makes the mmap call to
&gt; the bufring, the kernel checks if that read-only flag is set on the
&gt; bufring and if so returns a read-only mapping. 

Yes, tat&#x27;s what I though.  But Pavel seems to disagree?

</pre>
</details>
<div class="review-comment-signals">Signals: requested changes, flexible use cases</div>
</div>
</div>
<div class="thread-node depth-1" id="2026-02-12">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Christoph Hellwig</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-12">2026-02-12</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Christoph Hellwig raised concerns about the kernel-managed buffer rings implementation, specifically that it doesn&#x27;t guarantee user processes can only read the memory and not write to it. He suggested implementing a version of io_uring_register_buffers where buffers are provided by the kernel and mapped into the application address space read-only.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Wed, Feb 11, 2026 at 02:06:18PM -0800, Joanne Koong wrote:
&gt; &gt; I don&#x27;t think I follow. I&#x27;m saying that it might be interesting
&gt; &gt; to separate rings from how and with what they&#x27;re populated on the
&gt; &gt; kernel API level, but the fuse kernel module can do the population
&gt; 
&gt; Oh okay, from your first message I (and I think christoph too) thought
&gt; what you were saying is that the user should be responsible for
&gt; allocating the buffers with complete ownership over them, and then
&gt; just pass those allocated to the kernel to use. But what you&#x27;re saying
&gt; is that just use a different way for getting the kernel to allocate
&gt; the buffers (eg through the IORING_REGISTER_MEM_REGION interface). Am
&gt; I reading this correctly?

I&#x27;m arguing exactly against this.  For my use case I need a setup
where the kernel controls the allocation fully and guarantees user
processes can only read the memory but never write to it.  I&#x27;d love
to be able to piggy back than onto your work.



---

On Thu, Feb 12, 2026 at 09:29:31AM -0800, Joanne Koong wrote:
&gt; &gt; &gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt; &gt; &gt; where the kernel controls the allocation fully and guarantees user
&gt; &gt; &gt; processes can only read the memory but never write to it.  I&#x27;d love
&gt; 
&gt; By &quot;control the allocation fully&quot; do you mean for your use case, the
&gt; allocation/setup isn&#x27;t triggered by userspace but is initiated by the
&gt; kernel (eg user never explicitly registers any kbuf ring, the kernel
&gt; just uses the kbuf ring data structure internally and users can read
&gt; the buffer contents)? If userspace initiates the setup of the kbuf
&gt; ring, going through IORING_REGISTER_MEM_REGION would be semantically
&gt; the same, except the buffer allocation by the kernel now happens
&gt; before the ring is created and then later populated into the ring.
&gt; userspace would still need to make an mmap call to the region and the
&gt; kernel could enforce that as read-only. But if userspace doesn&#x27;t
&gt; initiate the setup, then going through IORING_REGISTER_MEM_REGION gets
&gt; uglier.

The idea is that the application tells the kernel that it wants to use
a fixed buffer pool for reads.  Right now the application does this
using io_uring_register_buffers().  The problem with that is that
io_uring_register_buffers ends up just doing a pin of the memory,
but the application or, in case of shared memory, someone else could
still modify the memory.  If the underlying file system or storage
device needs verify checksums, or worse rebuild data from parity
(or uncompress), it needs to ensure that the memory it is operating
on can&#x27;t be modified by someone else.

So I&#x27;ve been thinking of a version of io_uring_register_buffers where
the buffers are not provided by the application, but instead by the
kernel and mapped into the application address space read-only for
a while, and I thought I could implement this on top of your series,
but I have to admit I haven&#x27;t really looked into the details all
that much.

&gt; 
&gt; To be completely honest, the more I look at this the more this feels
&gt; like overkill / over-engineered to me. I get that now the user can do
&gt; the PMD optimization, but does that actually lead to noticeable
&gt; performance benefits? It seems especially confusing with them going
&gt; through the same pbuf ring interface but having totally different
&gt; expectations.

Yes.  The PMD mapping also is not that relevant.  Both AMD (implicit)
and ARM (explicit) have optimizations for contiguous PTEs that are
almost as valuable.

&gt; What about adding a straightforward kmbuf ring that goes through the
&gt; pbuf interface (eg the design in this patchset) and then in the future
&gt; adding an interface for pbuf rings (both kernel-managed and
&gt; non-kernel-managed) to go through IORING_REGISTERED_MEM_REGIONS if
&gt; users end up needing/wanting to have their rings populated that way?

That feels much simpler to me as well.



---

On Fri, Feb 13, 2026 at 11:09:13AM -0800, Joanne Koong wrote:
&gt; 
&gt; I think the circular buffer will be useful for Christoph&#x27;s use case in
&gt; the same way it&#x27;ll be useful for fuse&#x27;s. The read payload could be
&gt; differently sized across requests, so it&#x27;s a lot of wasted space to
&gt; have to allocate a buffer large enough to support the max-size request
&gt; per entry in the io_ring.

Yes.

&gt; With using a circular buffer, buffers have a
&gt; way to be shared across entries, which means we can significantly
&gt; reduce how much memory needs to be allocated.

Or enable such flexible use cases at all.



---

On Fri, Feb 13, 2026 at 11:14:03AM -0800, Joanne Koong wrote:
&gt; I think we have the exact same use case, except your buffers need to
&gt; be read-only. I think your use case benefits from the same memory wins
&gt; we&#x27;ll get with incremental buffer consumption, which is the primary
&gt; reason fuse is using a bufring instead of fixed buffers.

Yeah.

&gt; I think you can and it&#x27;ll be very easy to do so. All that would be
&gt; needed is to pass in a read-only flag from the userspace side when it
&gt; registers the bufring, and then when userspace makes the mmap call to
&gt; the bufring, the kernel checks if that read-only flag is set on the
&gt; bufring and if so returns a read-only mapping. 

Yes, tat&#x27;s what I though.  But Pavel seems to disagree?

</pre>
</details>
<div class="review-comment-signals">Signals: requested changes, alternative implementation</div>
</div>
</div>
<div class="thread-node depth-1" id="2026-02-13">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Bernd Schubert</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-13">2026-02-13</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Bernd expressed skepticism about sharing buffers across io_uring entries, questioning its usefulness and suggesting it might only reduce ring size.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">

On 2/13/26 20:09, Joanne Koong wrote:
&gt; On Fri, Feb 13, 2026 at 7:31\u202fAM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt;&gt;
&gt;&gt; On 2/13/26 07:27, Christoph Hellwig wrote:
&gt;&gt;&gt; On Thu, Feb 12, 2026 at 09:29:31AM -0800, Joanne Koong wrote:
&gt;&gt;&gt;&gt;&gt;&gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt;&gt;&gt;&gt;&gt;&gt; where the kernel controls the allocation fully and guarantees user
&gt;&gt;&gt;&gt;&gt;&gt; processes can only read the memory but never write to it.  I&#x27;d love
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; By &quot;control the allocation fully&quot; do you mean for your use case, the
&gt;&gt;&gt;&gt; allocation/setup isn&#x27;t triggered by userspace but is initiated by the
&gt;&gt;&gt;&gt; kernel (eg user never explicitly registers any kbuf ring, the kernel
&gt;&gt;&gt;&gt; just uses the kbuf ring data structure internally and users can read
&gt;&gt;&gt;&gt; the buffer contents)? If userspace initiates the setup of the kbuf
&gt;&gt;&gt;&gt; ring, going through IORING_REGISTER_MEM_REGION would be semantically
&gt;&gt;&gt;&gt; the same, except the buffer allocation by the kernel now happens
&gt;&gt;&gt;&gt; before the ring is created and then later populated into the ring.
&gt;&gt;&gt;&gt; userspace would still need to make an mmap call to the region and the
&gt;&gt;&gt;&gt; kernel could enforce that as read-only. But if userspace doesn&#x27;t
&gt;&gt;&gt;&gt; initiate the setup, then going through IORING_REGISTER_MEM_REGION gets
&gt;&gt;&gt;&gt; uglier.
&gt;&gt;&gt;
&gt;&gt;&gt; The idea is that the application tells the kernel that it wants to use
&gt;&gt;&gt; a fixed buffer pool for reads.  Right now the application does this
&gt;&gt;&gt; using io_uring_register_buffers().  The problem with that is that
&gt;&gt;&gt; io_uring_register_buffers ends up just doing a pin of the memory,
&gt;&gt;&gt; but the application or, in case of shared memory, someone else could
&gt;&gt;&gt; still modify the memory.  If the underlying file system or storage
&gt;&gt;&gt; device needs verify checksums, or worse rebuild data from parity
&gt;&gt;&gt; (or uncompress), it needs to ensure that the memory it is operating
&gt;&gt;&gt; on can&#x27;t be modified by someone else.
&gt;&gt;&gt;
&gt;&gt;&gt; So I&#x27;ve been thinking of a version of io_uring_register_buffers where
&gt;&gt;&gt; the buffers are not provided by the application, but instead by the
&gt;&gt;&gt; kernel and mapped into the application address space read-only for
&gt;&gt;&gt; a while, and I thought I could implement this on top of your series,
&gt;&gt;&gt; but I have to admit I haven&#x27;t really looked into the details all
&gt;&gt;&gt; that much.
&gt;&gt;
&gt;&gt; There is nothing about registered buffers in this series. And even
&gt;&gt; if you try to reuse buffer allocation out of it, it&#x27;ll come with
&gt;&gt; a circular buffer you&#x27;ll have no need for. And I&#x27;m pretty much
&gt; 
&gt; I think the circular buffer will be useful for Christoph&#x27;s use case in
&gt; the same way it&#x27;ll be useful for fuse&#x27;s. The read payload could be
&gt; differently sized across requests, so it&#x27;s a lot of wasted space to
&gt; have to allocate a buffer large enough to support the max-size request
&gt; per entry in the io_ring. With using a circular buffer, buffers have a
&gt; way to be shared across entries, which means we can significantly
&gt; reduce how much memory needs to be allocated.

Dunno, what we actually want is requests of multiple sizes. Sharing
buffers across entries sounds like just reducing the ring size - I
personally don&#x27;t see the point here.


Thanks,
Bernd
</pre>
</details>
<div class="review-comment-signals">Signals: skeptical, questioning</div>
</div>
<div class="thread-children">
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-18">2026-02-18</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Joanne raised a question about the meaning of &#x27;sharing buffers across entries&#x27; in kernel-managed buffer rings.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Fri, Feb 13, 2026 at 11:30 AM Bernd Schubert &lt;bernd@bsbernd.com&gt; wrote:
&gt;
&gt;
&gt;
&gt; On 2/13/26 20:09, Joanne Koong wrote:
&gt; &gt; On Fri, Feb 13, 2026 at 7:31 AM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt; &gt;&gt;
&gt; &gt;&gt; On 2/13/26 07:27, Christoph Hellwig wrote:
&gt; &gt;&gt;&gt; On Thu, Feb 12, 2026 at 09:29:31AM -0800, Joanne Koong wrote:
&gt; &gt;&gt;&gt;&gt;&gt;&gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt; &gt;&gt;&gt;&gt;&gt;&gt; where the kernel controls the allocation fully and guarantees user
&gt; &gt;&gt;&gt;&gt;&gt;&gt; processes can only read the memory but never write to it.  I&#x27;d love
&gt; &gt;&gt;&gt;&gt;
&gt; &gt;&gt;&gt;&gt; By &quot;control the allocation fully&quot; do you mean for your use case, the
&gt; &gt;&gt;&gt;&gt; allocation/setup isn&#x27;t triggered by userspace but is initiated by the
&gt; &gt;&gt;&gt;&gt; kernel (eg user never explicitly registers any kbuf ring, the kernel
&gt; &gt;&gt;&gt;&gt; just uses the kbuf ring data structure internally and users can read
&gt; &gt;&gt;&gt;&gt; the buffer contents)? If userspace initiates the setup of the kbuf
&gt; &gt;&gt;&gt;&gt; ring, going through IORING_REGISTER_MEM_REGION would be semantically
&gt; &gt;&gt;&gt;&gt; the same, except the buffer allocation by the kernel now happens
&gt; &gt;&gt;&gt;&gt; before the ring is created and then later populated into the ring.
&gt; &gt;&gt;&gt;&gt; userspace would still need to make an mmap call to the region and the
&gt; &gt;&gt;&gt;&gt; kernel could enforce that as read-only. But if userspace doesn&#x27;t
&gt; &gt;&gt;&gt;&gt; initiate the setup, then going through IORING_REGISTER_MEM_REGION gets
&gt; &gt;&gt;&gt;&gt; uglier.
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt; The idea is that the application tells the kernel that it wants to use
&gt; &gt;&gt;&gt; a fixed buffer pool for reads.  Right now the application does this
&gt; &gt;&gt;&gt; using io_uring_register_buffers().  The problem with that is that
&gt; &gt;&gt;&gt; io_uring_register_buffers ends up just doing a pin of the memory,
&gt; &gt;&gt;&gt; but the application or, in case of shared memory, someone else could
&gt; &gt;&gt;&gt; still modify the memory.  If the underlying file system or storage
&gt; &gt;&gt;&gt; device needs verify checksums, or worse rebuild data from parity
&gt; &gt;&gt;&gt; (or uncompress), it needs to ensure that the memory it is operating
&gt; &gt;&gt;&gt; on can&#x27;t be modified by someone else.
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt; So I&#x27;ve been thinking of a version of io_uring_register_buffers where
&gt; &gt;&gt;&gt; the buffers are not provided by the application, but instead by the
&gt; &gt;&gt;&gt; kernel and mapped into the application address space read-only for
&gt; &gt;&gt;&gt; a while, and I thought I could implement this on top of your series,
&gt; &gt;&gt;&gt; but I have to admit I haven&#x27;t really looked into the details all
&gt; &gt;&gt;&gt; that much.
&gt; &gt;&gt;
&gt; &gt;&gt; There is nothing about registered buffers in this series. And even
&gt; &gt;&gt; if you try to reuse buffer allocation out of it, it&#x27;ll come with
&gt; &gt;&gt; a circular buffer you&#x27;ll have no need for. And I&#x27;m pretty much
&gt; &gt;
&gt; &gt; I think the circular buffer will be useful for Christoph&#x27;s use case in
&gt; &gt; the same way it&#x27;ll be useful for fuse&#x27;s. The read payload could be
&gt; &gt; differently sized across requests, so it&#x27;s a lot of wasted space to
&gt; &gt; have to allocate a buffer large enough to support the max-size request
&gt; &gt; per entry in the io_ring. With using a circular buffer, buffers have a
&gt; &gt; way to be shared across entries, which means we can significantly
&gt; &gt; reduce how much memory needs to be allocated.
&gt;
&gt; Dunno, what we actually want is requests of multiple sizes. Sharing
&gt; buffers across entries sounds like just reducing the ring size - I
&gt; personally don&#x27;t see the point here.

By &quot;sharing buffers across entries&quot; what I mean is different regions
of the buffer can now be used concurrently by multiple entries.

Thanks,
Joanne
&gt;
&gt;
&gt; Thanks,
&gt; Bernd
</pre>
</details>
<div class="review-comment-signals">Signals: clarification</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-20_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-20">2026-02-20</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The reviewer, Joanne Koong, raised no specific technical concerns or objections to the patch.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Fri, Feb 13, 2026 at 11:30 AM Bernd Schubert &lt;bernd@bsbernd.com&gt; wrote:
&gt;
&gt;
&gt;
&gt; On 2/13/26 20:09, Joanne Koong wrote:
&gt; &gt; On Fri, Feb 13, 2026 at 7:31 AM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt; &gt;&gt;
&gt; &gt;&gt; On 2/13/26 07:27, Christoph Hellwig wrote:
&gt; &gt;&gt;&gt; On Thu, Feb 12, 2026 at 09:29:31AM -0800, Joanne Koong wrote:
&gt; &gt;&gt;&gt;&gt;&gt;&gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt; &gt;&gt;&gt;&gt;&gt;&gt; where the kernel controls the allocation fully and guarantees user
&gt; &gt;&gt;&gt;&gt;&gt;&gt; processes can only read the memory but never write to it.  I&#x27;d love
&gt; &gt;&gt;&gt;&gt;
&gt; &gt;&gt;&gt;&gt; By &quot;control the allocation fully&quot; do you mean for your use case, the
&gt; &gt;&gt;&gt;&gt; allocation/setup isn&#x27;t triggered by userspace but is initiated by the
&gt; &gt;&gt;&gt;&gt; kernel (eg user never explicitly registers any kbuf ring, the kernel
&gt; &gt;&gt;&gt;&gt; just uses the kbuf ring data structure internally and users can read
&gt; &gt;&gt;&gt;&gt; the buffer contents)? If userspace initiates the setup of the kbuf
&gt; &gt;&gt;&gt;&gt; ring, going through IORING_REGISTER_MEM_REGION would be semantically
&gt; &gt;&gt;&gt;&gt; the same, except the buffer allocation by the kernel now happens
&gt; &gt;&gt;&gt;&gt; before the ring is created and then later populated into the ring.
&gt; &gt;&gt;&gt;&gt; userspace would still need to make an mmap call to the region and the
&gt; &gt;&gt;&gt;&gt; kernel could enforce that as read-only. But if userspace doesn&#x27;t
&gt; &gt;&gt;&gt;&gt; initiate the setup, then going through IORING_REGISTER_MEM_REGION gets
&gt; &gt;&gt;&gt;&gt; uglier.
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt; The idea is that the application tells the kernel that it wants to use
&gt; &gt;&gt;&gt; a fixed buffer pool for reads.  Right now the application does this
&gt; &gt;&gt;&gt; using io_uring_register_buffers().  The problem with that is that
&gt; &gt;&gt;&gt; io_uring_register_buffers ends up just doing a pin of the memory,
&gt; &gt;&gt;&gt; but the application or, in case of shared memory, someone else could
&gt; &gt;&gt;&gt; still modify the memory.  If the underlying file system or storage
&gt; &gt;&gt;&gt; device needs verify checksums, or worse rebuild data from parity
&gt; &gt;&gt;&gt; (or uncompress), it needs to ensure that the memory it is operating
&gt; &gt;&gt;&gt; on can&#x27;t be modified by someone else.
&gt; &gt;&gt;&gt;
&gt; &gt;&gt;&gt; So I&#x27;ve been thinking of a version of io_uring_register_buffers where
&gt; &gt;&gt;&gt; the buffers are not provided by the application, but instead by the
&gt; &gt;&gt;&gt; kernel and mapped into the application address space read-only for
&gt; &gt;&gt;&gt; a while, and I thought I could implement this on top of your series,
&gt; &gt;&gt;&gt; but I have to admit I haven&#x27;t really looked into the details all
&gt; &gt;&gt;&gt; that much.
&gt; &gt;&gt;
&gt; &gt;&gt; There is nothing about registered buffers in this series. And even
&gt; &gt;&gt; if you try to reuse buffer allocation out of it, it&#x27;ll come with
&gt; &gt;&gt; a circular buffer you&#x27;ll have no need for. And I&#x27;m pretty much
&gt; &gt;
&gt; &gt; I think the circular buffer will be useful for Christoph&#x27;s use case in
&gt; &gt; the same way it&#x27;ll be useful for fuse&#x27;s. The read payload could be
&gt; &gt; differently sized across requests, so it&#x27;s a lot of wasted space to
&gt; &gt; have to allocate a buffer large enough to support the max-size request
&gt; &gt; per entry in the io_ring. With using a circular buffer, buffers have a
&gt; &gt; way to be shared across entries, which means we can significantly
&gt; &gt; reduce how much memory needs to be allocated.
&gt;
&gt; Dunno, what we actually want is requests of multiple sizes. Sharing
&gt; buffers across entries sounds like just reducing the ring size - I
&gt; personally don&#x27;t see the point here.

By &quot;sharing buffers across entries&quot; what I mean is different regions
of the buffer can now be used concurrently by multiple entries.

Thanks,
Joanne
&gt;
&gt;
&gt; Thanks,
&gt; Bernd
</pre>
</details>
<div class="review-comment-signals">Signals: no clear signal</div>
</div>
</div>
</div>
</div>
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Jens Axboe</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-18">2026-02-18</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Jens Axboe raised several minor issues and suggestions for improvement in the patch series, including adding a WARN_ON_ONCE() check to avoid int promotion, using req-&gt;buf_index instead of returning buffer id, and refactoring code for better readability.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On 2/9/26 5:28 PM, Joanne Koong wrote:
&gt; +int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,
&gt; +			   u64 addr, unsigned int len, unsigned int bid,
&gt; +			   unsigned int issue_flags)
&gt; +{
&gt; +	struct io_kiocb *req = cmd_to_io_kiocb(cmd);
&gt; +	struct io_ring_ctx *ctx = req-&gt;ctx;
&gt; +	struct io_uring_buf_ring *br;
&gt; +	struct io_uring_buf *buf;
&gt; +	struct io_buffer_list *bl;
&gt; +	int ret = -EINVAL;
&gt; +
&gt; +	if (WARN_ON_ONCE(req-&gt;flags &amp; REQ_F_BUFFERS_COMMIT))
&gt; +		return ret;
&gt; +
&gt; +	io_ring_submit_lock(ctx, issue_flags);
&gt; +
&gt; +	bl = io_buffer_get_list(ctx, buf_group);
&gt; +
&gt; +	if (!bl || WARN_ON_ONCE(!(bl-&gt;flags &amp; IOBL_BUF_RING)) ||
&gt; +	    WARN_ON_ONCE(!(bl-&gt;flags &amp; IOBL_KERNEL_MANAGED)))
&gt; +		goto done;
&gt; +
&gt; +	br = bl-&gt;buf_ring;
&gt; +
&gt; +	if (WARN_ON_ONCE((br-&gt;tail - bl-&gt;head) &gt;= bl-&gt;nr_entries))
&gt; +		goto done;

I think you want:

	if (WARN_ON_ONCE((__u16)(br-&gt;tail - bl-&gt;head) &gt;= bl-&gt;nr_entries))

here to avoid int promotion from messing this up if tail has wrapped.

In general, across the patches for the WARN_ON_ONCE(), it&#x27;s not a huge
issue to have a litter of them for now. Hopefully we can prune some of
these down the line, however.

-- 
Jens Axboe


---

On 2/9/26 5:28 PM, Joanne Koong wrote:
&gt; Return the id of the selected buffer in io_buffer_select(). This is
&gt; needed for kernel-managed buffer rings to later recycle the selected
&gt; buffer.
&gt; 
&gt; Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
&gt; ---
&gt;  include/linux/io_uring/cmd.h   | 2 +-
&gt;  include/linux/io_uring_types.h | 2 ++
&gt;  io_uring/kbuf.c                | 7 +++++--
&gt;  3 files changed, 8 insertions(+), 3 deletions(-)
&gt; 
&gt; diff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h
&gt; index d4b5943bdeb1..94df2bdebe77 100644
&gt; --- a/include/linux/io_uring/cmd.h
&gt; +++ b/include/linux/io_uring/cmd.h
&gt; @@ -71,7 +71,7 @@ void io_uring_cmd_issue_blocking(struct io_uring_cmd *ioucmd);
&gt;  
&gt;  /*
&gt;   * Select a buffer from the provided buffer group for multishot uring_cmd.
&gt; - * Returns the selected buffer address and size.
&gt; + * Returns the selected buffer address, size, and id.
&gt;   */
&gt;  struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,
&gt;  					    unsigned buf_group, size_t *len,
&gt; diff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h
&gt; index 36cc2e0346d9..5a56bb341337 100644
&gt; --- a/include/linux/io_uring_types.h
&gt; +++ b/include/linux/io_uring_types.h
&gt; @@ -100,6 +100,8 @@ struct io_br_sel {
&gt;  		void *kaddr;
&gt;  	};
&gt;  	ssize_t val;
&gt; +	/* id of the selected buffer */
&gt; +	unsigned buf_id;
&gt;  };

I&#x27;m probably missing something here, but why can&#x27;t the caller just use
req-&gt;buf_index for this?

-- 
Jens Axboe


---

On 2/9/26 5:28 PM, Joanne Koong wrote:
&gt; Currently, io_uring buffer rings require the application to allocate and
&gt; manage the backing buffers. This series introduces kernel-managed buffer
&gt; rings, where the kernel allocates and manages the buffers on behalf of
&gt; the application.
&gt; 
&gt; This is split out from the fuse over io_uring series in [1], which needs the
&gt; kernel to own and manage buffers shared between the fuse server and the
&gt; kernel.
&gt; 
&gt; This series is on top of the for-next branch in Jens&#x27; io-uring tree. The
&gt; corresponding liburing changes are in [2] and will be submitted after the
&gt; changes in this patchset are accepted.

Generally looks pretty good - for context, do you have a branch with
these patches and the users on top too? Makes it a bit easier for cross
referencing, as some of these really do need an exposed user to make a
good judgement on the helpers.

I know there&#x27;s the older series, but I&#x27;m assuming the latter patches
changed somewhat too, and it&#x27;d be nicer to look at a current set rather
than go back to the older ones.

-- 
Jens Axboe


---

On 2/9/26 5:28 PM, Joanne Koong wrote:
&gt; Add support for mmapping kernel-managed buffer rings (kmbuf) to
&gt; userspace, allowing applications to access the kernel-allocated buffers.
&gt; 
&gt; Similar to application-provided buffer rings (pbuf), kmbuf rings use the
&gt; buffer group ID encoded in the mmap offset to identify which buffer ring
&gt; to map. The implementation follows the same pattern as pbuf rings.
&gt; 
&gt; New mmap offset constants are introduced:
&gt;   - IORING_OFF_KMBUF_RING (0x88000000): Base offset for kmbuf mappings
&gt;   - IORING_OFF_KMBUF_SHIFT (16): Shift value to encode buffer group ID
&gt; 
&gt; The mmap offset encodes the bgid shifted by IORING_OFF_KMBUF_SHIFT.
&gt; The io_buf_get_region() helper retrieves the appropriate region.
&gt; 
&gt; This allows userspace to mmap the kernel-allocated buffer region and
&gt; access the buffers directly.
&gt; 
&gt; Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
&gt; ---
&gt;  include/uapi/linux/io_uring.h |  2 ++
&gt;  io_uring/kbuf.c               | 11 +++++++++--
&gt;  io_uring/kbuf.h               |  5 +++--
&gt;  io_uring/memmap.c             |  5 ++++-
&gt;  4 files changed, 18 insertions(+), 5 deletions(-)
&gt; 
&gt; diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
&gt; index a0889c1744bd..42a2812c9922 100644
&gt; --- a/include/uapi/linux/io_uring.h
&gt; +++ b/include/uapi/linux/io_uring.h
&gt; @@ -545,6 +545,8 @@ struct io_uring_cqe {
&gt;  #define IORING_OFF_SQES			0x10000000ULL
&gt;  #define IORING_OFF_PBUF_RING		0x80000000ULL
&gt;  #define IORING_OFF_PBUF_SHIFT		16
&gt; +#define IORING_OFF_KMBUF_RING		0x88000000ULL
&gt; +#define IORING_OFF_KMBUF_SHIFT		16
&gt;  #define IORING_OFF_MMAP_MASK		0xf8000000ULL
&gt;  
&gt;  /*
&gt; diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
&gt; index 9bc36451d083..ccf5b213087b 100644
&gt; --- a/io_uring/kbuf.c
&gt; +++ b/io_uring/kbuf.c
&gt; @@ -770,16 +770,23 @@ int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg)
&gt;  	return 0;
&gt;  }
&gt;  
&gt; -struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,
&gt; -					    unsigned int bgid)
&gt; +struct io_mapped_region *io_buf_get_region(struct io_ring_ctx *ctx,
&gt; +					   unsigned int bgid,
&gt; +					   bool kernel_managed)
&gt;  {
&gt;  	struct io_buffer_list *bl;
&gt; +	bool is_kernel_managed;
&gt;  
&gt;  	lockdep_assert_held(&amp;ctx-&gt;mmap_lock);
&gt;  
&gt;  	bl = xa_load(&amp;ctx-&gt;io_bl_xa, bgid);
&gt;  	if (!bl || !(bl-&gt;flags &amp; IOBL_BUF_RING))
&gt;  		return NULL;
&gt; +
&gt; +	is_kernel_managed = !!(bl-&gt;flags &amp; IOBL_KERNEL_MANAGED);
&gt; +	if (is_kernel_managed != kernel_managed)
&gt; +		return NULL;
&gt; +
&gt;  	return &amp;bl-&gt;region;
&gt;  }

For this, I think just add another helper - leave io_pbuf_get_region()
and add a bl-&gt;flags &amp; IOBL_KERNEL_MANAGED error check in there, and
add a io_kbuf_get_region() or similar and have a !(bl-&gt;flags &amp;
IOBL_KERNEL_MANAGED) error check in that one.

That&#x27;s easier to read, and there&#x27;s little reason to avoid duplicating
the xa_load() part.

Minor nit, but imho it&#x27;s more readable that way.

-- 
Jens Axboe


---

On 2/9/26 5:28 PM, Joanne Koong wrote:
&gt; +int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,
&gt; +			  unsigned issue_flags, struct io_buffer_list **bl)
&gt; +{
&gt; +	struct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)-&gt;ctx;
&gt; +	struct io_buffer_list *buffer_list;
&gt; +	int ret = -EINVAL;

Probably use the usual struct io_buffer_list *bl here and either use an
ERR_PTR return, or rename the passed on **bl to **blret or something.

&gt; +int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,
&gt; +		       unsigned issue_flags)
&gt; +{
&gt; +	struct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)-&gt;ctx;
&gt; +	struct io_buffer_list *bl;
&gt; +	int ret = -EINVAL;
&gt; +
&gt; +	io_ring_submit_lock(ctx, issue_flags);
&gt; +
&gt; +	bl = io_buffer_get_list(ctx, buf_group);
&gt; +	if (bl &amp;&amp; (bl-&gt;flags &amp; IOBL_BUF_RING) &amp;&amp; (bl-&gt;flags &amp; IOBL_PINNED)) {

Usually done as:

	if ((bl-&gt;flags &amp; (IOBL_BUF_RING|IOBL_PINNED)) == (IOBL_BUF_RING|IOBL_PINNED))

and maybe then just have an earlier

	if (!bl)
		goto err;

&gt; +		bl-&gt;flags &amp;= ~IOBL_PINNED;
&gt; +		ret = 0;
&gt; +	}
err:
&gt; +	io_ring_submit_unlock(ctx, issue_flags);
&gt; +	return ret;
&gt; +}

to avoid making it way too long. For io_uring, it&#x27;s fine to exceed 80
chars where it makes sense.

-- 
Jens Axboe
</pre>
</details>
<div class="review-comment-signals">Signals: minor issues, suggestions</div>
</div>
</div>
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Pavel Begunkov</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-18">2026-02-18</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Pavel Begunkov raised concerns about the kernel-managed buffer rings, specifically that they should not be responsible for allocating buffers. He suggested stripping buffer allocation from IORING_REGISTER_KMBUF_RING and instead requiring users to register a memory region of appropriate size.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On 2/10/26 00:28, Joanne Koong wrote:
&gt; Add support for kernel-managed buffer rings (kmbuf rings), which allow
&gt; the kernel to allocate and manage the backing buffers for a buffer
&gt; ring, rather than requiring the application to provide and manage them.
&gt; 
&gt; This introduces two new registration opcodes:
&gt; - IORING_REGISTER_KMBUF_RING: Register a kernel-managed buffer ring
&gt; - IORING_UNREGISTER_KMBUF_RING: Unregister a kernel-managed buffer ring
&gt; 
&gt; The existing io_uring_buf_reg structure is extended with a union to
&gt; support both application-provided buffer rings (pbuf) and kernel-managed
&gt; buffer rings (kmbuf):
&gt; - For pbuf rings: ring_addr specifies the user-provided ring address
&gt; - For kmbuf rings: buf_size specifies the size of each buffer. buf_size
&gt;    must be non-zero and page-aligned.
&gt; 
&gt; The implementation follows the same pattern as pbuf ring registration,
&gt; reusing the validation and buffer list allocation helpers introduced in
&gt; earlier refactoring. The IOBL_KERNEL_MANAGED flag marks buffer lists as
&gt; kernel-managed for appropriate handling in the I/O path.
&gt; 
&gt; Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
&gt; ---
&gt;   include/uapi/linux/io_uring.h |  15 ++++-
&gt;   io_uring/kbuf.c               |  81 ++++++++++++++++++++++++-
&gt;   io_uring/kbuf.h               |   7 ++-
&gt;   io_uring/memmap.c             | 111 ++++++++++++++++++++++++++++++++++
&gt;   io_uring/memmap.h             |   4 ++
&gt;   io_uring/register.c           |   7 +++
&gt;   6 files changed, 219 insertions(+), 6 deletions(-)
&gt; 
&gt; diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
&gt; index fc473af6feb4..a0889c1744bd 100644
&gt; --- a/include/uapi/linux/io_uring.h
&gt; +++ b/include/uapi/linux/io_uring.h
&gt; @@ -715,6 +715,10 @@ enum io_uring_register_op {
&gt;   	/* register bpf filtering programs */
&gt;   	IORING_REGISTER_BPF_FILTER		= 37,
&gt;   
&gt; +	/* register/unregister kernel-managed ring buffer group */
&gt; +	IORING_REGISTER_KMBUF_RING		= 38,
&gt; +	IORING_UNREGISTER_KMBUF_RING		= 39,
&gt; +
&gt;   	/* this goes last */
&gt;   	IORING_REGISTER_LAST,
&gt;   
&gt; @@ -891,9 +895,16 @@ enum io_uring_register_pbuf_ring_flags {
&gt;   	IOU_PBUF_RING_INC	= 2,
&gt;   };
&gt;   
&gt; -/* argument for IORING_(UN)REGISTER_PBUF_RING */
&gt; +/* argument for IORING_(UN)REGISTER_PBUF_RING and
&gt; + * IORING_(UN)REGISTER_KMBUF_RING
&gt; + */
&gt;   struct io_uring_buf_reg {
&gt; -	__u64	ring_addr;
&gt; +	union {
&gt; +		/* used for pbuf rings */
&gt; +		__u64	ring_addr;
&gt; +		/* used for kmbuf rings */
&gt; +		__u32   buf_size;

If you&#x27;re creating a region, there should be no reason why it
can&#x27;t work with user passed memory. You&#x27;re fencing yourself off
optimisations that are already there like huge pages.

&gt; +	};
&gt;   	__u32	ring_entries;
&gt;   	__u16	bgid;
&gt;   	__u16	flags;
&gt; diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
&gt; index aa9b70b72db4..9bc36451d083 100644
&gt; --- a/io_uring/kbuf.c
&gt; +++ b/io_uring/kbuf.c
...
&gt; +static int io_setup_kmbuf_ring(struct io_ring_ctx *ctx,
&gt; +			       struct io_buffer_list *bl,
&gt; +			       struct io_uring_buf_reg *reg)
&gt; +{
&gt; +	struct io_uring_buf_ring *ring;
&gt; +	unsigned long ring_size;
&gt; +	void *buf_region;
&gt; +	unsigned int i;
&gt; +	int ret;
&gt; +
&gt; +	/* allocate pages for the ring structure */
&gt; +	ring_size = flex_array_size(ring, bufs, bl-&gt;nr_entries);
&gt; +	ring = kzalloc(ring_size, GFP_KERNEL_ACCOUNT);
&gt; +	if (!ring)
&gt; +		return -ENOMEM;
&gt; +
&gt; +	ret = io_create_region_multi_buf(ctx, &amp;bl-&gt;region, bl-&gt;nr_entries,
&gt; +					 reg-&gt;buf_size);

Please use io_create_region(), the new function does nothing new
and only violates abstractions.

Provided buffer rings with kernel addresses could be an interesting
abstraction, but why is it also responsible for allocating buffers?
What I&#x27;d do:

1. Strip buffer allocation from IORING_REGISTER_KMBUF_RING.
2. Replace *_REGISTER_KMBUF_RING with *_REGISTER_PBUF_RING + a new flag.
    Or maybe don&#x27;t expose it to the user at all and create it from
    fuse via internal API.
3. Require the user to register a memory region of appropriate size,
    see IORING_REGISTER_MEM_REGION, ctx-&gt;param_region. Make fuse
    populating the buffer ring using the memory region.

I wanted to make regions shareable anyway (need it for other purposes),
I can toss patches for that tomorrow.

A separate question is whether extending buffer rings is the right
approach as it seems like you&#x27;re only using it for fuse requests and
not for passing buffers to normal requests, but I don&#x27;t see the
big picture here.

&gt; +	if (ret) {
&gt; +		kfree(ring);
&gt; +		return ret;
&gt; +	}
&gt; +
&gt; +	/* initialize ring buf entries to point to the buffers */
&gt; +	buf_region = bl-&gt;region.ptr;

io_region_get_ptr()

&gt; +	for (i = 0; i &lt; bl-&gt;nr_entries; i++) {
&gt; +		struct io_uring_buf *buf = &amp;ring-&gt;bufs[i];
&gt; +
&gt; +		buf-&gt;addr = (u64)(uintptr_t)buf_region;
&gt; +		buf-&gt;len = reg-&gt;buf_size;
&gt; +		buf-&gt;bid = i;
&gt; +
&gt; +		buf_region += reg-&gt;buf_size;
&gt; +	}
&gt; +	ring-&gt;tail = bl-&gt;nr_entries;
&gt; +
&gt; +	bl-&gt;buf_ring = ring;
&gt; +	bl-&gt;flags |= IOBL_KERNEL_MANAGED;
&gt; +
&gt; +	return 0;
&gt; +}
&gt; +
&gt; +int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
&gt; +{
&gt; +	struct io_uring_buf_reg reg;
&gt; +	struct io_buffer_list *bl;
&gt; +	int ret;
&gt; +
&gt; +	lockdep_assert_held(&amp;ctx-&gt;uring_lock);
&gt; +
&gt; +	ret = io_copy_and_validate_buf_reg(arg, &amp;reg, 0);
&gt; +	if (ret)
&gt; +		return ret;
&gt; +
&gt; +	if (!reg.buf_size || !PAGE_ALIGNED(reg.buf_size))

With io_create_region_multi_buf() gone, you shouldn&#x27;t need
to align every buffer, that could be a lot of wasted memory
(thinking about 64KB pages).

&gt; +		return -EINVAL;
&gt; +
&gt; +	bl = io_alloc_new_buffer_list(ctx, &amp;reg);
&gt; +	if (IS_ERR(bl))
&gt; +		return PTR_ERR(bl);
&gt; +
&gt; +	ret = io_setup_kmbuf_ring(ctx, bl, &amp;reg);
&gt; +	if (ret) {
&gt; +		kfree(bl);
&gt; +		return ret;
&gt; +	}
&gt; +
&gt; +	ret = io_buffer_add_list(ctx, bl, reg.bgid);
&gt; +	if (ret)
&gt; +		io_put_bl(ctx, bl);
&gt; +
&gt; +	return ret;

-- 
Pavel Begunkov



---

On 2/10/26 19:39, Joanne Koong wrote:
&gt; On Tue, Feb 10, 2026 at 8:34\u202fAM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
...
&gt;&gt;&gt; -/* argument for IORING_(UN)REGISTER_PBUF_RING */
&gt;&gt;&gt; +/* argument for IORING_(UN)REGISTER_PBUF_RING and
&gt;&gt;&gt; + * IORING_(UN)REGISTER_KMBUF_RING
&gt;&gt;&gt; + */
&gt;&gt;&gt;    struct io_uring_buf_reg {
&gt;&gt;&gt; -     __u64   ring_addr;
&gt;&gt;&gt; +     union {
&gt;&gt;&gt; +             /* used for pbuf rings */
&gt;&gt;&gt; +             __u64   ring_addr;
&gt;&gt;&gt; +             /* used for kmbuf rings */
&gt;&gt;&gt; +             __u32   buf_size;
&gt;&gt;
&gt;&gt; If you&#x27;re creating a region, there should be no reason why it
&gt;&gt; can&#x27;t work with user passed memory. You&#x27;re fencing yourself off
&gt;&gt; optimisations that are already there like huge pages.
&gt; 
&gt; Are there any optimizations with user-allocated buffers that wouldn&#x27;t
&gt; be possible with kernel-allocated buffers? For huge pages, can&#x27;t the
&gt; kernel do this as well (eg I see in io_mem_alloc_compound(), it calls
&gt; into alloc_pages() with order &gt; 0)?

Yes, there is handful of differences. To name one, 1MB allocation won&#x27;t
get you a PMD mappable huge page, while user space can allocate 2MB,
register the first 1MB and reuse the rest for other purposes.

&gt;&gt;&gt; +     };
&gt;&gt;&gt;        __u32   ring_entries;
&gt;&gt;&gt;        __u16   bgid;
&gt;&gt;&gt;        __u16   flags;
&gt;&gt;&gt; diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
&gt;&gt;&gt; index aa9b70b72db4..9bc36451d083 100644
&gt;&gt;&gt; --- a/io_uring/kbuf.c
&gt;&gt;&gt; +++ b/io_uring/kbuf.c
&gt;&gt; ...
&gt;&gt;&gt; +static int io_setup_kmbuf_ring(struct io_ring_ctx *ctx,
&gt;&gt;&gt; +                            struct io_buffer_list *bl,
&gt;&gt;&gt; +                            struct io_uring_buf_reg *reg)
&gt;&gt;&gt; +{
&gt;&gt;&gt; +     struct io_uring_buf_ring *ring;
&gt;&gt;&gt; +     unsigned long ring_size;
&gt;&gt;&gt; +     void *buf_region;
&gt;&gt;&gt; +     unsigned int i;
&gt;&gt;&gt; +     int ret;
&gt;&gt;&gt; +
&gt;&gt;&gt; +     /* allocate pages for the ring structure */
&gt;&gt;&gt; +     ring_size = flex_array_size(ring, bufs, bl-&gt;nr_entries);
&gt;&gt;&gt; +     ring = kzalloc(ring_size, GFP_KERNEL_ACCOUNT);
&gt;&gt;&gt; +     if (!ring)
&gt;&gt;&gt; +             return -ENOMEM;
&gt;&gt;&gt; +
&gt;&gt;&gt; +     ret = io_create_region_multi_buf(ctx, &amp;bl-&gt;region, bl-&gt;nr_entries,
&gt;&gt;&gt; +                                      reg-&gt;buf_size);
&gt;&gt;
&gt;&gt; Please use io_create_region(), the new function does nothing new
&gt;&gt; and only violates abstractions.
&gt; 
&gt; There&#x27;s separate checks needed between io_create_region() and
&gt; io_create_region_multi_buf() (eg IORING_MEM_REGION_TYPE_USER flag

If io_create_region() is too strict, let&#x27;s discuss that in
examples if there are any, but it&#x27;s likely not a good idea changing
that. If it&#x27;s too lax, filter arguments in the caller. IOW, don&#x27;t
pass IORING_MEM_REGION_TYPE_USER if it&#x27;s not used.

&gt; checking) and different allocation calls (eg
&gt; io_region_allocate_pages() vs io_region_allocate_pages_multi_buf()).

I saw that and saying that all memmap.c changes can get dropped.
You&#x27;re using it as one big virtually contig kernel memory range then
chunked into buffers, and that&#x27;s pretty much what you&#x27;re getting with
normal io_create_region(). I get that you only need it to be
contiguous within a single buffer, but that&#x27;s not what you&#x27;re doing,
and it&#x27;ll be only worse than default io_create_region() e.g.
effectively disabling any usefulness of io_mem_alloc_compound(),
and ultimately you don&#x27;t need to care.

Regions shouldn&#x27;t know anything about your buffers, how it&#x27;s
subdivided after, etc.

&gt; Maybe I&#x27;m misinterpreting your comment (or the code), but I&#x27;m not
&gt; seeing how this can just use io_create_region().

struct io_uring_region_desc rd = {};
total_size = nr_bufs * buf_size;
rd.size = PAGE_ALIGN(total_size);
io_create_region(&amp;region, &amp;rd);

Add something like this for user provided memory:

if (use_user_memory) {
	rd.user_addr = uaddr;
	rd.flags |= IORING_MEM_REGION_TYPE_USER;
}


&gt;&gt; Provided buffer rings with kernel addresses could be an interesting
&gt;&gt; abstraction, but why is it also responsible for allocating buffers?
&gt; 
&gt; Conceptually, I think it makes the interface and lifecycle management
&gt; simpler/cleaner. With registering it from userspace, imo there&#x27;s
&gt; additional complications with no tangible benefits, eg it&#x27;s not
&gt; guaranteed that the memory regions registered for the buffers are the
&gt; same size, with allocating it from the kernel-side we can guarantee
&gt; that the pages are allocated physically contiguously, userspace setup
&gt; with user-allocated buffers is less straightforward, etc. In general,
&gt; I&#x27;m just not really seeing what advantages there are in allocating the
&gt; buffers from userspace. Could you elaborate on that part more?

I don&#x27;t think I follow. I&#x27;m saying that it might be interesting
to separate rings from how and with what they&#x27;re populated on the
kernel API level, but the fuse kernel module can do the population
and get exactly same layout as you currently have:

int fuse_create_ring(size_t region_offset /* user space argument */) {
	struct io_mapped_region *mr = get_mem_region(ctx);
	// that can take full control of the ring
	ring = grab_empty_ring(io_uring_ctx);

	size = nr_bufs * buf_size;
	if (region_offset + size &gt; get_size(mr)) // + other validation
		return error;

	buf = mr_get_ptr(mr) + offset;
	for (i = 0; i &lt; nr_bufs; i++) {
		ring_push_buffer(ring, buf, buf_size);
		buf += buf_size;
	}
}

fuse might not care, but with empty rings other users will get a
channel they can use to do IO (e.g. read requests) using their
kernel addresses in the future. 	

&gt;&gt; What I&#x27;d do:
&gt;&gt;
&gt;&gt; 1. Strip buffer allocation from IORING_REGISTER_KMBUF_RING.
&gt;&gt; 2. Replace *_REGISTER_KMBUF_RING with *_REGISTER_PBUF_RING + a new flag.
&gt;&gt;      Or maybe don&#x27;t expose it to the user at all and create it from
&gt;&gt;      fuse via internal API.
&gt; 
&gt; If kmbuf rings are squashed into pbuf rings, then pbuf rings will need
&gt; to support pinning. In fuse, there are some contexts where you can&#x27;t

It&#x27;d change uapi but not internals, you already piggy back it
on pbuf implementation and differentiate with a flag.

It could basically be:

if (flags &amp; IOU_PBUF_RING_KM)
	bl-&gt;flags |= IOBL_KERNEL_MANAGED;

Pinning can be gated on that flag as well. Pretty likely uapi
and internals will be a bit cleaner, but that&#x27;s not a huge deal,
just don&#x27;t see why would you roll out a separate set of uapi
([un]register, offsets, etc.) when essentially it can be treated
as the same thing.

&gt; grab the uring mutex because you&#x27;re running in atomic context and this
&gt; can be encountered while recycling the buffer. I originally had a
&gt; patch adding pinning to pbuf rings (to mitigate the overhead of
&gt; registered buffers lookups) 

IIRC, you was pinning the registered buffer table and not provided
buffer rings? Which would indeed be a bad idea. Thinking about it,
fwiw, instead of creating multiple registered buffers and trying to
lock the entire table, you could&#x27;ve kept all memory in one larger
registered buffer and pinned only it. It&#x27;s already refcounted, so
shouldn&#x27;t have been much of a problem.

&gt; but dropped it when Jens and Caleb didn&#x27;t
&gt; like the idea. But for kmbuf rings, pinning will be necessary for
&gt; fuse.
&gt; 
&gt;&gt; 3. Require the user to register a memory region of appropriate size,
&gt;&gt;      see IORING_REGISTER_MEM_REGION, ctx-&gt;param_region. Make fuse
&gt;&gt;      populating the buffer ring using the memory region.

To explain why, I don&#x27;t think that creating many small regions
is a good direction going forward. In case of kernel allocation,
it&#x27;s extra mmap()s, extra user space management, and wasted space.
For user provided memory it&#x27;s over-accounting and extra memory
footprint. It&#x27;ll also give you better lifecycle guarantees, i.e.
you won&#x27;t be able to free buffers while there are requests for the
context. I&#x27;m not so sure about ring bound memory, let&#x27;s say I have
my suspicions, and you&#x27;d need to be extra careful about buffer
lifetimes even after a fuse instance dies.

&gt;&gt; I wanted to make regions shareable anyway (need it for other purposes),
&gt;&gt; I can toss patches for that tomorrow.
&gt;&gt;
&gt;&gt; A separate question is whether extending buffer rings is the right
&gt;&gt; approach as it seems like you&#x27;re only using it for fuse requests and
&gt;&gt; not for passing buffers to normal requests, but I don&#x27;t see the
&gt; 
&gt; What are &#x27;normal requests&#x27;? For fuse&#x27;s use case, there are only fuse requests.

Any kind of read/recv/etc. that can use provided buffers. It&#x27;s
where kernel memory filled rings would shine, as you&#x27;d be able
to use them together without changing any opcode specific code.
I.e. not changes in read request implementation, only kbuf.c

-- 
Pavel Begunkov



---

On 2/11/26 22:06, Joanne Koong wrote:
&gt; On Wed, Feb 11, 2026 at 4:01\u202fAM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt;&gt; On 2/10/26 19:39, Joanne Koong wrote:
&gt;&gt;&gt; On Tue, Feb 10, 2026 at 8:34\u202fAM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
...
&gt;&gt;&gt; checking) and different allocation calls (eg
&gt;&gt;&gt; io_region_allocate_pages() vs io_region_allocate_pages_multi_buf()).
&gt;&gt;
&gt;&gt; I saw that and saying that all memmap.c changes can get dropped.
&gt;&gt; You&#x27;re using it as one big virtually contig kernel memory range then
&gt;&gt; chunked into buffers, and that&#x27;s pretty much what you&#x27;re getting with
&gt;&gt; normal io_create_region(). I get that you only need it to be
&gt;&gt; contiguous within a single buffer, but that&#x27;s not what you&#x27;re doing,
&gt;&gt; and it&#x27;ll be only worse than default io_create_region() e.g.
&gt;&gt; effectively disabling any usefulness of io_mem_alloc_compound(),
&gt;&gt; and ultimately you don&#x27;t need to care.
&gt; 
&gt; When I originally implemented it, I had it use
&gt; io_region_allocate_pages() but this fails because it&#x27;s allocating way
&gt; too much memory at once. For fuse&#x27;s use case, each buffer is usually
&gt; at least 1 MB if not more. Allocating the memory one buffer a time in
&gt; io_region_allocate_pages_multi_buf() bypasses the allocation errors I
&gt; was seeing. That&#x27;s the main reason I don&#x27;t think this can just use
&gt; io_create_region().

Let&#x27;s fix that then. For now, just work it around by wrapping
into a loop.

Btw, I thought you&#x27;re going to use it for metadata like some
fuse headers and payloads would be zero copied by installing
it as registered buffers.

...
&gt;&gt;&gt;&gt; Provided buffer rings with kernel addresses could be an interesting
&gt;&gt;&gt;&gt; abstraction, but why is it also responsible for allocating buffers?
&gt;&gt;&gt;
&gt;&gt;&gt; Conceptually, I think it makes the interface and lifecycle management
&gt;&gt;&gt; simpler/cleaner. With registering it from userspace, imo there&#x27;s
&gt;&gt;&gt; additional complications with no tangible benefits, eg it&#x27;s not
&gt;&gt;&gt; guaranteed that the memory regions registered for the buffers are the
&gt;&gt;&gt; same size, with allocating it from the kernel-side we can guarantee
&gt;&gt;&gt; that the pages are allocated physically contiguously, userspace setup
&gt;&gt;&gt; with user-allocated buffers is less straightforward, etc. In general,
&gt;&gt;&gt; I&#x27;m just not really seeing what advantages there are in allocating the
&gt;&gt;&gt; buffers from userspace. Could you elaborate on that part more?
&gt;&gt;
&gt;&gt; I don&#x27;t think I follow. I&#x27;m saying that it might be interesting
&gt;&gt; to separate rings from how and with what they&#x27;re populated on the
&gt;&gt; kernel API level, but the fuse kernel module can do the population
&gt; 
&gt; Oh okay, from your first message I (and I think christoph too) thought
&gt; what you were saying is that the user should be responsible for
&gt; allocating the buffers with complete ownership over them, and then
&gt; just pass those allocated to the kernel to use. But what you&#x27;re saying
&gt; is that just use a different way for getting the kernel to allocate
&gt; the buffers (eg through the IORING_REGISTER_MEM_REGION interface). Am
&gt; I reading this correctly?

The main point is disentangling memory allocation from ring
creation in the io_uring uapi, and moving ring population
into fuse instead of doing it at creation. And it&#x27;ll still be
populated by the kernel (fuse), user space doesn&#x27;t have access
to the ring. IORING_REGISTER_MEM_REGION is just the easiest way
to achieve that without any extra uapi.

...
&gt;&gt; Pinning can be gated on that flag as well. Pretty likely uapi
&gt;&gt; and internals will be a bit cleaner, but that&#x27;s not a huge deal,
&gt;&gt; just don&#x27;t see why would you roll out a separate set of uapi
&gt;&gt; ([un]register, offsets, etc.) when essentially it can be treated
&gt;&gt; as the same thing.
&gt; 
&gt; imo, it looked cleaner as a separate api because it has different
&gt; expectations and behaviors and squashing kmbuf into the pbuf api makes
&gt; the pbuf api needlessly more complex. Though I guess from the

It appeared to me that they&#x27;re different because of special
region path and embedded buffer allocations, and otherwise
differences would be minimal. But if you think it&#x27;s still
better to be made as a separate opcode, I&#x27;m not opposing it,
go for it.

&gt; userspace pov, liburing could have a wrapper that takes care of
&gt; setting up the pbuf details for kernel-managed pbufs. But in my head,
&gt; having pbufs vs. kmbufs makes it clearer what each one does vs regular
&gt; pbufs vs. pbufs that are kernel-managed.
&gt; 
&gt; Especially with now having kmbufs go through the ioring mem region
&gt; interface, it makes things more confusing imo if they&#x27;re combined, eg
&gt; pbufs that are kernel-managed are created empty and then populated
&gt; from the kernel side by whatever subsystem is using them. Right now
&gt; there&#x27;s only one mem region supported per ring, but in the future if
&gt; there&#x27;s the possibility that multiple mem regions can be registered

That shouldn&#x27;t be a problem

&gt; (eg if userspace doesn&#x27;t know upfront what mem region length they&#x27;ll
&gt; need), then we should also probably add in a region id param for the
&gt; registration arg, which if kmbuf rings go through the pbuf ring
&gt; registration api, is not possible to do.

Not having patches using the functionality is inconvenient. How
fuse looks up the buffer ring from io_uring? I could imagine you
have some control path io-uring command:

case FUSE_CMD_BIND_BUFFER_RING:
	return bind_queue(params);

Then you can pass all necessary parameters to it, pseudo code:

struct fuse_bind_kmbuf_ring_params {
	region_id;
	buf_ring_id;
	...
};

bind_queue(cmd, struct fuse_bind_kmbuf_ring_params *p)
{
	region = io_uring_get_region(cmd, p-&gt;region_id);
	// get exclusive access:
	buf_ring = io_uring_get_buf_ring(cmd, p-&gt;buf_ring_id);

	if (!validate_buf_ring(buf_ring))
		return NOTSUPPORTED;

	io_uring_pin(buf_ring);
	fuse_populate_buf_ring(buf_ring, region, ...);
}

Does that match expectations? I don&#x27;t think you even need
the ring part exposed as an io_uring uapi, tbh, as it
stays completely in fuse and doesn&#x27;t meaningfully interact
with the rest of io_uring.

...
&gt;&gt;&gt;&gt; 3. Require the user to register a memory region of appropriate size,
&gt;&gt;&gt;&gt;       see IORING_REGISTER_MEM_REGION, ctx-&gt;param_region. Make fuse
&gt;&gt;&gt;&gt;       populating the buffer ring using the memory region.
&gt;&gt;
&gt;&gt; To explain why, I don&#x27;t think that creating many small regions
&gt;&gt; is a good direction going forward. In case of kernel allocation,
&gt;&gt; it&#x27;s extra mmap()s, extra user space management, and wasted space.
&gt; 
&gt; To clarify, is this in reply to why the individual buffers shouldn&#x27;t
&gt; be allocated separately by the kernel?

That was about an argument for using IORING_REGISTER_MEM_REGION
instead a separate region. And it&#x27;s separate from whether
buffers should be bound to the ring.

&gt; I added a comment about this above in the discussion about
&gt; io_region_allocate_pages_multi_buf(), and if the memory allocation
&gt; issue I was seeing is bypassable and the region can be allocated all
&gt; at once, I&#x27;m happy to make that change. With having the allocation be
&gt; separate buffers though, I&#x27;m not sure I agree that there are extra
&gt; mmaps / userspace management. All the pages across the buffers are
&gt; vmapped together and the userspace just needs to do 1 mmap call for
&gt; them. On the userspace side, I don&#x27;t think there&#x27;s more management
&gt; since the mmapped address represents the range across all the buffers.
&gt; I&#x27;m not seeing how there&#x27;s wasted space either since the only

I shouldn&#x27;t affect you much since you have such large buffers,
but imagine the total allocation size is not being pow2, and
the kernel allocating it as a single folio. E.g. 3 buffers,
0.5 MB each, total = 1.5MB, and the kernel allocates a 2MB
huge page.

-- 
Pavel Begunkov



---

On 2/12/26 17:29, Joanne Koong wrote:
&gt; On Thu, Feb 12, 2026 at 2:52\u202fAM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt;&gt;
&gt;&gt; On 2/12/26 10:07, Christoph Hellwig wrote:
&gt;&gt;&gt; On Wed, Feb 11, 2026 at 02:06:18PM -0800, Joanne Koong wrote:
&gt;&gt;&gt;&gt;&gt; I don&#x27;t think I follow. I&#x27;m saying that it might be interesting
&gt;&gt;&gt;&gt;&gt; to separate rings from how and with what they&#x27;re populated on the
&gt;&gt;&gt;&gt;&gt; kernel API level, but the fuse kernel module can do the population
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; Oh okay, from your first message I (and I think christoph too) thought
&gt;&gt;&gt;&gt; what you were saying is that the user should be responsible for
&gt;&gt;&gt;&gt; allocating the buffers with complete ownership over them, and then
&gt;&gt;&gt;&gt; just pass those allocated to the kernel to use. But what you&#x27;re saying
&gt;&gt;&gt;&gt; is that just use a different way for getting the kernel to allocate
&gt;&gt;&gt;&gt; the buffers (eg through the IORING_REGISTER_MEM_REGION interface). Am
&gt;&gt;&gt;&gt; I reading this correctly?
&gt;&gt;&gt;
&gt;&gt;&gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt;&gt;&gt; where the kernel controls the allocation fully and guarantees user
&gt;&gt;&gt; processes can only read the memory but never write to it.  I&#x27;d love
&gt; 
&gt; By &quot;control the allocation fully&quot; do you mean for your use case, the
&gt; allocation/setup isn&#x27;t triggered by userspace but is initiated by the
&gt; kernel (eg user never explicitly registers any kbuf ring, the kernel
&gt; just uses the kbuf ring data structure internally and users can read
&gt; the buffer contents)? If userspace initiates the setup of the kbuf
&gt; ring, going through IORING_REGISTER_MEM_REGION would be semantically
&gt; the same, except the buffer allocation by the kernel now happens
&gt; before the ring is created and then later populated into the ring.
&gt; userspace would still need to make an mmap call to the region and the
&gt; kernel could enforce that as read-only. But if userspace doesn&#x27;t
&gt; initiate the setup, then going through IORING_REGISTER_MEM_REGION gets
&gt; uglier.
&gt; 
&gt;&gt;&gt; to be able to piggy back than onto your work.
&gt;&gt;
&gt;&gt; IORING_REGISTER_MEM_REGION supports both types of allocations. It can
&gt;&gt; have a new registration flag for read-only, and then you either make
&gt;&gt; the bounce avoidance optional or reject binding fuse to unsupported
&gt;&gt; setups during init. Any arguments against that? I need to go over
&gt;&gt; Joanne&#x27;s reply, but I don&#x27;t see any contradiction in principal with
&gt;&gt; your use case.
&gt; 
&gt; So i guess the flow would have to be:
&gt; a) user calls io_uring_register_region(&amp;ring, &amp;mem_region_reg) with
&gt; mem_region_reg.region_uptr&#x27;s size field set to the total buffer size
&gt; (and mem_region_reg.flags read-only bit set if needed)
&gt;       kernel allocates region
&gt; b) user calls mmap() to get the address of the region. If read-only
&gt; bit was set, it gets a read-only address
&gt; c) user calls io_uring_register_buf_ring(&amp;ring, &amp;buf_reg, flags) with
&gt; buf_reg.flags |= IOU_PBUF_RING_KERNEL_MANAGED
&gt;       kernel creates an empty kernel-managed ring. None of the buffers
&gt; are populated
&gt; d) user tells X subsystem to populate the ring starting from offset Z
&gt; in the registered mem region
&gt; e) on the kernel side, the subsystem populates the ring starting from
&gt; offset Z, filling it up using the buf_size and ring_entries values
&gt; that the user registered the ring with in c)
&gt; 
&gt; To be completely honest, the more I look at this the more this feels
&gt; like overkill / over-engineered to me. I get that now the user can do
&gt; the PMD optimization, but does that actually lead to noticeable
&gt; performance benefits? It seems especially confusing with them going

No, it&#x27;s mainly about not keeping payload buffers and rings in the same
object from the io_uring uapi perspective.

1. If it&#x27;s an io_uring uapi, it shouldn&#x27;t be fuse specific or with
a bunch of use case specific expectations attached. Why does it
require all buffers to be uniform in size? Why does it require
the ring size to match the number of buffers? Why does it require
buffers to be allocated by io_uring in the first place? Maybe some
subsystem got memory from somewhere else and wants to do use it
with io_uring. Why does it need to know the total size at creation,
and what would you do if you want to add more memory at runtime
while using the same ring?

2. If it&#x27;s meant to be fuse specific and _not_ used with other requests
like recv/read/etc., then what&#x27;s the point of having it as an io_uring
uapi? Which also adds additional trouble like the once you&#x27;re solving
with pinning.

If it&#x27;s supposed to be used with other requests, then buffers and
rings will have different in-kernel lifetime expectations imposed
by io_uring, so having them together won&#x27;t even help with
management.

I have a strong opinion about the memmap.c change. For the
rest, if you believe it&#x27;s fine, just send it out and let Jens
decide.

&gt; through the same pbuf ring interface but having totally different
&gt; expectations.

It&#x27;s predicated on separating buffers from rings, see above,
and assuming that I&#x27;m not sure what expectations are different
apart from one being in-kernel with kernel addresses and the
other user visible with user addresses.

-- 
Pavel Begunkov



---

On 2/13/26 22:04, Joanne Koong wrote:
&gt; On Fri, Feb 13, 2026 at 4:41\u202fAM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
...
&gt;&gt; Fuse is doing both adding (kernel) buffers to the ring and consuming
&gt;&gt; them. At which point it&#x27;s not clear:
&gt;&gt;
&gt;&gt; 1. Why it even needs io_uring provided buffer rings, it can be all
&gt;&gt;      contained in fuse. Maybe it&#x27;s trying to reuse pbuf ring code as
&gt;&gt;      basically an internal memory allocator, but then why expose buffer
&gt;&gt;      rings as an io_uring uapi instead of keeping it internally.
&gt;&gt;
&gt;&gt;      That&#x27;s also why I mentioned whether those buffers are supposed to
&gt;&gt;      be used with other types of io_uring requests like recv, etc.
&gt; 
&gt; On the userspace/server side, it uses the buffers for other io-uring
&gt; operations (eg reading or writing the contents from/to a
&gt; locally-backed file).

Oops, typo. I was asking whether the buffer rings (not buffers) are
supposed to be used with other requests. E.g. submitting a
IORING_OP_RECV with IOSQE_BUFFER_SELECT set and the bgid specifying
your kernel-managed buffer ring.

&gt;&gt; 2. Why making io_uring to allocate payload memory. The answer to which
&gt;&gt;      is probably to reuse the region api with mmap and so on. And why
&gt;&gt;      payload buffers are inseparably created together with the ring
&gt; 
&gt; My main motivation for this is simplicity. I see (and thanks for
&gt; explaining) that using a registered mem region allows the use of some
&gt; optimizations (the only one I know of right now is the PMD one you
&gt; mentioned but maybe there&#x27;s more I&#x27;m missing) that could be useful for
&gt; some workloads, but I don&#x27;t think (and this could just be my lack of
&gt; understanding of what more optimizations there are) most use cases of
&gt; kmbufs benefit from those optimizations, so to me it feels like we&#x27;re
&gt; adding non-trivial complexity for no noticeable benefit.

There are two separate arguments. The first is about not making buffers
inseparable from buffer rings in the io_uring user API. Whether it&#x27;s
IORING_REGISTER_MEM_REGION or something else is not that important.
I have no objection if it&#x27;s a part of fuse instead though, e.g. if
fuse binds two objects together when you register it with fuse, or even
if fuse create a buffer ring internally (assuming it doesn&#x27;t indirectly
leak into io_uring uapi).

And the second was about optionally allowing user memory for buffer
creation as you&#x27;re reusing the region abstraction. You can find pros
and cons for both modes, and funnily enough, SQ/CQ were first kernel
allocated and then people asked for backing it by user memory, and IIRC
it was in the reverse order for pbuf rings.

Implementing this is trivial as well, you just need to pass an argument
while creating a region. All new region users use struct
io_uring_region_desc for uapi and forward it to io_create_region()
without caring if it&#x27;s user or kernel allocated memory.

&gt; I feel like we get the best of both worlds by letting users have both:
&gt; the simple kernel-managed pbuf where the kernel allocates the buffers
&gt; and the buffers are tied to the lifecycle of the ring, and the more
&gt; advanced kernel-managed pbuf where buffers are tied to a registered
&gt; memory region that the subsystem is responsible for later populating
&gt; the ring with.
&gt; 
&gt;&gt;      and via a new io_uring uapi.
&gt; 
&gt; imo it felt cleaner to have a new uapi for it because kmbufs and pbufs

The stress is on why it&#x27;s an _io_uring_ API. It doesn&#x27;t matter to me
whether it&#x27;s a separate opcode or not. Currently, buffer rings don&#x27;t give
you anything that can&#x27;t be pure fuse, and it might be simpler to have
it implemented in fuse than binding to some io_uring object. Or it could
create buffer rings internally to reuse code but it doesn&#x27;t become an
io_uring uapi but rather implementation detail. And that predicates on
whether km rings are intended to be used with other / non-fuse requests.

&gt; have different expectations and behaviors (eg pbufs only work with
&gt; user-provided buffers and requires userspace to populate the ring
&gt; before using it, whereas for kmbufs the kernel allocates the buffers
&gt; and populates it for you; pbufs require userspace to recycle back the
&gt; buffer, whereas for kmbufs the kernel is the one in control of
&gt; recycling) and from the user pov it seemed confusing to have kmbufs as
&gt; part of the pbuf ring uapi, instead of separating it out as a
&gt; different type of ringbuffer with a different expectation and

I believe the source of disagreement is that you&#x27;re thinking
about how it&#x27;s going to look like for fuse specifically, and I
believe you that it&#x27;ll be nicer for the fuse use case. However,
on the other hand it&#x27;s an io_uring uapi, and if it is an io_uring
uapi, we need reusable blocks that are not specific to particular
users.

If it km rings has to stay an io_uring uapi, I guess a middle
ground would be to allow registering km rings together with memory,
but make it a pure region without a notion of a buffer, and let
fuse to chunk it. Later, we can make payload memory allocation
optional.

&gt; behavior. I was trying to make the point that combining the interface
&gt; if we go with IORING_MEM_REGION gets even more confusing because now
&gt; pbufs that are kernel-managed are also empty at initialization and
&gt; only can point to areas inside a registered mem region and the
&gt; responsibility of populating it is now on whatever subsystem is using
&gt; it.

Right, intentionally so, because otherwise it&#x27;s a fuse uapi that
pretends to be a generic io_uring uapi but it&#x27;s not because of
all assumptions in different places.

&gt; I still have this opinion but I also think in general, you likely know
&gt; better than I do what kind of io-uring uapi is best for io-uring&#x27;s
&gt; users. For v2 I&#x27;ll have kmbufs go through the pbuf uapi.
&gt; 
&gt;&gt;
&gt;&gt;      And yes, I believe in the current form it&#x27;s inflexible, it requires
&gt;&gt;      a new io_uring uapi. It requires the number of buffers to match
&gt;&gt;      the number of ring entries, which are related but not the same
&gt; 
&gt; I&#x27;m not really seeing what the purpose of having a ring entry with no
&gt; buffer associated with it is. In the existing code for non-kernel
&gt; managed pbuf rings, there&#x27;s the same tie between reg-&gt;ring_entries
&gt; being used as the marker for how many buffers the ring supports. But

Not really, it tells the buffer ring depth but says nothing about
how much memory user space allocated and how it&#x27;s pushed. It&#x27;s a
reasonable default but they could be different. For example, if you
expect adding more memory at runtime, you might create the buffer
ring a bit larger. Or when server processing takes a while and you
can&#x27;t recycle until it finishes, you might have more buffers than
you need ring entries. Or you might might decide to split buffers
and as you mentioned incremental consumption, which is an entire
separate topic because it doesn&#x27;t do de-fragmentation and you&#x27;d
need to have it in fuse, just like user space does with pbufs.

&gt; if the number of buffers should be different than the number of ring
&gt; entries, this can be easily fixed by passing in the number of buffers
&gt; from the uapi for kernel-managed pbuf rings.

My entire point is that we&#x27;re making lots of assumptions for io_uring
uapi, and if it&#x27;s moved to fuse because it knows better what it
needs, it should be a win.

IOW, it sounds better if instead of passing the number of buffers to
io_uring, you just ask it to create a large chunk of memory, and then
fuse chunks it up and puts into the ring.

&gt;&gt;      thing. You can&#x27;t easily add more memory as it&#x27;s bound to the ring
&gt;&gt;      object. The buffer memory won&#x27;t even have same lifetime as the
&gt; 
&gt; To play devil&#x27;s advocate, we also can&#x27;t easily add more memory to the
&gt; mem region once it&#x27;s been registered. I think there&#x27;s also a worse
&gt; penalty where the user needs to know upfront how much memory to
&gt; allocate for the mem region for the lifetime of the ring, which imo
&gt; may be hard to do (eg if a kernel-managed buf ring only needs to be
&gt; registered for some code paths and not others, the mem region
&gt; registration would still have to allocate the memory a potential kbuf
&gt; ring would use).

I agree, and you&#x27;d need something new in either case to add more
memory, and it doesn&#x27;t need to be IORING_REGISTER_MEM_REGION
specifically.

&gt;&gt;      ring object -- allow using that km buffer ring with recv requests
&gt;&gt;      and highly likely I&#x27;ll most likely give you a way to crash the
&gt;&gt;      kernel.
&gt; 
&gt; I&#x27;m a bit confused by this part. The buffer memory does have the same
&gt; lifetime as the ring object, no? The buffers only get freed when the
&gt; ring itself is freed.

Unregistering a buffer ring doesn&#x27;t guarantee that there are no
inflight requests that are still using buffers that came out of
the buffer ring. The fuse driver can wait/terminate its requests
before unregisteration, but allow userspace issued IORING_OP_RECV
to use this km buffer ring, and you&#x27;ll need to somehow synchronise
with all other io_uring requests.

-- 
Pavel Begunkov



---

On 2/18/26 21:43, Joanne Koong wrote:
&gt; On Wed, Feb 18, 2026 at 4:36\u202fAM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt;&gt;
&gt;&gt; On 2/13/26 22:04, Joanne Koong wrote:
&gt;&gt;&gt; On Fri, Feb 13, 2026 at 4:41\u202fAM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt;&gt; ...
&gt;&gt;&gt;&gt; Fuse is doing both adding (kernel) buffers to the ring and consuming
&gt;&gt;&gt;&gt; them. At which point it&#x27;s not clear:
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; 1. Why it even needs io_uring provided buffer rings, it can be all
&gt;&gt;&gt;&gt;       contained in fuse. Maybe it&#x27;s trying to reuse pbuf ring code as
&gt;&gt;&gt;&gt;       basically an internal memory allocator, but then why expose buffer
&gt;&gt;&gt;&gt;       rings as an io_uring uapi instead of keeping it internally.
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;       That&#x27;s also why I mentioned whether those buffers are supposed to
&gt;&gt;&gt;&gt;       be used with other types of io_uring requests like recv, etc.
&gt;&gt;&gt;
&gt;&gt;&gt; On the userspace/server side, it uses the buffers for other io-uring
&gt;&gt;&gt; operations (eg reading or writing the contents from/to a
&gt;&gt;&gt; locally-backed file).
&gt;&gt;
&gt; 
&gt; Sorry, I submitted v2 last night thinking the conversation on this
&gt; thread had died. After reading through your reply, I&#x27;ll modify v2.

No worries at all, and sorry I&#x27;m a bit slow to reply

&gt;&gt; Oops, typo. I was asking whether the buffer rings (not buffers) are
&gt;&gt; supposed to be used with other requests. E.g. submitting a
&gt;&gt; IORING_OP_RECV with IOSQE_BUFFER_SELECT set and the bgid specifying
&gt;&gt; your kernel-managed buffer ring.
&gt; 
&gt; Yes the buffer rings are intended to be used with other io-uring
&gt; requests. The ideal scenario is that the user can then do the
&gt; equivalent of IORING_OP_READ/WRITE_FIXED operations on the
&gt; kernel-managed buffers and avoid the per-i/o page pinning overhead
&gt; costs.

You mention OP_READ_FIXED and below agreed not exposing km rings
an io_uring uapi, which makes me believe we&#x27;re still talking about
different things.

Correct me if I&#x27;m wrong. Currently, only fuse cmds use the buffer
ring itself, I&#x27;m not talking about buffer, i.e. fuse cmds consume
entries from the ring (!!! that&#x27;s the part I&#x27;m interested in), then
process them and tell the server &quot;this offset in the region has user
data to process or should be populated with data&quot;.

Naturally, the server should be able to use the buffers to issue
some I/O and process it in other ways, whether it&#x27;s a normal
OP_READ to which you pass the user space address (you can since
it&#x27;s mmap()&#x27;ed by the server) or something else is important but
a separate question than the one I&#x27;m trying to understand.

So I&#x27;m asking whether you expect that a server or other user space
program should be able to issue a READ_OP_RECV, READ_OP_READ or any
other similar request, which would consume buffers/entries from the
km ring without any fuse kernel code involved? Do you have some
use case for that in mind?

Understanding that is the key in deciding whether km rings should
be exposed as io_uring uapi or not, regardless of where buffers
to populate the ring come from.

...
&gt; With it going through a mem region, I don&#x27;t think it should even go
&gt; through the &quot;pbuf ring&quot; interface then if it&#x27;s not going to specify
&gt; the number of entries and buffer sizes upfront, if support is added
&gt; for io-uring normal requests (eg IORING_OP_READ/WRITE) to use the
&gt; backing pages from a memory region and if we&#x27;re able to guarantee that
&gt; the registered memory region will never be able to be unregistered by
&gt; the user. I think if we repurpose the
&gt; 
&gt; union {
&gt;    __u64 addr; /* pointer to buffer or iovecs */
&gt;    __u64 splice_off_in;
&gt; };
&gt; 
&gt; fields in the struct io_uring_sqe to
&gt; 
&gt; union {
&gt;    __u64 addr; /* pointer to buffer or iovecs */
&gt;    __u64 splice_off_in;
&gt;    __u64 offset; /* offset into registered mem region */
&gt; };
&gt; 
&gt; and add some IOSQE_ flag to indicate it should find the pages from the
&gt; registered mem region, then that should work for normal requests.
&gt; Where on the kernel side, it looks up the associated pages stored in
&gt; the io_mapped_region&#x27;s pages array for the offset passed in.

So you already can do all that using the mmap()&#x27;ed region user
pointer, and you just want it to be more efficient, right?
For that let&#x27;s just reuse registered buffers, we don&#x27;t need a
new mechanism that needs to be propagated to all request types.
And registered buffer are already optimised for I/O in a bunch
of ways. And as a bonus, it&#x27;ll be similar to the zero-copy
internally registered buffers if you still plan to add them.

The simplest way to do that is to create a registered buffer out
of the mmap&#x27;ed region pointer. Pseudo code:

// mmap&#x27;ed if it&#x27;s kernel allocated.
{region_ptr, region_size} = create_region();

struct iovec iov;
iov.iov_base = region_ptr;
iov.iov_len = region_size;
io_uring_register_buffers(ring, &amp;iov, 1);

// later instead of this:
ptr = region_ptr + off;
io_uring_prep_read(sqe, fd, ptr, ...);

// you use registered buffers as usual:
io_uring_prep_read_fixed(sqe, fd, off, regbuf_idx, ...);


IIRC the registration would fail because it doesn&#x27;t allow file
backed pages, but it should be fine if we know it&#x27;s io_uring
region memory, so that would need to be patched.

There might be a bunch of other ways you can do that like
create a kernel allocated registered buffer like what Cristoph
wants, and then register it as a region. Or allow creating
registered buffers out of a region. etc.

I wanted to unify registered buffers and regions internally
at some point, but then drifted away from active io_uring core
infrastructure development, so I guess that could&#x27;ve been useful.

&gt; Right now there&#x27;s only a uapi to register a memory region and none to
&gt; unregister one. Is it guaranteed that io-uring will never add
&gt; something in the future that will let userspace unregister the memory
&gt; region or at least unregister it while it&#x27;s being used (eg if we add
&gt; future refcounting to it to track active uses of it)?

Let&#x27;s talk about it when it&#x27;s needed or something changes, but if
you do registered buffers instead as per above, they&#x27;ll be holding
page references and or have to pin the region in some other way.

&gt; If so, then end-to-end, with it going through the mem region, it would
&gt; be something like:
&gt; * user creates a mem region for the io-uring
&gt; * user mmaps the mem region

FWIW, we should just add a liburing helper, so that fuse server
doesn&#x27;t need to deal with mmap&#x27;ing.

&gt; * user passes in offset into region, length of each buffer, and number
&gt; of entries in the ring to the subsystem
&gt; * subsystem creates a locally managed bufring and adds buffers to that
&gt; ring from the mem region

That&#x27;s sounds clean to me _if_ it allows you to achieve all
(fast path) optimisations you want to have. I hope it does?

-- 
Pavel Begunkov


</pre>
</details>
<div class="review-comment-signals">Signals: requested changes, alternative approach</div>
</div>
</div>
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Christoph Hellwig</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-18">2026-02-18</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Christoph Hellwig raised concerns about the kernel-managed buffer rings feature, specifically that it doesn&#x27;t guarantee user processes can only read memory and not write to it. He wants a setup where the kernel controls allocation fully.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Wed, Feb 11, 2026 at 02:06:18PM -0800, Joanne Koong wrote:
&gt; &gt; I don&#x27;t think I follow. I&#x27;m saying that it might be interesting
&gt; &gt; to separate rings from how and with what they&#x27;re populated on the
&gt; &gt; kernel API level, but the fuse kernel module can do the population
&gt; 
&gt; Oh okay, from your first message I (and I think christoph too) thought
&gt; what you were saying is that the user should be responsible for
&gt; allocating the buffers with complete ownership over them, and then
&gt; just pass those allocated to the kernel to use. But what you&#x27;re saying
&gt; is that just use a different way for getting the kernel to allocate
&gt; the buffers (eg through the IORING_REGISTER_MEM_REGION interface). Am
&gt; I reading this correctly?

I&#x27;m arguing exactly against this.  For my use case I need a setup
where the kernel controls the allocation fully and guarantees user
processes can only read the memory but never write to it.  I&#x27;d love
to be able to piggy back than onto your work.



---

On Thu, Feb 12, 2026 at 09:29:31AM -0800, Joanne Koong wrote:
&gt; &gt; &gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt; &gt; &gt; where the kernel controls the allocation fully and guarantees user
&gt; &gt; &gt; processes can only read the memory but never write to it.  I&#x27;d love
&gt; 
&gt; By &quot;control the allocation fully&quot; do you mean for your use case, the
&gt; allocation/setup isn&#x27;t triggered by userspace but is initiated by the
&gt; kernel (eg user never explicitly registers any kbuf ring, the kernel
&gt; just uses the kbuf ring data structure internally and users can read
&gt; the buffer contents)? If userspace initiates the setup of the kbuf
&gt; ring, going through IORING_REGISTER_MEM_REGION would be semantically
&gt; the same, except the buffer allocation by the kernel now happens
&gt; before the ring is created and then later populated into the ring.
&gt; userspace would still need to make an mmap call to the region and the
&gt; kernel could enforce that as read-only. But if userspace doesn&#x27;t
&gt; initiate the setup, then going through IORING_REGISTER_MEM_REGION gets
&gt; uglier.

The idea is that the application tells the kernel that it wants to use
a fixed buffer pool for reads.  Right now the application does this
using io_uring_register_buffers().  The problem with that is that
io_uring_register_buffers ends up just doing a pin of the memory,
but the application or, in case of shared memory, someone else could
still modify the memory.  If the underlying file system or storage
device needs verify checksums, or worse rebuild data from parity
(or uncompress), it needs to ensure that the memory it is operating
on can&#x27;t be modified by someone else.

So I&#x27;ve been thinking of a version of io_uring_register_buffers where
the buffers are not provided by the application, but instead by the
kernel and mapped into the application address space read-only for
a while, and I thought I could implement this on top of your series,
but I have to admit I haven&#x27;t really looked into the details all
that much.

&gt; 
&gt; To be completely honest, the more I look at this the more this feels
&gt; like overkill / over-engineered to me. I get that now the user can do
&gt; the PMD optimization, but does that actually lead to noticeable
&gt; performance benefits? It seems especially confusing with them going
&gt; through the same pbuf ring interface but having totally different
&gt; expectations.

Yes.  The PMD mapping also is not that relevant.  Both AMD (implicit)
and ARM (explicit) have optimizations for contiguous PTEs that are
almost as valuable.

&gt; What about adding a straightforward kmbuf ring that goes through the
&gt; pbuf interface (eg the design in this patchset) and then in the future
&gt; adding an interface for pbuf rings (both kernel-managed and
&gt; non-kernel-managed) to go through IORING_REGISTERED_MEM_REGIONS if
&gt; users end up needing/wanting to have their rings populated that way?

That feels much simpler to me as well.



---

On Fri, Feb 13, 2026 at 11:09:13AM -0800, Joanne Koong wrote:
&gt; 
&gt; I think the circular buffer will be useful for Christoph&#x27;s use case in
&gt; the same way it&#x27;ll be useful for fuse&#x27;s. The read payload could be
&gt; differently sized across requests, so it&#x27;s a lot of wasted space to
&gt; have to allocate a buffer large enough to support the max-size request
&gt; per entry in the io_ring.

Yes.

&gt; With using a circular buffer, buffers have a
&gt; way to be shared across entries, which means we can significantly
&gt; reduce how much memory needs to be allocated.

Or enable such flexible use cases at all.



---

On Fri, Feb 13, 2026 at 11:14:03AM -0800, Joanne Koong wrote:
&gt; I think we have the exact same use case, except your buffers need to
&gt; be read-only. I think your use case benefits from the same memory wins
&gt; we&#x27;ll get with incremental buffer consumption, which is the primary
&gt; reason fuse is using a bufring instead of fixed buffers.

Yeah.

&gt; I think you can and it&#x27;ll be very easy to do so. All that would be
&gt; needed is to pass in a read-only flag from the userspace side when it
&gt; registers the bufring, and then when userspace makes the mmap call to
&gt; the bufring, the kernel checks if that read-only flag is set on the
&gt; bufring and if so returns a read-only mapping. 

Yes, tat&#x27;s what I though.  But Pavel seems to disagree?

</pre>
</details>
<div class="review-comment-signals">Signals: requested changes, additional requirements</div>
</div>
</div>
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Bernd Schubert</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-18">2026-02-18</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The reviewer, Bernd Schubert, expressed skepticism about the purpose of kernel-managed buffer rings and questioned their usefulness.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">

On 2/13/26 20:09, Joanne Koong wrote:
&gt; On Fri, Feb 13, 2026 at 7:31\u202fAM Pavel Begunkov &lt;asml.silence@gmail.com&gt; wrote:
&gt;&gt;
&gt;&gt; On 2/13/26 07:27, Christoph Hellwig wrote:
&gt;&gt;&gt; On Thu, Feb 12, 2026 at 09:29:31AM -0800, Joanne Koong wrote:
&gt;&gt;&gt;&gt;&gt;&gt; I&#x27;m arguing exactly against this.  For my use case I need a setup
&gt;&gt;&gt;&gt;&gt;&gt; where the kernel controls the allocation fully and guarantees user
&gt;&gt;&gt;&gt;&gt;&gt; processes can only read the memory but never write to it.  I&#x27;d love
&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt; By &quot;control the allocation fully&quot; do you mean for your use case, the
&gt;&gt;&gt;&gt; allocation/setup isn&#x27;t triggered by userspace but is initiated by the
&gt;&gt;&gt;&gt; kernel (eg user never explicitly registers any kbuf ring, the kernel
&gt;&gt;&gt;&gt; just uses the kbuf ring data structure internally and users can read
&gt;&gt;&gt;&gt; the buffer contents)? If userspace initiates the setup of the kbuf
&gt;&gt;&gt;&gt; ring, going through IORING_REGISTER_MEM_REGION would be semantically
&gt;&gt;&gt;&gt; the same, except the buffer allocation by the kernel now happens
&gt;&gt;&gt;&gt; before the ring is created and then later populated into the ring.
&gt;&gt;&gt;&gt; userspace would still need to make an mmap call to the region and the
&gt;&gt;&gt;&gt; kernel could enforce that as read-only. But if userspace doesn&#x27;t
&gt;&gt;&gt;&gt; initiate the setup, then going through IORING_REGISTER_MEM_REGION gets
&gt;&gt;&gt;&gt; uglier.
&gt;&gt;&gt;
&gt;&gt;&gt; The idea is that the application tells the kernel that it wants to use
&gt;&gt;&gt; a fixed buffer pool for reads.  Right now the application does this
&gt;&gt;&gt; using io_uring_register_buffers().  The problem with that is that
&gt;&gt;&gt; io_uring_register_buffers ends up just doing a pin of the memory,
&gt;&gt;&gt; but the application or, in case of shared memory, someone else could
&gt;&gt;&gt; still modify the memory.  If the underlying file system or storage
&gt;&gt;&gt; device needs verify checksums, or worse rebuild data from parity
&gt;&gt;&gt; (or uncompress), it needs to ensure that the memory it is operating
&gt;&gt;&gt; on can&#x27;t be modified by someone else.
&gt;&gt;&gt;
&gt;&gt;&gt; So I&#x27;ve been thinking of a version of io_uring_register_buffers where
&gt;&gt;&gt; the buffers are not provided by the application, but instead by the
&gt;&gt;&gt; kernel and mapped into the application address space read-only for
&gt;&gt;&gt; a while, and I thought I could implement this on top of your series,
&gt;&gt;&gt; but I have to admit I haven&#x27;t really looked into the details all
&gt;&gt;&gt; that much.
&gt;&gt;
&gt;&gt; There is nothing about registered buffers in this series. And even
&gt;&gt; if you try to reuse buffer allocation out of it, it&#x27;ll come with
&gt;&gt; a circular buffer you&#x27;ll have no need for. And I&#x27;m pretty much
&gt; 
&gt; I think the circular buffer will be useful for Christoph&#x27;s use case in
&gt; the same way it&#x27;ll be useful for fuse&#x27;s. The read payload could be
&gt; differently sized across requests, so it&#x27;s a lot of wasted space to
&gt; have to allocate a buffer large enough to support the max-size request
&gt; per entry in the io_ring. With using a circular buffer, buffers have a
&gt; way to be shared across entries, which means we can significantly
&gt; reduce how much memory needs to be allocated.

Dunno, what we actually want is requests of multiple sizes. Sharing
buffers across entries sounds like just reducing the ring size - I
personally don&#x27;t see the point here.


Thanks,
Bernd
</pre>
</details>
<div class="review-comment-signals">Signals: skepticism, questioning usefulness</div>
</div>
</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Pavel Begunkov</span>
<a class="date-chip" href="../2026-02-18_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-18">2026-02-18</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Pavel Begunkov suggested reusing regions for allocations and mmap()ing, wrapping them into a registered buffer to simplify the implementation.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On 2/13/26 15:31, Pavel Begunkov wrote:
&gt; On 2/13/26 07:27, Christoph Hellwig wrote:
&gt;&gt; On Thu, Feb 12, 2026 at 09:29:31AM -0800, Joanne Koong wrote:
&gt;&gt;&gt;&gt;&gt; I&#x27;m arguing exactly against this. For my use case I need a setup
&gt;&gt;&gt;&gt;&gt; where the kernel controls the allocation fully and guarantees user
&gt;&gt;&gt;&gt;&gt; processes can only read the memory but never write to it. I&#x27;d love
&gt;&gt;&gt;
&gt;&gt;&gt; By &quot;control the allocation fully&quot; do you mean for your use case, the
&gt;&gt;&gt; allocation/setup isn&#x27;t triggered by userspace but is initiated by the
&gt;&gt;&gt; kernel (eg user never explicitly registers any kbuf ring, the kernel
&gt;&gt;&gt; just uses the kbuf ring data structure internally and users can read
&gt;&gt;&gt; the buffer contents)? If userspace initiates the setup of the kbuf
&gt;&gt;&gt; ring, going through IORING_REGISTER_MEM_REGION would be semantically
&gt;&gt;&gt; the same, except the buffer allocation by the kernel now happens
&gt;&gt;&gt; before the ring is created and then later populated into the ring.
&gt;&gt;&gt; userspace would still need to make an mmap call to the region and the
&gt;&gt;&gt; kernel could enforce that as read-only. But if userspace doesn&#x27;t
&gt;&gt;&gt; initiate the setup, then going through IORING_REGISTER_MEM_REGION gets
&gt;&gt;&gt; uglier.
&gt;&gt;
&gt;&gt; The idea is that the application tells the kernel that it wants to use
&gt;&gt; a fixed buffer pool for reads. Right now the application does this
&gt;&gt; using io_uring_register_buffers(). The problem with that is that
&gt;&gt; io_uring_register_buffers ends up just doing a pin of the memory,
&gt;&gt; but the application or, in case of shared memory, someone else could
&gt;&gt; still modify the memory. If the underlying file system or storage
&gt;&gt; device needs verify checksums, or worse rebuild data from parity
&gt;&gt; (or uncompress), it needs to ensure that the memory it is operating
&gt;&gt; on can&#x27;t be modified by someone else.
&gt;&gt;
&gt;&gt; So I&#x27;ve been thinking of a version of io_uring_register_buffers where
&gt;&gt; the buffers are not provided by the application, but instead by the
&gt;&gt; kernel and mapped into the application address space read-only for
&gt;&gt; a while, and I thought I could implement this on top of your series,
&gt;&gt; but I have to admit I haven&#x27;t really looked into the details all
&gt;&gt; that much.
&gt; 
&gt; There is nothing about registered buffers in this series. And even
&gt; if you try to reuse buffer allocation out of it, it&#x27;ll come with
&gt; a circular buffer you&#x27;ll have no need for. And I&#x27;m pretty much
&gt; arguing about separating those for io_uring.

FWIW, the easiest solution is to internally reuse regions for
allocations and mmap()&#x27;ing and wrap it into a registered buffer.
It just need to make vmap&#x27;ing optional as it won&#x27;t be needed.

-- 
Pavel Begunkov

</pre>
</details>
<div class="review-comment-signals">Signals: requested changes</div>
</div>
</div>
<div class="thread-node depth-0" id="2026-02-20">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-20_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-20">2026-02-20</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The reviewer, Joanne Koong (author), raised a concern about the patch [PATCH v1 11/11] io_uring/cmd: set selected buffer index in __io_uring_cmd_done(). They pointed out that when uring_cmd operations select a buffer, the completion queue entry should indicate which buffer was selected. The reviewer suggested setting IORING_CQE_F_BUFFER on the completed entry and encoding the buffer index if a buffer was selected.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Refactor the logic in io_register_pbuf_ring() into generic helpers:
- io_copy_and_validate_buf_reg(): Copy out user arg and validate user
  arg and buffer registration parameters
- io_alloc_new_buffer_list(): Allocate and initialize a new buffer
  list for the given buffer group ID
- io_setup_pbuf_ring(): Sets up the physical buffer ring region and
  handles memory mapping for provided buffer rings

This is a preparatory change for upcoming kernel-managed buffer ring
support which will need to reuse some of these helpers.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 io_uring/kbuf.c | 129 +++++++++++++++++++++++++++++++-----------------
 1 file changed, 85 insertions(+), 44 deletions(-)

diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index 67d4fe576473..850b836f32ee 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -596,55 +596,73 @@ int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags)
 	return IOU_COMPLETE;
 }
 
-int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
+static int io_copy_and_validate_buf_reg(const void __user *arg,
+					struct io_uring_buf_reg *reg,
+					unsigned int permitted_flags)
 {
-	struct io_uring_buf_reg reg;
-	struct io_buffer_list *bl;
-	struct io_uring_region_desc rd;
-	struct io_uring_buf_ring *br;
-	unsigned long mmap_offset;
-	unsigned long ring_size;
-	int ret;
-
-	lockdep_assert_held(&amp;ctx-&gt;uring_lock);
-
-	if (copy_from_user(&amp;reg, arg, sizeof(reg)))
+	if (copy_from_user(reg, arg, sizeof(*reg)))
 		return -EFAULT;
-	if (!mem_is_zero(reg.resv, sizeof(reg.resv)))
+
+	if (!mem_is_zero(reg-&gt;resv, sizeof(reg-&gt;resv)))
 		return -EINVAL;
-	if (reg.flags &amp; ~(IOU_PBUF_RING_MMAP | IOU_PBUF_RING_INC))
+	if (reg-&gt;flags &amp; ~permitted_flags)
 		return -EINVAL;
-	if (!is_power_of_2(reg.ring_entries))
+	if (!is_power_of_2(reg-&gt;ring_entries))
 		return -EINVAL;
 	/* cannot disambiguate full vs empty due to head/tail size */
-	if (reg.ring_entries &gt;= 65536)
+	if (reg-&gt;ring_entries &gt;= 65536)
 		return -EINVAL;
+	return 0;
+}
 
-	bl = io_buffer_get_list(ctx, reg.bgid);
-	if (bl) {
+static struct io_buffer_list *
+io_alloc_new_buffer_list(struct io_ring_ctx *ctx,
+			 const struct io_uring_buf_reg *reg)
+{
+	struct io_buffer_list *list;
+
+	list = io_buffer_get_list(ctx, reg-&gt;bgid);
+	if (list) {
 		/* if mapped buffer ring OR classic exists, don&#x27;t allow */
-		if (bl-&gt;flags &amp; IOBL_BUF_RING || !list_empty(&amp;bl-&gt;buf_list))
-			return -EEXIST;
-		io_destroy_bl(ctx, bl);
+		if (list-&gt;flags &amp; IOBL_BUF_RING || !list_empty(&amp;list-&gt;buf_list))
+			return ERR_PTR(-EEXIST);
+		io_destroy_bl(ctx, list);
 	}
 
-	bl = kzalloc(sizeof(*bl), GFP_KERNEL_ACCOUNT);
-	if (!bl)
-		return -ENOMEM;
+	list = kzalloc(sizeof(*list), GFP_KERNEL_ACCOUNT);
+	if (!list)
+		return ERR_PTR(-ENOMEM);
+
+	list-&gt;nr_entries = reg-&gt;ring_entries;
+	list-&gt;mask = reg-&gt;ring_entries - 1;
+	list-&gt;flags = IOBL_BUF_RING;
+
+	return list;
+}
+
+static int io_setup_pbuf_ring(struct io_ring_ctx *ctx,
+			      const struct io_uring_buf_reg *reg,
+			      struct io_buffer_list *bl)
+{
+	struct io_uring_region_desc rd;
+	unsigned long mmap_offset;
+	unsigned long ring_size;
+	int ret;
 
-	mmap_offset = (unsigned long)reg.bgid &lt;&lt; IORING_OFF_PBUF_SHIFT;
-	ring_size = flex_array_size(br, bufs, reg.ring_entries);
+	mmap_offset = (unsigned long)reg-&gt;bgid &lt;&lt; IORING_OFF_PBUF_SHIFT;
+	ring_size = flex_array_size(bl-&gt;buf_ring, bufs, reg-&gt;ring_entries);
 
 	memset(&amp;rd, 0, sizeof(rd));
 	rd.size = PAGE_ALIGN(ring_size);
-	if (!(reg.flags &amp; IOU_PBUF_RING_MMAP)) {
-		rd.user_addr = reg.ring_addr;
+	if (!(reg-&gt;flags &amp; IOU_PBUF_RING_MMAP)) {
+		rd.user_addr = reg-&gt;ring_addr;
 		rd.flags |= IORING_MEM_REGION_TYPE_USER;
 	}
+
 	ret = io_create_region(ctx, &amp;bl-&gt;region, &amp;rd, mmap_offset);
 	if (ret)
-		goto fail;
-	br = io_region_get_ptr(&amp;bl-&gt;region);
+		return ret;
+	bl-&gt;buf_ring = io_region_get_ptr(&amp;bl-&gt;region);
 
 #ifdef SHM_COLOUR
 	/*
@@ -656,25 +674,48 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
 	 * should use IOU_PBUF_RING_MMAP instead, and liburing will handle
 	 * this transparently.
 	 */
-	if (!(reg.flags &amp; IOU_PBUF_RING_MMAP) &amp;&amp;
-	    ((reg.ring_addr | (unsigned long)br) &amp; (SHM_COLOUR - 1))) {
-		ret = -EINVAL;
-		goto fail;
+	if (!(reg-&gt;flags &amp; IOU_PBUF_RING_MMAP) &amp;&amp;
+	    ((reg-&gt;ring_addr | (unsigned long)bl-&gt;buf_ring) &amp;
+	     (SHM_COLOUR - 1))) {
+		io_free_region(ctx-&gt;user, &amp;bl-&gt;region);
+		return -EINVAL;
 	}
 #endif
 
-	bl-&gt;nr_entries = reg.ring_entries;
-	bl-&gt;mask = reg.ring_entries - 1;
-	bl-&gt;flags |= IOBL_BUF_RING;
-	bl-&gt;buf_ring = br;
-	if (reg.flags &amp; IOU_PBUF_RING_INC)
+	if (reg-&gt;flags &amp; IOU_PBUF_RING_INC)
 		bl-&gt;flags |= IOBL_INC;
+
+	return 0;
+}
+
+int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
+{
+	unsigned int permitted_flags;
+	struct io_uring_buf_reg reg;
+	struct io_buffer_list *bl;
+	int ret;
+
+	lockdep_assert_held(&amp;ctx-&gt;uring_lock);
+
+	permitted_flags = IOU_PBUF_RING_MMAP | IOU_PBUF_RING_INC;
+	ret = io_copy_and_validate_buf_reg(arg, &amp;reg, permitted_flags);
+	if (ret)
+		return ret;
+
+	bl = io_alloc_new_buffer_list(ctx, &amp;reg);
+	if (IS_ERR(bl))
+		return PTR_ERR(bl);
+
+	ret = io_setup_pbuf_ring(ctx, &amp;reg, bl);
+	if (ret) {
+		kfree(bl);
+		return ret;
+	}
+
 	ret = io_buffer_add_list(ctx, bl, reg.bgid);
-	if (!ret)
-		return 0;
-fail:
-	io_free_region(ctx-&gt;user, &amp;bl-&gt;region);
-	kfree(bl);
+	if (ret)
+		io_put_bl(ctx, bl);
+
 	return ret;
 }
 
-- 
2.47.3



---

Use the more generic name io_unregister_buf_ring() as this function will
be used for unregistering both provided buffer rings and kernel-managed
buffer rings.

This is a preparatory change for upcoming kernel-managed buffer ring
support.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 io_uring/kbuf.c     | 2 +-
 io_uring/kbuf.h     | 2 +-
 io_uring/register.c | 2 +-
 3 files changed, 3 insertions(+), 3 deletions(-)

diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index 850b836f32ee..aa9b70b72db4 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -719,7 +719,7 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
 	return ret;
 }
 
-int io_unregister_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
+int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg)
 {
 	struct io_uring_buf_reg reg;
 	struct io_buffer_list *bl;
diff --git a/io_uring/kbuf.h b/io_uring/kbuf.h
index bf15e26520d3..40b44f4fdb15 100644
--- a/io_uring/kbuf.h
+++ b/io_uring/kbuf.h
@@ -74,7 +74,7 @@ int io_provide_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe
 int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags);
 
 int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);
-int io_unregister_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);
+int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg);
 int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg);
 
 bool io_kbuf_recycle_legacy(struct io_kiocb *req, unsigned issue_flags);
diff --git a/io_uring/register.c b/io_uring/register.c
index 594b1f2ce875..0882cb34f851 100644
--- a/io_uring/register.c
+++ b/io_uring/register.c
@@ -841,7 +841,7 @@ static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,
 		ret = -EINVAL;
 		if (!arg || nr_args != 1)
 			break;
-		ret = io_unregister_pbuf_ring(ctx, arg);
+		ret = io_unregister_buf_ring(ctx, arg);
 		break;
 	case IORING_REGISTER_SYNC_CANCEL:
 		ret = -EINVAL;
-- 
2.47.3



---

Add support for kernel-managed buffer rings (kmbuf rings), which allow
the kernel to allocate and manage the backing buffers for a buffer
ring, rather than requiring the application to provide and manage them.

This introduces two new registration opcodes:
- IORING_REGISTER_KMBUF_RING: Register a kernel-managed buffer ring
- IORING_UNREGISTER_KMBUF_RING: Unregister a kernel-managed buffer ring

The existing io_uring_buf_reg structure is extended with a union to
support both application-provided buffer rings (pbuf) and kernel-managed
buffer rings (kmbuf):
- For pbuf rings: ring_addr specifies the user-provided ring address
- For kmbuf rings: buf_size specifies the size of each buffer. buf_size
  must be non-zero and page-aligned.

The implementation follows the same pattern as pbuf ring registration,
reusing the validation and buffer list allocation helpers introduced in
earlier refactoring. The IOBL_KERNEL_MANAGED flag marks buffer lists as
kernel-managed for appropriate handling in the I/O path.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 include/uapi/linux/io_uring.h |  15 ++++-
 io_uring/kbuf.c               |  81 ++++++++++++++++++++++++-
 io_uring/kbuf.h               |   7 ++-
 io_uring/memmap.c             | 111 ++++++++++++++++++++++++++++++++++
 io_uring/memmap.h             |   4 ++
 io_uring/register.c           |   7 +++
 6 files changed, 219 insertions(+), 6 deletions(-)

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index fc473af6feb4..a0889c1744bd 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -715,6 +715,10 @@ enum io_uring_register_op {
 	/* register bpf filtering programs */
 	IORING_REGISTER_BPF_FILTER		= 37,
 
+	/* register/unregister kernel-managed ring buffer group */
+	IORING_REGISTER_KMBUF_RING		= 38,
+	IORING_UNREGISTER_KMBUF_RING		= 39,
+
 	/* this goes last */
 	IORING_REGISTER_LAST,
 
@@ -891,9 +895,16 @@ enum io_uring_register_pbuf_ring_flags {
 	IOU_PBUF_RING_INC	= 2,
 };
 
-/* argument for IORING_(UN)REGISTER_PBUF_RING */
+/* argument for IORING_(UN)REGISTER_PBUF_RING and
+ * IORING_(UN)REGISTER_KMBUF_RING
+ */
 struct io_uring_buf_reg {
-	__u64	ring_addr;
+	union {
+		/* used for pbuf rings */
+		__u64	ring_addr;
+		/* used for kmbuf rings */
+		__u32   buf_size;
+	};
 	__u32	ring_entries;
 	__u16	bgid;
 	__u16	flags;
diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index aa9b70b72db4..9bc36451d083 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -427,10 +427,13 @@ static int io_remove_buffers_legacy(struct io_ring_ctx *ctx,
 
 static void io_put_bl(struct io_ring_ctx *ctx, struct io_buffer_list *bl)
 {
-	if (bl-&gt;flags &amp; IOBL_BUF_RING)
+	if (bl-&gt;flags &amp; IOBL_BUF_RING) {
 		io_free_region(ctx-&gt;user, &amp;bl-&gt;region);
-	else
+		if (bl-&gt;flags &amp; IOBL_KERNEL_MANAGED)
+			kfree(bl-&gt;buf_ring);
+	} else {
 		io_remove_buffers_legacy(ctx, bl, -1U);
+	}
 
 	kfree(bl);
 }
@@ -779,3 +782,77 @@ struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,
 		return NULL;
 	return &amp;bl-&gt;region;
 }
+
+static int io_setup_kmbuf_ring(struct io_ring_ctx *ctx,
+			       struct io_buffer_list *bl,
+			       struct io_uring_buf_reg *reg)
+{
+	struct io_uring_buf_ring *ring;
+	unsigned long ring_size;
+	void *buf_region;
+	unsigned int i;
+	int ret;
+
+	/* allocate pages for the ring structure */
+	ring_size = flex_array_size(ring, bufs, bl-&gt;nr_entries);
+	ring = kzalloc(ring_size, GFP_KERNEL_ACCOUNT);
+	if (!ring)
+		return -ENOMEM;
+
+	ret = io_create_region_multi_buf(ctx, &amp;bl-&gt;region, bl-&gt;nr_entries,
+					 reg-&gt;buf_size);
+	if (ret) {
+		kfree(ring);
+		return ret;
+	}
+
+	/* initialize ring buf entries to point to the buffers */
+	buf_region = bl-&gt;region.ptr;
+	for (i = 0; i &lt; bl-&gt;nr_entries; i++) {
+		struct io_uring_buf *buf = &amp;ring-&gt;bufs[i];
+
+		buf-&gt;addr = (u64)(uintptr_t)buf_region;
+		buf-&gt;len = reg-&gt;buf_size;
+		buf-&gt;bid = i;
+
+		buf_region += reg-&gt;buf_size;
+	}
+	ring-&gt;tail = bl-&gt;nr_entries;
+
+	bl-&gt;buf_ring = ring;
+	bl-&gt;flags |= IOBL_KERNEL_MANAGED;
+
+	return 0;
+}
+
+int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
+{
+	struct io_uring_buf_reg reg;
+	struct io_buffer_list *bl;
+	int ret;
+
+	lockdep_assert_held(&amp;ctx-&gt;uring_lock);
+
+	ret = io_copy_and_validate_buf_reg(arg, &amp;reg, 0);
+	if (ret)
+		return ret;
+
+	if (!reg.buf_size || !PAGE_ALIGNED(reg.buf_size))
+		return -EINVAL;
+
+	bl = io_alloc_new_buffer_list(ctx, &amp;reg);
+	if (IS_ERR(bl))
+		return PTR_ERR(bl);
+
+	ret = io_setup_kmbuf_ring(ctx, bl, &amp;reg);
+	if (ret) {
+		kfree(bl);
+		return ret;
+	}
+
+	ret = io_buffer_add_list(ctx, bl, reg.bgid);
+	if (ret)
+		io_put_bl(ctx, bl);
+
+	return ret;
+}
diff --git a/io_uring/kbuf.h b/io_uring/kbuf.h
index 40b44f4fdb15..62c80a1ebf03 100644
--- a/io_uring/kbuf.h
+++ b/io_uring/kbuf.h
@@ -7,9 +7,11 @@
 
 enum {
 	/* ring mapped provided buffers */
-	IOBL_BUF_RING	= 1,
+	IOBL_BUF_RING		= 1,
 	/* buffers are consumed incrementally rather than always fully */
-	IOBL_INC	= 2,
+	IOBL_INC		= 2,
+	/* buffers are kernel managed */
+	IOBL_KERNEL_MANAGED	= 4,
 };
 
 struct io_buffer_list {
@@ -74,6 +76,7 @@ int io_provide_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe
 int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags);
 
 int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);
+int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg);
 int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg);
 int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg);
 
diff --git a/io_uring/memmap.c b/io_uring/memmap.c
index 89f56609e50a..8d37e93c0433 100644
--- a/io_uring/memmap.c
+++ b/io_uring/memmap.c
@@ -15,6 +15,28 @@
 #include &quot;rsrc.h&quot;
 #include &quot;zcrx.h&quot;
 
+static void release_multi_buf_pages(struct page **pages, unsigned long nr_pages)
+{
+	struct page *page;
+	unsigned int nr, i = 0;
+
+	while (nr_pages) {
+		page = pages[i];
+
+		if (!page || WARN_ON_ONCE(page != compound_head(page)))
+			return;
+
+		nr = compound_nr(page);
+		put_page(page);
+
+		if (WARN_ON_ONCE(nr &gt; nr_pages))
+			return;
+
+		i += nr;
+		nr_pages -= nr;
+	}
+}
+
 static bool io_mem_alloc_compound(struct page **pages, int nr_pages,
 				  size_t size, gfp_t gfp)
 {
@@ -86,6 +108,8 @@ enum {
 	IO_REGION_F_USER_PROVIDED		= 2,
 	/* only the first page in the array is ref&#x27;ed */
 	IO_REGION_F_SINGLE_REF			= 4,
+	/* pages in the array belong to multiple discrete allocations */
+	IO_REGION_F_MULTI_BUF			= 8,
 };
 
 void io_free_region(struct user_struct *user, struct io_mapped_region *mr)
@@ -98,6 +122,8 @@ void io_free_region(struct user_struct *user, struct io_mapped_region *mr)
 
 		if (mr-&gt;flags &amp; IO_REGION_F_USER_PROVIDED)
 			unpin_user_pages(mr-&gt;pages, nr_refs);
+		else if (mr-&gt;flags &amp; IO_REGION_F_MULTI_BUF)
+			release_multi_buf_pages(mr-&gt;pages, nr_refs);
 		else
 			release_pages(mr-&gt;pages, nr_refs);
 
@@ -149,6 +175,54 @@ static int io_region_pin_pages(struct io_mapped_region *mr,
 	return 0;
 }
 
+static int io_region_allocate_pages_multi_buf(struct io_mapped_region *mr,
+					      unsigned int nr_bufs,
+					      unsigned int buf_size)
+{
+	gfp_t gfp = GFP_USER | __GFP_ACCOUNT | __GFP_ZERO | __GFP_NOWARN;
+	struct page **pages, **cur_pages;
+	unsigned int nr_allocated;
+	unsigned int buf_pages;
+	unsigned int i;
+
+	if (!PAGE_ALIGNED(buf_size))
+		return -EINVAL;
+
+	buf_pages = buf_size &gt;&gt; PAGE_SHIFT;
+
+	pages = kvmalloc_array(mr-&gt;nr_pages, sizeof(*pages), gfp);
+	if (!pages)
+		return -ENOMEM;
+
+	cur_pages = pages;
+
+	for (i = 0; i &lt; nr_bufs; i++) {
+		if (io_mem_alloc_compound(cur_pages, buf_pages, buf_size,
+					  gfp)) {
+			cur_pages += buf_pages;
+			continue;
+		}
+
+		nr_allocated = alloc_pages_bulk_node(gfp, NUMA_NO_NODE,
+						     buf_pages, cur_pages);
+		if (nr_allocated != buf_pages) {
+			unsigned int total =
+				(cur_pages - pages) + nr_allocated;
+
+			release_multi_buf_pages(pages, total);
+			kvfree(pages);
+			return -ENOMEM;
+		}
+
+		cur_pages += buf_pages;
+	}
+
+	mr-&gt;flags |= IO_REGION_F_MULTI_BUF;
+	mr-&gt;pages = pages;
+
+	return 0;
+}
+
 static int io_region_allocate_pages(struct io_mapped_region *mr,
 				    struct io_uring_region_desc *reg,
 				    unsigned long mmap_offset)
@@ -181,6 +255,43 @@ static int io_region_allocate_pages(struct io_mapped_region *mr,
 	return 0;
 }
 
+int io_create_region_multi_buf(struct io_ring_ctx *ctx,
+			       struct io_mapped_region *mr,
+			       unsigned int nr_bufs, unsigned int buf_size)
+{
+	unsigned int nr_pages;
+	int ret;
+
+	if (WARN_ON_ONCE(mr-&gt;pages || mr-&gt;ptr || mr-&gt;nr_pages))
+		return -EFAULT;
+
+	if (WARN_ON_ONCE(!nr_bufs || !buf_size || !PAGE_ALIGNED(buf_size)))
+		return -EINVAL;
+
+	if (check_mul_overflow(buf_size &gt;&gt; PAGE_SHIFT, nr_bufs, &amp;nr_pages))
+		return -EINVAL;
+
+	if (ctx-&gt;user) {
+		ret = __io_account_mem(ctx-&gt;user, nr_pages);
+		if (ret)
+			return ret;
+	}
+	mr-&gt;nr_pages = nr_pages;
+
+	ret = io_region_allocate_pages_multi_buf(mr, nr_bufs, buf_size);
+	if (ret)
+		goto out_free;
+
+	ret = io_region_init_ptr(mr);
+	if (ret)
+		goto out_free;
+
+	return 0;
+out_free:
+	io_free_region(ctx-&gt;user, mr);
+	return ret;
+}
+
 int io_create_region(struct io_ring_ctx *ctx, struct io_mapped_region *mr,
 		     struct io_uring_region_desc *reg,
 		     unsigned long mmap_offset)
diff --git a/io_uring/memmap.h b/io_uring/memmap.h
index f4cfbb6b9a1f..3aa1167462ae 100644
--- a/io_uring/memmap.h
+++ b/io_uring/memmap.h
@@ -22,6 +22,10 @@ int io_create_region(struct io_ring_ctx *ctx, struct io_mapped_region *mr,
 		     struct io_uring_region_desc *reg,
 		     unsigned long mmap_offset);
 
+int io_create_region_multi_buf(struct io_ring_ctx *ctx,
+			       struct io_mapped_region *mr,
+			       unsigned int nr_bufs, unsigned int buf_size);
+
 static inline void *io_region_get_ptr(struct io_mapped_region *mr)
 {
 	return mr-&gt;ptr;
diff --git a/io_uring/register.c b/io_uring/register.c
index 0882cb34f851..2db8daaf8fde 100644
--- a/io_uring/register.c
+++ b/io_uring/register.c
@@ -837,7 +837,14 @@ static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,
 			break;
 		ret = io_register_pbuf_ring(ctx, arg);
 		break;
+	case IORING_REGISTER_KMBUF_RING:
+		ret = -EINVAL;
+		if (!arg || nr_args != 1)
+			break;
+		ret = io_register_kmbuf_ring(ctx, arg);
+		break;
 	case IORING_UNREGISTER_PBUF_RING:
+	case IORING_UNREGISTER_KMBUF_RING:
 		ret = -EINVAL;
 		if (!arg || nr_args != 1)
 			break;
-- 
2.47.3



---

Add support for mmapping kernel-managed buffer rings (kmbuf) to
userspace, allowing applications to access the kernel-allocated buffers.

Similar to application-provided buffer rings (pbuf), kmbuf rings use the
buffer group ID encoded in the mmap offset to identify which buffer ring
to map. The implementation follows the same pattern as pbuf rings.

New mmap offset constants are introduced:
  - IORING_OFF_KMBUF_RING (0x88000000): Base offset for kmbuf mappings
  - IORING_OFF_KMBUF_SHIFT (16): Shift value to encode buffer group ID

The mmap offset encodes the bgid shifted by IORING_OFF_KMBUF_SHIFT.
The io_buf_get_region() helper retrieves the appropriate region.

This allows userspace to mmap the kernel-allocated buffer region and
access the buffers directly.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 include/uapi/linux/io_uring.h |  2 ++
 io_uring/kbuf.c               | 11 +++++++++--
 io_uring/kbuf.h               |  5 +++--
 io_uring/memmap.c             |  5 ++++-
 4 files changed, 18 insertions(+), 5 deletions(-)

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index a0889c1744bd..42a2812c9922 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -545,6 +545,8 @@ struct io_uring_cqe {
 #define IORING_OFF_SQES			0x10000000ULL
 #define IORING_OFF_PBUF_RING		0x80000000ULL
 #define IORING_OFF_PBUF_SHIFT		16
+#define IORING_OFF_KMBUF_RING		0x88000000ULL
+#define IORING_OFF_KMBUF_SHIFT		16
 #define IORING_OFF_MMAP_MASK		0xf8000000ULL
 
 /*
diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index 9bc36451d083..ccf5b213087b 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -770,16 +770,23 @@ int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg)
 	return 0;
 }
 
-struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,
-					    unsigned int bgid)
+struct io_mapped_region *io_buf_get_region(struct io_ring_ctx *ctx,
+					   unsigned int bgid,
+					   bool kernel_managed)
 {
 	struct io_buffer_list *bl;
+	bool is_kernel_managed;
 
 	lockdep_assert_held(&amp;ctx-&gt;mmap_lock);
 
 	bl = xa_load(&amp;ctx-&gt;io_bl_xa, bgid);
 	if (!bl || !(bl-&gt;flags &amp; IOBL_BUF_RING))
 		return NULL;
+
+	is_kernel_managed = !!(bl-&gt;flags &amp; IOBL_KERNEL_MANAGED);
+	if (is_kernel_managed != kernel_managed)
+		return NULL;
+
 	return &amp;bl-&gt;region;
 }
 
diff --git a/io_uring/kbuf.h b/io_uring/kbuf.h
index 62c80a1ebf03..11d165888b8e 100644
--- a/io_uring/kbuf.h
+++ b/io_uring/kbuf.h
@@ -88,8 +88,9 @@ unsigned int __io_put_kbufs(struct io_kiocb *req, struct io_buffer_list *bl,
 bool io_kbuf_commit(struct io_kiocb *req,
 		    struct io_buffer_list *bl, int len, int nr);
 
-struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,
-					    unsigned int bgid);
+struct io_mapped_region *io_buf_get_region(struct io_ring_ctx *ctx,
+					   unsigned int bgid,
+					   bool kernel_managed);
 
 static inline bool io_kbuf_recycle_ring(struct io_kiocb *req,
 					struct io_buffer_list *bl)
diff --git a/io_uring/memmap.c b/io_uring/memmap.c
index 8d37e93c0433..916315122323 100644
--- a/io_uring/memmap.c
+++ b/io_uring/memmap.c
@@ -356,7 +356,10 @@ static struct io_mapped_region *io_mmap_get_region(struct io_ring_ctx *ctx,
 		return &amp;ctx-&gt;sq_region;
 	case IORING_OFF_PBUF_RING:
 		id = (offset &amp; ~IORING_OFF_MMAP_MASK) &gt;&gt; IORING_OFF_PBUF_SHIFT;
-		return io_pbuf_get_region(ctx, id);
+		return io_buf_get_region(ctx, id, false);
+	case IORING_OFF_KMBUF_RING:
+		id = (offset &amp; ~IORING_OFF_MMAP_MASK) &gt;&gt; IORING_OFF_KMBUF_SHIFT;
+		return io_buf_get_region(ctx, id, true);
 	case IORING_MAP_OFF_PARAM_REGION:
 		return &amp;ctx-&gt;param_region;
 	case IORING_MAP_OFF_ZCRX_REGION:
-- 
2.47.3



---

Allow kernel-managed buffers to be selected. This requires modifying the
io_br_sel struct to separate the fields for address and val, since a
kernel address cannot be distinguished from a negative val when error
checking.

Auto-commit any selected kernel-managed buffer.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 include/linux/io_uring_types.h |  8 ++++----
 io_uring/kbuf.c                | 16 ++++++++++++----
 2 files changed, 16 insertions(+), 8 deletions(-)

diff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h
index 3e4a82a6f817..36cc2e0346d9 100644
--- a/include/linux/io_uring_types.h
+++ b/include/linux/io_uring_types.h
@@ -93,13 +93,13 @@ struct io_mapped_region {
  */
 struct io_br_sel {
 	struct io_buffer_list *buf_list;
-	/*
-	 * Some selection parts return the user address, others return an error.
-	 */
 	union {
+		/* for classic/ring provided buffers */
 		void __user *addr;
-		ssize_t val;
+		/* for kernel-managed buffers */
+		void *kaddr;
 	};
+	ssize_t val;
 };
 
 
diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index ccf5b213087b..1e8395270227 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -155,7 +155,8 @@ static int io_provided_buffers_select(struct io_kiocb *req, size_t *len,
 	return 1;
 }
 
-static bool io_should_commit(struct io_kiocb *req, unsigned int issue_flags)
+static bool io_should_commit(struct io_kiocb *req, struct io_buffer_list *bl,
+			     unsigned int issue_flags)
 {
 	/*
 	* If we came in unlocked, we have no choice but to consume the
@@ -170,7 +171,11 @@ static bool io_should_commit(struct io_kiocb *req, unsigned int issue_flags)
 	if (issue_flags &amp; IO_URING_F_UNLOCKED)
 		return true;
 
-	/* uring_cmd commits kbuf upfront, no need to auto-commit */
+	/* kernel-managed buffers are auto-committed */
+	if (bl-&gt;flags &amp; IOBL_KERNEL_MANAGED)
+		return true;
+
+	/* multishot uring_cmd commits kbuf upfront, no need to auto-commit */
 	if (!io_file_can_poll(req) &amp;&amp; req-&gt;opcode != IORING_OP_URING_CMD)
 		return true;
 	return false;
@@ -200,9 +205,12 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,
 	req-&gt;flags |= REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT;
 	req-&gt;buf_index = READ_ONCE(buf-&gt;bid);
 	sel.buf_list = bl;
-	sel.addr = u64_to_user_ptr(READ_ONCE(buf-&gt;addr));
+	if (bl-&gt;flags &amp; IOBL_KERNEL_MANAGED)
+		sel.kaddr = (void *)(uintptr_t)READ_ONCE(buf-&gt;addr);
+	else
+		sel.addr = u64_to_user_ptr(READ_ONCE(buf-&gt;addr));
 
-	if (io_should_commit(req, issue_flags)) {
+	if (io_should_commit(req, bl, issue_flags)) {
 		io_kbuf_commit(req, sel.buf_list, *len, 1);
 		sel.buf_list = NULL;
 	}
-- 
2.47.3



---

Add kernel APIs to pin and unpin buffer rings, preventing userspace from
unregistering a buffer ring while it is pinned by the kernel.

This provides a mechanism for kernel subsystems to safely access buffer
ring contents while ensuring the buffer ring remains valid. A pinned
buffer ring cannot be unregistered until explicitly unpinned. On the
userspace side, trying to unregister a pinned buffer will return -EBUSY.

This is a preparatory change for upcoming fuse usage of kernel-managed
buffer rings. It is necessary for fuse to pin the buffer ring because
fuse may need to select a buffer in atomic contexts, which it can only
do so by using the underlying buffer list pointer.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 include/linux/io_uring/cmd.h | 17 +++++++++++++
 io_uring/kbuf.c              | 48 ++++++++++++++++++++++++++++++++++++
 io_uring/kbuf.h              |  5 ++++
 3 files changed, 70 insertions(+)

diff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h
index 375fd048c4cb..702b1903e6ee 100644
--- a/include/linux/io_uring/cmd.h
+++ b/include/linux/io_uring/cmd.h
@@ -84,6 +84,10 @@ struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,
 bool io_uring_mshot_cmd_post_cqe(struct io_uring_cmd *ioucmd,
 				 struct io_br_sel *sel, unsigned int issue_flags);
 
+int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,
+			  unsigned issue_flags, struct io_buffer_list **bl);
+int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,
+			    unsigned issue_flags);
 #else
 static inline int
 io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,
@@ -126,6 +130,19 @@ static inline bool io_uring_mshot_cmd_post_cqe(struct io_uring_cmd *ioucmd,
 {
 	return true;
 }
+static inline int io_uring_buf_ring_pin(struct io_uring_cmd *cmd,
+					unsigned buf_group,
+					unsigned issue_flags,
+					struct io_buffer_list **bl)
+{
+	return -EOPNOTSUPP;
+}
+static inline int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd,
+					  unsigned buf_group,
+					  unsigned issue_flags)
+{
+	return -EOPNOTSUPP;
+}
 #endif
 
 static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)
diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index 1e8395270227..dee1764ed19f 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -9,6 +9,7 @@
 #include &lt;linux/poll.h&gt;
 #include &lt;linux/vmalloc.h&gt;
 #include &lt;linux/io_uring.h&gt;
+#include &lt;linux/io_uring/cmd.h&gt;
 
 #include &lt;uapi/linux/io_uring.h&gt;
 
@@ -237,6 +238,51 @@ struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,
 	return sel;
 }
 
+int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,
+			  unsigned issue_flags, struct io_buffer_list **bl)
+{
+	struct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)-&gt;ctx;
+	struct io_buffer_list *buffer_list;
+	int ret = -EINVAL;
+
+	io_ring_submit_lock(ctx, issue_flags);
+
+	buffer_list = io_buffer_get_list(ctx, buf_group);
+	if (buffer_list &amp;&amp; (buffer_list-&gt;flags &amp; IOBL_BUF_RING)) {
+		if (unlikely(buffer_list-&gt;flags &amp; IOBL_PINNED)) {
+			ret = -EALREADY;
+		} else {
+			buffer_list-&gt;flags |= IOBL_PINNED;
+			ret = 0;
+			*bl = buffer_list;
+		}
+	}
+
+	io_ring_submit_unlock(ctx, issue_flags);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(io_uring_buf_ring_pin);
+
+int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,
+		       unsigned issue_flags)
+{
+	struct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)-&gt;ctx;
+	struct io_buffer_list *bl;
+	int ret = -EINVAL;
+
+	io_ring_submit_lock(ctx, issue_flags);
+
+	bl = io_buffer_get_list(ctx, buf_group);
+	if (bl &amp;&amp; (bl-&gt;flags &amp; IOBL_BUF_RING) &amp;&amp; (bl-&gt;flags &amp; IOBL_PINNED)) {
+		bl-&gt;flags &amp;= ~IOBL_PINNED;
+		ret = 0;
+	}
+
+	io_ring_submit_unlock(ctx, issue_flags);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(io_uring_buf_ring_unpin);
+
 /* cap it at a reasonable 256, will be one page even for 4K */
 #define PEEK_MAX_IMPORT		256
 
@@ -747,6 +793,8 @@ int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg)
 		return -ENOENT;
 	if (!(bl-&gt;flags &amp; IOBL_BUF_RING))
 		return -EINVAL;
+	if (bl-&gt;flags &amp; IOBL_PINNED)
+		return -EBUSY;
 
 	scoped_guard(mutex, &amp;ctx-&gt;mmap_lock)
 		xa_erase(&amp;ctx-&gt;io_bl_xa, bl-&gt;bgid);
diff --git a/io_uring/kbuf.h b/io_uring/kbuf.h
index 11d165888b8e..781630c2cc10 100644
--- a/io_uring/kbuf.h
+++ b/io_uring/kbuf.h
@@ -12,6 +12,11 @@ enum {
 	IOBL_INC		= 2,
 	/* buffers are kernel managed */
 	IOBL_KERNEL_MANAGED	= 4,
+	/*
+	 * buffer ring is pinned and cannot be unregistered by userspace until
+	 * it has been unpinned
+	 */
+	IOBL_PINNED		= 8,
 };
 
 struct io_buffer_list {
-- 
2.47.3



---

Add an interface for buffers to be recycled back into a kernel-managed
buffer ring.

This is a preparatory patch for fuse over io-uring.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 include/linux/io_uring/cmd.h | 11 +++++++++
 io_uring/kbuf.c              | 44 ++++++++++++++++++++++++++++++++++++
 2 files changed, 55 insertions(+)

diff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h
index 702b1903e6ee..a488e945f883 100644
--- a/include/linux/io_uring/cmd.h
+++ b/include/linux/io_uring/cmd.h
@@ -88,6 +88,10 @@ int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,
 			  unsigned issue_flags, struct io_buffer_list **bl);
 int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,
 			    unsigned issue_flags);
+
+int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,
+			   u64 addr, unsigned int len, unsigned int bid,
+			   unsigned int issue_flags);
 #else
 static inline int
 io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,
@@ -143,6 +147,13 @@ static inline int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd,
 {
 	return -EOPNOTSUPP;
 }
+static inline int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd,
+					 unsigned int buf_group, u64 addr,
+					 unsigned int len, unsigned int bid,
+					 unsigned int issue_flags)
+{
+	return -EOPNOTSUPP;
+}
 #endif
 
 static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)
diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index dee1764ed19f..17b6178be4ce 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -102,6 +102,50 @@ void io_kbuf_drop_legacy(struct io_kiocb *req)
 	req-&gt;kbuf = NULL;
 }
 
+int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,
+			   u64 addr, unsigned int len, unsigned int bid,
+			   unsigned int issue_flags)
+{
+	struct io_kiocb *req = cmd_to_io_kiocb(cmd);
+	struct io_ring_ctx *ctx = req-&gt;ctx;
+	struct io_uring_buf_ring *br;
+	struct io_uring_buf *buf;
+	struct io_buffer_list *bl;
+	int ret = -EINVAL;
+
+	if (WARN_ON_ONCE(req-&gt;flags &amp; REQ_F_BUFFERS_COMMIT))
+		return ret;
+
+	io_ring_submit_lock(ctx, issue_flags);
+
+	bl = io_buffer_get_list(ctx, buf_group);
+
+	if (!bl || WARN_ON_ONCE(!(bl-&gt;flags &amp; IOBL_BUF_RING)) ||
+	    WARN_ON_ONCE(!(bl-&gt;flags &amp; IOBL_KERNEL_MANAGED)))
+		goto done;
+
+	br = bl-&gt;buf_ring;
+
+	if (WARN_ON_ONCE((br-&gt;tail - bl-&gt;head) &gt;= bl-&gt;nr_entries))
+		goto done;
+
+	buf = &amp;br-&gt;bufs[(br-&gt;tail) &amp; bl-&gt;mask];
+
+	buf-&gt;addr = addr;
+	buf-&gt;len = len;
+	buf-&gt;bid = bid;
+
+	req-&gt;flags &amp;= ~REQ_F_BUFFER_RING;
+
+	br-&gt;tail++;
+	ret = 0;
+
+done:
+	io_ring_submit_unlock(ctx, issue_flags);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(io_uring_kmbuf_recycle);
+
 bool io_kbuf_recycle_legacy(struct io_kiocb *req, unsigned issue_flags)
 {
 	struct io_ring_ctx *ctx = req-&gt;ctx;
-- 
2.47.3



---

io_uring_is_kmbuf_ring() returns true if there is a kernel-managed
buffer ring at the specified buffer group.

This is a preparatory patch for upcoming fuse kernel-managed buffer
support, which needs to ensure the buffer ring registered by the server
is a kernel-managed buffer ring.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 include/linux/io_uring/cmd.h |  9 +++++++++
 io_uring/kbuf.c              | 20 ++++++++++++++++++++
 2 files changed, 29 insertions(+)

diff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h
index a488e945f883..04a937f6f4d3 100644
--- a/include/linux/io_uring/cmd.h
+++ b/include/linux/io_uring/cmd.h
@@ -92,6 +92,9 @@ int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,
 int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,
 			   u64 addr, unsigned int len, unsigned int bid,
 			   unsigned int issue_flags);
+
+bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,
+			    unsigned int issue_flags);
 #else
 static inline int
 io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,
@@ -154,6 +157,12 @@ static inline int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd,
 {
 	return -EOPNOTSUPP;
 }
+static inline bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd,
+					  unsigned int buf_group,
+					  unsigned int issue_flags)
+{
+	return false;
+}
 #endif
 
 static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)
diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index 17b6178be4ce..797cc2f0a5e9 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -963,3 +963,23 @@ int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
 
 	return ret;
 }
+
+bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,
+			    unsigned int issue_flags)
+{
+	struct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)-&gt;ctx;
+	struct io_buffer_list *bl;
+	bool is_kmbuf_ring = false;
+
+	io_ring_submit_lock(ctx, issue_flags);
+
+	bl = io_buffer_get_list(ctx, buf_group);
+	if (likely(bl) &amp;&amp; (bl-&gt;flags &amp; IOBL_KERNEL_MANAGED)) {
+		WARN_ON_ONCE(!(bl-&gt;flags &amp; IOBL_BUF_RING));
+		is_kmbuf_ring = true;
+	}
+
+	io_ring_submit_unlock(ctx, issue_flags);
+	return is_kmbuf_ring;
+}
+EXPORT_SYMBOL_GPL(io_uring_is_kmbuf_ring);
-- 
2.47.3



---

Export io_ring_buffer_select() so that it may be used by callers who
pass in a pinned bufring without needing to grab the io_uring mutex.

This is a preparatory patch that will be needed by fuse io-uring, which
will need to select a buffer from a kernel-managed bufring while the
uring mutex may already be held by in-progress commits, and may need to
select a buffer in atomic contexts.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 include/linux/io_uring/cmd.h | 14 ++++++++++++++
 io_uring/kbuf.c              |  7 ++++---
 2 files changed, 18 insertions(+), 3 deletions(-)

diff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h
index 04a937f6f4d3..d4b5943bdeb1 100644
--- a/include/linux/io_uring/cmd.h
+++ b/include/linux/io_uring/cmd.h
@@ -95,6 +95,10 @@ int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,
 
 bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,
 			    unsigned int issue_flags);
+
+struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,
+				       struct io_buffer_list *bl,
+				       unsigned int issue_flags);
 #else
 static inline int
 io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,
@@ -163,6 +167,16 @@ static inline bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd,
 {
 	return false;
 }
+static inline struct io_br_sel io_ring_buffer_select(struct io_kiocb *req,
+						     size_t *len,
+						     struct io_buffer_list *bl,
+						     unsigned int issue_flags)
+{
+	struct io_br_sel sel = {
+		.val = -EOPNOTSUPP,
+	};
+	return sel;
+}
 #endif
 
 static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)
diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index 797cc2f0a5e9..9a93f10d3214 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -226,9 +226,9 @@ static bool io_should_commit(struct io_kiocb *req, struct io_buffer_list *bl,
 	return false;
 }
 
-static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,
-					      struct io_buffer_list *bl,
-					      unsigned int issue_flags)
+struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,
+				       struct io_buffer_list *bl,
+				       unsigned int issue_flags)
 {
 	struct io_uring_buf_ring *br = bl-&gt;buf_ring;
 	__u16 tail, head = bl-&gt;head;
@@ -261,6 +261,7 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,
 	}
 	return sel;
 }
+EXPORT_SYMBOL_GPL(io_ring_buffer_select);
 
 struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,
 				  unsigned buf_group, unsigned int issue_flags)
-- 
2.47.3



---

Return the id of the selected buffer in io_buffer_select(). This is
needed for kernel-managed buffer rings to later recycle the selected
buffer.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 include/linux/io_uring/cmd.h   | 2 +-
 include/linux/io_uring_types.h | 2 ++
 io_uring/kbuf.c                | 7 +++++--
 3 files changed, 8 insertions(+), 3 deletions(-)

diff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h
index d4b5943bdeb1..94df2bdebe77 100644
--- a/include/linux/io_uring/cmd.h
+++ b/include/linux/io_uring/cmd.h
@@ -71,7 +71,7 @@ void io_uring_cmd_issue_blocking(struct io_uring_cmd *ioucmd);
 
 /*
  * Select a buffer from the provided buffer group for multishot uring_cmd.
- * Returns the selected buffer address and size.
+ * Returns the selected buffer address, size, and id.
  */
 struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,
 					    unsigned buf_group, size_t *len,
diff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h
index 36cc2e0346d9..5a56bb341337 100644
--- a/include/linux/io_uring_types.h
+++ b/include/linux/io_uring_types.h
@@ -100,6 +100,8 @@ struct io_br_sel {
 		void *kaddr;
 	};
 	ssize_t val;
+	/* id of the selected buffer */
+	unsigned buf_id;
 };
 
 
diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index 9a93f10d3214..24c1e34ea23e 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -250,6 +250,7 @@ struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,
 	req-&gt;flags |= REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT;
 	req-&gt;buf_index = READ_ONCE(buf-&gt;bid);
 	sel.buf_list = bl;
+	sel.buf_id = req-&gt;buf_index;
 	if (bl-&gt;flags &amp; IOBL_KERNEL_MANAGED)
 		sel.kaddr = (void *)(uintptr_t)READ_ONCE(buf-&gt;addr);
 	else
@@ -274,10 +275,12 @@ struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,
 
 	bl = io_buffer_get_list(ctx, buf_group);
 	if (likely(bl)) {
-		if (bl-&gt;flags &amp; IOBL_BUF_RING)
+		if (bl-&gt;flags &amp; IOBL_BUF_RING) {
 			sel = io_ring_buffer_select(req, len, bl, issue_flags);
-		else
+		} else {
 			sel.addr = io_provided_buffer_select(req, len, bl);
+			sel.buf_id = req-&gt;buf_index;
+		}
 	}
 	io_ring_submit_unlock(req-&gt;ctx, issue_flags);
 	return sel;
-- 
2.47.3



---

When uring_cmd operations select a buffer, the completion queue entry
should indicate which buffer was selected.

Set IORING_CQE_F_BUFFER on the completed entry and encode the buffer
index if a buffer was selected.

This will be needed for fuse, which needs to relay to userspace which
selected buffer contains the data.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 io_uring/uring_cmd.c | 6 +++++-
 1 file changed, 5 insertions(+), 1 deletion(-)

diff --git a/io_uring/uring_cmd.c b/io_uring/uring_cmd.c
index ee7b49f47cb5..6d38df1a812d 100644
--- a/io_uring/uring_cmd.c
+++ b/io_uring/uring_cmd.c
@@ -151,6 +151,7 @@ void __io_uring_cmd_done(struct io_uring_cmd *ioucmd, s32 ret, u64 res2,
 		       unsigned issue_flags, bool is_cqe32)
 {
 	struct io_kiocb *req = cmd_to_io_kiocb(ioucmd);
+	u32 cflags = 0;
 
 	if (WARN_ON_ONCE(req-&gt;flags &amp; REQ_F_APOLL_MULTISHOT))
 		return;
@@ -160,7 +161,10 @@ void __io_uring_cmd_done(struct io_uring_cmd *ioucmd, s32 ret, u64 res2,
 	if (ret &lt; 0)
 		req_set_fail(req);
 
-	io_req_set_res(req, ret, 0);
+	if (req-&gt;flags &amp; (REQ_F_BUFFER_SELECTED | REQ_F_BUFFER_RING))
+		cflags |= IORING_CQE_F_BUFFER |
+			(req-&gt;buf_index &lt;&lt; IORING_CQE_BUFFER_SHIFT);
+	io_req_set_res(req, ret, cflags);
 	if (is_cqe32) {
 		if (req-&gt;ctx-&gt;flags &amp; IORING_SETUP_CQE_MIXED)
 			req-&gt;cqe.flags |= IORING_CQE_F_32;
-- 
2.47.3

</pre>
</details>
<div class="review-comment-signals">Signals: requested changes</div>
</div>
</div>
</div>

    <footer>LKML Daily Activity Tracker</footer>
    <script>
    // When arriving via a date anchor (e.g. #2026-02-15 from a daily report),
    // scroll the anchor into view after a brief delay so layout is complete.
    (function () {
        var hash = window.location.hash;
        if (!hash) return;
        var target = document.getElementById(hash.slice(1));
        if (!target) return;
        setTimeout(function () {
            target.scrollIntoView({behavior: 'smooth', block: 'start'});
        }, 80);
    })();
    </script>
</body>
</html>