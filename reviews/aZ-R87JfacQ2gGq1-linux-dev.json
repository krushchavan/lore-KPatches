{
  "thread_id": "aZ-R87JfacQ2gGq1@linux.dev",
  "subject": "[PATCH v5 29/32] mm: memcontrol: prepare for reparenting non-hierarchical stats",
  "url": "https://lore.kernel.org/all/aZ-R87JfacQ2gGq1@linux.dev/",
  "dates": {
    "2026-02-26": {
      "report_file": "2026-02-26_ollama_llama3.1-8b.html",
      "developer": "Shakeel Butt",
      "reviews": [
        {
          "author": "Shakeel Butt",
          "summary": "Raised a question about whether page/folio/slab points to the same node's objcg for a given memcg, but didn't express an opinion on the patch itself.",
          "sentiment": "neutral",
          "sentiment_signals": [],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "On Thu, Feb 26, 2026 at 07:16:50AM -0800, Yosry Ahmed wrote:\n> > > Did you measure the impact of making state_local atomic on the flush\n> > > path? It's a slow path but we've seen pain from it being too slow\n> > > before, because it extends the critical section of the rstat flush\n> > > lock.\n> >\n> > Qi, please measure the impact on flushing and if no impact then no need to do\n> > anything as I don't want anymore churn in this series.\n> >\n> > >\n> > > Can we keep this non-atomic and use mod_memcg_lruvec_state() here? It\n> > > will update the stat on the local counter and it will be added to\n> > > state_local in the flush path when needed. We can even force another\n> > > flush in reparent_state_local () after reparenting is completed, if we\n> > > want to avoid leaving a potentially large stat update pending, as it\n> > > can be missed by mem_cgroup_flush_stats_ratelimited().\n> > >\n> > > Same for reparent_memcg_state_local(), we can probably use mod_memcg_state()?\n> >\n> > Yosry, do you mind sending the patch you are thinking about over this series?\n> \n> Honestly, I'd rather squash it into this patch if possible. It avoids\n> churn in the history (switch to atomics and back), and is arguably\n> simpler than checking for regressions in the flush path.\n\nYup, let's squash it into the original patch. Please add your sign-off tag.\n\n> \n> What I have in mind is the diff below (build tested only). Qi, would\n> you be able to test this? It applies directly on this patch in mm-new:\n\nQi, please squash this diff into the patch and test. You might need to change\nthe subsequent patches. Once you are done with testing, you can post the diffs\nfor those in reply to those patches and we will ask Andrew to squash into\norinigal ones.\n\nThe diff looks good to me though.\n> \n> diff --git a/mm/memcontrol.c b/mm/memcontrol.c\n> index d82dbfcc28057..404565e80cbf3 100644\n> --- a/mm/memcontrol.c\n> +++ b/mm/memcontrol.c\n> @@ -234,11 +234,18 @@ static inline void reparent_state_local(struct\n> mem_cgroup *memcg, struct mem_cgr\n>         if (cgroup_subsys_on_dfl(memory_cgrp_subsys))\n>                 return;\n> \n> +       /*\n> +        * Reparent stats exposed non-hierarchically. Flush @memcg's\n> stats first to\n> +        * read its stats accurately , and conservatively flush @parent's stats\n> +        * after reparenting to avoid hiding a potentially large stat update\n> +        * (e.g. from callers of mem_cgroup_flush_stats_ratelimited()).\n> +        */\n>         __mem_cgroup_flush_stats(memcg, true);\n> \n> -       /* The following counts are all non-hierarchical and need to\n> be reparented. */\n>         reparent_memcg1_state_local(memcg, parent);\n>         reparent_memcg1_lruvec_state_local(memcg, parent);\n> +\n> +       __mem_cgroup_flush_stats(parent, true);\n>  }\n>  #else\n>  static inline void reparent_state_local(struct mem_cgroup *memcg,\n> struct mem_cgroup *parent)\n> @@ -442,7 +449,7 @@ struct lruvec_stats {\n>         long state[NR_MEMCG_NODE_STAT_ITEMS];\n> \n>         /* Non-hierarchical (CPU aggregated) state */\n> -       atomic_long_t state_local[NR_MEMCG_NODE_STAT_ITEMS];\n> +       long state_local[NR_MEMCG_NODE_STAT_ITEMS];\n> \n>         /* Pending child counts during tree propagation */\n>         long state_pending[NR_MEMCG_NODE_STAT_ITEMS];\n> @@ -485,7 +492,7 @@ unsigned long lruvec_page_state_local(struct lruvec *lruvec,\n>                 return 0;\n> \n>         pn = container_of(lruvec, struct mem_cgroup_per_node, lruvec);\n> -       x = atomic_long_read(&(pn->lruvec_stats->state_local[i]));\n> +       x = READ_ONCE(pn->lruvec_stats->state_local[i]);\n>  #ifdef CONFIG_SMP\n>         if (x < 0)\n>                 x = 0;\n> @@ -493,6 +500,10 @@ unsigned long lruvec_page_state_local(struct\n> lruvec *lruvec,\n>         return x;\n>  }\n> \n> +static void mod_memcg_lruvec_state(struct lruvec *lruvec,\n> +                                  enum node_stat_item idx,\n> +                                  int val);\n> +\n>  #ifdef CONFIG_MEMCG_V1\n>  void reparent_memcg_lruvec_state_local(struct mem_cgroup *memcg,\n>                                        struct mem_cgroup *parent, int idx)\n> @@ -506,12 +517,10 @@ void reparent_memcg_lruvec_state_local(struct\n> mem_cgroup *memcg,\n>         for_each_node(nid) {\n>                 struct lruvec *child_lruvec = mem_cgroup_lruvec(memcg,\n> NODE_DATA(nid));\n>                 struct lruvec *parent_lruvec =\n> mem_cgroup_lruvec(parent, NODE_DATA(nid));\n> -               struct mem_cgroup_per_node *parent_pn;\n>                 unsigned long value =\n> lruvec_page_state_local(child_lruvec, idx);\n> \n> -               parent_pn = container_of(parent_lruvec, struct\n> mem_cgroup_per_node, lruvec);\n> -\n> -               atomic_long_add(value,\n> &(parent_pn->lruvec_stats->state_local[i]));\n> +               mod_memcg_lruvec_state(child_lruvec, idx, -value);\n> +               mod_memcg_lruvec_state(parent_lruvec, idx, value);\n>         }\n>  }\n>  #endif\n> @@ -598,7 +607,7 @@ struct memcg_vmstats {\n>         unsigned long           events[NR_MEMCG_EVENTS];\n> \n>         /* Non-hierarchical (CPU aggregated) page state & events */\n> -       atomic_long_t           state_local[MEMCG_VMSTAT_SIZE];\n> +       long                    state_local[MEMCG_VMSTAT_SIZE];\n>         unsigned long           events_local[NR_MEMCG_EVENTS];\n> \n>         /* Pending child counts during tree propagation */\n> @@ -835,7 +844,7 @@ unsigned long memcg_page_state_local(struct\n> mem_cgroup *memcg, int idx)\n>         if (WARN_ONCE(BAD_STAT_IDX(i), \"%s: missing stat item %d\\n\",\n> __func__, idx))\n>                 return 0;\n> \n> -       x = atomic_long_read(&(memcg->vmstats->state_local[i]));\n> +       x = READ_ONCE(memcg->vmstats->state_local[i]);\n>  #ifdef CONFIG_SMP\n>         if (x < 0)\n>                 x = 0;\n> @@ -852,7 +861,8 @@ void reparent_memcg_state_local(struct mem_cgroup *memcg,\n>         if (WARN_ONCE(BAD_STAT_IDX(i), \"%s: missing stat item %d\\n\",\n> __func__, idx))\n>                 return;\n> \n> -       atomic_long_add(value, &(parent->vmstats->state_local[i]));\n> +       mod_memcg_state(memcg, idx, -value);\n> +       mod_memcg_state(parent, idx, value);\n>  }\n>  #endif\n> \n> @@ -4174,8 +4184,6 @@ struct aggregate_control {\n>         long *aggregate;\n>         /* pointer to the non-hierarchichal (CPU aggregated) counters */\n>         long *local;\n> -       /* pointer to the atomic non-hierarchichal (CPU aggregated) counters */\n> -       atomic_long_t *alocal;\n>         /* pointer to the pending child counters during tree propagation */\n>         long *pending;\n>         /* pointer to the parent's pending counters, could be NULL */\n> @@ -4213,12 +4221,8 @@ static void mem_cgroup_stat_aggregate(struct\n> aggregate_control *ac)\n>                 }\n> \n>                 /* Aggregate counts on this level and propagate upwards */\n> -               if (delta_cpu) {\n> -                       if (ac->local)\n> -                               ac->local[i] += delta_cpu;\n> -                       else if (ac->alocal)\n> -                               atomic_long_add(delta_cpu, &(ac->alocal[i]));\n> -               }\n> +               if (delta_cpu)\n> +                       ac->local[i] += delta_cpu;\n> \n>                 if (delta) {\n>                         ac->aggregate[i] += delta;\n> @@ -4289,8 +4293,7 @@ static void mem_cgroup_css_rstat_flush(struct\n> cgroup_subsys_state *css, int cpu)\n> \n>         ac = (struct aggregate_control) {\n>                 .aggregate = memcg->vmstats->state,\n> -               .local = NULL,\n> -               .alocal = memcg->vmstats->state_local,\n> +               .local = memcg->vmstats->state_local,\n>                 .pending = memcg->vmstats->state_pending,\n>                 .ppending = parent ? parent->vmstats->state_pending : NULL,\n>                 .cstat = statc->state,\n> @@ -4323,8 +4326,7 @@ static void mem_cgroup_css_rstat_flush(struct\n> cgroup_subsys_state *css, int cpu)\n> \n>                 ac = (struct aggregate_control) {\n>                         .aggregate = lstats->state,\n> -                       .local = NULL,\n> -                       .alocal = lstats->state_local,\n> +                       .local = lstats->state_local,\n>                         .pending = lstats->state_pending,\n>                         .ppending = plstats ? plstats->state_pending : NULL,\n>                         .cstat = lstatc->state,\n",
          "reply_to": "",
          "message_date": "2026-02-26",
          "message_id": ""
        }
      ],
      "analysis_source": "llm",
      "patch_summary": "Conversion of objcg to per-memcg per-node type in mm: memcontrol"
    },
    "2026-02-27": {
      "report_file": "2026-02-26_ollama_llama3.1-8b.html",
      "developer": "Shakeel Butt",
      "reviews": [
        {
          "author": "Qi Zheng",
          "summary": "Addressed Shakeel's concern by stating that maybe not, and suggested changing the comparison to objcg->memcg equality.",
          "sentiment": "neutral",
          "sentiment_signals": [],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "\n\nOn 2/27/26 4:05 AM, Shakeel Butt wrote:\n> On Wed, Feb 25, 2026 at 05:44:56PM +0800, Qi Zheng wrote:\n>> From: Qi Zheng <zhengqi.arch@bytedance.com>\n>>\n>> Convert objcg to be per-memcg per-node type, so that when reparent LRU\n>> folios later, we can hold the lru lock at the node level, thus avoiding\n>> holding too many lru locks at once.\n>>\n>> Signed-off-by: Qi Zheng <zhengqi.arch@bytedance.com>\n>> ---\n>> changlog:\n>>   - fix a missing root_obj_cgroup conversion and completely delete\n>>     root_obj_cgroup.\n>>\n> \n> After this patch, do we care that page/folio/slab points to the objcg of the\n> same node as them for a given memcg?\n\nMaybe not. My only concern is whether the kernel has a way of\ndetermining whether two folios belong to the same memcg by checking if\nthe objcg pointers are equal. If so, it needs to be changed to check if\nobjcg->memcg are equal.\n\n> \n\n\n",
          "reply_to": "Shakeel Butt",
          "message_date": "2026-02-27",
          "message_id": ""
        }
      ],
      "analysis_source": "llm",
      "patch_summary": "Conversion of objcg to per-memcg per-node type in mm: memcontrol"
    }
  }
}