{
  "thread_id": "0c81121c23a9b1016425da100f11cb31feddd7ad.camel@surriel.com",
  "subject": "Re: [LSF/MM/BPF TOPIC] Beyond 2MB: Why Terabyte-Scale Machines Need 1GB Transparent Huge Pages",
  "url": "https://lore.kernel.org/all/0c81121c23a9b1016425da100f11cb31feddd7ad.camel@surriel.com/",
  "dates": {
    "2026-02-19": {
      "report_file": "2026-02-19_ollama_llama3.1-8b.html",
      "developer": "Rik van Riel",
      "reviews": [
        {
          "author": "David (Arm)",
          "summary": "Reviewer noted that migrating all memory away from a 1 GiB THP instead of splitting it could be an interesting approach, but this method does not work when remapping the THP to be mapped by PMDs or is not desired in certain cases.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "NEUTRAL"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "There once was this proposal where we would, instead of splitting a THP, \nmigrate all memory away instead. That means, instead of splitting the 1 \nGiB THP, you would instead return it to the page allocator where \nsomebody else could use it.\n\nHowever, we cannot easily do the same when remapping a 1 GiB THP to be \nmapped by PMDs etc. I think there are examples where that just doesn't \nwork or is not desired.\n\nBut I considered that in general (avoid folio_split()) an interesting \napproach. The remapping part is a bit different though.\n\n-- \nCheers,\n\nDavid",
          "reply_to": "Usama Arif",
          "message_date": "2026-02-19"
        },
        {
          "author": "Johannes Weiner",
          "summary": "Reviewer noted that preserving contiguity in TLB coalescing is beneficial and suggested doing it lazily by scanning deferred split lists when a larger page cannot be allocated, instead of immediately splitting the huge page.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "With TLB coalescing, there is benefit in preserving contiguity. If you\nlop off the last 4k of a 2M-backed range, a split still gives you 511\ncontiguously mapped pfns that can be coalesced.\n\nIt would be unfortunate to lose that for pure virtual memory splits,\nwhile there is no demand or no shortage of huge pages. But it might be\npossible to do this lazily, e.g. when somebody has trouble getting a\nlarger page, scan the deferred split lists for candidates to migrate.",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-19"
        },
        {
          "author": "Zi Yan",
          "summary": "Reviewer Zi Yan suggested using non-uniform splitting for THPs to keep after-split folios as large as possible when mapping folios larger than PMD",
          "sentiment": "neutral",
          "sentiment_signals": [
            "suggestion",
            "improvement"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "With mapping of folios > PMD with PMDs, you can use non uniform split to keep\nafter-split folios as large as possible.",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-19"
        },
        {
          "author": "Zi Yan",
          "summary": "Reviewer Yan noted that if hardware supports multiple TLB entries translating to the same physical frame and allows translation priority of TLB entries, it would be possible to keep a 1GB PUD mapping by having one TLB entry pointing to the 1GB folio and another 4KB TLB entry overriding the part in the original 1GB vaddr region. Without this hardware support, software would need to split the PUD into PMDs and PTEs.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "proposed an alternative solution",
            "acknowledged a limitation"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "If HW can support multiple TLB entries translating to the same physical frame\nand allow translation priority of TLB entries, this remapping would be easy\nand we can still keep the 1GB PUD mapping. Basically, we can have 1GB TLB entry\npointing to the 1GB folio and another 4KB TLB entry pointing to the remapped\nregion and overriding the part in the original 1GB vaddr region.\n\nWithout that, SW will need to split the PUD into PMDs and PTEs.\n\n\nBest Regards,\nYan, Zi",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-19"
        },
        {
          "author": "Zi Yan",
          "summary": "Reviewer Zi Yan asked for clarification on the specific CPU architecture being targeted, pointing out that AMD's PTE coalescing works up to 32KB and ARM's contig PTE supports larger sizes, and inquired about PMD level ARM contiguous bit support.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "request for clarification",
            "inquiry about specific feature"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Which CPU are you referring to? AMD\\u2019s PTE coalescing works up to 32KB\nand ARM\\u2019s contig PTE supports larger sizes. BTW, do we have PMD level\nARM contiguous bit support?",
          "reply_to": "Johannes Weiner",
          "message_date": "2026-02-19"
        },
        {
          "author": "Zi Yan",
          "summary": "Reviewer noted that the patch does not properly handle the case where a huge page is split into smaller pages, and requested that the patch be updated to correctly handle this scenario.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Best Regards,\nYan, Zi",
          "reply_to": "Johannes Weiner",
          "message_date": "2026-02-19"
        },
        {
          "author": "Johannes Weiner",
          "summary": "Reviewer Johannes Weiner noted that any potential benefits of coalescing 511 entries into a single one would be lost when the range is scattered into discontiguous 4k pagelets, raising concerns about the effectiveness of this approach.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "potential issue with coalescing"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "I'm not aware of a CPU that will coalesce the 511 entries into a\nsingle one. But *any* coalescing effects will be lost when the range\nis scattered into discontiguous 4k pagelets.",
          "reply_to": "Zi Yan",
          "message_date": "2026-02-19"
        },
        {
          "author": "Matthew Wilcox",
          "summary": "Reviewer Matthew Wilcox expressed concern about the feasibility of supporting 1GB transparent huge pages due to potential hardware compatibility issues, citing notes from various CPU vendors that suggest attempting this would lead to significant problems.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Uh, do you know any hardware that supports that?  Every CPU I'm familiar\nwith has notes suggesting that trying to do this will cause you to Have\nA Very Bad Day.",
          "reply_to": "Zi Yan",
          "message_date": "2026-02-19"
        },
        {
          "author": "Zi Yan",
          "summary": "Reviewer Yan noted that the TLB would need to handle additional translations for sub-ranges of a large page, potentially requiring a per-sub-range bitmap or rewalks of each sub-range",
          "sentiment": "neutral",
          "sentiment_signals": [
            "raised a new consideration",
            "acknowledged the reviewer's comment with 'thank you'"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "No. I was imagining it. :)\n\nBut thinking about it more, that means for every >PTE TLB hit, HW needs to know\nwhether any sub-range has an additional translation. It is easy if all sub-range\ntranslations are present in the TLB. Otherwise, a per sub range bitmap or rewalks\nof each sub range is needed. Never mind, thank you for waking me up in my\ndaydream.\n\nBest Regards,\nYan, Zi",
          "reply_to": "Matthew Wilcox",
          "message_date": "2026-02-19"
        },
        {
          "author": "Rik Riel",
          "summary": "Reviewer Rik Riel questioned whether the current approach to handling large memory allocations is suitable for future-proofing against 1TB page usage, suggesting that a new system memory partitioning scheme may be necessary to prevent fragmentation and false OOMs.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "lack of clear direction"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "While I agree with the idea of starting simple, I think\nwe should ask the question of what we want physical memory\nhandling to look like if 1TB pages become more common,\nand applications start to rely on them to meet their\nperformance goals.\n\nWe have CMA balancing code today. It seems to work, but\nit likely is not the long term direction we want to go,\nmostly due to the way CMA does allocations.\n\nIt seems clear that in order to prevent memory fragmentation,\nwe need to split up system memory in some way between an area\nthat is used only for movable allocations, and an area where\nany kind of allocation can go.\n\nThis would need something similar to CMA balancing to prevent\nfalse OOMs for non-movable allocations.\n\nHowever, beyond that I really do not have any idea of what\nthings should look like.\n\nWhat do we want the kernel to do here?\n\n\n-- \nAll Rights Reversed.",
          "reply_to": "Usama Arif",
          "message_date": "2026-02-19"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-20": {
      "report_file": "2026-02-19_ollama_llama3.1-8b.html",
      "developer": "Rik van Riel",
      "reviews": [
        {
          "author": "David (Arm)",
          "summary": "Reviewer noted that teaching the buddy to manage pages larger than the current maximum buddy order would be a necessary step, and suggested exploring ways to compact/group on a larger granularity, such as using sub-zones or superblocks.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "This subtopic is certainly worth a separate session as it's quite \ninvolved, but I assume the right (tm) thing to do will be\n\n(a) Teaching the buddy to manage pages larger than the current maximum\n     buddy order. There will certainly be some work required to get to\n     that point (and Zi Yan already did some work). It might also be\n     fair to say that order > current  buddy order might behave different\n     at least to some degree (thinking about relation to zone alignment,\n     section sizes etc).\n\n     If we require vmemmap for these larger orders, maybe the buddy order\n     could more easily exceed the section size; I don't remember all of\n     the details why that limitation was in place (but one of them was\n     memmap continuity within a high-order buddy page, which is only\n     guaranteed within a memory section with CONFIG_SPARSEMEM).\n\n(b) Teaching compaction etc. to *also* compact/group on a larger\n     granularity (in addition to current sized pageblocks). When we\n     discussed that in the past we used the term superblock, that\n     Zi Yan just brought up again in another thread [1].\n\n\n\nThere was a proposal a while ago to internally separate zones into \nchunks of memory (I think the proposal used DRAM banks, such that you \ncould more easily power down unused DRAM banks). I'm not saying we \nshould do that, but maybe something like sub-zones could be something to \nexplore. Maybe not.\n\nBig, more complex topic :)\n\n\n[1] \nhttps://lore.kernel.org/r/34730030-48F6-4D0C-91EA-998A5AF93F5F@nvidia.com\n\n-- \nCheers,\n\nDavid",
          "reply_to": "Rik Riel",
          "message_date": "2026-02-20"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    }
  }
}