<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Review Comments: Re: [PATCH v1 03/11] io_uring/kbuf: add support for kernel-managed buffer rings</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto,
                         "Helvetica Neue", Arial, sans-serif;
            background: #f5f5f5;
            color: #333;
            line-height: 1.6;
            padding: 20px;
            max-width: 900px;
            margin: 0 auto;
        }
        .home-link { margin-bottom: 12px; display: block; }
        .home-link a { color: #0366d6; text-decoration: none; font-size: 0.9em; }
        .home-link a:hover { text-decoration: underline; }

        h1 { font-size: 1.3em; margin-bottom: 2px; color: #1a1a1a; line-height: 1.3; }

        .lore-link { font-size: 0.85em; margin: 4px 0 6px; display: block; }
        .lore-link a { color: #0366d6; text-decoration: none; }
        .lore-link a:hover { text-decoration: underline; }

        .date-range {
            font-size: 0.8em;
            color: #888;
            margin-bottom: 16px;
        }
        .date-range a { color: #0366d6; text-decoration: none; }
        .date-range a:hover { text-decoration: underline; }

        /* thread-node scroll margin so the card isn't clipped at the top */
        .thread-node { scroll-margin-top: 8px; }

        /* ── Patch summary ──────────────────────────────────────────── */
        .patch-summary-block {
            background: #fff;
            border-radius: 8px;
            padding: 12px 16px;
            margin-bottom: 20px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            border-left: 3px solid #4a90d9;
        }
        .patch-summary-label {
            font-size: 0.72em;
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 0.06em;
            color: #4a90d9;
            margin-bottom: 4px;
        }
        .patch-summary-text {
            font-size: 0.88em;
            color: #444;
            line-height: 1.55;
        }

        /* ── Thread tree ────────────────────────────────────────────── */
        .thread-tree {
            display: flex;
            flex-direction: column;
            gap: 6px;
        }

        /* Depth indentation via left border */
        .thread-node { position: relative; }
        .thread-children {
            margin-left: 20px;
            padding-left: 12px;
            border-left: 2px solid #e0e0e0;
            margin-top: 6px;
            display: flex;
            flex-direction: column;
            gap: 6px;
        }

        /* ── Review comment card ────────────────────────────────────── */
        .review-comment {
            background: #fff;
            border-radius: 6px;
            padding: 10px 14px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            font-size: 0.88em;
        }
        .review-comment-header {
            display: flex;
            flex-wrap: wrap;
            align-items: center;
            gap: 6px;
            margin-bottom: 5px;
        }
        .review-author {
            font-weight: 700;
            color: #1a1a1a;
            font-size: 0.95em;
        }

        /* Date chip — links back to the daily report */
        .date-chip {
            font-size: 0.75em;
            color: #777;
            background: #f0f0f0;
            border-radius: 10px;
            padding: 1px 7px;
            text-decoration: none;
            white-space: nowrap;
        }
        a.date-chip:hover { background: #e0e8f5; color: #0366d6; }

        .badge {
            display: inline-block;
            padding: 1px 8px;
            border-radius: 10px;
            font-size: 0.75em;
            font-weight: 600;
        }
        .inline-review-badge {
            display: inline-block;
            padding: 0 6px;
            border-radius: 8px;
            font-size: 0.78em;
            font-weight: 500;
            background: #e3f2fd;
            color: #1565c0;
        }
        .review-tag-badge {
            display: inline-block;
            padding: 0 6px;
            border-radius: 8px;
            font-size: 0.78em;
            font-weight: 500;
            background: #e8f5e9;
            color: #2e7d32;
        }
        .analysis-source-badge {
            display: inline-block;
            padding: 1px 7px;
            border-radius: 10px;
            font-size: 0.72em;
            font-weight: 600;
            border: 1px solid rgba(0,0,0,0.1);
        }

        .review-comment-text {
            color: #444;
            line-height: 1.55;
            margin-bottom: 4px;
        }
        .review-comment-signals {
            margin-top: 3px;
            font-size: 0.85em;
            color: #aaa;
            font-style: italic;
        }

        /* ── Collapsible raw body ───────────────────────────────────── */
        .raw-body-toggle {
            margin-top: 5px;
            font-size: 0.85em;
        }
        .raw-body-toggle summary {
            cursor: pointer;
            color: #888;
            padding: 2px 0;
            font-weight: 500;
            font-size: 0.9em;
            list-style: none;
        }
        .raw-body-toggle summary::-webkit-details-marker { display: none; }
        .raw-body-toggle summary::before { content: "▶ "; font-size: 0.7em; }
        .raw-body-toggle[open] summary::before { content: "▼ "; }
        .raw-body-toggle summary:hover { color: #555; }
        .raw-body-text {
            white-space: pre-wrap;
            font-size: 0.95em;
            background: #f8f8f8;
            padding: 8px 10px;
            border-radius: 4px;
            max-height: 360px;
            overflow-y: auto;
            margin-top: 4px;
            line-height: 1.5;
            color: #444;
            border: 1px solid #e8e8e8;
        }
        .reply-to-label {
            font-size: 0.8em;
            color: #999;
            font-style: italic;
            margin-top: 3px;
        }
        .lore-link {
            display: inline-block;
            margin-top: 4px;
            font-size: 0.82em;
            color: #0366d6;
            text-decoration: none;
            font-weight: 500;
            white-space: nowrap;
        }
        .lore-link:hover { text-decoration: underline; color: #0056b3; }

        .no-reviews {
            color: #aaa;
            font-size: 0.85em;
            font-style: italic;
            padding: 8px 0;
        }

        footer {
            text-align: center;
            color: #bbb;
            font-size: 0.78em;
            margin-top: 36px;
            padding: 16px;
        }
    </style>
</head>
<body>
    <div class="home-link"><a href="../index.html">&larr; Back to reports</a></div>
    <h1>Re: [PATCH v1 03/11] io_uring/kbuf: add support for kernel-managed buffer rings</h1>
    <div class="lore-link"><a href="https://lore.kernel.org/all/CAJnrk1Zr=9RMGpNXpe6=fSDkG2uVijB9qa1vENHpQozB3iPQtg@mail.gmail.com/" target="_blank">View on lore.kernel.org &rarr;</a></div>
    <div class="date-range">Active on: <a href="#2026-02-23">2026-02-23</a> &bull; <a href="#2026-02-21">2026-02-21</a> &bull; <a href="#2026-02-20">2026-02-20</a> &bull; <a href="#2026-02-18">2026-02-18</a> &bull; <a href="#2026-02-13">2026-02-13</a> &bull; <a href="#2026-02-12">2026-02-12</a> &bull; <a href="#2026-02-11">2026-02-11</a> &bull; <a href="#2026-02-10">2026-02-10</a> &bull; <a href="#2026-02-09">2026-02-09</a></div>
    
    <div class="thread-tree">
<div class="thread-node depth-0" id="2026-02-09">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-09">2026-02-09</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed a concern about the refactoring of io_register_pbuf_ring() logic, explaining that it is being split into generic helpers to prepare for upcoming kernel-managed buffer ring support.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Refactor the logic in io_register_pbuf_ring() into generic helpers:
- io_copy_and_validate_buf_reg(): Copy out user arg and validate user
  arg and buffer registration parameters
- io_alloc_new_buffer_list(): Allocate and initialize a new buffer
  list for the given buffer group ID
- io_setup_pbuf_ring(): Sets up the physical buffer ring region and
  handles memory mapping for provided buffer rings

This is a preparatory change for upcoming kernel-managed buffer ring
support which will need to reuse some of these helpers.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 io_uring/kbuf.c | 129 +++++++++++++++++++++++++++++++-----------------
 1 file changed, 85 insertions(+), 44 deletions(-)

diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index 67d4fe576473..850b836f32ee 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -596,55 +596,73 @@ int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags)
 	return IOU_COMPLETE;
 }
 
-int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
+static int io_copy_and_validate_buf_reg(const void __user *arg,
+					struct io_uring_buf_reg *reg,
+					unsigned int permitted_flags)
 {
-	struct io_uring_buf_reg reg;
-	struct io_buffer_list *bl;
-	struct io_uring_region_desc rd;
-	struct io_uring_buf_ring *br;
-	unsigned long mmap_offset;
-	unsigned long ring_size;
-	int ret;
-
-	lockdep_assert_held(&amp;ctx-&gt;uring_lock);
-
-	if (copy_from_user(&amp;reg, arg, sizeof(reg)))
+	if (copy_from_user(reg, arg, sizeof(*reg)))
 		return -EFAULT;
-	if (!mem_is_zero(reg.resv, sizeof(reg.resv)))
+
+	if (!mem_is_zero(reg-&gt;resv, sizeof(reg-&gt;resv)))
 		return -EINVAL;
-	if (reg.flags &amp; ~(IOU_PBUF_RING_MMAP | IOU_PBUF_RING_INC))
+	if (reg-&gt;flags &amp; ~permitted_flags)
 		return -EINVAL;
-	if (!is_power_of_2(reg.ring_entries))
+	if (!is_power_of_2(reg-&gt;ring_entries))
 		return -EINVAL;
 	/* cannot disambiguate full vs empty due to head/tail size */
-	if (reg.ring_entries &gt;= 65536)
+	if (reg-&gt;ring_entries &gt;= 65536)
 		return -EINVAL;
+	return 0;
+}
 
-	bl = io_buffer_get_list(ctx, reg.bgid);
-	if (bl) {
+static struct io_buffer_list *
+io_alloc_new_buffer_list(struct io_ring_ctx *ctx,
+			 const struct io_uring_buf_reg *reg)
+{
+	struct io_buffer_list *list;
+
+	list = io_buffer_get_list(ctx, reg-&gt;bgid);
+	if (list) {
 		/* if mapped buffer ring OR classic exists, don&#x27;t allow */
-		if (bl-&gt;flags &amp; IOBL_BUF_RING || !list_empty(&amp;bl-&gt;buf_list))
-			return -EEXIST;
-		io_destroy_bl(ctx, bl);
+		if (list-&gt;flags &amp; IOBL_BUF_RING || !list_empty(&amp;list-&gt;buf_list))
+			return ERR_PTR(-EEXIST);
+		io_destroy_bl(ctx, list);
 	}
 
-	bl = kzalloc(sizeof(*bl), GFP_KERNEL_ACCOUNT);
-	if (!bl)
-		return -ENOMEM;
+	list = kzalloc(sizeof(*list), GFP_KERNEL_ACCOUNT);
+	if (!list)
+		return ERR_PTR(-ENOMEM);
+
+	list-&gt;nr_entries = reg-&gt;ring_entries;
+	list-&gt;mask = reg-&gt;ring_entries - 1;
+	list-&gt;flags = IOBL_BUF_RING;
+
+	return list;
+}
+
+static int io_setup_pbuf_ring(struct io_ring_ctx *ctx,
+			      const struct io_uring_buf_reg *reg,
+			      struct io_buffer_list *bl)
+{
+	struct io_uring_region_desc rd;
+	unsigned long mmap_offset;
+	unsigned long ring_size;
+	int ret;
 
-	mmap_offset = (unsigned long)reg.bgid &lt;&lt; IORING_OFF_PBUF_SHIFT;
-	ring_size = flex_array_size(br, bufs, reg.ring_entries);
+	mmap_offset = (unsigned long)reg-&gt;bgid &lt;&lt; IORING_OFF_PBUF_SHIFT;
+	ring_size = flex_array_size(bl-&gt;buf_ring, bufs, reg-&gt;ring_entries);
 
 	memset(&amp;rd, 0, sizeof(rd));
 	rd.size = PAGE_ALIGN(ring_size);
-	if (!(reg.flags &amp; IOU_PBUF_RING_MMAP)) {
-		rd.user_addr = reg.ring_addr;
+	if (!(reg-&gt;flags &amp; IOU_PBUF_RING_MMAP)) {
+		rd.user_addr = reg-&gt;ring_addr;
 		rd.flags |= IORING_MEM_REGION_TYPE_USER;
 	}
+
 	ret = io_create_region(ctx, &amp;bl-&gt;region, &amp;rd, mmap_offset);
 	if (ret)
-		goto fail;
-	br = io_region_get_ptr(&amp;bl-&gt;region);
+		return ret;
+	bl-&gt;buf_ring = io_region_get_ptr(&amp;bl-&gt;region);
 
 #ifdef SHM_COLOUR
 	/*
@@ -656,25 +674,48 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
 	 * should use IOU_PBUF_RING_MMAP instead, and liburing will handle
 	 * this transparently.
 	 */
-	if (!(reg.flags &amp; IOU_PBUF_RING_MMAP) &amp;&amp;
-	    ((reg.ring_addr | (unsigned long)br) &amp; (SHM_COLOUR - 1))) {
-		ret = -EINVAL;
-		goto fail;
+	if (!(reg-&gt;flags &amp; IOU_PBUF_RING_MMAP) &amp;&amp;
+	    ((reg-&gt;ring_addr | (unsigned long)bl-&gt;buf_ring) &amp;
+	     (SHM_COLOUR - 1))) {
+		io_free_region(ctx-&gt;user, &amp;bl-&gt;region);
+		return -EINVAL;
 	}
 #endif
 
-	bl-&gt;nr_entries = reg.ring_entries;
-	bl-&gt;mask = reg.ring_entries - 1;
-	bl-&gt;flags |= IOBL_BUF_RING;
-	bl-&gt;buf_ring = br;
-	if (reg.flags &amp; IOU_PBUF_RING_INC)
+	if (reg-&gt;flags &amp; IOU_PBUF_RING_INC)
 		bl-&gt;flags |= IOBL_INC;
+
+	return 0;
+}
+
+int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
+{
+	unsigned int permitted_flags;
+	struct io_uring_buf_reg reg;
+	struct io_buffer_list *bl;
+	int ret;
+
+	lockdep_assert_held(&amp;ctx-&gt;uring_lock);
+
+	permitted_flags = IOU_PBUF_RING_MMAP | IOU_PBUF_RING_INC;
+	ret = io_copy_and_validate_buf_reg(arg, &amp;reg, permitted_flags);
+	if (ret)
+		return ret;
+
+	bl = io_alloc_new_buffer_list(ctx, &amp;reg);
+	if (IS_ERR(bl))
+		return PTR_ERR(bl);
+
+	ret = io_setup_pbuf_ring(ctx, &amp;reg, bl);
+	if (ret) {
+		kfree(bl);
+		return ret;
+	}
+
 	ret = io_buffer_add_list(ctx, bl, reg.bgid);
-	if (!ret)
-		return 0;
-fail:
-	io_free_region(ctx-&gt;user, &amp;bl-&gt;region);
-	kfree(bl);
+	if (ret)
+		io_put_bl(ctx, bl);
+
 	return ret;
 }
 
-- 
2.47.3</pre>
</details>
<div class="review-comment-signals">Signals: neutral explanation, no clear resolution signal</div>
</div>
<div class="thread-children">
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Jens Axboe</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-09">2026-02-09</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Jens Axboe suggested adding a WARN_ON_ONCE() check to prevent int promotion from causing incorrect results when calculating the difference between br-&gt;tail and bl-&gt;head, recommending this change for all patches containing WARN_ON_ONCE().</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">I think you want:

	if (WARN_ON_ONCE((__u16)(br-&gt;tail - bl-&gt;head) &gt;= bl-&gt;nr_entries))

here to avoid int promotion from messing this up if tail has wrapped.

In general, across the patches for the WARN_ON_ONCE(), it&#x27;s not a huge
issue to have a litter of them for now. Hopefully we can prune some of
these down the line, however.

-- 
Jens Axboe</pre>
</details>
<div class="reply-to-label">&#8627; replying to Joanne Koong</div>
<div class="review-comment-signals">Signals: requested changes</div>
</div>
<div class="thread-children">
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Caleb Mateos</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-10">2026-02-10</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Caleb Mateos noted that the patch&#x27;s optimization !(~bl-&gt;flags &amp; (IOBL_BUF_RING|IOBL_PINNED)) is unnecessary, as modern compilers will automatically perform this optimization and potentially optimize it further.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">FWIW, modern compilers will perform this optimization automatically.
They&#x27;ll even optimize it further to !(~bl-&gt;flags &amp;
(IOBL_BUF_RING|IOBL_PINNED)): https://godbolt.org/z/xGoP4TfhP

Best,
Caleb</pre>
</details>
<div class="reply-to-label">&#8627; replying to Jens Axboe</div>
<div class="review-comment-signals">Signals: NEUTRAL, OPTIMIZATION</div>
</div>
<div class="thread-children">
<div class="thread-node depth-3">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Jens Axboe</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-10">2026-02-10</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Jens Axboe suggested that the patch&#x27;s implementation should follow a more common and readable approach, citing an example where his own version is easier to read than the original.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Sure, it&#x27;s not about that, it&#x27;s more about the common way of doing it,
which makes it easier to read for people. FWIW, your example is easier
to read too than the original.

-- 
Jens Axboe</pre>
</details>
<div class="reply-to-label">&#8627; replying to Caleb Mateos</div>
<div class="review-comment-signals">Signals: suggestion, example</div>
</div>
</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-10">2026-02-10</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Author addressed Jens&#x27; concern about accessing buffer IDs from within the io_uring internals by suggesting an alternative approach: adding a helper function to retrieve the buffer ID, such as io_uring_cmd_buf_id() or io_uring_req_buf_id().</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">The caller can, but from the caller side they only have access to the
cmd so they would need to do something like

struct io_kiocb *req = cmd_to_iocb_kiocb(ent-&gt;cmd);
buf_id = req-&gt;buf_index;

which may be kind of ugly with looking inside io-uring internals.
Maybe a helper here would be nicer, something like
io_uring_cmd_buf_id() or io_uring_req_buf_id(). It seemed cleaner to
me to just return the buf id as part of the io_br_sel struct, but I&#x27;m
happy to do it another way if you have a preference.

Thanks,
Joanne</pre>
</details>
<div class="reply-to-label">&#8627; replying to Jens Axboe</div>
<div class="review-comment-signals">Signals: clarifying question, open to suggestions</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-10">2026-02-10</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Author acknowledged the need for further changes, specifically mentioned making changes pointed out in other comments as part of v2 and waiting on discussion with Pavel before submitting a revised version.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Thanks for reviewing the patches. The branch containing the userside
changes on top of these patches is in [1]. I&#x27;ll make the changes you
pointed out in your other comments as part of v2. Once the discussion
with Pavel is resolved / figured out with the changes he wants for v2,
I&#x27;ll submit v2.

Thanks,
Joanne

[1] https://github.com/joannekoong/linux/commits/fuse_zero_copy/</pre>
</details>
<div class="reply-to-label">&#8627; replying to Jens Axboe</div>
<div class="review-comment-signals">Signals: acknowledged need for further changes, waiting on discussion with Pavel</div>
</div>
</div>
</div>
</div>
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Jens Axboe</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-09">2026-02-09</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Jens Axboe questioned the necessity of introducing a new field &#x27;kmbuf_ring&#x27; in the io_uring registration structure, suggesting that the existing &#x27;req-&gt;buf_index&#x27; could be used instead.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">I&#x27;m probably missing something here, but why can&#x27;t the caller just use
req-&gt;buf_index for this?

-- 
Jens Axboe</pre>
</details>
<div class="reply-to-label">&#8627; replying to Joanne Koong</div>
<div class="review-comment-signals">Signals: requested changes</div>
</div>
</div>
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Jens Axboe</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-09">2026-02-09</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Jens Axboe requested a branch with all patches, including users, for easier cross-referencing and evaluation of helper functions.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Generally looks pretty good - for context, do you have a branch with
these patches and the users on top too? Makes it a bit easier for cross
referencing, as some of these really do need an exposed user to make a
good judgement on the helpers.

I know there&#x27;s the older series, but I&#x27;m assuming the latter patches
changed somewhat too, and it&#x27;d be nicer to look at a current set rather
than go back to the older ones.

-- 
Jens Axboe</pre>
</details>
<div class="reply-to-label">&#8627; replying to Joanne Koong</div>
<div class="review-comment-signals">Signals: requested information, asked for clarification</div>
</div>
</div>
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Jens Axboe</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-09">2026-02-09</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Jens Axboe suggested refactoring io_pbuf_get_region() to handle kernel-managed buffer rings by adding a new helper function, io_kbuf_get_region(), and modifying the existing function to include an error check for IOBL_KERNEL_MANAGED flags.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">For this, I think just add another helper - leave io_pbuf_get_region()
and add a bl-&gt;flags &amp; IOBL_KERNEL_MANAGED error check in there, and
add a io_kbuf_get_region() or similar and have a !(bl-&gt;flags &amp;
IOBL_KERNEL_MANAGED) error check in that one.

That&#x27;s easier to read, and there&#x27;s little reason to avoid duplicating
the xa_load() part.

Minor nit, but imho it&#x27;s more readable that way.

-- 
Jens Axboe</pre>
</details>
<div class="reply-to-label">&#8627; replying to Joanne Koong</div>
<div class="review-comment-signals">Signals: requested changes</div>
</div>
</div>
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Jens Axboe</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-09">2026-02-09</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Jens Axboe suggested using a pointer to struct io_buffer_list instead of a double pointer, and recommended either returning an error pointer or renaming the passed-on pointer

Jens Axboe suggested a more efficient way to check for pinned buffer rings by combining two flags into one condition, and also recommended adding an early return statement when bl is NULL.

Reviewer Jens Axboe suggested allowing io_uring commands to exceed the standard 80 character limit, citing that it is acceptable for io_uring due to its specific requirements.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Probably use the usual struct io_buffer_list *bl here and either use an
ERR_PTR return, or rename the passed on **bl to **blret or something.

---

Usually done as:

	if ((bl-&gt;flags &amp; (IOBL_BUF_RING|IOBL_PINNED)) == (IOBL_BUF_RING|IOBL_PINNED))

and maybe then just have an earlier

	if (!bl)
		goto err;

---

to avoid making it way too long. For io_uring, it&#x27;s fine to exceed 80
chars where it makes sense.

-- 
Jens Axboe</pre>
</details>
<div class="reply-to-label">&#8627; replying to Joanne Koong</div>
<div class="review-comment-signals">Signals: requested change, suggestion, requested changes</div>
</div>
</div>
<div class="thread-node depth-1" id="2026-02-10">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Pavel Begunkov</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-10">2026-02-10</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Pavel Begunkov noted that the code should allow regions to work with user-passed memory, which would enable optimizations such as huge pages, and requested a change to remove this limitation.

The reviewer suggested refactoring io_uring/kbuf: IORING_REGISTER_KMBUF_RING should not allocate buffers, instead it should register a memory region and use that for buffer allocation. They also proposed using a new flag or internal API to create the buffer ring.

Reviewer Pavel Begunkov noted that the removal of io_create_region_multi_buf() eliminates the need for aligning every buffer, which could result in wasted memory due to 64KB page sizes.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">If you&#x27;re creating a region, there should be no reason why it
can&#x27;t work with user passed memory. You&#x27;re fencing yourself off
optimisations that are already there like huge pages.

---

Please use io_create_region(), the new function does nothing new
and only violates abstractions.

Provided buffer rings with kernel addresses could be an interesting
abstraction, but why is it also responsible for allocating buffers?
What I&#x27;d do:

1. Strip buffer allocation from IORING_REGISTER_KMBUF_RING.
2. Replace *_REGISTER_KMBUF_RING with *_REGISTER_PBUF_RING + a new flag.
    Or maybe don&#x27;t expose it to the user at all and create it from
    fuse via internal API.
3. Require the user to register a memory region of appropriate size,
    see IORING_REGISTER_MEM_REGION, ctx-&gt;param_region. Make fuse
    populating the buffer ring using the memory region.

I wanted to make regions shareable anyway (need it for other purposes),
I can toss patches for that tomorrow.

A separate question is whether extending buffer rings is the right
approach as it seems like you&#x27;re only using it for fuse requests and
not for passing buffers to normal requests, but I don&#x27;t see the
big picture here.

---

With io_create_region_multi_buf() gone, you shouldn&#x27;t need
to align every buffer, that could be a lot of wasted memory
(thinking about 64KB pages).</pre>
</details>
<div class="reply-to-label">&#8627; replying to Joanne Koong</div>
<div class="review-comment-signals">Signals: requested changes, suggested refactoring, requested optimization</div>
</div>
<div class="thread-children">
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-10">2026-02-10</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Author asks for clarification on whether kernel-managed buffers can optimize huge page allocation, suggesting that the kernel could handle this as well.

Author Joanne Koong addressed Pavel Begunkov&#x27;s feedback about using a single function call to create regions, explaining that separate checks and allocation calls are needed for different types of regions.

The author addresses Pavel Begunkov&#x27;s feedback about kernel-managed buffer rings, specifically questioning the benefits of registering buffers from userspace and explaining how kernel allocation simplifies interface and lifecycle management.

Author addressed Pavel Begunkov&#x27;s concern about squashing kernel-managed buffer rings into existing pbuf rings, explaining that this would require adding pinning support to pbuf rings, which was previously dropped due to feedback from Jens and Caleb. The author plans to re-add pinning for kmbuf rings.

Author responded to Pavel Begunkov&#x27;s question about the definition of &#x27;normal requests&#x27; in the context of io_uring buffer rings, explaining that for fuse&#x27;s use case there are only fuse requests.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Are there any optimizations with user-allocated buffers that wouldn&#x27;t
be possible with kernel-allocated buffers? For huge pages, can&#x27;t the
kernel do this as well (eg I see in io_mem_alloc_compound(), it calls
into alloc_pages() with order &gt; 0)?

---

There&#x27;s separate checks needed between io_create_region() and
io_create_region_multi_buf() (eg IORING_MEM_REGION_TYPE_USER flag
checking) and different allocation calls (eg
io_region_allocate_pages() vs io_region_allocate_pages_multi_buf()).
Maybe I&#x27;m misinterpreting your comment (or the code), but I&#x27;m not
seeing how this can just use io_create_region().

---

Conceptually, I think it makes the interface and lifecycle management
simpler/cleaner. With registering it from userspace, imo there&#x27;s
additional complications with no tangible benefits, eg it&#x27;s not
guaranteed that the memory regions registered for the buffers are the
same size, with allocating it from the kernel-side we can guarantee
that the pages are allocated physically contiguously, userspace setup
with user-allocated buffers is less straightforward, etc. In general,
I&#x27;m just not really seeing what advantages there are in allocating the
buffers from userspace. Could you elaborate on that part more?

---

If kmbuf rings are squashed into pbuf rings, then pbuf rings will need
to support pinning. In fuse, there are some contexts where you can&#x27;t
grab the uring mutex because you&#x27;re running in atomic context and this
can be encountered while recycling the buffer. I originally had a
patch adding pinning to pbuf rings (to mitigate the overhead of
registered buffers lookups) but dropped it when Jens and Caleb didn&#x27;t
like the idea. But for kmbuf rings, pinning will be necessary for
fuse.

---

What are &#x27;normal requests&#x27;? For fuse&#x27;s use case, there are only fuse requests.

Thanks,
Joanne</pre>
</details>
<div class="reply-to-label">&#8627; replying to Pavel Begunkov</div>
<div class="review-comment-signals">Signals: asking for clarification, questioning assumption, clarification</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Christoph Hellwig</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-11">2026-02-11</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#155724;background:#d4edda">Positive</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Christoph Hellwig noted that pages mapped to userspace can be allocated in the kernel, allowing for a buffer ring that is only mapped read-only into userspace, enabling zero-copy raids if the device requires stable pages for checksumming or raid.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Any pages mapped to userspace can be allocated in the kernel as well.

And I really do like this design, because it means we can have a
buffer ring that is only mapped read-only into userspace.  That way
we can still do zero-copy raids if the device requires stable pages
for checksumming or raid.  I was going to implement this as soon
as this series lands upstream.</pre>
</details>
<div class="reply-to-label">&#8627; replying to Pavel Begunkov</div>
<div class="review-comment-signals">Signals: likes the design, zero-copy raids</div>
</div>
<div class="thread-children">
<div class="thread-node depth-3">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Pavel Begunkov</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-12">2026-02-12</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The reviewer pointed out that the current implementation of pow2 round ups will waste memory, as 1MB allocations will not become 2MB huge pages and there is also a concern about 1GB huge pages. They suggested that users could make better placement decisions.

Reviewer Pavel Begunkov suggested that the io_uring uapi should include fields for user-provided memory as an optional feature, and noted that fuse can refuse to bind to buffer rings it doesn&#x27;t like.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">pow2 round ups will waste memory. 1MB allocations will never
become 2MB huge pages. And there is a separate question of
1GB huge pages. The user can be smarter about all placement
decisions.

---

That&#x27;s an interesting case. To be clear, user provided memory is
an optional feature for pbuf rings / regions / etc., and I think
the io_uring uapi should leave fields for the feature. However, I
have nothing against fuse refusing to bind to buffer rings it
doesn&#x27;t like.

-- 
Pavel Begunkov</pre>
</details>
<div class="reply-to-label">&#8627; replying to Christoph Hellwig</div>
<div class="review-comment-signals">Signals: waste of memory, user can be smarter, requested additional consideration for user-provided memory</div>
</div>
</div>
<div class="thread-node depth-3">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Pavel Begunkov</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-12">2026-02-12</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Pavel Begunkov suggested modifying IORING_REGISTER_MEM_REGION to support read-only registrations, and proposed adding a new registration flag or rejecting unsupported setups during initialization.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">IORING_REGISTER_MEM_REGION supports both types of allocations. It can
have a new registration flag for read-only, and then you either make
the bounce avoidance optional or reject binding fuse to unsupported
setups during init. Any arguments against that? I need to go over
Joanne&#x27;s reply, but I don&#x27;t see any contradiction in principal with
your use case.

-- 
Pavel Begunkov</pre>
</details>
<div class="reply-to-label">&#8627; replying to Christoph Hellwig</div>
<div class="review-comment-signals">Signals: requested changes</div>
</div>
</div>
<div class="thread-node depth-3" id="2026-02-13">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Pavel Begunkov</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-13">2026-02-13</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The reviewer pointed out that the patch introduces a new type of buffer ring that allocates kernel buffers and maps them into user space, but this is not clearly related to the original purpose of io_uring provided buffer rings. He questioned why fuse needs to reuse pbuf ring code as an internal memory allocator and suggested that it could be contained within fuse instead of exposing buffer rings as an io_uring uapi.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Registered, aka fixed, buffers are the ones you pass to
IORING_OP_[READ,WRITE]_FIXED and some other requests. It&#x27;s normally
created by io_uring_register_buffers*() / IORING_REGISTER_BUFFERS*
with user memory, but there are special cases when it&#x27;s installed
internally by other kernel components, e.g. ublk.
This series has nothing to do with them, and relevant parts of
the discussion here don&#x27;t mention them either.

Provided buffer rings, a.k.a pbuf rings, IORING_REGISTER_PBUF_RING
is a kernel-user shared ring. The entries are user buffers
{uaddr, size}. The user space adds entries, the kernel (io_uring
requests) consumes them and issues I/O using the user addresses.
E.g. you can issue a IORING_OP_RECV request (+IOSQE_BUFFER_SELECT)
and it&#x27;ll grab a buffer from the ring instead of using sqe-&gt;addr.

pbuf rings, IORING_REGISTER_MEM_REGION, completion/submission
queues and all other kernel-user rings/etc. are internally based
on so called regions. All of them support both user allocated
memory and kernel allocations + mmap.

This series essentially creates provided buffer rings, where
1. the ring now contains kernel addresses
2. the ring itself is in-kernel only and not shared with user space
3. it also allocates kernel buffers (as a region), populates the ring
    with them, and allows mapping the buffers into the user space.

Fuse is doing both adding (kernel) buffers to the ring and consuming
them. At which point it&#x27;s not clear:

1. Why it even needs io_uring provided buffer rings, it can be all
    contained in fuse. Maybe it&#x27;s trying to reuse pbuf ring code as
    basically an internal memory allocator, but then why expose buffer
    rings as an io_uring uapi instead of keeping it internally.

    That&#x27;s also why I mentioned whether those buffers are supposed to
    be used with other types of io_uring requests like recv, etc.

2. Why making io_uring to allocate payload memory. The answer to which
    is probably to reuse the region api with mmap and so on. And why
    payload buffers are inseparably created together with the ring
    and via a new io_uring uapi.

    And yes, I believe in the current form it&#x27;s inflexible, it requires
    a new io_uring uapi. It requires the number of buffers to match
    the number of ring entries, which are related but not the same
    thing. You can&#x27;t easily add more memory as it&#x27;s bound to the ring
    object. The buffer memory won&#x27;t even have same lifetime as the
    ring object -- allow using that km buffer ring with recv requests
    and highly likely I&#x27;ll most likely give you a way to crash the
    kernel.

But hey, I&#x27;m tired. I don&#x27;t have any beef here and am only trying
to make it a bit cleaner and flexible for fuse in the first place
without even questioning the I/O path. If everyone believes
everything is right, just ask Jens to merge it.

-- 
Pavel Begunkov</pre>
</details>
<div class="reply-to-label">&#8627; replying to Christoph Hellwig</div>
<div class="review-comment-signals">Signals: inflexibility, potential for kernel crashes</div>
</div>
</div>
<div class="thread-node depth-3">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Pavel Begunkov</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-13">2026-02-13</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Pavel Begunkov expressed concern that the kernel-managed buffer rings are being used for large payload buffers, which was not their intended use case; they suggested respinning patches to place SQ/CQ onto a different area of memory

Reviewer Pavel Begunkov expressed confusion about how the kernel-managed buffer rings can work without a kernel component returning buffers into the ring, as io_uring does not currently support this.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Think of it as an area of memory for kernel-user communication. Used
for syscall parameters passing to avoid copy_from_user, but I added
it for a bunch of use cases. We&#x27;ll hopefully get support at some
point for passing request arguments like struct iovec. BPF patches
use it for communication. I need to respin patches placing SQ/CQ onto
it (avoid some memory waste).

Tbh, I never meant it nor io_uring regions to be used for huge
payload buffers, but this series already uses regions for that.

---

Then I&#x27;m confused. Take a look at the other reply, this series is
about buffer rings with kernel memory, it can&#x27;t work without a kernel
component returning buffers into the ring, and io_uring doesn&#x27;t do
that. But maybe you&#x27;re thinking about adding some more elaborate API.

IIUC, Joanne also wants to add support for fuse installing registered
buffers, which would allow zero-copy, but those got split out of
this series.

-- 
Pavel Begunkov</pre>
</details>
<div class="reply-to-label">&#8627; replying to Christoph Hellwig</div>
<div class="review-comment-signals">Signals: concern about misuse of kernel-managed buffer rings, confusion, requested clarification</div>
</div>
</div>
<div class="thread-node depth-3">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Pavel Begunkov</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-13">2026-02-13</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Pavel Begunkov expressed concern that the patch series does not address registered buffers and suggested separating kernel-managed buffer rings from io_uring, arguing that reusing buffer allocation would introduce unnecessary complexity.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">There is nothing about registered buffers in this series. And even
if you try to reuse buffer allocation out of it, it&#x27;ll come with
a circular buffer you&#x27;ll have no need for. And I&#x27;m pretty much
arguing about separating those for io_uring.

-- 
Pavel Begunkov</pre>
</details>
<div class="reply-to-label">&#8627; replying to Christoph Hellwig</div>
<div class="review-comment-signals">Signals: requested changes, suggested separation</div>
</div>
</div>
<div class="thread-node depth-3">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-13">2026-02-13</a>
<span class="badge" style="color:#155724;background:#d4edda">Positive</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Author acknowledged that Christoph Hellwig&#x27;s use case for read-only buffers aligns with the benefits of kernel-managed buffer rings, which will provide memory wins through incremental buffer consumption.

Author addressed Christoph&#x27;s concern about how to handle read-only mappings for kernel-managed buffer rings, suggesting a simple solution by passing a read-only flag from userspace and checking it when mmap is called. She offered to add this patch to the series if needed.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">(resending because I hit reply instead of reply-all)

I think we have the exact same use case, except your buffers need to
be read-only. I think your use case benefits from the same memory wins
we&#x27;ll get with incremental buffer consumption, which is the primary
reason fuse is using a bufring instead of fixed buffers.

---

I think you can and it&#x27;ll be very easy to do so. All that would be
needed is to pass in a read-only flag from the userspace side when it
registers the bufring, and then when userspace makes the mmap call to
the bufring, the kernel checks if that read-only flag is set on the
bufring and if so returns a read-only mapping. I&#x27;m happy to add that
patch to this series if that would make things easier for you. The
io_uring_register_buffers() api registers fixed buffers (which have to
be user-allocated memory) so you would need to go through the
io_uring_register_buf_ring() api once kmbufs are squashed into the
pbuf interface.

With going through IORING_MEM_REGION, this would work for your use
case as well. The user would have to register the mem region with
io_uring_register_region() and pass in a read-only flag, and then the
kernel will allocate the memory region. Then userspace would mmap the
memory region and on the kernel side, it would set the mapping to be
read-only. When the kmbufring then gets registered, the buffers in it
will be empty. The filesystem will then have to populate the buffers
in it from the mem region that was previously registered.

Thanks,
Joanne</pre>
</details>
<div class="reply-to-label">&#8627; replying to Christoph Hellwig</div>
<div class="review-comment-signals">Signals: alignment, benefits, agreed</div>
</div>
</div>
<div class="thread-node depth-3" id="2026-02-18">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Pavel Begunkov</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-18">2026-02-18</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Pavel Begunkov noted that buffer rings are not suitable for storage read/write requests because they bind to a buffer immediately, whereas other types of requests like recv allow io_uring to poll the socket before taking a buffer from the ring. He also pointed out that someone needs to return buffers back into the kernel-private ring, which is currently assumed to be handled by the fuse driver but is unclear for normal rw requests.

The reviewer suggests using IORING_MEM_REGION or a standalone registered buffer extension to provide buffers/memory without extra semantics, potentially leading to a finer API.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Provided buffer rings are not useful for storage read/write requests
because they bind to a buffer right away, that&#x27;s in contrast to some
recv request, where io_uring will first poll the socket to confirm
the data is there, and only then take a buffer from the buffer ring
and copy into it. With storage rw it makes more sense to specify
the buffer directly gain control over where exactly data lands
IOW, instead of the usual &quot;read data into a given pointer&quot; request
semantics like what read(2) gives you, buffer rings are rather
&quot;read data somewhere and return a pointer to where you placed it&quot;.

Another problem is that someone needs to return buffers back into
the buffer ring, and it&#x27;s a kernel private ring. For this patchset
it&#x27;s assumed the fuse driver is going to be doing that, but there
is no one for normal rw requests.

---

Yes. You only need buffers, and it&#x27;ll be better to base on sth that
gives you buffers/memory without extra semantics, i.e.
IORING_MEM_REGION. Or it can be a standalone registered buffer
extension, likely reusing regions internally. That might even yield
a finer API.

-- 
Pavel Begunkov</pre>
</details>
<div class="reply-to-label">&#8627; replying to Christoph Hellwig</div>
<div class="review-comment-signals">Signals: requested changes, unclear implementation</div>
</div>
</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-11">2026-02-11</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Author Joanne Koong addressed Pavel Begunkov&#x27;s concern that the patch uses io_region_allocate_pages_multi_buf() instead of io_create_region(), explaining that it was a necessary change to avoid memory allocation errors due to large buffer sizes in fuse&#x27;s use case.

Author clarifies her understanding of reviewer&#x27;s feedback, confirming that she and Christoph thought the user should allocate buffers before passing them to kernel, but now realizes the patch uses IORING_REGISTER_MEM_REGION for kernel allocation instead.

The author addressed a concern about the complexity of the pbuf API, explaining that separating kernel-managed buffer rings (kmbufs) from regular pbufs makes it clearer what each does and avoids unnecessary complexity. They agreed to combine the interfaces in v2.

Author acknowledged that she misremembered the issue being addressed as pinning the registered buffer table, not the pbuf ring.

Author addressed Pavel Begunkov&#x27;s concern about sparse buffers populated by the kernel, explaining that they would need to be automatically pinned and potentially requiring users to unregister buffers individually. The author notes that performance differences between pinned and unpinned registered buffers were negligible in their benchmarking.

The author is addressing a concern about buffer allocation, specifically whether individual buffers should be allocated separately by the kernel. The author acknowledges the memory allocation issue but disagrees that separate buffer allocation would lead to extra mmapping or userspace management.

Author asks for clarification on reviewer&#x27;s concerns about over-accounting and extra memory footprint in kernel-managed buffer rings.

Author is addressing concerns about the API and kernel buffer allocation in the io_uring/kbuf series. She plans to make changes for v2, including removing the KMBUF_RING API interface, having kernel buffer allocation go through IORING_REGISTER_MEM_REGION, and adding APIs for subsystems to populate a kernel-managed buffer ring with addresses from registered memory regions.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">When I originally implemented it, I had it use
io_region_allocate_pages() but this fails because it&#x27;s allocating way
too much memory at once. For fuse&#x27;s use case, each buffer is usually
at least 1 MB if not more. Allocating the memory one buffer a time in
io_region_allocate_pages_multi_buf() bypasses the allocation errors I
was seeing. That&#x27;s the main reason I don&#x27;t think this can just use
io_create_region().

---

Oh okay, from your first message I (and I think christoph too) thought
what you were saying is that the user should be responsible for
allocating the buffers with complete ownership over them, and then
just pass those allocated to the kernel to use. But what you&#x27;re saying
is that just use a different way for getting the kernel to allocate
the buffers (eg through the IORING_REGISTER_MEM_REGION interface). Am
I reading this correctly?

---

imo, it looked cleaner as a separate api because it has different
expectations and behaviors and squashing kmbuf into the pbuf api makes
the pbuf api needlessly more complex. Though I guess from the
userspace pov, liburing could have a wrapper that takes care of
setting up the pbuf details for kernel-managed pbufs. But in my head,
having pbufs vs. kmbufs makes it clearer what each one does vs regular
pbufs vs. pbufs that are kernel-managed.

Especially with now having kmbufs go through the ioring mem region
interface, it makes things more confusing imo if they&#x27;re combined, eg
pbufs that are kernel-managed are created empty and then populated
from the kernel side by whatever subsystem is using them. Right now
there&#x27;s only one mem region supported per ring, but in the future if
there&#x27;s the possibility that multiple mem regions can be registered
(eg if userspace doesn&#x27;t know upfront what mem region length they&#x27;ll
need), then we should also probably add in a region id param for the
registration arg, which if kmbuf rings go through the pbuf ring
registration api, is not possible to do.

But I&#x27;m happy to combine the interfaces and go with your suggestion.
I&#x27;ll make this change for v2 unless someone else objects.

---

Yeah, you&#x27;re right I misremembered and the objections / patch I
dropped was pinning the registered buffer table, not the pbuf ring

---

Hmm, I&#x27;m not sure this idea would work for sparse buffers populated by
the kernel, unless those are automatically pinned too but then from
the user POV for unregistration they&#x27;d need to unregister buffers
individually instead of just calling IORING_UNREGISTER_BUFFERS but it
might be annoying for them to now need to know which buffers are
pinned vs not. When i benchmarked the fuse code with vs without pinned
registered buffers, it didn&#x27;t seem to make much of a difference
performance-wise thankfully, so I just dropped it.

---

To clarify, is this in reply to why the individual buffers shouldn&#x27;t
be allocated separately by the kernel?
I added a comment about this above in the discussion about
io_region_allocate_pages_multi_buf(), and if the memory allocation
issue I was seeing is bypassable and the region can be allocated all
at once, I&#x27;m happy to make that change. With having the allocation be
separate buffers though, I&#x27;m not sure I agree that there are extra
mmaps / userspace management. All the pages across the buffers are
vmapped together and the userspace just needs to do 1 mmap call for
them. On the userspace side, I don&#x27;t think there&#x27;s more management
since the mmapped address represents the range across all the buffers.
I&#x27;m not seeing how there&#x27;s wasted space either since the only
requirement is that the buffer size is page aligned. I think also
there&#x27;s a higher chance of the entire buffer region being physically
contiguous if each buffer is allocated separately vs. all the buffers
are allocated as 1 region. I don&#x27;t feel strongly about this either way
and I&#x27;m happy to allocate the entire region at once if that&#x27;s
possible.

---

Just out of curiosity, could you elaborate on the over-accounting and
extra memory footprint? I was under the impression it would be the
same since the accounting gets adjusted by the total bytes allocated?
For the extra memory footprint, is the extra footprint from the
metadata to describe each buffer region, or are you referring to
something else?

---

Thanks for your input on the series. To iterate / sum up, these are
changes for v2 I&#x27;ll be making:
- api-wise from userspace/liburing: get rid of KMBUF_RING api
interface and have users go through PBUF_RING api instead with a flag
indicating the ring is kernel-managed
- have kernel buffer allocation go through IORING_REGISTER_MEM_REGION
instead, which means when the pbuf ring is created and the
kernel-managed flag is set, the ring will be empty. The memory region
will need to be registered before the mmap call to the ring fd.
- add apis for subsystems to populate a kernel-managed buffer ring
with addresses from the registered mem region

Does this align with your understanding of the conversation as well or
is there anything I&#x27;m missing?

And Christoph, do these changes for v2 work for your use case as well?

Thanks,
Joanne</pre>
</details>
<div class="reply-to-label">&#8627; replying to Pavel Begunkov</div>
<div class="review-comment-signals">Signals: clarification, explanation, understanding</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-12">2026-02-12</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Author clarified that kernel-managed buffer rings can be used internally by the kernel without explicit user registration, and explained that this would simplify the process compared to using IORING_REGISTER_MEM_REGION.

Author acknowledged a concern about the complexity of kernel-managed buffer rings, suggesting an alternative design where a straightforward kmbuf ring uses the pbuf interface and a future interface for pbuf rings to use IORING_REGISTERED_MEM_REGIONS.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">By &quot;control the allocation fully&quot; do you mean for your use case, the
allocation/setup isn&#x27;t triggered by userspace but is initiated by the
kernel (eg user never explicitly registers any kbuf ring, the kernel
just uses the kbuf ring data structure internally and users can read
the buffer contents)? If userspace initiates the setup of the kbuf
ring, going through IORING_REGISTER_MEM_REGION would be semantically
the same, except the buffer allocation by the kernel now happens
before the ring is created and then later populated into the ring.
userspace would still need to make an mmap call to the region and the
kernel could enforce that as read-only. But if userspace doesn&#x27;t
initiate the setup, then going through IORING_REGISTER_MEM_REGION gets
uglier.

---

So i guess the flow would have to be:
a) user calls io_uring_register_region(&amp;ring, &amp;mem_region_reg) with
mem_region_reg.region_uptr&#x27;s size field set to the total buffer size
(and mem_region_reg.flags read-only bit set if needed)
     kernel allocates region
b) user calls mmap() to get the address of the region. If read-only
bit was set, it gets a read-only address
c) user calls io_uring_register_buf_ring(&amp;ring, &amp;buf_reg, flags) with
buf_reg.flags |= IOU_PBUF_RING_KERNEL_MANAGED
     kernel creates an empty kernel-managed ring. None of the buffers
are populated
d) user tells X subsystem to populate the ring starting from offset Z
in the registered mem region
e) on the kernel side, the subsystem populates the ring starting from
offset Z, filling it up using the buf_size and ring_entries values
that the user registered the ring with in c)

To be completely honest, the more I look at this the more this feels
like overkill / over-engineered to me. I get that now the user can do
the PMD optimization, but does that actually lead to noticeable
performance benefits? It seems especially confusing with them going
through the same pbuf ring interface but having totally different
expectations.

What about adding a straightforward kmbuf ring that goes through the
pbuf interface (eg the design in this patchset) and then in the future
adding an interface for pbuf rings (both kernel-managed and
non-kernel-managed) to go through IORING_REGISTERED_MEM_REGIONS if
users end up needing/wanting to have their rings populated that way?

Thanks,
Joanne</pre>
</details>
<div class="reply-to-label">&#8627; replying to Pavel Begunkov</div>
<div class="review-comment-signals">Signals: clarifying question, explanation, overkill</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Christoph Hellwig</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-12">2026-02-12</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer noted that if an application is concerned about TLB pressure, a simple solution would be to allocate buffers in multiples of page table entry (PTE) levels.

Reviewer Christoph Hellwig expressed confusion about the term &#x27;pbuf&#x27; used in the patch, specifically asking for clarification on what it refers to and questioning whether it&#x27;s a fixed buffer API.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Sure.  But if the application cares that much about TLB pressure
I&#x27;d just round up to nice multtiple of PTE levels.

---

Can you clarify what you mean with &#x27;pbuf&#x27;?  The only fixed buffer API I
know is io_uring_register_buffers* which always takes user provided
buffers, so I have a hard time parsing what you&#x27;re saying there.  But
that might just be sign that I&#x27;m no expert in io_uring APIs, and that
web searches have degraded to the point of not being very useful
anymore.</pre>
</details>
<div class="reply-to-label">&#8627; replying to Pavel Begunkov</div>
<div class="review-comment-signals">Signals: requested alternative approach, confusion, lack of clarity</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Christoph Hellwig</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-12">2026-02-12</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">reviewer questioned the purpose of IORING_REGISTER_MEM_REGION, citing a mismatch between its description in the commit message and public documentation

The reviewer noted that their use case does not involve fuse, but rather block and file system I/O, implying that the patch may not be applicable to their specific needs.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">IORING_REGISTER_MEM_REGION seems to be all about cqs from both your
commit message and the public documentation.  I&#x27;m confused.

---

My use case is not about fuse, but good old block and file system
I/O.</pre>
</details>
<div class="reply-to-label">&#8627; replying to Pavel Begunkov</div>
<div class="review-comment-signals">Signals: confusion, requested clarification, highlighted a potential limitation</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-13">2026-02-13</a>
<span class="badge" style="color:#155724;background:#d4edda">Positive</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Author addressed Pavel Begunkov&#x27;s concern about wasted space in the io_ring buffer by explaining that a circular buffer will allow buffers to be shared across entries, reducing memory allocation.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">I think the circular buffer will be useful for Christoph&#x27;s use case in
the same way it&#x27;ll be useful for fuse&#x27;s. The read payload could be
differently sized across requests, so it&#x27;s a lot of wasted space to
have to allocate a buffer large enough to support the max-size request
per entry in the io_ring. With using a circular buffer, buffers have a
way to be shared across entries, which means we can significantly
reduce how much memory needs to be allocated.

Thanks,
Joanne</pre>
</details>
<div class="reply-to-label">&#8627; replying to Pavel Begunkov</div>
<div class="review-comment-signals">Signals: acknowledged a potential issue, explained a technical solution</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-13">2026-02-13</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author is addressing Pavel Begunkov&#x27;s concern about the need for kernel-managed buffer rings, specifically in the context of fuse&#x27;s use case where buffers are shared between the kernel and server. The author explains that the kernel needs to control when buffers get recycled back into the ring because the server writes data back to the kernel in those buffers after submitting an sqe.

Author addressed Pavel Begunkov&#x27;s concern about userspace applications using kernel-managed buffer rings for operations other than io_uring, explaining that they are used for reading and writing contents from/to a locally-backed file.

The author is addressing Pavel Begunkov&#x27;s feedback about using a registered memory region, which allows optimizations like PMD. The author acknowledges that these optimizations may be beneficial for some workloads but believes most kmbuf use cases don&#x27;t require them, and thus doesn&#x27;t see the need to add complexity.

The author addressed Pavel Begunkov&#x27;s concern that combining kernel-managed buffer rings (kmbufs) with user-provided buffer rings (pbufs) in a single UAPI is confusing, as kmbufs have different expectations and behaviors. The author initially thought it was cleaner to separate them into distinct UAPIs but now agrees to put kmbufs through the pbuf UAPI for v2.

Author Joanne Koong is addressing a concern about the purpose and behavior of ring entries without associated buffers, explaining that this can be fixed by passing in the number of buffers from the uapi for kernel-managed pbuf rings.

Author addressed Pavel Begunkov&#x27;s concern about the limitations of kernel-managed buffer rings by explaining that adding more memory to the mem region is difficult and that users need to know upfront how much memory to allocate, which may be hard to do.

Author addressed Pavel Begunkov&#x27;s concern that the buffer memory has a different lifetime than the ring object by explaining that the buffers are freed when the ring is freed.

Author is addressing Pavel&#x27;s concern about whether kernel-managed buffer rings should support both simple and registered memory regions, offering to make a change if that&#x27;s the case.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">The most important part and the whole reason fuse needs the buffer
ring to be kernel-managed is because the kernel needs to control when
buffers get recycled back into the ring. For fuse&#x27;s use case, the
buffer is used for passing data between the kernel and the server. We
can&#x27;t have the server recycle the buffer because the server writes
back data to the kernel in that buffer when it submits the sqe. After
fuse receives the sqe and reads the reply from the server, it then
needs to recycle that buffer back into the ring so it can be reused
for a future cqe (eg sending a future request).

---

On the userspace/server side, it uses the buffers for other io-uring
operations (eg reading or writing the contents from/to a
locally-backed file).

---

My main motivation for this is simplicity. I see (and thanks for
explaining) that using a registered mem region allows the use of some
optimizations (the only one I know of right now is the PMD one you
mentioned but maybe there&#x27;s more I&#x27;m missing) that could be useful for
some workloads, but I don&#x27;t think (and this could just be my lack of
understanding of what more optimizations there are) most use cases of
kmbufs benefit from those optimizations, so to me it feels like we&#x27;re
adding non-trivial complexity for no noticeable benefit.

I feel like we get the best of both worlds by letting users have both:
the simple kernel-managed pbuf where the kernel allocates the buffers
and the buffers are tied to the lifecycle of the ring, and the more
advanced kernel-managed pbuf where buffers are tied to a registered
memory region that the subsystem is responsible for later populating
the ring with.

---

imo it felt cleaner to have a new uapi for it because kmbufs and pbufs
have different expectations and behaviors (eg pbufs only work with
user-provided buffers and requires userspace to populate the ring
before using it, whereas for kmbufs the kernel allocates the buffers
and populates it for you; pbufs require userspace to recycle back the
buffer, whereas for kmbufs the kernel is the one in control of
recycling) and from the user pov it seemed confusing to have kmbufs as
part of the pbuf ring uapi, instead of separating it out as a
different type of ringbuffer with a different expectation and
behavior. I was trying to make the point that combining the interface
if we go with IORING_MEM_REGION gets even more confusing because now
pbufs that are kernel-managed are also empty at initialization and
only can point to areas inside a registered mem region and the
responsibility of populating it is now on whatever subsystem is using
it.

I still have this opinion but I also think in general, you likely know
better than I do what kind of io-uring uapi is best for io-uring&#x27;s
users. For v2 I&#x27;ll have kmbufs go through the pbuf uapi.

---

I&#x27;m not really seeing what the purpose of having a ring entry with no
buffer associated with it is. In the existing code for non-kernel
managed pbuf rings, there&#x27;s the same tie between reg-&gt;ring_entries
being used as the marker for how many buffers the ring supports. But
if the number of buffers should be different than the number of ring
entries, this can be easily fixed by passing in the number of buffers
from the uapi for kernel-managed pbuf rings.

---

To play devil&#x27;s advocate, we also can&#x27;t easily add more memory to the
mem region once it&#x27;s been registered. I think there&#x27;s also a worse
penalty where the user needs to know upfront how much memory to
allocate for the mem region for the lifetime of the ring, which imo
may be hard to do (eg if a kernel-managed buf ring only needs to be
registered for some code paths and not others, the mem region
registration would still have to allocate the memory a potential kbuf
ring would use).

---

I&#x27;m a bit confused by this part. The buffer memory does have the same
lifetime as the ring object, no? The buffers only get freed when the
ring itself is freed.

---

I appreciate you looking at this and giving your feedback and insight.
Thank you for doing so. I don&#x27;t want to merge in something you&#x27;re
unhappy with.

Are you open to having support for both a simple kernel-managed pbuf
interface and later on if/when the need arises, a kernel-managed pbuf
interface that goes through a registered memory region? If the answer
is no, then I&#x27;ll make the change to have kmbufs go through the
registered memory region.

Thanks,
Joanne</pre>
</details>
<div class="reply-to-label">&#8627; replying to Pavel Begunkov</div>
<div class="review-comment-signals">Signals: clarification, explanation, acknowledges feedback</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-18">2026-02-18</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Author acknowledged that a fix is needed and promised to modify version 2 of the patch in response to reviewer feedback.

Author clarified that kernel-managed buffer rings are intended for use with other io-uring requests, specifically to avoid per-i/o page pinning overhead costs.

Author Joanne Koong addressed Pavel Begunkov&#x27;s concern about the design of kernel-managed buffer rings, agreeing that having buffers owned by the ring and tied to its lifetime is more generically useful. She proposed a new design where the API would allow users to specify an offset into a registered memory region for normal requests, using a new IOSQE_ flag and looking up associated pages stored in the io_mapped_region&#x27;s pages array.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Sorry, I submitted v2 last night thinking the conversation on this
thread had died. After reading through your reply, I&#x27;ll modify v2.

---

Yes the buffer rings are intended to be used with other io-uring
requests. The ideal scenario is that the user can then do the
equivalent of IORING_OP_READ/WRITE_FIXED operations on the
kernel-managed buffers and avoid the per-i/o page pinning overhead
costs.

---

I agree 100%. The api we add should be what&#x27;s best for io-uring, not fuse.

For the majority of use cases, it seemed to me that having the buffers
separated from the buffer rings didn&#x27;t yield perceptible benefits but
added complexity and more restrictions like having to statically know
up front how big the mem region needs to be across the lifetime of the
io-uring for anything the io-uring might use the mem region for. It
seems more generically useful as a concept to have the buffers owned
by the ring and tied to the lifetime of the ring. I like how with this
design everything is self-contained and multiple subsystems can use it
without having to reimplement functionality locally in the subsystem.
On the other hand, I see your point about how it might be something
users want in the future if they want complete control over which
parts of the mem region get used as the backing buffers to do stuff
like PMD optimizations.

I think this is a matter of opinion/preference and I think in general
for anything io-uring related, yours should take precedence.

With it going through a mem region, I don&#x27;t think it should even go
through the &quot;pbuf ring&quot; interface then if it&#x27;s not going to specify
the number of entries and buffer sizes upfront, if support is added
for io-uring normal requests (eg IORING_OP_READ/WRITE) to use the
backing pages from a memory region and if we&#x27;re able to guarantee that
the registered memory region will never be able to be unregistered by
the user. I think if we repurpose the

union {
  __u64 addr; /* pointer to buffer or iovecs */
  __u64 splice_off_in;
};

fields in the struct io_uring_sqe to

union {
  __u64 addr; /* pointer to buffer or iovecs */
  __u64 splice_off_in;
  __u64 offset; /* offset into registered mem region */
};

and add some IOSQE_ flag to indicate it should find the pages from the
registered mem region, then that should work for normal requests.
Where on the kernel side, it looks up the associated pages stored in
the io_mapped_region&#x27;s pages array for the offset passed in.

Right now there&#x27;s only a uapi to register a memory region and none to
unregister one. Is it guaranteed that io-uring will never add
something in the future that will let userspace unregister the memory
region or at least unregister it while it&#x27;s being used (eg if we add
future refcounting to it to track active uses of it)?

If so, then end-to-end, with it going through the mem region, it would
be something like:
* user creates a mem region for the io-uring
* user mmaps the mem region
* user passes in offset into region, length of each buffer, and number
of entries in the ring to the subsystem
* subsystem creates a locally managed bufring and adds buffers to that
ring from the mem region
* on the cqe side, it sends the buffer id of the registered mem region
through the same &quot;IORING_CQE_F_BUFFER |  (buf_id &lt;&lt;
IORING_CQE_BUFFER_SHIFT)&quot; mechanism

Does this design match what you had in mind / prefer?

I think the above works for Christoph&#x27;s use case too (as his and my
use case are the same) but if not, please let me know.

Thanks,
Joanne</pre>
</details>
<div class="reply-to-label">&#8627; replying to Pavel Begunkov</div>
<div class="review-comment-signals">Signals: acknowledged need for modification, promised to revise, clarification</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-20">2026-02-20</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author is addressing a question from Pavel Begunkov about whether kernel-managed buffer rings are fuse-specific, and they acknowledge that while it&#x27;s useful for fuse servers with certain workloads, the concept isn&#x27;t exclusive to fuse and other subsystems/users could also benefit from this optimization.

The author is addressing concerns about the added complexity and convoluted interface introduced by kernel-managed buffer rings, specifically questioning the need for tying together different concepts and the benefits of this approach.

Author addressed Pavel&#x27;s concern that kernel-managed buffer rings require the caller to register memory regions as fixed buffers, explaining that this cannot be guaranteed and proposing two possible solutions: adding pinning to registered memory regions or introducing extra overhead for every I/O operation.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Thanks for clarifying your question. Yes, this would be a useful
optimization in the future for fuse servers with certain workload
characteristics (eg network-backed servers with high concurrency and
unpredictable latencies). I don&#x27;t think the concept of kmbufrings is
exclusively fuse-specific though (for example, Christoph&#x27;s use case
being a recent instance); I think other subsystems/users that&#x27;ll use
kmbuf rings would also generically find it useful to have the option
of READ_OP_RECV/READ_OP_READ operating directly on the ring.

---

I feel like this design makes the interface more convoluted and now
muddies different concepts together by adding new complexity /
relationships between them whereas they were otherwise cleanly
isolated. Maybe I&#x27;m just not seeing/understanding the overarching
vision for why conceptually it makes sense for them to be tied
together besides as a mechanism to tell io-uring requests where to
copy from by reusing what exists for fixed buffer ids. There&#x27;s more
complexity now on the kernel side (eg having to detect if the buffer
passed in is kernel-allocated to know whether to pin the pages /
charge it against the user&#x27;s RLIMIT_MEMLOCK limit) but I&#x27;m not
understanding what we gain from it. I got the sense from your previous
comments that memory regions are the de facto way to go and should be
decoupled from other structures, so if that&#x27;s the case, why doesn&#x27;t it
make sense for io-uring to add native support for using memory regions
for io-uring requests? I feel like from the userspace side it makes
things more confusing with this extra layer of indirection that now
has to go through a fixed buffer.

---

I don&#x27;t think we can guarantee that the caller will register the
memory region as a fixed buffer (eg if it doesn&#x27;t need/want to use the
buffer for normal io-uring requests). On the kernel side, the internal
buffer entry uses the kaddr of the registered memory region buffer for
any memcpys. If it&#x27;s not guaranteed that registered memory regions
persist for the lifetime of the ring, there&#x27;ll have to be extra
overhead for every I/O (eg grab the io-uring lock, checking if the mem
region is still registered, grab a refcount to that mem region, unlock
the ring, do the memcpy to the kaddr, then grab the io-uring lock
again, decrement the refcount, and unlock). Or I guess we could add
pinning to a registered memory region.

Thanks,
Joanne</pre>
</details>
<div class="reply-to-label">&#8627; replying to Pavel Begunkov</div>
<div class="review-comment-signals">Signals: acknowledged a question, explained their reasoning, questioning the design</div>
</div>
</div>
</div>
</div>
<div class="thread-node depth-1" id="2026-02-11">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Pavel Begunkov</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-11">2026-02-11</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer noted that allocating 1MB in kernel-managed buffer rings will not result in a PMD mappable huge page, whereas user space can allocate 2MB and reuse the rest for other purposes.

Reviewer Pavel Begunkov suggested that instead of modifying io_create_region() to be less strict, the caller should filter its arguments to only pass IORING_MEM_REGION_TYPE_USER when it&#x27;s actually used.

Reviewer Pavel Begunkov noted that the memmap.c changes can be dropped as they are not necessary for kernel-managed buffer rings and may even introduce issues such as disabling io_mem_alloc_compound(), suggesting a simpler approach where regions do not need to know about buffer subdivision.

Reviewer Pavel Begunkov suggested adding a check for user-provided memory to the io_create_region() call, and provided an example of how this could be done by setting rd.user_addr and rd.flags accordingly.

The reviewer suggests separating ring creation from population on the kernel API level, allowing other users like fuse to create rings and populate them independently.

Reviewer Pavel Begunkov suggested that instead of introducing new UAPI and internal changes for kernel-managed buffer rings, the existing pbuf implementation could be modified to support this feature by adding a flag to indicate kernel-managed buffers.

reviewer pointed out that the patch was pinning the registered buffer table without providing buffer rings, which is a bad idea; instead suggested keeping all memory in one larger registered buffer and pinning only it

Reviewer Pavel Begunkov expressed concerns that creating many small regions for kernel-managed buffer rings would lead to unnecessary mmap()s, extra user space management, and wasted space, and also questioned the ring bound memory approach due to potential issues with buffer lifetimes.

Reviewer Pavel Begunkov noted that kernel-managed buffer rings would be particularly useful for operations like read and recv, where the kernel can fill provided buffers without requiring opcode-specific code changes in kbuf.c.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Yes, there is handful of differences. To name one, 1MB allocation won&#x27;t
get you a PMD mappable huge page, while user space can allocate 2MB,
register the first 1MB and reuse the rest for other purposes.

---

If io_create_region() is too strict, let&#x27;s discuss that in
examples if there are any, but it&#x27;s likely not a good idea changing
that. If it&#x27;s too lax, filter arguments in the caller. IOW, don&#x27;t
pass IORING_MEM_REGION_TYPE_USER if it&#x27;s not used.

---

I saw that and saying that all memmap.c changes can get dropped.
You&#x27;re using it as one big virtually contig kernel memory range then
chunked into buffers, and that&#x27;s pretty much what you&#x27;re getting with
normal io_create_region(). I get that you only need it to be
contiguous within a single buffer, but that&#x27;s not what you&#x27;re doing,
and it&#x27;ll be only worse than default io_create_region() e.g.
effectively disabling any usefulness of io_mem_alloc_compound(),
and ultimately you don&#x27;t need to care.

Regions shouldn&#x27;t know anything about your buffers, how it&#x27;s
subdivided after, etc.

---

struct io_uring_region_desc rd = {};
total_size = nr_bufs * buf_size;
rd.size = PAGE_ALIGN(total_size);
io_create_region(&amp;region, &amp;rd);

Add something like this for user provided memory:

if (use_user_memory) {
	rd.user_addr = uaddr;
	rd.flags |= IORING_MEM_REGION_TYPE_USER;
}

---

I don&#x27;t think I follow. I&#x27;m saying that it might be interesting
to separate rings from how and with what they&#x27;re populated on the
kernel API level, but the fuse kernel module can do the population
and get exactly same layout as you currently have:

int fuse_create_ring(size_t region_offset /* user space argument */) {
	struct io_mapped_region *mr = get_mem_region(ctx);
	// that can take full control of the ring
	ring = grab_empty_ring(io_uring_ctx);

	size = nr_bufs * buf_size;
	if (region_offset + size &gt; get_size(mr)) // + other validation
		return error;

	buf = mr_get_ptr(mr) + offset;
	for (i = 0; i &lt; nr_bufs; i++) {
		ring_push_buffer(ring, buf, buf_size);
		buf += buf_size;
	}
}

fuse might not care, but with empty rings other users will get a
channel they can use to do IO (e.g. read requests) using their
kernel addresses in the future.

---

It&#x27;d change uapi but not internals, you already piggy back it
on pbuf implementation and differentiate with a flag.

It could basically be:

if (flags &amp; IOU_PBUF_RING_KM)
	bl-&gt;flags |= IOBL_KERNEL_MANAGED;

Pinning can be gated on that flag as well. Pretty likely uapi
and internals will be a bit cleaner, but that&#x27;s not a huge deal,
just don&#x27;t see why would you roll out a separate set of uapi
([un]register, offsets, etc.) when essentially it can be treated
as the same thing.

---

IIRC, you was pinning the registered buffer table and not provided
buffer rings? Which would indeed be a bad idea. Thinking about it,
fwiw, instead of creating multiple registered buffers and trying to
lock the entire table, you could&#x27;ve kept all memory in one larger
registered buffer and pinned only it. It&#x27;s already refcounted, so
shouldn&#x27;t have been much of a problem.

---

To explain why, I don&#x27;t think that creating many small regions
is a good direction going forward. In case of kernel allocation,
it&#x27;s extra mmap()s, extra user space management, and wasted space.
For user provided memory it&#x27;s over-accounting and extra memory
footprint. It&#x27;ll also give you better lifecycle guarantees, i.e.
you won&#x27;t be able to free buffers while there are requests for the
context. I&#x27;m not so sure about ring bound memory, let&#x27;s say I have
my suspicions, and you&#x27;d need to be extra careful about buffer
lifetimes even after a fuse instance dies.

---

Any kind of read/recv/etc. that can use provided buffers. It&#x27;s
where kernel memory filled rings would shine, as you&#x27;d be able
to use them together without changing any opcode specific code.
I.e. not changes in read request implementation, only kbuf.c

-- 
Pavel Begunkov</pre>
</details>
<div class="reply-to-label">&#8627; replying to Joanne Koong</div>
<div class="review-comment-signals">Signals: requested change, requested changes, suggested alternative approach</div>
</div>
</div>
<div class="thread-node depth-1" id="2026-02-12">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Christoph Hellwig</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-12">2026-02-12</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Christoph Hellwig argued against kernel-managed buffer rings, citing a need for the kernel to control allocation and guarantee user processes can only read memory without writing to it.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">I&#x27;m arguing exactly against this.  For my use case I need a setup
where the kernel controls the allocation fully and guarantees user
processes can only read the memory but never write to it.  I&#x27;d love
to be able to piggy back than onto your work.</pre>
</details>
<div class="reply-to-label">&#8627; replying to Joanne Koong</div>
<div class="review-comment-signals">Signals: requested changes</div>
</div>
</div>
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Christoph Hellwig</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-12">2026-02-12</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Christoph Hellwig noted that io_uring_register_buffers() only pins memory, allowing applications or other processes to modify it, which can cause issues for file systems and storage devices that need to verify checksums or rebuild data from parity.

reviewer noted that the PMD mapping optimization is almost as valuable on both AMD and ARM, making it relevant despite initial concerns</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">The idea is that the application tells the kernel that it wants to use
a fixed buffer pool for reads.  Right now the application does this
using io_uring_register_buffers().  The problem with that is that
io_uring_register_buffers ends up just doing a pin of the memory,
but the application or, in case of shared memory, someone else could
still modify the memory.  If the underlying file system or storage
device needs verify checksums, or worse rebuild data from parity
(or uncompress), it needs to ensure that the memory it is operating
on can&#x27;t be modified by someone else.

So I&#x27;ve been thinking of a version of io_uring_register_buffers where
the buffers are not provided by the application, but instead by the
kernel and mapped into the application address space read-only for
a while, and I thought I could implement this on top of your series,
but I have to admit I haven&#x27;t really looked into the details all
that much.

---

Yes.  The PMD mapping also is not that relevant.  Both AMD (implicit)
and ARM (explicit) have optimizations for contiguous PTEs that are
almost as valuable.</pre>
</details>
<div class="reply-to-label">&#8627; replying to Joanne Koong</div>
<div class="review-comment-signals">Signals: requested changes, revised opinion, acknowledged relevance</div>
</div>
</div>
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Pavel Begunkov</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-13">2026-02-13</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Pavel Begunkov suggested a workaround for an issue, noting that the patch should instead use the kernel-managed buffer ring for metadata like fuse headers and payloads, which would allow zero-copying.

Reviewer Pavel Begunkov suggested moving ring population from io_uring uapi to fuse, and using IORING_REGISTER_MEM_REGION to simplify the process

Reviewer Pavel Begunkov noted that the differences between io_uring and kmbuf are mostly due to special region path and embedded buffer allocations, but suggested that making a separate opcode might still be beneficial.

The reviewer noted that the current implementation does not provide a clear control path for fuse to bind kernel-managed buffer rings, and suggested adding an io-uring command (FUSE_CMD_BIND_BUFFER_RING) to handle this scenario. They also proposed passing necessary parameters through this command and pinning the buffer ring before populating it with data.

Reviewer Pavel Begunkov pointed out that the patch could use IORING_REGISTER_MEM_REGION instead of a separate region, which is a distinct issue from whether buffers should be bound to the ring.

The reviewer noted that if the total allocation size is not a power of two, the kernel may allocate a huge page larger than the actual buffer size, wasting memory.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Let&#x27;s fix that then. For now, just work it around by wrapping
into a loop.

Btw, I thought you&#x27;re going to use it for metadata like some
fuse headers and payloads would be zero copied by installing
it as registered buffers.

...

---

The main point is disentangling memory allocation from ring
creation in the io_uring uapi, and moving ring population
into fuse instead of doing it at creation. And it&#x27;ll still be
populated by the kernel (fuse), user space doesn&#x27;t have access
to the ring. IORING_REGISTER_MEM_REGION is just the easiest way
to achieve that without any extra uapi.

...

---

It appeared to me that they&#x27;re different because of special
region path and embedded buffer allocations, and otherwise
differences would be minimal. But if you think it&#x27;s still
better to be made as a separate opcode, I&#x27;m not opposing it,
go for it.

---

Not having patches using the functionality is inconvenient. How
fuse looks up the buffer ring from io_uring? I could imagine you
have some control path io-uring command:

case FUSE_CMD_BIND_BUFFER_RING:
	return bind_queue(params);

Then you can pass all necessary parameters to it, pseudo code:

struct fuse_bind_kmbuf_ring_params {
	region_id;
	buf_ring_id;
	...
};

bind_queue(cmd, struct fuse_bind_kmbuf_ring_params *p)
{
	region = io_uring_get_region(cmd, p-&gt;region_id);
	// get exclusive access:
	buf_ring = io_uring_get_buf_ring(cmd, p-&gt;buf_ring_id);

	if (!validate_buf_ring(buf_ring))
		return NOTSUPPORTED;

	io_uring_pin(buf_ring);
	fuse_populate_buf_ring(buf_ring, region, ...);
}

Does that match expectations? I don&#x27;t think you even need
the ring part exposed as an io_uring uapi, tbh, as it
stays completely in fuse and doesn&#x27;t meaningfully interact
with the rest of io_uring.

...

---

That was about an argument for using IORING_REGISTER_MEM_REGION
instead a separate region. And it&#x27;s separate from whether
buffers should be bound to the ring.

---

I shouldn&#x27;t affect you much since you have such large buffers,
but imagine the total allocation size is not being pow2, and
the kernel allocating it as a single folio. E.g. 3 buffers,
0.5 MB each, total = 1.5MB, and the kernel allocates a 2MB
huge page.

-- 
Pavel Begunkov</pre>
</details>
<div class="reply-to-label">&#8627; replying to Joanne Koong</div>
<div class="review-comment-signals">Signals: requested changes, workaround, suggested changes</div>
</div>
</div>
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Pavel Begunkov</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-13">2026-02-13</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Pavel Begunkov expressed concerns that the io_uring uapi should not be tied to specific use cases or requirements, such as uniform buffer sizes, matching ring and buffer counts, and kernel-allocated buffers. He questioned the need for the total size at creation and suggested allowing runtime addition of memory while using the same ring.

Reviewer Pavel Begunkov expressed confusion about the distinction between kernel-managed buffer rings and user-visible buffer rings, questioning what expectations are different apart from one being in-kernel with kernel addresses and the other user-visible with user addresses.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">No, it&#x27;s mainly about not keeping payload buffers and rings in the same
object from the io_uring uapi perspective.

1. If it&#x27;s an io_uring uapi, it shouldn&#x27;t be fuse specific or with
a bunch of use case specific expectations attached. Why does it
require all buffers to be uniform in size? Why does it require
the ring size to match the number of buffers? Why does it require
buffers to be allocated by io_uring in the first place? Maybe some
subsystem got memory from somewhere else and wants to do use it
with io_uring. Why does it need to know the total size at creation,
and what would you do if you want to add more memory at runtime
while using the same ring?

2. If it&#x27;s meant to be fuse specific and _not_ used with other requests
like recv/read/etc., then what&#x27;s the point of having it as an io_uring
uapi? Which also adds additional trouble like the once you&#x27;re solving
with pinning.

If it&#x27;s supposed to be used with other requests, then buffers and
rings will have different in-kernel lifetime expectations imposed
by io_uring, so having them together won&#x27;t even help with
management.

I have a strong opinion about the memmap.c change. For the
rest, if you believe it&#x27;s fine, just send it out and let Jens
decide.

---

It&#x27;s predicated on separating buffers from rings, see above,
and assuming that I&#x27;m not sure what expectations are different
apart from one being in-kernel with kernel addresses and the
other user visible with user addresses.

-- 
Pavel Begunkov</pre>
</details>
<div class="reply-to-label">&#8627; replying to Joanne Koong</div>
<div class="review-comment-signals">Signals: requested changes, strong opinion, confusion</div>
</div>
</div>
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Bernd Schubert</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-13">2026-02-13</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Bernd Schubert expressed skepticism about sharing buffers across io_uring entries, suggesting it would only reduce ring size and questioned its purpose.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Dunno, what we actually want is requests of multiple sizes. Sharing
buffers across entries sounds like just reducing the ring size - I
personally don&#x27;t see the point here.


Thanks,
Bernd</pre>
</details>
<div class="reply-to-label">&#8627; replying to Joanne Koong</div>
<div class="review-comment-signals">Signals: skepticism, questioning</div>
</div>
<div class="thread-children">
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-13">2026-02-13</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Author clarified that kernel-managed buffer rings allow concurrent access to different regions of a shared buffer, addressing concerns about buffer sharing.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">By &quot;sharing buffers across entries&quot; what I mean is different regions
of the buffer can now be used concurrently by multiple entries.

Thanks,
Joanne</pre>
</details>
<div class="reply-to-label">&#8627; replying to Bernd Schubert</div>
<div class="review-comment-signals">Signals: clarification, explanation</div>
</div>
</div>
</div>
</div>
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Pavel Begunkov</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-18">2026-02-18</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Pavel Begunkov asked whether kernel-managed buffer rings can be used with other requests, specifically IORING_OP_RECV with IOSQE_BUFFER_SELECT set and bgid specifying the kernel-managed buffer ring.

Reviewer Pavel Begunkov raised two separate concerns: first, the io_uring user API should not make buffers inseparable from buffer rings; second, the patch should optionally allow user memory for buffer creation by reusing the region abstraction.

Pavel Begunkov questioned the necessity of making kernel-managed buffer rings an io_uring API, suggesting that it could be simpler to implement in fuse or as an implementation detail within io_uring

Reviewer Pavel Begunkov expressed concerns that the kernel-managed buffer rings (km rings) API in io_uring is not reusable and specific to the fuse use case, suggesting a middle ground where km rings can be registered together with memory as a pure region without a notion of a buffer, allowing users like fuse to chunk it.

reviewer noted that the kernel-managed buffer ring API is not fully generic and makes assumptions specific to fuse, which could lead to issues if used by other io_uring users

The reviewer noted that the current implementation only specifies the buffer ring depth but not the amount of memory allocated by userspace and how it&#x27;s pushed, which could lead to issues such as over-allocation or under-allocation of buffers.

Reviewer Pavel Begunkov suggested that instead of passing the number of buffers to io_uring, the kernel should create a large chunk of memory and then have fuse divide it up and put into the ring, reducing assumptions about uapi

Reviewer Pavel Begunkov agreed with the patch but noted that adding new memory would require a new mechanism, not necessarily tied to IORING_REGISTER_MEM_REGION.

The reviewer noted that unregistering a kernel-managed buffer ring does not guarantee that there are no in-flight requests using buffers from the ring, and suggested synchronizing with all other io_uring requests to ensure proper cleanup.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Oops, typo. I was asking whether the buffer rings (not buffers) are
supposed to be used with other requests. E.g. submitting a
IORING_OP_RECV with IOSQE_BUFFER_SELECT set and the bgid specifying
your kernel-managed buffer ring.

---

There are two separate arguments. The first is about not making buffers
inseparable from buffer rings in the io_uring user API. Whether it&#x27;s
IORING_REGISTER_MEM_REGION or something else is not that important.
I have no objection if it&#x27;s a part of fuse instead though, e.g. if
fuse binds two objects together when you register it with fuse, or even
if fuse create a buffer ring internally (assuming it doesn&#x27;t indirectly
leak into io_uring uapi).

And the second was about optionally allowing user memory for buffer
creation as you&#x27;re reusing the region abstraction. You can find pros
and cons for both modes, and funnily enough, SQ/CQ were first kernel
allocated and then people asked for backing it by user memory, and IIRC
it was in the reverse order for pbuf rings.

Implementing this is trivial as well, you just need to pass an argument
while creating a region. All new region users use struct
io_uring_region_desc for uapi and forward it to io_create_region()
without caring if it&#x27;s user or kernel allocated memory.

---

The stress is on why it&#x27;s an _io_uring_ API. It doesn&#x27;t matter to me
whether it&#x27;s a separate opcode or not. Currently, buffer rings don&#x27;t give
you anything that can&#x27;t be pure fuse, and it might be simpler to have
it implemented in fuse than binding to some io_uring object. Or it could
create buffer rings internally to reuse code but it doesn&#x27;t become an
io_uring uapi but rather implementation detail. And that predicates on
whether km rings are intended to be used with other / non-fuse requests.

---

I believe the source of disagreement is that you&#x27;re thinking
about how it&#x27;s going to look like for fuse specifically, and I
believe you that it&#x27;ll be nicer for the fuse use case. However,
on the other hand it&#x27;s an io_uring uapi, and if it is an io_uring
uapi, we need reusable blocks that are not specific to particular
users.

If it km rings has to stay an io_uring uapi, I guess a middle
ground would be to allow registering km rings together with memory,
but make it a pure region without a notion of a buffer, and let
fuse to chunk it. Later, we can make payload memory allocation
optional.

---

Right, intentionally so, because otherwise it&#x27;s a fuse uapi that
pretends to be a generic io_uring uapi but it&#x27;s not because of
all assumptions in different places.

---

Not really, it tells the buffer ring depth but says nothing about
how much memory user space allocated and how it&#x27;s pushed. It&#x27;s a
reasonable default but they could be different. For example, if you
expect adding more memory at runtime, you might create the buffer
ring a bit larger. Or when server processing takes a while and you
can&#x27;t recycle until it finishes, you might have more buffers than
you need ring entries. Or you might might decide to split buffers
and as you mentioned incremental consumption, which is an entire
separate topic because it doesn&#x27;t do de-fragmentation and you&#x27;d
need to have it in fuse, just like user space does with pbufs.

---

My entire point is that we&#x27;re making lots of assumptions for io_uring
uapi, and if it&#x27;s moved to fuse because it knows better what it
needs, it should be a win.

IOW, it sounds better if instead of passing the number of buffers to
io_uring, you just ask it to create a large chunk of memory, and then
fuse chunks it up and puts into the ring.

---

I agree, and you&#x27;d need something new in either case to add more
memory, and it doesn&#x27;t need to be IORING_REGISTER_MEM_REGION
specifically.

---

Unregistering a buffer ring doesn&#x27;t guarantee that there are no
inflight requests that are still using buffers that came out of
the buffer ring. The fuse driver can wait/terminate its requests
before unregisteration, but allow userspace issued IORING_OP_RECV
to use this km buffer ring, and you&#x27;ll need to somehow synchronise
with all other io_uring requests.

-- 
Pavel Begunkov</pre>
</details>
<div class="reply-to-label">&#8627; replying to Joanne Koong</div>
<div class="review-comment-signals">Signals: clarification, question, no objection</div>
</div>
</div>
<div class="thread-node depth-1" id="2026-02-20">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Pavel Begunkov</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-20">2026-02-20</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Pavel Begunkov questioned whether kernel-managed buffer rings should be exposed in the io_uring uapi, specifically asking if a server or user space program can issue I/O requests that consume buffers from the km ring without fuse kernel code involved. He requested clarification on this point to inform the decision about exposing km rings in the uapi.

Reviewer Pavel Begunkov suggested reusing registered buffers instead of introducing a new mechanism for kernel-managed buffer rings, citing efficiency and similarity to zero-copy internally registered buffers as benefits.

Reviewer noted that kernel-managed buffer rings would require handling page references and/or pinning regions when using registered buffers, suggesting an alternative approach.

Reviewer Pavel Begunkov suggested adding a liburing helper to handle mmap&#x27;ing for the fuse server, eliminating its need to directly manage memory mappings.

reviewer expressed conditional approval, requesting confirmation that the patch enables fast path optimizations</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">You mention OP_READ_FIXED and below agreed not exposing km rings
an io_uring uapi, which makes me believe we&#x27;re still talking about
different things.

Correct me if I&#x27;m wrong. Currently, only fuse cmds use the buffer
ring itself, I&#x27;m not talking about buffer, i.e. fuse cmds consume
entries from the ring (!!! that&#x27;s the part I&#x27;m interested in), then
process them and tell the server &quot;this offset in the region has user
data to process or should be populated with data&quot;.

Naturally, the server should be able to use the buffers to issue
some I/O and process it in other ways, whether it&#x27;s a normal
OP_READ to which you pass the user space address (you can since
it&#x27;s mmap()&#x27;ed by the server) or something else is important but
a separate question than the one I&#x27;m trying to understand.

So I&#x27;m asking whether you expect that a server or other user space
program should be able to issue a READ_OP_RECV, READ_OP_READ or any
other similar request, which would consume buffers/entries from the
km ring without any fuse kernel code involved? Do you have some
use case for that in mind?

Understanding that is the key in deciding whether km rings should
be exposed as io_uring uapi or not, regardless of where buffers
to populate the ring come from.

...

---

So you already can do all that using the mmap()&#x27;ed region user
pointer, and you just want it to be more efficient, right?
For that let&#x27;s just reuse registered buffers, we don&#x27;t need a
new mechanism that needs to be propagated to all request types.
And registered buffer are already optimised for I/O in a bunch
of ways. And as a bonus, it&#x27;ll be similar to the zero-copy
internally registered buffers if you still plan to add them.

The simplest way to do that is to create a registered buffer out
of the mmap&#x27;ed region pointer. Pseudo code:

// mmap&#x27;ed if it&#x27;s kernel allocated.
{region_ptr, region_size} = create_region();

struct iovec iov;
iov.iov_base = region_ptr;
iov.iov_len = region_size;
io_uring_register_buffers(ring, &amp;iov, 1);

// later instead of this:
ptr = region_ptr + off;
io_uring_prep_read(sqe, fd, ptr, ...);

// you use registered buffers as usual:
io_uring_prep_read_fixed(sqe, fd, off, regbuf_idx, ...);


IIRC the registration would fail because it doesn&#x27;t allow file
backed pages, but it should be fine if we know it&#x27;s io_uring
region memory, so that would need to be patched.

There might be a bunch of other ways you can do that like
create a kernel allocated registered buffer like what Cristoph
wants, and then register it as a region. Or allow creating
registered buffers out of a region. etc.

I wanted to unify registered buffers and regions internally
at some point, but then drifted away from active io_uring core
infrastructure development, so I guess that could&#x27;ve been useful.

---

Let&#x27;s talk about it when it&#x27;s needed or something changes, but if
you do registered buffers instead as per above, they&#x27;ll be holding
page references and or have to pin the region in some other way.

---

FWIW, we should just add a liburing helper, so that fuse server
doesn&#x27;t need to deal with mmap&#x27;ing.

---

That&#x27;s sounds clean to me _if_ it allows you to achieve all
(fast path) optimisations you want to have. I hope it does?

-- 
Pavel Begunkov</pre>
</details>
<div class="reply-to-label">&#8627; replying to Joanne Koong</div>
<div class="review-comment-signals">Signals: requested changes, clarification needed, suggested alternative approach</div>
</div>
</div>
<div class="thread-node depth-1" id="2026-02-23">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Pavel Begunkov</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-23">2026-02-23</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Pavel Begunkov expressed confusion about the relevance of kernel-managed buffer rings to a previous request from Christoph, and asked for clarification on why this feature is necessary.

Reviewer Pavel Begunkov questioned the exposure of internal kernel fuse API as an io_uring uapi, suggesting that it may have been discussed previously but was not clear from the patchset

Reviewer Pavel Begunkov noted that the patch introduces an additional way to pass buffers through io_uring, which could make the I/O path more complex and suggested reusing an existing uapi instead of creating a new one.

Reviewer Pavel Begunkov suggested using registered buffers instead of copying client&#x27;s data into user space, citing that it would be a better abstraction; he also mentioned that he was following the main I/O path and trying to make the setup path more flexible and reusable.

Reviewer Pavel Begunkov expressed skepticism about the necessity of introducing a new interface for kernel-managed buffer rings, citing an existing interface as sufficient for efficient user-space implementation.

Reviewer Pavel Begunkov noted that the patch does not provide a clear mechanism for the fuse server to use kernel-managed buffer rings when performing I/O operations, instead requiring the user to either use OP_READ/etc. with user addresses from mmap()ing regions or registering and using OP_READ_FIXED.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Sorry, I don&#x27;t see relevance b/w km rings and what Christoph wants.
I explained why in some sub-thread, but maybe someone can tell
what I&#x27;m missing.

---

Yep, it could be, potentially, it&#x27;s just the patchset doesn&#x27;t plumb
it to other requests and uses it within fuse. It&#x27;s just cases like
that always make me wonder, here it was why what is basically an
internal kernel fuse API is exposed as an io_uring uapi. Maybe there
was a discussion about it I missed?

---

That would avoid doing a large revamp of uapi and plumbing it
to each every request type when there is already a uapi that does
what you want, does it well and have lots of things figured out.
Keeping the I/O path sane is important, io_uring already has 3
different ways of passing buffers, let&#x27;s not add a 4th one
unless it achieves something meaningful.

---

Sorry, maybe I wasn&#x27;t clear. With what I see you&#x27;re trying to do,
i.e. copying client&#x27;s data into user space (server), I think
registered buffers would be a better abstraction. However, I just
went with your design on top of regions, since it&#x27;s not the first
iteration of the series and I wasn&#x27;t following previous ones, and
IIRC you was already using registered buffers in previous revisions
but moved from that for some reason. IOW, I was taking you main I/O
path and was trying to make the setup path a bit more flexible and
reusable.

---

There is a high bar for adding a new interface for passing buffers
that needs to be propagated to a good number of request handlers,
and there is already one that gives you all you need to write
efficient user space.

---

It&#x27;s up to the user (i.e. fuse server) to either use OP_READ/etc. using
user addresses that you have in your design from mmap()ing regions, or
registering it and using OP_READ_FIXED.</pre>
</details>
<div class="reply-to-label">&#8627; replying to Joanne Koong</div>
<div class="review-comment-signals">Signals: confusion, lack of understanding, uncertainty</div>
</div>
</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-09">2026-02-09</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed a concern about inconsistent function naming by renaming io_unregister_pbuf_ring() to io_unregister_buf_ring(), which will be used for both provided buffer rings and kernel-managed buffer rings, as a preparatory change for upcoming kernel-managed buffer ring support.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Use the more generic name io_unregister_buf_ring() as this function will
be used for unregistering both provided buffer rings and kernel-managed
buffer rings.

This is a preparatory change for upcoming kernel-managed buffer ring
support.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 io_uring/kbuf.c     | 2 +-
 io_uring/kbuf.h     | 2 +-
 io_uring/register.c | 2 +-
 3 files changed, 3 insertions(+), 3 deletions(-)

diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index 850b836f32ee..aa9b70b72db4 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -719,7 +719,7 @@ int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
 	return ret;
 }
 
-int io_unregister_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
+int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg)
 {
 	struct io_uring_buf_reg reg;
 	struct io_buffer_list *bl;
diff --git a/io_uring/kbuf.h b/io_uring/kbuf.h
index bf15e26520d3..40b44f4fdb15 100644
--- a/io_uring/kbuf.h
+++ b/io_uring/kbuf.h
@@ -74,7 +74,7 @@ int io_provide_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe
 int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags);
 
 int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);
-int io_unregister_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);
+int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg);
 int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg);
 
 bool io_kbuf_recycle_legacy(struct io_kiocb *req, unsigned issue_flags);
diff --git a/io_uring/register.c b/io_uring/register.c
index 594b1f2ce875..0882cb34f851 100644
--- a/io_uring/register.c
+++ b/io_uring/register.c
@@ -841,7 +841,7 @@ static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,
 		ret = -EINVAL;
 		if (!arg || nr_args != 1)
 			break;
-		ret = io_unregister_pbuf_ring(ctx, arg);
+		ret = io_unregister_buf_ring(ctx, arg);
 		break;
 	case IORING_REGISTER_SYNC_CANCEL:
 		ret = -EINVAL;
-- 
2.47.3</pre>
</details>
<div class="review-comment-signals">Signals: preparatory change, renaming function</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-09">2026-02-09</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed a concern about the implementation of kernel-managed buffer rings, explaining that they follow the same pattern as pbuf ring registration and reusing validation and buffer list allocation helpers.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Add support for kernel-managed buffer rings (kmbuf rings), which allow
the kernel to allocate and manage the backing buffers for a buffer
ring, rather than requiring the application to provide and manage them.

This introduces two new registration opcodes:
- IORING_REGISTER_KMBUF_RING: Register a kernel-managed buffer ring
- IORING_UNREGISTER_KMBUF_RING: Unregister a kernel-managed buffer ring

The existing io_uring_buf_reg structure is extended with a union to
support both application-provided buffer rings (pbuf) and kernel-managed
buffer rings (kmbuf):
- For pbuf rings: ring_addr specifies the user-provided ring address
- For kmbuf rings: buf_size specifies the size of each buffer. buf_size
  must be non-zero and page-aligned.

The implementation follows the same pattern as pbuf ring registration,
reusing the validation and buffer list allocation helpers introduced in
earlier refactoring. The IOBL_KERNEL_MANAGED flag marks buffer lists as
kernel-managed for appropriate handling in the I/O path.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 include/uapi/linux/io_uring.h |  15 ++++-
 io_uring/kbuf.c               |  81 ++++++++++++++++++++++++-
 io_uring/kbuf.h               |   7 ++-
 io_uring/memmap.c             | 111 ++++++++++++++++++++++++++++++++++
 io_uring/memmap.h             |   4 ++
 io_uring/register.c           |   7 +++
 6 files changed, 219 insertions(+), 6 deletions(-)

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index fc473af6feb4..a0889c1744bd 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -715,6 +715,10 @@ enum io_uring_register_op {
 	/* register bpf filtering programs */
 	IORING_REGISTER_BPF_FILTER		= 37,
 
+	/* register/unregister kernel-managed ring buffer group */
+	IORING_REGISTER_KMBUF_RING		= 38,
+	IORING_UNREGISTER_KMBUF_RING		= 39,
+
 	/* this goes last */
 	IORING_REGISTER_LAST,
 
@@ -891,9 +895,16 @@ enum io_uring_register_pbuf_ring_flags {
 	IOU_PBUF_RING_INC	= 2,
 };
 
-/* argument for IORING_(UN)REGISTER_PBUF_RING */
+/* argument for IORING_(UN)REGISTER_PBUF_RING and
+ * IORING_(UN)REGISTER_KMBUF_RING
+ */
 struct io_uring_buf_reg {
-	__u64	ring_addr;
+	union {
+		/* used for pbuf rings */
+		__u64	ring_addr;
+		/* used for kmbuf rings */
+		__u32   buf_size;
+	};
 	__u32	ring_entries;
 	__u16	bgid;
 	__u16	flags;
diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index aa9b70b72db4..9bc36451d083 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -427,10 +427,13 @@ static int io_remove_buffers_legacy(struct io_ring_ctx *ctx,
 
 static void io_put_bl(struct io_ring_ctx *ctx, struct io_buffer_list *bl)
 {
-	if (bl-&gt;flags &amp; IOBL_BUF_RING)
+	if (bl-&gt;flags &amp; IOBL_BUF_RING) {
 		io_free_region(ctx-&gt;user, &amp;bl-&gt;region);
-	else
+		if (bl-&gt;flags &amp; IOBL_KERNEL_MANAGED)
+			kfree(bl-&gt;buf_ring);
+	} else {
 		io_remove_buffers_legacy(ctx, bl, -1U);
+	}
 
 	kfree(bl);
 }
@@ -779,3 +782,77 @@ struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,
 		return NULL;
 	return &amp;bl-&gt;region;
 }
+
+static int io_setup_kmbuf_ring(struct io_ring_ctx *ctx,
+			       struct io_buffer_list *bl,
+			       struct io_uring_buf_reg *reg)
+{
+	struct io_uring_buf_ring *ring;
+	unsigned long ring_size;
+	void *buf_region;
+	unsigned int i;
+	int ret;
+
+	/* allocate pages for the ring structure */
+	ring_size = flex_array_size(ring, bufs, bl-&gt;nr_entries);
+	ring = kzalloc(ring_size, GFP_KERNEL_ACCOUNT);
+	if (!ring)
+		return -ENOMEM;
+
+	ret = io_create_region_multi_buf(ctx, &amp;bl-&gt;region, bl-&gt;nr_entries,
+					 reg-&gt;buf_size);
+	if (ret) {
+		kfree(ring);
+		return ret;
+	}
+
+	/* initialize ring buf entries to point to the buffers */
+	buf_region = bl-&gt;region.ptr;
+	for (i = 0; i &lt; bl-&gt;nr_entries; i++) {
+		struct io_uring_buf *buf = &amp;ring-&gt;bufs[i];
+
+		buf-&gt;addr = (u64)(uintptr_t)buf_region;
+		buf-&gt;len = reg-&gt;buf_size;
+		buf-&gt;bid = i;
+
+		buf_region += reg-&gt;buf_size;
+	}
+	ring-&gt;tail = bl-&gt;nr_entries;
+
+	bl-&gt;buf_ring = ring;
+	bl-&gt;flags |= IOBL_KERNEL_MANAGED;
+
+	return 0;
+}
+
+int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
+{
+	struct io_uring_buf_reg reg;
+	struct io_buffer_list *bl;
+	int ret;
+
+	lockdep_assert_held(&amp;ctx-&gt;uring_lock);
+
+	ret = io_copy_and_validate_buf_reg(arg, &amp;reg, 0);
+	if (ret)
+		return ret;
+
+	if (!reg.buf_size || !PAGE_ALIGNED(reg.buf_size))
+		return -EINVAL;
+
+	bl = io_alloc_new_buffer_list(ctx, &amp;reg);
+	if (IS_ERR(bl))
+		return PTR_ERR(bl);
+
+	ret = io_setup_kmbuf_ring(ctx, bl, &amp;reg);
+	if (ret) {
+		kfree(bl);
+		return ret;
+	}
+
+	ret = io_buffer_add_list(ctx, bl, reg.bgid);
+	if (ret)
+		io_put_bl(ctx, bl);
+
+	return ret;
+}
diff --git a/io_uring/kbuf.h b/io_uring/kbuf.h
index 40b44f4fdb15..62c80a1ebf03 100644
--- a/io_uring/kbuf.h
+++ b/io_uring/kbuf.h
@@ -7,9 +7,11 @@
 
 enum {
 	/* ring mapped provided buffers */
-	IOBL_BUF_RING	= 1,
+	IOBL_BUF_RING		= 1,
 	/* buffers are consumed incrementally rather than always fully */
-	IOBL_INC	= 2,
+	IOBL_INC		= 2,
+	/* buffers are kernel managed */
+	IOBL_KERNEL_MANAGED	= 4,
 };
 
 struct io_buffer_list {
@@ -74,6 +76,7 @@ int io_provide_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe
 int io_manage_buffers_legacy(struct io_kiocb *req, unsigned int issue_flags);
 
 int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg);
+int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg);
 int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg);
 int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg);
 
diff --git a/io_uring/memmap.c b/io_uring/memmap.c
index 89f56609e50a..8d37e93c0433 100644
--- a/io_uring/memmap.c
+++ b/io_uring/memmap.c
@@ -15,6 +15,28 @@
 #include &quot;rsrc.h&quot;
 #include &quot;zcrx.h&quot;
 
+static void release_multi_buf_pages(struct page **pages, unsigned long nr_pages)
+{
+	struct page *page;
+	unsigned int nr, i = 0;
+
+	while (nr_pages) {
+		page = pages[i];
+
+		if (!page || WARN_ON_ONCE(page != compound_head(page)))
+			return;
+
+		nr = compound_nr(page);
+		put_page(page);
+
+		if (WARN_ON_ONCE(nr &gt; nr_pages))
+			return;
+
+		i += nr;
+		nr_pages -= nr;
+	}
+}
+
 static bool io_mem_alloc_compound(struct page **pages, int nr_pages,
 				  size_t size, gfp_t gfp)
 {
@@ -86,6 +108,8 @@ enum {
 	IO_REGION_F_USER_PROVIDED		= 2,
 	/* only the first page in the array is ref&#x27;ed */
 	IO_REGION_F_SINGLE_REF			= 4,
+	/* pages in the array belong to multiple discrete allocations */
+	IO_REGION_F_MULTI_BUF			= 8,
 };
 
 void io_free_region(struct user_struct *user, struct io_mapped_region *mr)
@@ -98,6 +122,8 @@ void io_free_region(struct user_struct *user, struct io_mapped_region *mr)
 
 		if (mr-&gt;flags &amp; IO_REGION_F_USER_PROVIDED)
 			unpin_user_pages(mr-&gt;pages, nr_refs);
+		else if (mr-&gt;flags &amp; IO_REGION_F_MULTI_BUF)
+			release_multi_buf_pages(mr-&gt;pages, nr_refs);
 		else
 			release_pages(mr-&gt;pages, nr_refs);
 
@@ -149,6 +175,54 @@ static int io_region_pin_pages(struct io_mapped_region *mr,
 	return 0;
 }
 
+static int io_region_allocate_pages_multi_buf(struct io_mapped_region *mr,
+					      unsigned int nr_bufs,
+					      unsigned int buf_size)
+{
+	gfp_t gfp = GFP_USER | __GFP_ACCOUNT | __GFP_ZERO | __GFP_NOWARN;
+	struct page **pages, **cur_pages;
+	unsigned int nr_allocated;
+	unsigned int buf_pages;
+	unsigned int i;
+
+	if (!PAGE_ALIGNED(buf_size))
+		return -EINVAL;
+
+	buf_pages = buf_size &gt;&gt; PAGE_SHIFT;
+
+	pages = kvmalloc_array(mr-&gt;nr_pages, sizeof(*pages), gfp);
+	if (!pages)
+		return -ENOMEM;
+
+	cur_pages = pages;
+
+	for (i = 0; i &lt; nr_bufs; i++) {
+		if (io_mem_alloc_compound(cur_pages, buf_pages, buf_size,
+					  gfp)) {
+			cur_pages += buf_pages;
+			continue;
+		}
+
+		nr_allocated = alloc_pages_bulk_node(gfp, NUMA_NO_NODE,
+						     buf_pages, cur_pages);
+		if (nr_allocated != buf_pages) {
+			unsigned int total =
+				(cur_pages - pages) + nr_allocated;
+
+			release_multi_buf_pages(pages, total);
+			kvfree(pages);
+			return -ENOMEM;
+		}
+
+		cur_pages += buf_pages;
+	}
+
+	mr-&gt;flags |= IO_REGION_F_MULTI_BUF;
+	mr-&gt;pages = pages;
+
+	return 0;
+}
+
 static int io_region_allocate_pages(struct io_mapped_region *mr,
 				    struct io_uring_region_desc *reg,
 				    unsigned long mmap_offset)
@@ -181,6 +255,43 @@ static int io_region_allocate_pages(struct io_mapped_region *mr,
 	return 0;
 }
 
+int io_create_region_multi_buf(struct io_ring_ctx *ctx,
+			       struct io_mapped_region *mr,
+			       unsigned int nr_bufs, unsigned int buf_size)
+{
+	unsigned int nr_pages;
+	int ret;
+
+	if (WARN_ON_ONCE(mr-&gt;pages || mr-&gt;ptr || mr-&gt;nr_pages))
+		return -EFAULT;
+
+	if (WARN_ON_ONCE(!nr_bufs || !buf_size || !PAGE_ALIGNED(buf_size)))
+		return -EINVAL;
+
+	if (check_mul_overflow(buf_size &gt;&gt; PAGE_SHIFT, nr_bufs, &amp;nr_pages))
+		return -EINVAL;
+
+	if (ctx-&gt;user) {
+		ret = __io_account_mem(ctx-&gt;user, nr_pages);
+		if (ret)
+			return ret;
+	}
+	mr-&gt;nr_pages = nr_pages;
+
+	ret = io_region_allocate_pages_multi_buf(mr, nr_bufs, buf_size);
+	if (ret)
+		goto out_free;
+
+	ret = io_region_init_ptr(mr);
+	if (ret)
+		goto out_free;
+
+	return 0;
+out_free:
+	io_free_region(ctx-&gt;user, mr);
+	return ret;
+}
+
 int io_create_region(struct io_ring_ctx *ctx, struct io_mapped_region *mr,
 		     struct io_uring_region_desc *reg,
 		     unsigned long mmap_offset)
diff --git a/io_uring/memmap.h b/io_uring/memmap.h
index f4cfbb6b9a1f..3aa1167462ae 100644
--- a/io_uring/memmap.h
+++ b/io_uring/memmap.h
@@ -22,6 +22,10 @@ int io_create_region(struct io_ring_ctx *ctx, struct io_mapped_region *mr,
 		     struct io_uring_region_desc *reg,
 		     unsigned long mmap_offset);
 
+int io_create_region_multi_buf(struct io_ring_ctx *ctx,
+			       struct io_mapped_region *mr,
+			       unsigned int nr_bufs, unsigned int buf_size);
+
 static inline void *io_region_get_ptr(struct io_mapped_region *mr)
 {
 	return mr-&gt;ptr;
diff --git a/io_uring/register.c b/io_uring/register.c
index 0882cb34f851..2db8daaf8fde 100644
--- a/io_uring/register.c
+++ b/io_uring/register.c
@@ -837,7 +837,14 @@ static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,
 			break;
 		ret = io_register_pbuf_ring(ctx, arg);
 		break;
+	case IORING_REGISTER_KMBUF_RING:
+		ret = -EINVAL;
+		if (!arg || nr_args != 1)
+			break;
+		ret = io_register_kmbuf_ring(ctx, arg);
+		break;
 	case IORING_UNREGISTER_PBUF_RING:
+	case IORING_UNREGISTER_KMBUF_RING:
 		ret = -EINVAL;
 		if (!arg || nr_args != 1)
 			break;
-- 
2.47.3</pre>
</details>
<div class="review-comment-signals">Signals: neutral explanation, no clear resolution signal</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-09">2026-02-09</a>
<span class="badge" style="color:#155724;background:#d4edda">Positive</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Author addressed a concern about the implementation of kernel-managed buffer rings (kmbuf) mmap support, explaining that it follows the same pattern as application-provided buffer rings and introducing new constants for kmbuf mappings. The author confirmed that userspace can access kernel-allocated buffers directly through mmap.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Add support for mmapping kernel-managed buffer rings (kmbuf) to
userspace, allowing applications to access the kernel-allocated buffers.

Similar to application-provided buffer rings (pbuf), kmbuf rings use the
buffer group ID encoded in the mmap offset to identify which buffer ring
to map. The implementation follows the same pattern as pbuf rings.

New mmap offset constants are introduced:
  - IORING_OFF_KMBUF_RING (0x88000000): Base offset for kmbuf mappings
  - IORING_OFF_KMBUF_SHIFT (16): Shift value to encode buffer group ID

The mmap offset encodes the bgid shifted by IORING_OFF_KMBUF_SHIFT.
The io_buf_get_region() helper retrieves the appropriate region.

This allows userspace to mmap the kernel-allocated buffer region and
access the buffers directly.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 include/uapi/linux/io_uring.h |  2 ++
 io_uring/kbuf.c               | 11 +++++++++--
 io_uring/kbuf.h               |  5 +++--
 io_uring/memmap.c             |  5 ++++-
 4 files changed, 18 insertions(+), 5 deletions(-)

diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index a0889c1744bd..42a2812c9922 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -545,6 +545,8 @@ struct io_uring_cqe {
 #define IORING_OFF_SQES			0x10000000ULL
 #define IORING_OFF_PBUF_RING		0x80000000ULL
 #define IORING_OFF_PBUF_SHIFT		16
+#define IORING_OFF_KMBUF_RING		0x88000000ULL
+#define IORING_OFF_KMBUF_SHIFT		16
 #define IORING_OFF_MMAP_MASK		0xf8000000ULL
 
 /*
diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index 9bc36451d083..ccf5b213087b 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -770,16 +770,23 @@ int io_register_pbuf_status(struct io_ring_ctx *ctx, void __user *arg)
 	return 0;
 }
 
-struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,
-					    unsigned int bgid)
+struct io_mapped_region *io_buf_get_region(struct io_ring_ctx *ctx,
+					   unsigned int bgid,
+					   bool kernel_managed)
 {
 	struct io_buffer_list *bl;
+	bool is_kernel_managed;
 
 	lockdep_assert_held(&amp;ctx-&gt;mmap_lock);
 
 	bl = xa_load(&amp;ctx-&gt;io_bl_xa, bgid);
 	if (!bl || !(bl-&gt;flags &amp; IOBL_BUF_RING))
 		return NULL;
+
+	is_kernel_managed = !!(bl-&gt;flags &amp; IOBL_KERNEL_MANAGED);
+	if (is_kernel_managed != kernel_managed)
+		return NULL;
+
 	return &amp;bl-&gt;region;
 }
 
diff --git a/io_uring/kbuf.h b/io_uring/kbuf.h
index 62c80a1ebf03..11d165888b8e 100644
--- a/io_uring/kbuf.h
+++ b/io_uring/kbuf.h
@@ -88,8 +88,9 @@ unsigned int __io_put_kbufs(struct io_kiocb *req, struct io_buffer_list *bl,
 bool io_kbuf_commit(struct io_kiocb *req,
 		    struct io_buffer_list *bl, int len, int nr);
 
-struct io_mapped_region *io_pbuf_get_region(struct io_ring_ctx *ctx,
-					    unsigned int bgid);
+struct io_mapped_region *io_buf_get_region(struct io_ring_ctx *ctx,
+					   unsigned int bgid,
+					   bool kernel_managed);
 
 static inline bool io_kbuf_recycle_ring(struct io_kiocb *req,
 					struct io_buffer_list *bl)
diff --git a/io_uring/memmap.c b/io_uring/memmap.c
index 8d37e93c0433..916315122323 100644
--- a/io_uring/memmap.c
+++ b/io_uring/memmap.c
@@ -356,7 +356,10 @@ static struct io_mapped_region *io_mmap_get_region(struct io_ring_ctx *ctx,
 		return &amp;ctx-&gt;sq_region;
 	case IORING_OFF_PBUF_RING:
 		id = (offset &amp; ~IORING_OFF_MMAP_MASK) &gt;&gt; IORING_OFF_PBUF_SHIFT;
-		return io_pbuf_get_region(ctx, id);
+		return io_buf_get_region(ctx, id, false);
+	case IORING_OFF_KMBUF_RING:
+		id = (offset &amp; ~IORING_OFF_MMAP_MASK) &gt;&gt; IORING_OFF_KMBUF_SHIFT;
+		return io_buf_get_region(ctx, id, true);
 	case IORING_MAP_OFF_PARAM_REGION:
 		return &amp;ctx-&gt;param_region;
 	case IORING_MAP_OFF_ZCRX_REGION:
-- 
2.47.3</pre>
</details>
<div class="review-comment-signals">Signals: author provided a clear explanation of their implementation, author confirmed that userspace can access kernel-allocated buffers</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-09">2026-02-09</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed a concern about distinguishing between kernel-managed buffer addresses and negative values when error checking, explaining that the io_br_sel struct needs to separate address and value fields for kernel-managed buffers. The author modified the io_uring_types.h file to add a kaddr field to the union in io_br_sel, allowing for kernel-managed buffer selection.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Allow kernel-managed buffers to be selected. This requires modifying the
io_br_sel struct to separate the fields for address and val, since a
kernel address cannot be distinguished from a negative val when error
checking.

Auto-commit any selected kernel-managed buffer.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 include/linux/io_uring_types.h |  8 ++++----
 io_uring/kbuf.c                | 16 ++++++++++++----
 2 files changed, 16 insertions(+), 8 deletions(-)

diff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h
index 3e4a82a6f817..36cc2e0346d9 100644
--- a/include/linux/io_uring_types.h
+++ b/include/linux/io_uring_types.h
@@ -93,13 +93,13 @@ struct io_mapped_region {
  */
 struct io_br_sel {
 	struct io_buffer_list *buf_list;
-	/*
-	 * Some selection parts return the user address, others return an error.
-	 */
 	union {
+		/* for classic/ring provided buffers */
 		void __user *addr;
-		ssize_t val;
+		/* for kernel-managed buffers */
+		void *kaddr;
 	};
+	ssize_t val;
 };
 
 
diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index ccf5b213087b..1e8395270227 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -155,7 +155,8 @@ static int io_provided_buffers_select(struct io_kiocb *req, size_t *len,
 	return 1;
 }
 
-static bool io_should_commit(struct io_kiocb *req, unsigned int issue_flags)
+static bool io_should_commit(struct io_kiocb *req, struct io_buffer_list *bl,
+			     unsigned int issue_flags)
 {
 	/*
 	* If we came in unlocked, we have no choice but to consume the
@@ -170,7 +171,11 @@ static bool io_should_commit(struct io_kiocb *req, unsigned int issue_flags)
 	if (issue_flags &amp; IO_URING_F_UNLOCKED)
 		return true;
 
-	/* uring_cmd commits kbuf upfront, no need to auto-commit */
+	/* kernel-managed buffers are auto-committed */
+	if (bl-&gt;flags &amp; IOBL_KERNEL_MANAGED)
+		return true;
+
+	/* multishot uring_cmd commits kbuf upfront, no need to auto-commit */
 	if (!io_file_can_poll(req) &amp;&amp; req-&gt;opcode != IORING_OP_URING_CMD)
 		return true;
 	return false;
@@ -200,9 +205,12 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,
 	req-&gt;flags |= REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT;
 	req-&gt;buf_index = READ_ONCE(buf-&gt;bid);
 	sel.buf_list = bl;
-	sel.addr = u64_to_user_ptr(READ_ONCE(buf-&gt;addr));
+	if (bl-&gt;flags &amp; IOBL_KERNEL_MANAGED)
+		sel.kaddr = (void *)(uintptr_t)READ_ONCE(buf-&gt;addr);
+	else
+		sel.addr = u64_to_user_ptr(READ_ONCE(buf-&gt;addr));
 
-	if (io_should_commit(req, issue_flags)) {
+	if (io_should_commit(req, bl, issue_flags)) {
 		io_kbuf_commit(req, sel.buf_list, *len, 1);
 		sel.buf_list = NULL;
 	}
-- 
2.47.3</pre>
</details>
<div class="review-comment-signals">Signals: clarification, modification</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-09">2026-02-09</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed a concern about userspace unregistering a buffer ring while it is pinned by the kernel, explaining that adding kernel APIs to pin and unpin buffer rings will prevent this issue. The new APIs will allow kernel subsystems to safely access buffer ring contents while ensuring the buffer ring remains valid.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Add kernel APIs to pin and unpin buffer rings, preventing userspace from
unregistering a buffer ring while it is pinned by the kernel.

This provides a mechanism for kernel subsystems to safely access buffer
ring contents while ensuring the buffer ring remains valid. A pinned
buffer ring cannot be unregistered until explicitly unpinned. On the
userspace side, trying to unregister a pinned buffer will return -EBUSY.

This is a preparatory change for upcoming fuse usage of kernel-managed
buffer rings. It is necessary for fuse to pin the buffer ring because
fuse may need to select a buffer in atomic contexts, which it can only
do so by using the underlying buffer list pointer.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 include/linux/io_uring/cmd.h | 17 +++++++++++++
 io_uring/kbuf.c              | 48 ++++++++++++++++++++++++++++++++++++
 io_uring/kbuf.h              |  5 ++++
 3 files changed, 70 insertions(+)

diff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h
index 375fd048c4cb..702b1903e6ee 100644
--- a/include/linux/io_uring/cmd.h
+++ b/include/linux/io_uring/cmd.h
@@ -84,6 +84,10 @@ struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,
 bool io_uring_mshot_cmd_post_cqe(struct io_uring_cmd *ioucmd,
 				 struct io_br_sel *sel, unsigned int issue_flags);
 
+int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,
+			  unsigned issue_flags, struct io_buffer_list **bl);
+int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,
+			    unsigned issue_flags);
 #else
 static inline int
 io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,
@@ -126,6 +130,19 @@ static inline bool io_uring_mshot_cmd_post_cqe(struct io_uring_cmd *ioucmd,
 {
 	return true;
 }
+static inline int io_uring_buf_ring_pin(struct io_uring_cmd *cmd,
+					unsigned buf_group,
+					unsigned issue_flags,
+					struct io_buffer_list **bl)
+{
+	return -EOPNOTSUPP;
+}
+static inline int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd,
+					  unsigned buf_group,
+					  unsigned issue_flags)
+{
+	return -EOPNOTSUPP;
+}
 #endif
 
 static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)
diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index 1e8395270227..dee1764ed19f 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -9,6 +9,7 @@
 #include &lt;linux/poll.h&gt;
 #include &lt;linux/vmalloc.h&gt;
 #include &lt;linux/io_uring.h&gt;
+#include &lt;linux/io_uring/cmd.h&gt;
 
 #include &lt;uapi/linux/io_uring.h&gt;
 
@@ -237,6 +238,51 @@ struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,
 	return sel;
 }
 
+int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,
+			  unsigned issue_flags, struct io_buffer_list **bl)
+{
+	struct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)-&gt;ctx;
+	struct io_buffer_list *buffer_list;
+	int ret = -EINVAL;
+
+	io_ring_submit_lock(ctx, issue_flags);
+
+	buffer_list = io_buffer_get_list(ctx, buf_group);
+	if (buffer_list &amp;&amp; (buffer_list-&gt;flags &amp; IOBL_BUF_RING)) {
+		if (unlikely(buffer_list-&gt;flags &amp; IOBL_PINNED)) {
+			ret = -EALREADY;
+		} else {
+			buffer_list-&gt;flags |= IOBL_PINNED;
+			ret = 0;
+			*bl = buffer_list;
+		}
+	}
+
+	io_ring_submit_unlock(ctx, issue_flags);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(io_uring_buf_ring_pin);
+
+int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,
+		       unsigned issue_flags)
+{
+	struct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)-&gt;ctx;
+	struct io_buffer_list *bl;
+	int ret = -EINVAL;
+
+	io_ring_submit_lock(ctx, issue_flags);
+
+	bl = io_buffer_get_list(ctx, buf_group);
+	if (bl &amp;&amp; (bl-&gt;flags &amp; IOBL_BUF_RING) &amp;&amp; (bl-&gt;flags &amp; IOBL_PINNED)) {
+		bl-&gt;flags &amp;= ~IOBL_PINNED;
+		ret = 0;
+	}
+
+	io_ring_submit_unlock(ctx, issue_flags);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(io_uring_buf_ring_unpin);
+
 /* cap it at a reasonable 256, will be one page even for 4K */
 #define PEEK_MAX_IMPORT		256
 
@@ -747,6 +793,8 @@ int io_unregister_buf_ring(struct io_ring_ctx *ctx, void __user *arg)
 		return -ENOENT;
 	if (!(bl-&gt;flags &amp; IOBL_BUF_RING))
 		return -EINVAL;
+	if (bl-&gt;flags &amp; IOBL_PINNED)
+		return -EBUSY;
 
 	scoped_guard(mutex, &amp;ctx-&gt;mmap_lock)
 		xa_erase(&amp;ctx-&gt;io_bl_xa, bl-&gt;bgid);
diff --git a/io_uring/kbuf.h b/io_uring/kbuf.h
index 11d165888b8e..781630c2cc10 100644
--- a/io_uring/kbuf.h
+++ b/io_uring/kbuf.h
@@ -12,6 +12,11 @@ enum {
 	IOBL_INC		= 2,
 	/* buffers are kernel managed */
 	IOBL_KERNEL_MANAGED	= 4,
+	/*
+	 * buffer ring is pinned and cannot be unregistered by userspace until
+	 * it has been unpinned
+	 */
+	IOBL_PINNED		= 8,
 };
 
 struct io_buffer_list {
-- 
2.47.3</pre>
</details>
<div class="review-comment-signals">Signals: clarification, preparatory change</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-09">2026-02-09</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Author addressed a concern about buffer recycling by adding an interface for buffers to be recycled back into a kernel-managed buffer ring, and provided the implementation in io_uring/kbuf.c.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Add an interface for buffers to be recycled back into a kernel-managed
buffer ring.

This is a preparatory patch for fuse over io-uring.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 include/linux/io_uring/cmd.h | 11 +++++++++
 io_uring/kbuf.c              | 44 ++++++++++++++++++++++++++++++++++++
 2 files changed, 55 insertions(+)

diff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h
index 702b1903e6ee..a488e945f883 100644
--- a/include/linux/io_uring/cmd.h
+++ b/include/linux/io_uring/cmd.h
@@ -88,6 +88,10 @@ int io_uring_buf_ring_pin(struct io_uring_cmd *cmd, unsigned buf_group,
 			  unsigned issue_flags, struct io_buffer_list **bl);
 int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,
 			    unsigned issue_flags);
+
+int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,
+			   u64 addr, unsigned int len, unsigned int bid,
+			   unsigned int issue_flags);
 #else
 static inline int
 io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,
@@ -143,6 +147,13 @@ static inline int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd,
 {
 	return -EOPNOTSUPP;
 }
+static inline int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd,
+					 unsigned int buf_group, u64 addr,
+					 unsigned int len, unsigned int bid,
+					 unsigned int issue_flags)
+{
+	return -EOPNOTSUPP;
+}
 #endif
 
 static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)
diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index dee1764ed19f..17b6178be4ce 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -102,6 +102,50 @@ void io_kbuf_drop_legacy(struct io_kiocb *req)
 	req-&gt;kbuf = NULL;
 }
 
+int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,
+			   u64 addr, unsigned int len, unsigned int bid,
+			   unsigned int issue_flags)
+{
+	struct io_kiocb *req = cmd_to_io_kiocb(cmd);
+	struct io_ring_ctx *ctx = req-&gt;ctx;
+	struct io_uring_buf_ring *br;
+	struct io_uring_buf *buf;
+	struct io_buffer_list *bl;
+	int ret = -EINVAL;
+
+	if (WARN_ON_ONCE(req-&gt;flags &amp; REQ_F_BUFFERS_COMMIT))
+		return ret;
+
+	io_ring_submit_lock(ctx, issue_flags);
+
+	bl = io_buffer_get_list(ctx, buf_group);
+
+	if (!bl || WARN_ON_ONCE(!(bl-&gt;flags &amp; IOBL_BUF_RING)) ||
+	    WARN_ON_ONCE(!(bl-&gt;flags &amp; IOBL_KERNEL_MANAGED)))
+		goto done;
+
+	br = bl-&gt;buf_ring;
+
+	if (WARN_ON_ONCE((br-&gt;tail - bl-&gt;head) &gt;= bl-&gt;nr_entries))
+		goto done;
+
+	buf = &amp;br-&gt;bufs[(br-&gt;tail) &amp; bl-&gt;mask];
+
+	buf-&gt;addr = addr;
+	buf-&gt;len = len;
+	buf-&gt;bid = bid;
+
+	req-&gt;flags &amp;= ~REQ_F_BUFFER_RING;
+
+	br-&gt;tail++;
+	ret = 0;
+
+done:
+	io_ring_submit_unlock(ctx, issue_flags);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(io_uring_kmbuf_recycle);
+
 bool io_kbuf_recycle_legacy(struct io_kiocb *req, unsigned issue_flags)
 {
 	struct io_ring_ctx *ctx = req-&gt;ctx;
-- 
2.47.3</pre>
</details>
<div class="review-comment-signals">Signals: clarification, preparatory patch</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-09">2026-02-09</a>
<span class="badge" style="color:#155724;background:#d4edda">Positive</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed a concern about the io_uring_is_kmbuf_ring() function, which was missing an implementation to check if a buffer ring is kernel-managed. The author provided the updated code for this function in the patch, which checks the flags of the buffer list and returns true if it&#x27;s a kernel-managed buffer ring.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">io_uring_is_kmbuf_ring() returns true if there is a kernel-managed
buffer ring at the specified buffer group.

This is a preparatory patch for upcoming fuse kernel-managed buffer
support, which needs to ensure the buffer ring registered by the server
is a kernel-managed buffer ring.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 include/linux/io_uring/cmd.h |  9 +++++++++
 io_uring/kbuf.c              | 20 ++++++++++++++++++++
 2 files changed, 29 insertions(+)

diff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h
index a488e945f883..04a937f6f4d3 100644
--- a/include/linux/io_uring/cmd.h
+++ b/include/linux/io_uring/cmd.h
@@ -92,6 +92,9 @@ int io_uring_buf_ring_unpin(struct io_uring_cmd *cmd, unsigned buf_group,
 int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,
 			   u64 addr, unsigned int len, unsigned int bid,
 			   unsigned int issue_flags);
+
+bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,
+			    unsigned int issue_flags);
 #else
 static inline int
 io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,
@@ -154,6 +157,12 @@ static inline int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd,
 {
 	return -EOPNOTSUPP;
 }
+static inline bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd,
+					  unsigned int buf_group,
+					  unsigned int issue_flags)
+{
+	return false;
+}
 #endif
 
 static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)
diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index 17b6178be4ce..797cc2f0a5e9 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -963,3 +963,23 @@ int io_register_kmbuf_ring(struct io_ring_ctx *ctx, void __user *arg)
 
 	return ret;
 }
+
+bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,
+			    unsigned int issue_flags)
+{
+	struct io_ring_ctx *ctx = cmd_to_io_kiocb(cmd)-&gt;ctx;
+	struct io_buffer_list *bl;
+	bool is_kmbuf_ring = false;
+
+	io_ring_submit_lock(ctx, issue_flags);
+
+	bl = io_buffer_get_list(ctx, buf_group);
+	if (likely(bl) &amp;&amp; (bl-&gt;flags &amp; IOBL_KERNEL_MANAGED)) {
+		WARN_ON_ONCE(!(bl-&gt;flags &amp; IOBL_BUF_RING));
+		is_kmbuf_ring = true;
+	}
+
+	io_ring_submit_unlock(ctx, issue_flags);
+	return is_kmbuf_ring;
+}
+EXPORT_SYMBOL_GPL(io_uring_is_kmbuf_ring);
-- 
2.47.3</pre>
</details>
<div class="review-comment-signals">Signals: provided updated code, addressed concern</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-09">2026-02-09</a>
<span class="badge" style="color:#155724;background:#d4edda">Positive</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed a concern about the io_uring mutex being held in atomic contexts and needing to select a buffer from a kernel-managed bufring, so they exported the io_ring_buffer_select() function to allow callers to use it without grabbing the mutex.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Export io_ring_buffer_select() so that it may be used by callers who
pass in a pinned bufring without needing to grab the io_uring mutex.

This is a preparatory patch that will be needed by fuse io-uring, which
will need to select a buffer from a kernel-managed bufring while the
uring mutex may already be held by in-progress commits, and may need to
select a buffer in atomic contexts.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 include/linux/io_uring/cmd.h | 14 ++++++++++++++
 io_uring/kbuf.c              |  7 ++++---
 2 files changed, 18 insertions(+), 3 deletions(-)

diff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h
index 04a937f6f4d3..d4b5943bdeb1 100644
--- a/include/linux/io_uring/cmd.h
+++ b/include/linux/io_uring/cmd.h
@@ -95,6 +95,10 @@ int io_uring_kmbuf_recycle(struct io_uring_cmd *cmd, unsigned int buf_group,
 
 bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd, unsigned int buf_group,
 			    unsigned int issue_flags);
+
+struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,
+				       struct io_buffer_list *bl,
+				       unsigned int issue_flags);
 #else
 static inline int
 io_uring_cmd_import_fixed(u64 ubuf, unsigned long len, int rw,
@@ -163,6 +167,16 @@ static inline bool io_uring_is_kmbuf_ring(struct io_uring_cmd *cmd,
 {
 	return false;
 }
+static inline struct io_br_sel io_ring_buffer_select(struct io_kiocb *req,
+						     size_t *len,
+						     struct io_buffer_list *bl,
+						     unsigned int issue_flags)
+{
+	struct io_br_sel sel = {
+		.val = -EOPNOTSUPP,
+	};
+	return sel;
+}
 #endif
 
 static inline struct io_uring_cmd *io_uring_cmd_from_tw(struct io_tw_req tw_req)
diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index 797cc2f0a5e9..9a93f10d3214 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -226,9 +226,9 @@ static bool io_should_commit(struct io_kiocb *req, struct io_buffer_list *bl,
 	return false;
 }
 
-static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,
-					      struct io_buffer_list *bl,
-					      unsigned int issue_flags)
+struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,
+				       struct io_buffer_list *bl,
+				       unsigned int issue_flags)
 {
 	struct io_uring_buf_ring *br = bl-&gt;buf_ring;
 	__u16 tail, head = bl-&gt;head;
@@ -261,6 +261,7 @@ static struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,
 	}
 	return sel;
 }
+EXPORT_SYMBOL_GPL(io_ring_buffer_select);
 
 struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,
 				  unsigned buf_group, unsigned int issue_flags)
-- 
2.47.3</pre>
</details>
<div class="review-comment-signals">Signals: acknowledged a specific concern, provided a solution</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-09">2026-02-09</a>
<span class="badge" style="color:#155724;background:#d4edda">Positive</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author is addressing a concern about returning the id of the selected buffer in io_buffer_select(). They agree to modify the function to return the buffer id, which is necessary for kernel-managed buffer rings to recycle the selected buffer.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Return the id of the selected buffer in io_buffer_select(). This is
needed for kernel-managed buffer rings to later recycle the selected
buffer.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 include/linux/io_uring/cmd.h   | 2 +-
 include/linux/io_uring_types.h | 2 ++
 io_uring/kbuf.c                | 7 +++++--
 3 files changed, 8 insertions(+), 3 deletions(-)

diff --git a/include/linux/io_uring/cmd.h b/include/linux/io_uring/cmd.h
index d4b5943bdeb1..94df2bdebe77 100644
--- a/include/linux/io_uring/cmd.h
+++ b/include/linux/io_uring/cmd.h
@@ -71,7 +71,7 @@ void io_uring_cmd_issue_blocking(struct io_uring_cmd *ioucmd);
 
 /*
  * Select a buffer from the provided buffer group for multishot uring_cmd.
- * Returns the selected buffer address and size.
+ * Returns the selected buffer address, size, and id.
  */
 struct io_br_sel io_uring_cmd_buffer_select(struct io_uring_cmd *ioucmd,
 					    unsigned buf_group, size_t *len,
diff --git a/include/linux/io_uring_types.h b/include/linux/io_uring_types.h
index 36cc2e0346d9..5a56bb341337 100644
--- a/include/linux/io_uring_types.h
+++ b/include/linux/io_uring_types.h
@@ -100,6 +100,8 @@ struct io_br_sel {
 		void *kaddr;
 	};
 	ssize_t val;
+	/* id of the selected buffer */
+	unsigned buf_id;
 };
 
 
diff --git a/io_uring/kbuf.c b/io_uring/kbuf.c
index 9a93f10d3214..24c1e34ea23e 100644
--- a/io_uring/kbuf.c
+++ b/io_uring/kbuf.c
@@ -250,6 +250,7 @@ struct io_br_sel io_ring_buffer_select(struct io_kiocb *req, size_t *len,
 	req-&gt;flags |= REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT;
 	req-&gt;buf_index = READ_ONCE(buf-&gt;bid);
 	sel.buf_list = bl;
+	sel.buf_id = req-&gt;buf_index;
 	if (bl-&gt;flags &amp; IOBL_KERNEL_MANAGED)
 		sel.kaddr = (void *)(uintptr_t)READ_ONCE(buf-&gt;addr);
 	else
@@ -274,10 +275,12 @@ struct io_br_sel io_buffer_select(struct io_kiocb *req, size_t *len,
 
 	bl = io_buffer_get_list(ctx, buf_group);
 	if (likely(bl)) {
-		if (bl-&gt;flags &amp; IOBL_BUF_RING)
+		if (bl-&gt;flags &amp; IOBL_BUF_RING) {
 			sel = io_ring_buffer_select(req, len, bl, issue_flags);
-		else
+		} else {
 			sel.addr = io_provided_buffer_select(req, len, bl);
+			sel.buf_id = req-&gt;buf_index;
+		}
 	}
 	io_ring_submit_unlock(req-&gt;ctx, issue_flags);
 	return sel;
-- 
2.47.3</pre>
</details>
<div class="review-comment-signals">Signals: agreement, commitment</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Joanne Koong (author)</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-09">2026-02-09</a>
<span class="badge" style="color:#155724;background:#d4edda">Positive</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Author addressed a concern about indicating the selected buffer in uring_cmd operations, explaining that this is necessary for fuse to relay the information to userspace and agreeing to add the IORING_CQE_F_BUFFER flag along with the buffer index.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">When uring_cmd operations select a buffer, the completion queue entry
should indicate which buffer was selected.

Set IORING_CQE_F_BUFFER on the completed entry and encode the buffer
index if a buffer was selected.

This will be needed for fuse, which needs to relay to userspace which
selected buffer contains the data.

Signed-off-by: Joanne Koong &lt;joannelkoong@gmail.com&gt;
---
 io_uring/uring_cmd.c | 6 +++++-
 1 file changed, 5 insertions(+), 1 deletion(-)

diff --git a/io_uring/uring_cmd.c b/io_uring/uring_cmd.c
index ee7b49f47cb5..6d38df1a812d 100644
--- a/io_uring/uring_cmd.c
+++ b/io_uring/uring_cmd.c
@@ -151,6 +151,7 @@ void __io_uring_cmd_done(struct io_uring_cmd *ioucmd, s32 ret, u64 res2,
 		       unsigned issue_flags, bool is_cqe32)
 {
 	struct io_kiocb *req = cmd_to_io_kiocb(ioucmd);
+	u32 cflags = 0;
 
 	if (WARN_ON_ONCE(req-&gt;flags &amp; REQ_F_APOLL_MULTISHOT))
 		return;
@@ -160,7 +161,10 @@ void __io_uring_cmd_done(struct io_uring_cmd *ioucmd, s32 ret, u64 res2,
 	if (ret &lt; 0)
 		req_set_fail(req);
 
-	io_req_set_res(req, ret, 0);
+	if (req-&gt;flags &amp; (REQ_F_BUFFER_SELECTED | REQ_F_BUFFER_RING))
+		cflags |= IORING_CQE_F_BUFFER |
+			(req-&gt;buf_index &lt;&lt; IORING_CQE_BUFFER_SHIFT);
+	io_req_set_res(req, ret, cflags);
 	if (is_cqe32) {
 		if (req-&gt;ctx-&gt;flags &amp; IORING_SETUP_CQE_MIXED)
 			req-&gt;cqe.flags |= IORING_CQE_F_32;
-- 
2.47.3</pre>
</details>
<div class="review-comment-signals">Signals: agreed to implement a fix, explained reasoning</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Pavel Begunkov</span>
<a class="date-chip" href="../2026-02-21_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-13">2026-02-13</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Pavel Begunkov suggested reusing regions for allocations and mmap() operations, wrapping them into a registered buffer, and making vmap&#x27;ing optional to avoid unnecessary overhead.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">FWIW, the easiest solution is to internally reuse regions for
allocations and mmap()&#x27;ing and wrap it into a registered buffer.
It just need to make vmap&#x27;ing optional as it won&#x27;t be needed.

-- 
Pavel Begunkov</pre>
</details>
<div class="review-comment-signals">Signals: suggested improvement, optional optimization</div>
</div>
</div>
</div>

    <footer>LKML Daily Activity Tracker</footer>
    <script>
    // When arriving via a date anchor (e.g. #2026-02-15 from a daily report),
    // scroll the anchor into view after a brief delay so layout is complete.
    (function () {
        var hash = window.location.hash;
        if (!hash) return;
        var target = document.getElementById(hash.slice(1));
        if (!target) return;
        setTimeout(function () {
            target.scrollIntoView({behavior: 'smooth', block: 'start'});
        }, 80);
    })();
    </script>
</body>
</html>