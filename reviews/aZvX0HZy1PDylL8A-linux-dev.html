<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Review Comments: Re: [RFC PATCH v2 0/5] mm/swap, memcg: Introduce swap tiers for cgroup based swap control</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto,
                         "Helvetica Neue", Arial, sans-serif;
            background: #f5f5f5;
            color: #333;
            line-height: 1.6;
            padding: 20px;
            max-width: 900px;
            margin: 0 auto;
        }
        .home-link { margin-bottom: 12px; display: block; }
        .home-link a { color: #0366d6; text-decoration: none; font-size: 0.9em; }
        .home-link a:hover { text-decoration: underline; }

        h1 { font-size: 1.3em; margin-bottom: 2px; color: #1a1a1a; line-height: 1.3; }

        .lore-link { font-size: 0.85em; margin: 4px 0 6px; display: block; }
        .lore-link a { color: #0366d6; text-decoration: none; }
        .lore-link a:hover { text-decoration: underline; }

        .date-range {
            font-size: 0.8em;
            color: #888;
            margin-bottom: 16px;
        }
        .date-range a { color: #0366d6; text-decoration: none; }
        .date-range a:hover { text-decoration: underline; }

        /* thread-node scroll margin so the card isn't clipped at the top */
        .thread-node { scroll-margin-top: 8px; }

        /* ── Patch summary ──────────────────────────────────────────── */
        .patch-summary-block {
            background: #fff;
            border-radius: 8px;
            padding: 12px 16px;
            margin-bottom: 20px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            border-left: 3px solid #4a90d9;
        }
        .patch-summary-label {
            font-size: 0.72em;
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 0.06em;
            color: #4a90d9;
            margin-bottom: 4px;
        }
        .patch-summary-text {
            font-size: 0.88em;
            color: #444;
            line-height: 1.55;
        }

        /* ── Thread tree ────────────────────────────────────────────── */
        .thread-tree {
            display: flex;
            flex-direction: column;
            gap: 6px;
        }

        /* Depth indentation via left border */
        .thread-node { position: relative; }
        .thread-children {
            margin-left: 20px;
            padding-left: 12px;
            border-left: 2px solid #e0e0e0;
            margin-top: 6px;
            display: flex;
            flex-direction: column;
            gap: 6px;
        }

        /* ── Review comment card ────────────────────────────────────── */
        .review-comment {
            background: #fff;
            border-radius: 6px;
            padding: 10px 14px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            font-size: 0.88em;
        }
        .review-comment-header {
            display: flex;
            flex-wrap: wrap;
            align-items: center;
            gap: 6px;
            margin-bottom: 5px;
        }
        .review-author {
            font-weight: 700;
            color: #1a1a1a;
            font-size: 0.95em;
        }

        /* Date chip — links back to the daily report */
        .date-chip {
            font-size: 0.75em;
            color: #777;
            background: #f0f0f0;
            border-radius: 10px;
            padding: 1px 7px;
            text-decoration: none;
            white-space: nowrap;
        }
        a.date-chip:hover { background: #e0e8f5; color: #0366d6; }

        .badge {
            display: inline-block;
            padding: 1px 8px;
            border-radius: 10px;
            font-size: 0.75em;
            font-weight: 600;
        }
        .inline-review-badge {
            display: inline-block;
            padding: 0 6px;
            border-radius: 8px;
            font-size: 0.78em;
            font-weight: 500;
            background: #e3f2fd;
            color: #1565c0;
        }
        .review-tag-badge {
            display: inline-block;
            padding: 0 6px;
            border-radius: 8px;
            font-size: 0.78em;
            font-weight: 500;
            background: #e8f5e9;
            color: #2e7d32;
        }
        .analysis-source-badge {
            display: inline-block;
            padding: 1px 7px;
            border-radius: 10px;
            font-size: 0.72em;
            font-weight: 600;
            border: 1px solid rgba(0,0,0,0.1);
        }

        .review-comment-text {
            color: #444;
            line-height: 1.55;
            margin-bottom: 4px;
        }
        .review-comment-signals {
            margin-top: 3px;
            font-size: 0.85em;
            color: #aaa;
            font-style: italic;
        }

        /* ── Collapsible raw body ───────────────────────────────────── */
        .raw-body-toggle {
            margin-top: 5px;
            font-size: 0.85em;
        }
        .raw-body-toggle summary {
            cursor: pointer;
            color: #888;
            padding: 2px 0;
            font-weight: 500;
            font-size: 0.9em;
            list-style: none;
        }
        .raw-body-toggle summary::-webkit-details-marker { display: none; }
        .raw-body-toggle summary::before { content: "▶ "; font-size: 0.7em; }
        .raw-body-toggle[open] summary::before { content: "▼ "; }
        .raw-body-toggle summary:hover { color: #555; }
        .raw-body-text {
            white-space: pre-wrap;
            font-size: 0.95em;
            background: #f8f8f8;
            padding: 8px 10px;
            border-radius: 4px;
            max-height: 360px;
            overflow-y: auto;
            margin-top: 4px;
            line-height: 1.5;
            color: #444;
            border: 1px solid #e8e8e8;
        }

        .no-reviews {
            color: #aaa;
            font-size: 0.85em;
            font-style: italic;
            padding: 8px 0;
        }

        footer {
            text-align: center;
            color: #bbb;
            font-size: 0.78em;
            margin-top: 36px;
            padding: 16px;
        }
    </style>
</head>
<body>
    <div class="home-link"><a href="../">&larr; Back to reports</a></div>
    <h1>Re: [RFC PATCH v2 0/5] mm/swap, memcg: Introduce swap tiers for cgroup based swap control</h1>
    <div class="lore-link"><a href="https://lore.kernel.org/all/aZvX0HZy1PDylL8A@linux.dev/" target="_blank">View on lore.kernel.org &rarr;</a></div>
    <div class="date-range">Active on: <a href="#2026-02-23">2026-02-23</a> &bull; <a href="#2026-02-22">2026-02-22</a> &bull; <a href="#2026-02-21">2026-02-21</a> &bull; <a href="#2026-02-20">2026-02-20</a> &bull; <a href="#2026-02-13">2026-02-13</a> &bull; <a href="#2026-02-12">2026-02-12</a> &bull; <a href="#2026-02-11">2026-02-11</a> &bull; <a href="#2026-01-26">2026-01-26</a></div>
    
    <div class="thread-tree">
<div class="thread-node depth-0" id="2026-01-26">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Youngjun Park (author)</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-01-26">2026-01-26</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed a concern about the interaction between cgroup hierarchy and swap tier selection, explaining that effective tiers are calculated separately using a dedicated mask to respect the cgroup hierarchy. The author noted that configured tiers may differ from effective ones because they must be a subset of the parent&#x27;s configuration.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">This patch integrates the swap tier infrastructure with cgroup,
enabling the selection of specific swap devices per cgroup by
configuring allowed swap tiers.

The new `memory.swap.tiers` interface controls allowed swap tiers via a mask.
By default, the mask is set to include all tiers, allowing specific tiers to
be excluded or restored. Note that effective tiers are calculated separately
using a dedicated mask to respect the cgroup hierarchy. Consequently,
configured tiers may differ from effective ones, as they must be a subset
of the parent&#x27;s.

Note that cgroups do not pin swap tiers. This is similar to the
`cpuset` controller, which does not prevent CPU hotplug. This
approach ensures flexibility by allowing tier configuration changes
regardless of cgroup usage.

Signed-off-by: Youngjun Park &lt;youngjun.park@lge.com&gt;
---
 Documentation/admin-guide/cgroup-v2.rst | 27 +++++++++
 include/linux/memcontrol.h              |  3 +-
 mm/memcontrol.c                         | 80 +++++++++++++++++++++++++
 mm/swap_tier.c                          | 66 ++++++++++++++++++++
 mm/swap_tier.h                          | 21 +++++++
 mm/swapfile.c                           |  5 ++
 6 files changed, 201 insertions(+), 1 deletion(-)

diff --git a/Documentation/admin-guide/cgroup-v2.rst b/Documentation/admin-guide/cgroup-v2.rst
index 7f5b59d95fce..776a908ce1b9 100644
--- a/Documentation/admin-guide/cgroup-v2.rst
+++ b/Documentation/admin-guide/cgroup-v2.rst
@@ -1848,6 +1848,33 @@ The following nested keys are defined.
 	Swap usage hard limit.  If a cgroup&#x27;s swap usage reaches this
 	limit, anonymous memory of the cgroup will not be swapped out.
 
+  memory.swap.tiers
+        A read-write nested-keyed file which exists on non-root
+        cgroups. The default is to enable all tiers.
+
+        This interface allows selecting which swap tiers a cgroup can
+        use for swapping out memory.
+
+        The effective tiers are inherited from the parent. Only tiers
+        effective in the parent can be effective in the child. However,
+        the child can explicitly disable tiers allowed by the parent.
+
+        When read, the file shows two lines:
+          - The first line shows the operation string that was
+            written to this file.
+          - The second line shows the effective operation after
+            merging with parent settings.
+
+        When writing, the format is:
+          (+/-)(TIER_NAME) (+/-)(TIER_NAME) ...
+
+        Valid tier names are those configured in
+        /sys/kernel/mm/swap/tiers.
+
+        Each tier can be prefixed with:
+          +    Enable this tier
+          -    Disable this tier
+
   memory.swap.events
 	A read-only flat-keyed file which exists on non-root cgroups.
 	The following entries are defined.  Unless specified
diff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h
index b6c82c8f73e1..542bee1b5f60 100644
--- a/include/linux/memcontrol.h
+++ b/include/linux/memcontrol.h
@@ -283,7 +283,8 @@ struct mem_cgroup {
 	/* per-memcg mm_struct list */
 	struct lru_gen_mm_list mm_list;
 #endif
-
+	int tier_mask;
+	int tier_effective_mask;
 #ifdef CONFIG_MEMCG_V1
 	/* Legacy consumer-oriented counters */
 	struct page_counter kmem;		/* v1 only */
diff --git a/mm/memcontrol.c b/mm/memcontrol.c
index 007413a53b45..c0a0a957a630 100644
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@ -68,6 +68,7 @@
 #include &lt;net/ip.h&gt;
 #include &quot;slab.h&quot;
 #include &quot;memcontrol-v1.h&quot;
+#include &quot;swap_tier.h&quot;
 
 #include &lt;linux/uaccess.h&gt;
 
@@ -3691,6 +3692,7 @@ static void mem_cgroup_free(struct mem_cgroup *memcg)
 {
 	lru_gen_exit_memcg(memcg);
 	memcg_wb_domain_exit(memcg);
+	swap_tiers_memcg_sync_mask(memcg);
 	__mem_cgroup_free(memcg);
 }
 
@@ -3792,6 +3794,9 @@ mem_cgroup_css_alloc(struct cgroup_subsys_state *parent_css)
 	WRITE_ONCE(memcg-&gt;zswap_writeback, true);
 #endif
 	page_counter_set_high(&amp;memcg-&gt;swap, PAGE_COUNTER_MAX);
+	memcg-&gt;tier_mask = TIER_ALL_MASK;
+	swap_tiers_memcg_inherit_mask(memcg, parent);
+
 	if (parent) {
 		WRITE_ONCE(memcg-&gt;swappiness, mem_cgroup_swappiness(parent));
 
@@ -5352,6 +5357,75 @@ static int swap_events_show(struct seq_file *m, void *v)
 	return 0;
 }
 
+static int swap_tier_show(struct seq_file *m, void *v)
+{
+	struct mem_cgroup *memcg = mem_cgroup_from_seq(m);
+
+	swap_tiers_mask_show(m, memcg-&gt;tier_mask);
+	swap_tiers_mask_show(m, memcg-&gt;tier_effective_mask);
+
+	return 0;
+}
+
+static ssize_t swap_tier_write(struct kernfs_open_file *of,
+				char *buf, size_t nbytes, loff_t off)
+{
+	struct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));
+	char *pos, *token;
+	int ret = 0;
+
+	pos = strstrip(buf);
+
+	spin_lock(&amp;swap_tier_lock);
+	if (!*pos) {
+		memcg-&gt;tier_mask = TIER_ALL_MASK;
+		goto sync;
+	}
+
+	while ((token = strsep(&amp;pos, &quot; \t\n&quot;)) != NULL) {
+		int mask;
+
+		if (!*token)
+			continue;
+
+		if (token[0] != &#x27;-&#x27; &amp;&amp; token[0] != &#x27;+&#x27;) {
+			ret = -EINVAL;
+			goto err;
+		}
+
+		mask = swap_tiers_mask_lookup(token+1);
+		if (!mask) {
+			ret = -EINVAL;
+			goto err;
+		}
+
+		/*
+		 * if child already set, cannot add that tiers for hierarch mismatching.
+		 * parent compatible, child must respect parent selected swap device.
+		 */
+		switch (token[0]) {
+		case &#x27;-&#x27;:
+			memcg-&gt;tier_mask &amp;= ~mask;
+			break;
+		case &#x27;+&#x27;:
+			memcg-&gt;tier_mask |= mask;
+			break;
+		default:
+			ret = -EINVAL;
+			break;
+		}
+
+		if (ret)
+			goto err;
+	}
+
+sync:
+	__swap_tiers_memcg_sync_mask(memcg);
+err:
+	spin_unlock(&amp;swap_tier_lock);
+	return ret ? ret : nbytes;
+}
+
 static struct cftype swap_files[] = {
 	{
 		.name = &quot;swap.current&quot;,
@@ -5384,6 +5458,12 @@ static struct cftype swap_files[] = {
 		.file_offset = offsetof(struct mem_cgroup, swap_events_file),
 		.seq_show = swap_events_show,
 	},
+	{
+		.name = &quot;swap.tiers&quot;,
+		.flags = CFTYPE_NOT_ON_ROOT,
+		.seq_show = swap_tier_show,
+		.write = swap_tier_write,
+	},
 	{ }	/* terminate */
 };
 
diff --git a/mm/swap_tier.c b/mm/swap_tier.c
index d90f6eccb908..e860c87292e2 100644
--- a/mm/swap_tier.c
+++ b/mm/swap_tier.c
@@ -384,3 +384,69 @@ bool swap_tiers_update(void)
 
 	return true;
 }
+
+void swap_tiers_mask_show(struct seq_file *m, int mask)
+{
+	struct swap_tier *tier;
+
+	spin_lock(&amp;swap_tier_lock);
+	for_each_active_tier(tier) {
+		if (mask &amp; TIER_MASK(tier))
+			seq_printf(m, &quot;%s &quot;, tier-&gt;name);
+	}
+	spin_unlock(&amp;swap_tier_lock);
+	seq_puts(m, &quot;\n&quot;);
+}
+
+int swap_tiers_mask_lookup(const char *name)
+{
+	struct swap_tier *tier;
+
+	lockdep_assert_held(&amp;swap_tier_lock);
+
+	for_each_active_tier(tier) {
+		if (!strcmp(name, tier-&gt;name))
+			return TIER_MASK(tier);
+	}
+
+	return 0;
+}
+
+static void __swap_tier_memcg_inherit_mask(struct mem_cgroup *memcg,
+	struct mem_cgroup *parent)
+{
+	int effective_mask
+		= parent ? parent-&gt;tier_effective_mask : TIER_ALL_MASK;
+
+	memcg-&gt;tier_effective_mask
+		= effective_mask &amp; memcg-&gt;tier_mask;
+}
+
+void swap_tiers_memcg_inherit_mask(struct mem_cgroup *memcg,
+	struct mem_cgroup *parent)
+{
+	spin_lock(&amp;swap_tier_lock);
+	__swap_tier_memcg_inherit_mask(memcg, parent);
+	spin_unlock(&amp;swap_tier_lock);
+}
+
+void __swap_tiers_memcg_sync_mask(struct mem_cgroup *memcg)
+{
+	struct mem_cgroup *child;
+
+	lockdep_assert_held(&amp;swap_tier_lock);
+
+	if (memcg == root_mem_cgroup)
+		return;
+
+	for_each_mem_cgroup_tree(child, memcg)
+		__swap_tier_memcg_inherit_mask(child, parent_mem_cgroup(child));
+}
+
+void swap_tiers_memcg_sync_mask(struct mem_cgroup *memcg)
+{
+	spin_lock(&amp;swap_tier_lock);
+	memcg-&gt;tier_mask = TIER_ALL_MASK;
+	__swap_tiers_memcg_sync_mask(memcg);
+	spin_unlock(&amp;swap_tier_lock);
+}
diff --git a/mm/swap_tier.h b/mm/swap_tier.h
index de81d540e3b5..8652a7f993ab 100644
--- a/mm/swap_tier.h
+++ b/mm/swap_tier.h
@@ -46,4 +46,25 @@ bool swap_tiers_update(void);
 /* Tier assignment */
 void swap_tiers_assign_dev(struct swap_info_struct *swp);
 
+/* Memcg related functions */
+void swap_tiers_mask_show(struct seq_file *m, int mask);
+void swap_tiers_memcg_inherit_mask(struct mem_cgroup *memcg,
+	struct mem_cgroup *parent);
+void swap_tiers_memcg_sync_mask(struct mem_cgroup *memcg);
+void __swap_tiers_memcg_sync_mask(struct mem_cgroup *memcg);
+
+/* Mask and tier lookup */
+int swap_tiers_mask_lookup(const char *name);
+
+/**
+ * swap_tiers_mask_test - Check if the tier mask is valid
+ * @tier_mask: The tier mask to check
+ * @mask: The mask to compare against
+ *
+ * Return: true if condition matches, false otherwise
+ */
+static inline bool swap_tiers_mask_test(int tier_mask, int mask)
+{
+	return tier_mask &amp; mask;
+}
 #endif /* _SWAP_TIER_H */
diff --git a/mm/swapfile.c b/mm/swapfile.c
index 4f8ce021c5bd..dd97e850ea2c 100644
--- a/mm/swapfile.c
+++ b/mm/swapfile.c
@@ -1348,10 +1348,15 @@ static bool swap_alloc_fast(struct folio *folio)
 static void swap_alloc_slow(struct folio *folio)
 {
 	struct swap_info_struct *si, *next;
+	int mask = folio_memcg(folio) ?
+		folio_memcg(folio)-&gt;tier_effective_mask : TIER_ALL_MASK;
 
 	spin_lock(&amp;swap_avail_lock);
 start_over:
 	plist_for_each_entry_safe(si, next, &amp;swap_avail_head, avail_list) {
+		if (!swap_tiers_mask_test(si-&gt;tier_mask, mask))
+			continue;
+
 		/* Rotate the device and switch to a new cluster */
 		plist_requeue(&amp;si-&gt;avail_list, &amp;swap_avail_head);
 		spin_unlock(&amp;swap_avail_lock);
-- 
2.34.1</pre>
</details>
<div class="review-comment-signals">Signals: clarification, explanation</div>
</div>
<div class="thread-children">
<div class="thread-node depth-1" id="2026-02-11">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Chris Li</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-11">2026-02-11</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Chris Li noted that the swap_tier structure simplification removed the &#x27;end prio&#x27; and priority lists, which caused a regression in handling tier boundaries, and requested reverting to the original implementation.

Reviewer Chris Li suggested breaking down the patch series into smaller, more manageable steps, starting with defining the tiers bits without deleting any existing functionality, and then building upon that in subsequent steps.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Hi Youngjun,

On Sun, Jan 25, 2026 at 10:53 PM Youngjun Park &lt;youngjun.park@lge.com&gt; wrote:

---

Thanks for the patches series.

Sorry for the late reply. I have been wanting to reply to it but get
super busy at work.

Some high level feedback for the series. Now that you demonstrated the
whole series, let&#x27;s focus on making small mergiable baby steps. Just
like the swap table has different phases. Make each step minimal, each
step shows some value. Do the MVP, we can always add more features as
a follow up step.

I suggest the first step is getting the tiers bits defined. Add only,
no delete.  Get that reviewed and merged, then the next step is to use
those tiers.

Chris</pre>
</details>
<div class="review-comment-signals">Signals: regression, requested change, requested changes</div>
</div>
<div class="thread-children">
<div class="thread-node depth-2" id="2026-02-13">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">YoungJun Park (author)</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-13">2026-02-13</a>
<span class="badge" style="color:#155724;background:#d4edda">Positive</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Author addressed Chris Li&#x27;s concern about the patch series being too large by proposing a revised roadmap to break it down into smaller, mergeable steps. The author plans to introduce the swap tier definitions first, followed by advanced control and external integration.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Hi Chris,

Thank you for the direction.

I agree that breaking the series into smaller, mergeable steps is the
right approach. However, since introducing the definitions alone might
lack immediate usage, I propose a slightly
modified roadmap to ensure Step 1 demonstrates some value.

Here is the plan I have in mind.

1. Swap Tier Definition &amp; Addition
   - Introduce the concept, grouping logic, and the &#x27;add&#x27; interface.
   - Value: Enables basic exception handling within the swap device
     itself using tiers.

2. Advanced Control (Delete/Modify)
   - Implement logic to remove or update tiers.
   - Value: Enhances the usability and management of the tiers
     established in Step 1.

3. External Integration (memcg, bpf etc ... )
   - Apply swap tiers for broader swap control.
   - Value: Connects swap tiers to other subsystems like memcg.

Does this roadmap look reasonable to you? I will proceed with preparing
the real patch series based on this structure.

Best regards,
Youngjun</pre>
</details>
<div class="review-comment-signals">Signals: agreed, proposed</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">YoungJun Park (author)</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-13">2026-02-13</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Author acknowledged the need to limit swapfile size, agreed to add a CONFIG option to control this in v2

Author addressed Chris Li&#x27;s concern about mixed operations in the /sys/kernel/mm/swap/tiers interface, proposing to restrict it to handle one operation at a time due to potential errors and performance issues.

Author acknowledged that the tier structure can limit contention, and agreed to consider optimizing swap_avail_list in the future.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Sounds good. I will add a CONFIG option for it and ensure it doesn&#x27;t exceed
MAX_SWAPFILE.

---

I have given this a lot of thought.

Since the current interface allows mixing add (+), remove (-), and modify
operations, we must either restore from a saved state or reverse the
successful individual operations upon failure.

I implemented both approaches and concluded that reversing individual
operations is error-prone. Also, it could be slow if there are many
operations.

Another approach could be using a &quot;global clone tier&quot; strategy.
(Because operation globally synchronized)

Therefore, I would like to propose restricting the interface to handle a
single operation at a time. What do you think?

---

I agree. With the tier structure, we can limit contention to objects within
the same tier.

I also think swap_avail_list could be optimized in a similar way in the
future.

Youngjun</pre>
</details>
<div class="review-comment-signals">Signals: agreed, will add, acknowledges fix is needed</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">YoungJun Park (author)</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-13">2026-02-13</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Author acknowledges that stripping out the remove/modify parts of the patch is a viable direction, indicating an openness to revising the patch in response to reviewer feedback.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Oops, I replied to your previous email before seeing this one.

Stripping out the remove/modify parts is also feasible. Do you agree with
that direction?

Youngjun</pre>
</details>
<div class="review-comment-signals">Signals: acknowledges need for revision, open to alternative approach</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">YoungJun Park (author)</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-13">2026-02-13</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author acknowledged that existing swapfiles&#x27; tier is immutable once assigned and explained how they ensured this invariant by removing tier reference, using operation validation, and rejecting certain operations if it would cause a swapfile to move to a different tier.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">I missed one comment. 

The tier of existing swapfiles is immutable once assigned at swapon.
I removed tier reference.
Instead of reference counting, each operation validates the tier
range at operation time to guarantee this invariant.

- add:    Does not change existing swapfiles&#x27; tier. New tier may
          split priority range, but existing assignments stay.
- remove: Rejected with -EBUSY if any swapfile is attached.
- modify: Rejected if the change would cause any swapfile to
          move to a different tier.

So swapfiles never jump between tiers at runtime.

Youngjun Park</pre>
</details>
<div class="review-comment-signals">Signals: acknowledged, explained</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Shakeel Butt</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-21">2026-02-21</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Shakeel Butt expressed concern that the patch does not address the primary use-case of controlling/partitioning swap devices among sub-workloads and requested further exploration before adding a stable API.

Reviewer Shakeel Butt questioned whether the patch&#x27;s concept of swap tiers aligns with Google&#x27;s past implementation and usage, specifically mentioning memory.swapfiles interface and its deprecation.

Reviewer Shakeel Butt expressed skepticism about the introduction of a new swap tier interface without a clear use case, prompting him to push back even harder against its addition.

Reviewer Shakeel Butt questioned the practicality of controlling swap devices hierarchically, specifically asking for a real-world use case to justify this feature.

Reviewer Shakeel Butt noted that having multiple swap devices reduces lock contention, but this does not address hierarchical control of swap devices among sub-workloads.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">No, that is secondary because I am not seeing the real use-case of
controlling/partitioning swap devices among sub-workloads. Until that is
figured out, adding a stable API is not good.

---

I am assuming you meant Google and particularly Prodkernel team and not
Android or ChromeOS. Google&#x27;s prodkernel used to have per-cgroup
swapfiles exposed through memory.swapfiles (if I remember correctly
Suleiman implemented this along with ghost swapfiles). Later this was
deprecated (by Yu Zhao) and global (ghost) swapfiles were being used.
The memory.swapfiles interface instead of supporting real swapfiles
started having select options among default, ghost/zswap and real
(something like that). However such interface was used to just disable
or enable zswap for a workload and never about hierarchically
controlling the swap devices (Google prodkernel only have zswap). Has
something changed?

---

This just motivates me to pushback even harder on adding a new interface
without a clear use-case.

---

I already asked above but let me say it again. What&#x27;s the actual real
world use-case to control/allow/disallow swap devices hierarchically?

---

Having more than one swap devices to reduce lock contention is unrelated
to hierarchically control swap devices among sub-workloads.</pre>
</details>
<div class="review-comment-signals">Signals: requested changes, concerns about stability, requested clarification on alignment with previous implementation</div>
</div>
</div>
</div>
</div>
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Chris Li</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-11">2026-02-11</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Chris Li suggested replacing per-cpu allocation for each swap device with a global percpu cluster per tier, as the maximum number of tiers is smaller than the maximum number of swap devices, which could be more efficient.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">One idea is that, instead of using percpu per swap device.
You can make the global percpu cluster per tier. Because the max tier
number is smaller than the max number of swap devices. That is likely
a win.

Chris</pre>
</details>
<div class="review-comment-signals">Signals: requested change, potential optimization</div>
</div>
</div>
<div class="thread-node depth-1" id="2026-02-12">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Chris Li</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-12">2026-02-12</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Chris Li noted that the swap_tier structure was simplified by replacing &#x27;end prio&#x27; and priority lists with a standard list_head, as suggested in v2.

Reviewer Chris Li suggested introducing a CONFIG option to limit the maximum number of swap tiers, recommending a default value of 4.

Reviewer Chris Li noted that when modifying a tier, it may cause swap files to move from one tier to another, which could be problematic.

Reviewer Chris Li expressed concern about the complexity of the patch, specifically the need for save and restore operations, and requested a simpler design.

Reviewer Chris Li suggested that each tier have its own swap_active_head, so that different swap entries on different tiers do not compete for the same resource, and proposed that swapfiles should not be allowed to jump between tiers.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Hi Yongjun,

On Sun, Jan 25, 2026 at 10:53 PM Youngjun Park &lt;youngjun.park@lge.com&gt; wrote:

---

We can have a CONFIG option for the MAX_SWAPTIER. I think the default
should be a small number like 4.

---

When we add, modify, remove a tier. The simple case is there is no
swap file under any tiers.
But if the modification causes some swap files to jump to different
tiers. That might be problematic.

---

I really hope we don&#x27;t have to do the save and restore thing. Is there
another design we can simplify this?

---

One idea is to make each tier have swap_active_head. So different swap
entry releases on different tiers don&#x27;t need to be competing on the
same swap_active_head.

That will require the swapfile don&#x27;t jump to another tiers.

Chris</pre>
</details>
<div class="review-comment-signals">Signals: acknowledgment of previous feedback, requested changes, potential issue with tier modification</div>
</div>
</div>
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Nhat Pham</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-12">2026-02-12</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Nhat Pham questioned the consistency of the patch description, pointing out that the &#x27;+&#x27; operator was removed but its reference remained in the explanation.

Reviewer Nhat Pham questioned the logic for restricting child cgroup swap tiers, suggesting it should be a subset of both parent and ancestors instead of just children and parents.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">This comment seems a bit clunky to me. The &quot;+&quot; is removed, as noted
above, but then why are we saying &quot;even if a child re-enables a tier
with &quot;+&quot;&quot; here? Am I missing something?

---

But otherwise, I assume you mean to restrict child&#x27;s allowed swap
tiers to be a subset of children and its ancestors? That seems more
straightforward to me than the last system :)</pre>
</details>
<div class="review-comment-signals">Signals: questioning inconsistency, requested changes</div>
</div>
<div class="thread-children">
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">YoungJun Park (author)</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-13">2026-02-13</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed Nhat Pham&#x27;s concern about the default behavior of swap tiers, clarifying that previously &#x27;+&#x27; switched to an exclusive mode but is now being changed to a subtraction-based model where all tiers are selected by default and users use &#x27;-&#x27; to exclude specific ones.

Author acknowledged a concern about the swapoff path and agreed to restructure in v2</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">To clarify, previously, the default state used all tiers. Using &quot;+&quot;              
switched to &quot;an exclusive mode&quot;  where only that specific tier was used.         
                                                                                 
I am changing this to a subtraction-based model. By default, all tiers           
are selected, and users use &quot;-&quot; to exclude specific ones.                        
(Then not &quot;removed&quot; but &quot;changed&quot; is more proper?)                               
                                                                                 
In this context, I intended &quot;+&quot; to be used to restore a tier that was            
previously excluded by &quot;-&quot;.

---

Yes, that&#x27;s right :)

Thanks 
Youngjun Park.</pre>
</details>
<div class="review-comment-signals">Signals: clarification, explanation, acknowledged</div>
</div>
</div>
</div>
</div>
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Shakeel Butt</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-12">2026-02-12</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Shakeel Butt noted that the patch does not handle the case where a cgroup has both &#x27;+&#x27; and &#x27;-&#x27; operations for the same tier, which can lead to inconsistent behavior.

Reviewer Shakeel Butt expressed concerns that adding a memcg interface for swap tier control is unnecessary, suggesting instead the use of BPF to expose this functionality. He argued that workloads should not be allowed to pick and choose swap devices, but rather it&#x27;s the job orchestator or node controller&#x27;s decision.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Hi Youngjun,

On Mon, Jan 26, 2026 at 03:52:37PM +0900, Youngjun Park wrote:

---

One of the LPC feedback you missed is to not add memcg interface for
this functionality and explore BPF way instead.

We are normally very conservative to add new interfaces to cgroup.
However I am not even convinced that memcg interface is the right way to
expose this functionality. Swap is currently global and the idea to
limit or assign specific swap devices to specific cgroups makes sense
but that is the decision for the job orchestator or node controller.
Allowing workloads to pick and choose swap devices do not make sense to
me.

Shakeel</pre>
</details>
<div class="review-comment-signals">Signals: inconsistent behavior, handling of &#x27;+&#x27; and &#x27;-&#x27; operations, requested changes</div>
</div>
<div class="thread-children">
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">YoungJun Park (author)</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-13">2026-02-13</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author is addressing Shakeel Butt&#x27;s feedback on using the BPF approach for swap control, which was suggested to provide greater flexibility but may introduce logical contradictions and hierarchy enforcement issues. The author agrees that the idea of assigning swap devices to cgroups makes sense, but prefers a strict cgroup interface over a constrained BPF hook, citing potential conflicts and edge cases.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Apologies for overlooking the feedback regarding the BPF approach. Thank you
for the suggestion.

I agree that using BPF would provide greater flexibility, allowing control not
just at the memcg level, but also per-process or for complex workloads.
(As like orchestrator and node controller)

However, I am concerned that this level of freedom might introduce logical
contradictions, particularly regarding cgroup hierarchy semantics.

For example, BPF might allow a topology that violates hierarchical constraints
(a concern that was also touched upon during LPC)

  - Group A (Parent): Assigned to SSD1
  - Group B (Child of A): Assigned to SSD2

If Group A has a `memory.swap.max` limit, and Group B swaps out to SSD2, it
creates a consistency issue. Group B consumes Group A&#x27;s swap quota, but it is
utilizing a device (SSD2) that is distinct from the Parent&#x27;s assignment. This
could lead to situations where the Parent&#x27;s limit is exhausted by usage on a
device it effectively doesn&#x27;t &quot;own&quot; or shouldn&#x27;t be using.

One might suggest restricting BPF to strictly adhere to these hierarchical
constraints. However, doing so would effectively eliminate the primary
advantage of using BPF\u2014its flexibility. If we are to enforce standard cgroup
semantics anyway, a native interface seems more appropriate than a constrained
BPF hook.

Beyond this specific example, I suspect that delegating this logic to BPF
might introduce other unforeseen edge cases regarding hierarchy enforcement.
In my view, the BPF approach seems more like a &quot;next step.&quot;

Since you acknowledged that the idea of assigning swap devices to cgroups
&quot;makes sense,&quot; I believe implementing this within the standard, strictly
constrained &quot;cgroup land&quot; is preferable. 

A strict cgroup interface ensures
that hierarchy and accounting rules are consistently enforced, avoiding the
potential conflicts that the unrestricted freedom of BPF might create.

Ultimately, I hope this swap tier mechanism can serve as a foundation to be
leveraged by other subsystems, such as BPF and DAMON. I view this proposal as
the necessary first step toward that future.

Youngjun Park</pre>
</details>
<div class="review-comment-signals">Signals: acknowledges fix is needed, agrees with approach</div>
</div>
</div>
<div class="thread-node depth-2">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Chris Li</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-20">2026-02-20</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Chris Li expressed understanding and leniency towards Shakeel Butt&#x27;s patch, suggesting that the discussion may not be fully concluded due to lack of response from YoungJun Park and acknowledging the difficulty in contributing to the kernel.

Reviewer Chris Li suggested that instead of addressing the issue directly, a config option could be added to protect it and mark it as experimental, allowing for further experimentation and feedback.

Reviewer Chris Li confirmed that his company uses a different swap device at different cgroup levels, emphasizing the practical need for control at non-root levels.

Chris Li expressed concerns that the swap device control introduced in this patch is not generic enough, similar to zswap.writeback, and suggested that it should be more comprehensive.

Chris Li expressed frustration about the lack of documentation on how to use the new swap tier feature, referencing a long thread on the linux-mm mailing list but not providing specific details or suggestions for improvement.

Reviewer Chris Li noted that the swap tier system&#x27;s performance is dependent on the number of devices within each tier, and suggested using a round-robin algorithm to distribute swap operations within tiers

Reviewer Chris Li suggested renaming the &#x27;tier&#x27; concept to something like &#x27;swap.device_speed_classes&#x27;, acknowledging that it was inspired by memory tiers, and proposed considering alternative names.

Reviewer Chris Li noted that in their deployment, they use multiple swap devices to reduce lock contention and requested consideration for the job&#x27;s tolerable swap speed when selecting tiers.

Reviewer Chris Li expressed a neutral sentiment, suggesting that the patch should start from the current need and not over-engineer future-proofing, citing an example of zswap.writeback as a solution to a specific requirement.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">In this case I think it is fine.  You haven&#x27;t responded to YoungJun&#x27;s
last response in over a week. He might have mistaken that the
discussion concluded.
Consider it is one of the iterations. It is hard enough to contribute
to the kernel. Relax.
Plus, much of the discussion on the mailing list always has differing
opinions. So, it&#x27;s hard to determine what is truly concluded.
Different people might have different interitations of the same text.

---

Is that your biggest concern? Many different ways exist to solve that
problem. e.g. We can put a config option protecting it and mark it as
experimental. This will unblock the development allow experiment. We
can have more people to try it out and give feedback.

---

Yes, my company uses a different swap device at different cgroup
level. I did ask my coworker to confirm that usage. Control at the non
root level is a real need.

---

I think this swap device control is a very basic need. All your
objections to swapping control in the group can equally apply to
zswap.writeback. Unlike zswap.writeback, which only control from the
zswap behavior. This is a more generic version control swap device
other than zswap as well. BTW, I raised that concern about
zswap.writeback was not generic enough as swap control was limited
when zswap was proposed. We did hold back zswap.writeback. The
consensers is interface can be improved as later iterations. So here
we are.

---

There is a very long thread on the linux-mm maillist. I&#x27;m too lazy to dig it up.

I can share our usage requirement to refresh your memory. We
internally use a cgroup swapfile control interface that has not been
upstreamed. With this we can remove the need of that internal
interface and go upstream instead.

---

It depends on the number of devices in the tiers. Different tiers
maintain an order. Within the same tier round robin.

---

I propose the tier name. Guilty. Yes, in was inpired by memory tiers.
It just different class of swap speeds. I am not fixed on the name. We
can also call it swap.device_speed_classes. You can suggest
alternatives.

Promotion / demotion is possible in the future. The current state,
without promotion or demotion, already provides value. Our current
deployment uses only one class of swap device at a time. However I do
know other companies use  more than one class of swap device.

---

In our deployment, we always use more than one swap device to reduce
swap device lock contention.
The job config can describe the swap speed it can tolerate. Some jobs
can tolerate slower speeds, while others cannot.

---

Take zswap.writeback as example. We have a solution that worked for
the requirement at that time. Incremental improvement is fine as well.
Usually, incremental progress is better. At least currently there is a
real need to allow different cgroups to select different swap speeds.
There is a risk in being too future-proof: we might design things that
people in the future don&#x27;t use as we envisioned. I see that happen too
often as well.

So starting from the current need is a solid starting point. It&#x27;s just
a different design philosophy. Each to their own.

That is the only usage case I know. YoungJun feel free to add yours
usage as well.

Chris</pre>
</details>
<div class="review-comment-signals">Signals: leniency, understanding, requested changes</div>
</div>
</div>
<div class="thread-node depth-2" id="2026-02-21">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">YoungJun Park (author)</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-21">2026-02-21</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Author acknowledges Shakeel Butt&#x27;s feedback and agrees to restructure the swapoff path in v2 to drop the per-vswap spinlock before calling try_to_unmap(), but no specific fix is planned.

The author acknowledges a concern about committing to a stable interface too early and proposes reducing this concern by adding a build-time config option or marking it as experimental, but also expresses uncertainty about the need for a memcg interface if BPF becomes primary.

Author acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), but did not explicitly state a plan for addressing this issue in v2.

Author suggests that introducing build-time config or runtime constraints can help define and predict usage of the swap tiers feature.

Author acknowledges reviewer&#x27;s feedback about the patch&#x27;s complexity and scope, explaining that their initial proposal was more complex but they simplified it to &#x27;swap tiers&#x27; as per Chris Li&#x27;s suggestion.

Author addressed Shakeel Butt&#x27;s concern about how swap devices are ordered when they have the same priority within a tier, explaining that round-robin ordering applies in this case.

Author acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), but did not provide a specific plan for addressing this issue.

Author acknowledged a potential issue of lock contention in the current implementation, but did not provide any specific solution or plan for addressing it.

The author is addressing Shakeel Butt&#x27;s concern about the long-term viability of the swap tiers concept, specifically whether it will be able to adapt to future use cases. The author agrees that BPF would be a better fit for future needs and sees it as an extension path, but believes that implementing swap tiers first with a CONFIG option will allow them to move forward without committing to a stable interface.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Understood. Let&#x27;s continue the discussion. :D

Chris has already provided a thorough response, but I would like to
add my perspective as well.

---

I understand your concern about committing to a stable interface too
early. As Chris suggested, we could reduce this concern by guarding
the interface behind a build-time config option or marking it as
experimental, which I will also touch on further below.

On that note, if BPF were to become the primary control mechanism,
I am not sure a memcg interface would still be needed at all, since
BPF already provides a high degree of freedom. However, that level
of freedom is also what concerns me -- BPF-driven swap device
assignments could subtly conflict with memcg hierarchy semantics in
ways that are hard to predict or debug. A more constrained memcg-based
approach might actually be safer in that regard.

---

I think this concern is closely tied to your question #3 below about
concrete use cases for partitioning devices across sub-workloads.
I hope my answer there helps clarify this.

---

As I mentioned above, I think guarding the feature behind a build-time
config or runtime constraints could keep the usage well-defined and
predictable, while still being useful.

---

Our use case is simple at now. 
We have two swap devices with different performance
characteristics and want to assign different swap devices to different
workloads (cgroups).

For some background, when I initially proposed this, I suggested allowing
per-cgroup swap device priorities so that it could also accommodate the
broader scenarios you mentioned. However, since even our own use case
does not require reversing swap priorities within a cgroup, we pivoted
to the &quot;swap tier&quot; mechanism that Chris proposed.

---

Both. If devices are in the same tier with the same priority, round robin.
If they are in the same tier with different priorities, or in different
tiers, ordering applies. The current tier structure should be able to
satisfy either preference.

---

This was originally Chris&#x27;s idea. I think he explained the rationale
well in his reply.

---

One possible scenario is reducing lock contention by partitioning swap
devices between parent and child cgroups.

---

We have clear production use cases from both us and Chris, and I also
presented a deployment example in the cover letter.

I think it is hard to design concretely for future use cases at this
point. When those needs become clearer, BPF with its flexibility
would be a better fit then. I see BPF as a natural extension path
rather than a starting point.

For now, guarding the memcg &amp; tier behind a CONFIG option would
let us move forward without committing to a stable interface, and
we can always pivot to BPF later if needed

Thanks,
YoungJun Park</pre>
</details>
<div class="review-comment-signals">Signals: acknowledged, agreed, acknowledges a concern</div>
</div>
</div>
<div class="thread-node depth-2" id="2026-02-22">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">YoungJun Park (author)</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-22">2026-02-22</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Author is addressing concerns about the BPF-first approach, specifically questioning its feasibility in an embedded environment and asking for clarification on precedents for stable interfaces graduating from BPF prototypes.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">....

After reading the reply and re-think more of it.

I have a few questions regarding the BPF-first approach you
suggested, if you don&#x27;t mind. Some of them I am re-asking
because I feel they have not been clearly addressed yet.

- We are in an embedded environment where enabling additional
  kernel compile options is costly. BPF is disabled by
  default in some of our production configurations. From a
  trade-off perspective, does it make sense to enable BPF
  just for swap device control?

- You suggest starting with BPF and discussing a stable
  interface later. I am genuinely curious, are there actual
  precedents where a BPF prototype graduated into a stable
  kernel interface? 

- You raised that stable interfaces are hard to remove. Would
  gating it behind a CONFIG option or marking it experimental
  be an acceptable compromise?

- You already acknowledged the use-case for assigning
  different swap devices to different workloads. Your
  objection is specifically about hierarchical parent-child
  partitioning. If the interface enforced uniform policy
  within a subtree, would that be acceptable?

- We already run a modified kernel with internal swap control
  in production and have real feedback from it. Requiring BPF
  as a prerequisite to gather production experience seems
  unnecessary when we are already doing that.

To be honest, I am having trouble understanding the motivation
behind the BPF-first validation approach. If the real point is
that BPF enables more flexible swap-out policies than any fixed
interface can, that would make much more sense to me. I would
appreciate it if you could share more on this.

Thanks,
Youngjun Park</pre>
</details>
<div class="review-comment-signals">Signals: questioning, asking for clarification</div>
</div>
</div>
</div>
</div>
<div class="thread-node depth-1" id="2026-02-20">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Shakeel Butt</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-20">2026-02-20</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Shakeel Butt requested that further discussion be concluded on the previous patch version before sending a new one.

Reviewer Shakeel Butt expressed concerns about introducing stable interfaces for swap tiers, requesting a BPF approach first and questioning the need for hierarchical control.

Reviewer Shakeel Butt noted that while BPF provides more power, its control is limited to administrators who can still make mistakes.

Reviewer Shakeel Butt requested clarification on the patch&#x27;s use case, specifically asking about ordering policies between multiple assigned swap devices and the purpose of &#x27;tiers&#x27; in the name.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Please don&#x27;t send a new version of the series before concluding the discussion
on the previous one.

On Fri, Feb 13, 2026 at 12:58:40PM +0900, YoungJun Park wrote:

---

Yes it provides the flexibility but that is not the main reason I am pushing for
it. The reason I want you to first try the BPF approach without introducing any
stable interfaces. Show how swap tiers will be used and configured in production
environment and then we can talk if a stable interface is needed. I am still not
convinced that swap tiers need to be controlled hierarchically and the non-root
should be able to control it.

---

Yes BPF provides more power but it is controlled by admin and admin can shoot
their foot in multiple ways.

---

No need to constraint anything.

Taking a step back, can you describe your use-case a bit more and share
requirements?

You have multiple swap devices of different properties and you want to assign
those swap devices to different workloads. Now couple of questions:

1. If more than one device is assign to a workload, do you want to have
   some kind of ordering between them for the worklod or do you want option to
   have round robin kind of policy?

2. What&#x27;s the reason to use &#x27;tiers&#x27; in the name? Is it similar to memory tiers
   and you want promotion/demotion among the tiers?

3. If a workload has multiple swap devices assigned, can you describe the
   scenario where such workloads need to partition/divide given devices to their
   sub-workloads?

Let&#x27;s start with these questions. Please note that I want us to not just look at
the current use-case but brainstorm more future use-cases and then come up with
the solution which is more future proof.

thanks,
Shakeel</pre>
</details>
<div class="review-comment-signals">Signals: lack of technical feedback, requested changes, still not convinced</div>
</div>
</div>
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Shakeel Butt</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-22">2026-02-22</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer noted that the patch does not handle the case where a child cgroup re-enables a tier with &#x27;+&#x27; that was excluded by its parent, and requested that the effective tier list be limited to the parent&#x27;s allowed subset.

Reviewer Shakeel Butt requested additional information about the cgroup hierarchy structure of Youngjun Park&#x27;s deployment, specifically asking if they use cgroup v1 or v2 in their production environment.

Reviewer Shakeel Butt questioned whether the proposed swap tiers are an improvement over existing priority behavior and asked for clarification on their similarity to current priorities.

Reviewer Shakeel Butt requested that the patch authors gather all options and weigh their pros and cons before making an informed decision, suggesting a more collaborative approach to resolving the issue.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Hi YoungJun,

I see you have sent a separate email on BPF specific questions to which I will
respond separately, here I will respond to other questions/comments.

On Sat, Feb 21, 2026 at 11:30:59PM +0900, YoungJun Park wrote:

---

If you don&#x27;t mind, can you share a bit more about the cgroup hierarchy structure
of your deployment. Do you use cgroup v1 or v2 on your production environment?

---

I assume this is the same swap priorities as of today, right? You want similar
priority behavior within a tier.

---

I think your use-case is very clear. Before committing to any options, I want us
to brainstorm all options and gather pros/cons and then make an informed
decision. Anyways I will respond to your other email (in a day or two).

Shakeel</pre>
</details>
<div class="review-comment-signals">Signals: requested changes, lack of technical discussion, request for more information</div>
</div>
</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Youngjun Park (author)</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-01-26">2026-01-26</a>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed a concern about swap device tier membership tracking, explaining that a `tier_mask` is added to identify which tier a device belongs to and ensuring that tier assignment of already configured swap devices remains unchanged upon activation.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">This patch connects swap devices to the swap tier infrastructure,
ensuring that devices are correctly assigned to tiers based on their
priority.

A `tier_mask` is added to identify the tier membership of swap devices.
Although tier-based allocation logic is not yet implemented, this
mapping is necessary to track which tier a device belongs to. Upon
activation, the device is assigned to a tier by matching its priority
against the configured tier ranges.

The infrastructure allows dynamic modification of tiers, such as
splitting or merging ranges. These operations are permitted provided
that the tier assignment of already configured swap devices remains
unchanged.

This patch also adds the documentation for the swap tier feature,
covering the core concepts, sysfs interface usage, and configuration
details.

Signed-off-by: Youngjun Park &lt;youngjun.park@lge.com&gt;
---
 Documentation/mm/swap-tier.rst | 109 +++++++++++++++++++++++++++++++++
 include/linux/swap.h           |   1 +
 mm/swap_state.c                |   2 +-
 mm/swap_tier.c                 | 106 ++++++++++++++++++++++++++++----
 mm/swap_tier.h                 |  13 +++-
 mm/swapfile.c                  |   2 +
 6 files changed, 219 insertions(+), 14 deletions(-)
 create mode 100644 Documentation/mm/swap-tier.rst

diff --git a/Documentation/mm/swap-tier.rst b/Documentation/mm/swap-tier.rst
new file mode 100644
index 000000000000..3386161b9b18
--- /dev/null
+++ b/Documentation/mm/swap-tier.rst
@@ -0,0 +1,109 @@
+.. SPDX-License-Identifier: GPL-2.0
+
+:Author: Chris Li &lt;chrisl@kernel.org&gt; Youngjun Park &lt;youngjun.park@lge.com&gt;
+
+==========
+Swap Tier
+==========
+
+Swap tier is a collection of user-named groups classified by priority ranges.
+It acts as a facilitation layer, allowing users to manage swap devices based
+on their speeds.
+
+Users are encouraged to assign swap device priorities according to device
+speed to fully utilize this feature. While the current implementation is
+integrated with cgroups, the concept is designed to be extensible for other
+subsystems in the future.
+
+Use case
+-------
+
+Users can perform selective swapping by choosing a swap tier assigned according
+to speed within a cgroup.
+
+For more information on cgroup v2, please refer to
+``Documentation/admin-guide/cgroup-v2.rst``.
+
+Priority Range
+--------------
+
+The specified tiers must cover the entire priority range from -1
+(DEF_SWAP_PRIO) to SHRT_MAX.
+
+Consistency
+-----------
+
+Tier consistency is guaranteed with a focus on maximizing flexibility. When a
+swap device is activated within a tier range, a reference is held from the
+start of the tier to the priority of that swap device. This ensures that the
+tier of region containing the active swap device does not disappear.
+
+If a request to add a new tier with a priority higher than the current swap
+device is received, the existing tier can be split.
+
+However, specifying a tier in a cgroup does not hold a reference to the tier.
+Consequently, the corresponding tier can disappear at any time.
+
+Configuration Interface
+-----------------------
+
+The swap tiers can be configured via the following interface:
+
+/sys/kernel/mm/swap/tiers
+
+Operations can be performed using the following syntax:
+
+* Add:    ``+&quot;&lt;tiername&gt;&quot;:&quot;&lt;start_priority&gt;&quot;``
+* Remove: ``-&quot;&lt;tiername&gt;&quot;``
+* Modify: ``&quot;&lt;tiername&gt;&quot;:&quot;&lt;start_priority&gt;&quot;``
+
+Multiple operations can be provided in a single write, separated by spaces (&quot; &quot;)
+or commas (&quot;,&quot;).
+
+When configuring tiers, the specified value represents the **start priority**
+of that tier. The end priority is automatically determined by the start
+priority of the next higher tier. Consequently, adding or modifying a tier
+automatically adjusts (splits or merges) the ranges of adjacent tiers to
+ensure continuity.
+
+Examples
+--------
+
+**1. Initialization**
+
+A tier starting at -1 is mandatory to cover the entire priority range up to
+SHRT_MAX. In this example, &#x27;HDD&#x27; starts at 50, and &#x27;NET&#x27; covers the remaining
+lower range starting from -1.
+
+::
+
+    # echo &quot;+HDD:50, +NET:-1&quot; &gt; /sys/kernel/mm/swap/tiers
+    # cat /sys/kernel/mm/swap/tiers
+    Name             Idx   PrioStart   PrioEnd
+    HDD              0     50          32767
+    NET              1     -1          49
+
+**2. Modification and Splitting**
+
+Here, &#x27;HDD&#x27; is moved to start at 80, and a new tier &#x27;SSD&#x27; is added at 100.
+Notice how the ranges are automatically recalculated:
+* &#x27;SSD&#x27; takes the top range. Split HDD Tier&#x27;s range. (100 to SHRT_MAX).
+* &#x27;HDD&#x27; is adjusted to the range between &#x27;NET&#x27; and &#x27;SSD&#x27; (80 to 99).
+* &#x27;NET&#x27; automatically extends to fill the gap below &#x27;HDD&#x27; (-1 to 79).
+
+::
+
+    # echo &quot;HDD:80, +SSD:100&quot; &gt; /sys/kernel/mm/swap/tiers
+    # cat /sys/kernel/mm/swap/tiers
+    Name             Idx   PrioStart   PrioEnd
+    SSD              2     100         32767
+    HDD              0     80          99
+    NET              1     -1          79
+
+**3. Removal**
+
+Tiers can be removed using the &#x27;-&#x27; prefix.
+
+::
+
+    # echo &quot;-SSD,-HDD,-NET&quot; &gt; /sys/kernel/mm/swap/tiers
diff --git a/include/linux/swap.h b/include/linux/swap.h
index 62fc7499b408..1e68c220a0e7 100644
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@ -262,6 +262,7 @@ struct swap_info_struct {
 	struct percpu_ref users;	/* indicate and keep swap device valid. */
 	unsigned long	flags;		/* SWP_USED etc: see above */
 	signed short	prio;		/* swap priority of this type */
+	int tier_mask;			/* swap tier mask */
 	struct plist_node list;		/* entry in swap_active_head */
 	signed char	type;		/* strange name for an index */
 	unsigned int	max;		/* extent of the swap_map */
diff --git a/mm/swap_state.c b/mm/swap_state.c
index f1a7d9cdc648..d46ca61d2e42 100644
--- a/mm/swap_state.c
+++ b/mm/swap_state.c
@@ -997,7 +997,7 @@ static ssize_t tiers_store(struct kobject *kobj,
 			goto restore;
 	}
 
-	if (!swap_tiers_validate()) {
+	if (!swap_tiers_update()) {
 		ret = -EINVAL;
 		goto restore;
 	}
diff --git a/mm/swap_tier.c b/mm/swap_tier.c
index 87882272eec8..d90f6eccb908 100644
--- a/mm/swap_tier.c
+++ b/mm/swap_tier.c
@@ -14,7 +14,7 @@
  * @name: name of the swap_tier.
  * @prio: starting value of priority.
  * @list: linked list of tiers.
-*/
+ */
 static struct swap_tier {
 	char name[MAX_TIERNAME];
 	short prio;
@@ -34,6 +34,8 @@ static LIST_HEAD(swap_tier_inactive_list);
 	(!list_is_first(&amp;(tier)-&gt;list, &amp;swap_tier_active_list) ? \
 	list_prev_entry((tier), list)-&gt;prio - 1 : SHRT_MAX)
 
+#define MASK_TO_TIER(mask) (&amp;swap_tiers[__ffs((mask))])
+
 #define for_each_tier(tier, idx) \
 	for (idx = 0, tier = &amp;swap_tiers[0]; idx &lt; MAX_SWAPTIER; \
 		idx++, tier = &amp;swap_tiers[idx])
@@ -55,6 +57,26 @@ static bool swap_tier_is_active(void)
 	return !list_empty(&amp;swap_tier_active_list) ? true : false;
 }
 
+static bool swap_tier_prio_in_range(struct swap_tier *tier, short prio)
+{
+	if (tier-&gt;prio &lt;= prio &amp;&amp; TIER_END_PRIO(tier) &gt;= prio)
+		return true;
+
+	return false;
+}
+
+static bool swap_tier_prio_is_used(struct swap_tier *self, short prio)
+{
+	struct swap_tier *tier;
+
+	for_each_active_tier(tier) {
+		if (tier != self &amp;&amp; tier-&gt;prio == prio)
+			return true;
+	}
+
+	return false;
+}
+
 static struct swap_tier *swap_tier_lookup(const char *name)
 {
 	struct swap_tier *tier;
@@ -67,12 +89,14 @@ static struct swap_tier *swap_tier_lookup(const char *name)
 	return NULL;
 }
 
+
 void swap_tiers_init(void)
 {
 	struct swap_tier *tier;
 	int idx;
 
 	BUILD_BUG_ON(BITS_PER_TYPE(int) &lt; MAX_SWAPTIER);
+	BUILD_BUG_ON(MAX_SWAPTIER &gt; TIER_DEFAULT_IDX);
 
 	for_each_tier(tier, idx) {
 		INIT_LIST_HEAD(&amp;tier-&gt;list);
@@ -145,17 +169,35 @@ static struct swap_tier *swap_tier_prepare(const char *name, short prio)
 	return tier;
 }
 
-static int swap_tier_check_range(short prio)
+static int swap_tier_can_split_range(struct swap_tier *orig_tier,
+	short new_prio)
 {
+	struct swap_info_struct *p;
 	struct swap_tier *tier;
 
 	lockdep_assert_held(&amp;swap_lock);
 	lockdep_assert_held(&amp;swap_tier_lock);
 
-	for_each_active_tier(tier) {
-		/* No overwrite */
-		if (tier-&gt;prio == prio)
-			return -EINVAL;
+	plist_for_each_entry(p, &amp;swap_active_head, list) {
+		if (p-&gt;tier_mask == TIER_DEFAULT_MASK)
+			continue;
+
+		tier = MASK_TO_TIER(p-&gt;tier_mask);
+		if (tier-&gt;prio &gt; new_prio)
+			continue;
+		/*
+                 * Prohibit implicit tier reassignment.
+                 * Case 1: Prevent orig_tier devices from dropping out
+                 *         of the new range.
+                 */
+		if (orig_tier == tier &amp;&amp; (p-&gt;prio &lt; new_prio))
+			return -EBUSY;
+                /*
+                 * Case 2: Prevent other tier devices from entering
+                 *         the new range.
+                 */
+		else if (orig_tier != tier &amp;&amp; (p-&gt;prio &gt;= new_prio))
+			return -EBUSY;
 	}
 
 	return 0;
@@ -173,7 +215,10 @@ int swap_tiers_add(const char *name, int prio)
 	if (swap_tier_lookup(name))
 		return -EPERM;
 
-	ret = swap_tier_check_range(prio);
+	if (swap_tier_prio_is_used(NULL, prio))
+		return -EBUSY;
+
+	ret = swap_tier_can_split_range(NULL, prio);
 	if (ret)
 		return ret;
 
@@ -183,7 +228,6 @@ int swap_tiers_add(const char *name, int prio)
 		return ret;
 	}
 
-
 	swap_tier_insert_by_prio(tier);
 	return ret;
 }
@@ -200,11 +244,18 @@ int swap_tiers_remove(const char *name)
 	if (!tier)
 		return -EINVAL;
 
+	/* Simulate adding a tier to check for conflicts */
+	ret = swap_tier_can_split_range(NULL, tier-&gt;prio);
+	if (ret)
+		return ret;
+
 	list_move(&amp;tier-&gt;list, &amp;swap_tier_inactive_list);
 
 	/* Removing DEF_SWAP_PRIO merges into the higher tier. */
-	if (swap_tier_is_active() &amp;&amp; tier-&gt;prio == DEF_SWAP_PRIO)
-		list_prev_entry(tier, list)-&gt;prio = DEF_SWAP_PRIO;
+	if (swap_tier_is_active() &amp;&amp; tier-&gt;prio == DEF_SWAP_PRIO) {
+		list_last_entry(&amp;swap_tier_active_list, struct swap_tier, list)
+			-&gt;prio = DEF_SWAP_PRIO;
+	}
 
 	return ret;
 }
@@ -225,7 +276,10 @@ int swap_tiers_modify(const char *name, int prio)
 	if (tier-&gt;prio == prio)
 		return 0;
 
-	ret = swap_tier_check_range(prio);
+	if (swap_tier_prio_is_used(tier, prio))
+		return -EBUSY;
+
+	ret = swap_tier_can_split_range(tier, prio);
 	if (ret)
 		return ret;
 
@@ -283,10 +337,27 @@ void swap_tiers_restore(struct swap_tier_save_ctx ctx[])
 	}
 }
 
-bool swap_tiers_validate(void)
+void swap_tiers_assign_dev(struct swap_info_struct *swp)
 {
 	struct swap_tier *tier;
 
+	lockdep_assert_held(&amp;swap_lock);
+
+	for_each_active_tier(tier) {
+		if (swap_tier_prio_in_range(tier, swp-&gt;prio)) {
+			swp-&gt;tier_mask = TIER_MASK(tier);
+			return;
+		}
+	}
+
+	swp-&gt;tier_mask = TIER_DEFAULT_MASK;
+}
+
+bool swap_tiers_update(void)
+{
+	struct swap_tier *tier;
+	struct swap_info_struct *swp;
+
 	/*
 	 * Initial setting might not cover DEF_SWAP_PRIO.
 	 * Swap tier must cover the full range (DEF_SWAP_PRIO to SHRT_MAX).
@@ -300,5 +371,16 @@ bool swap_tiers_validate(void)
 			return false;
 	}
 
+	/*
+	 * If applied initially, the swap tier_mask may change
+	 * from the default value.
+	 */
+	plist_for_each_entry(swp, &amp;swap_active_head, list) {
+		/* Tier is already configured */
+		if (swp-&gt;tier_mask != TIER_DEFAULT_MASK)
+			break;
+		swap_tiers_assign_dev(swp);
+	}
+
 	return true;
 }
diff --git a/mm/swap_tier.h b/mm/swap_tier.h
index 4b1b0602d691..de81d540e3b5 100644
--- a/mm/swap_tier.h
+++ b/mm/swap_tier.h
@@ -14,6 +14,9 @@
 #define MAX_SWAPTIER		8
 #endif
 
+/* Forward declarations */
+struct swap_info_struct;
+
 extern spinlock_t swap_tier_lock;
 
 struct swap_tier_save_ctx {
@@ -24,6 +27,10 @@ struct swap_tier_save_ctx {
 #define DEFINE_SWAP_TIER_SAVE_CTX(_name) \
 	struct swap_tier_save_ctx _name[MAX_SWAPTIER] = {0}
 
+#define TIER_ALL_MASK		(~0)
+#define TIER_DEFAULT_IDX	(31)
+#define TIER_DEFAULT_MASK	(1 &lt;&lt; TIER_DEFAULT_IDX)
+
 /* Initialization and application */
 void swap_tiers_init(void);
 ssize_t swap_tiers_sysfs_show(char *buf);
@@ -34,5 +41,9 @@ int swap_tiers_modify(const char *name, int prio);
 
 void swap_tiers_save(struct swap_tier_save_ctx ctx[]);
 void swap_tiers_restore(struct swap_tier_save_ctx ctx[]);
-bool swap_tiers_validate(void);
+bool swap_tiers_update(void);
+
+/* Tier assignment */
+void swap_tiers_assign_dev(struct swap_info_struct *swp);
+
 #endif /* _SWAP_TIER_H */
diff --git a/mm/swapfile.c b/mm/swapfile.c
index c27952b41d4f..4f8ce021c5bd 100644
--- a/mm/swapfile.c
+++ b/mm/swapfile.c
@@ -2672,6 +2672,8 @@ static void _enable_swap_info(struct swap_info_struct *si)
 
 	/* Add back to available list */
 	add_to_avail_list(si, true);
+
+	swap_tiers_assign_dev(si);
 }
 
 static void enable_swap_info(struct swap_info_struct *si, int prio,
-- 
2.34.1</pre>
</details>
<div class="review-comment-signals">Signals: clarification, explanation</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Youngjun Park (author)</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-01-26">2026-01-26</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Author acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), agreed to restructure in v2.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">This patch introduces the &quot;Swap tier&quot; concept, which serves as an
abstraction layer for managing swap devices based on their performance
characteristics (e.g., NVMe, HDD, Network swap).

Swap tiers are user-named groups representing priority ranges.
These tiers collectively cover the entire priority
space from -1 (`DEF_SWAP_PRIO`) to `SHRT_MAX`.

To configure tiers, a new sysfs interface is exposed at
`/sys/kernel/mm/swap/tiers`. The input parser evaluates commands from
left to right and supports batch input, allowing users to add, remove or
modify multiple tiers in a single write operation.

Tier management enforces continuous priority ranges anchored by start
priorities. Operations trigger range splitting or merging, but overwriting
start priorities is forbidden. Merging expands lower tiers upwards to
preserve configured start priorities, except when removing `DEF_SWAP_PRIO`,
which merges downwards.

Suggested-by: Chris Li &lt;chrisl@kernel.org&gt;
Signed-off-by: Youngjun Park &lt;youngjun.park@lge.com&gt;
---
 MAINTAINERS     |   2 +
 mm/Makefile     |   2 +-
 mm/swap.h       |   4 +
 mm/swap_state.c |  70 +++++++++++
 mm/swap_tier.c  | 304 ++++++++++++++++++++++++++++++++++++++++++++++++
 mm/swap_tier.h  |  38 ++++++
 mm/swapfile.c   |   7 +-
 7 files changed, 423 insertions(+), 4 deletions(-)
 create mode 100644 mm/swap_tier.c
 create mode 100644 mm/swap_tier.h

diff --git a/MAINTAINERS b/MAINTAINERS
index 18d1ebf053db..501bf46adfb4 100644
--- a/MAINTAINERS
+++ b/MAINTAINERS
@@ -16743,6 +16743,8 @@ F:	mm/swap.c
 F:	mm/swap.h
 F:	mm/swap_table.h
 F:	mm/swap_state.c
+F:	mm/swap_tier.c
+F:	mm/swap_tier.h
 F:	mm/swapfile.c
 
 MEMORY MANAGEMENT - THP (TRANSPARENT HUGE PAGE)
diff --git a/mm/Makefile b/mm/Makefile
index 53ca5d4b1929..3b3de2de7285 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -75,7 +75,7 @@ ifdef CONFIG_MMU
 	obj-$(CONFIG_ADVISE_SYSCALLS)	+= madvise.o
 endif
 
-obj-$(CONFIG_SWAP)	+= page_io.o swap_state.o swapfile.o
+obj-$(CONFIG_SWAP)	+= page_io.o swap_state.o swapfile.o swap_tier.o
 obj-$(CONFIG_ZSWAP)	+= zswap.o
 obj-$(CONFIG_HAS_DMA)	+= dmapool.o
 obj-$(CONFIG_HUGETLBFS)	+= hugetlb.o hugetlb_sysfs.o hugetlb_sysctl.o
diff --git a/mm/swap.h b/mm/swap.h
index bfafa637c458..55f230cbe4e7 100644
--- a/mm/swap.h
+++ b/mm/swap.h
@@ -16,6 +16,10 @@ extern int page_cluster;
 #define swap_entry_order(order)	0
 #endif
 
+#define DEF_SWAP_PRIO  -1
+
+extern spinlock_t swap_lock;
+extern struct plist_head swap_active_head;
 extern struct swap_info_struct *swap_info[];
 
 /*
diff --git a/mm/swap_state.c b/mm/swap_state.c
index 6d0eef7470be..f1a7d9cdc648 100644
--- a/mm/swap_state.c
+++ b/mm/swap_state.c
@@ -25,6 +25,7 @@
 #include &quot;internal.h&quot;
 #include &quot;swap_table.h&quot;
 #include &quot;swap.h&quot;
+#include &quot;swap_tier.h&quot;
 
 /*
  * swapper_space is a fiction, retained to simplify the path through
@@ -947,8 +948,77 @@ static ssize_t vma_ra_enabled_store(struct kobject *kobj,
 }
 static struct kobj_attribute vma_ra_enabled_attr = __ATTR_RW(vma_ra_enabled);
 
+static ssize_t tiers_show(struct kobject *kobj,
+				     struct kobj_attribute *attr, char *buf)
+{
+	return swap_tiers_sysfs_show(buf);
+}
+
+static ssize_t tiers_store(struct kobject *kobj,
+			struct kobj_attribute *attr,
+			const char *buf, size_t count)
+{
+	char *p, *token, *name, *tmp;
+	int ret = 0;
+	short prio;
+	DEFINE_SWAP_TIER_SAVE_CTX(ctx);
+
+	tmp = kstrdup(buf, GFP_KERNEL);
+	if (!tmp)
+		return -ENOMEM;
+
+	spin_lock(&amp;swap_lock);
+	spin_lock(&amp;swap_tier_lock);
+
+	p = tmp;
+	swap_tiers_save(ctx);
+
+	while (!ret &amp;&amp; (token = strsep(&amp;p, &quot;, \t\n&quot;)) != NULL) {
+		if (!*token)
+			continue;
+
+		if (token[0] == &#x27;-&#x27;) {
+			ret = swap_tiers_remove(token + 1);
+		} else {
+
+			name = strsep(&amp;token, &quot;:&quot;);
+			if (!token || kstrtos16(token, 10, &amp;prio)) {
+				ret = -EINVAL;
+				goto out;
+			}
+
+			if (name[0] == &#x27;+&#x27;)
+				ret = swap_tiers_add(name + 1, prio);
+			else
+				ret = swap_tiers_modify(name, prio);
+		}
+
+		if (ret)
+			goto restore;
+	}
+
+	if (!swap_tiers_validate()) {
+		ret = -EINVAL;
+		goto restore;
+	}
+
+out:
+	spin_unlock(&amp;swap_tier_lock);
+	spin_unlock(&amp;swap_lock);
+
+	kfree(tmp);
+	return ret ? ret : count;
+
+restore:
+	swap_tiers_restore(ctx);
+	goto out;
+}
+
+static struct kobj_attribute tier_attr = __ATTR_RW(tiers);
+
 static struct attribute *swap_attrs[] = {
 	&amp;vma_ra_enabled_attr.attr,
+	&amp;tier_attr.attr,
 	NULL,
 };
 
diff --git a/mm/swap_tier.c b/mm/swap_tier.c
new file mode 100644
index 000000000000..87882272eec8
--- /dev/null
+++ b/mm/swap_tier.c
@@ -0,0 +1,304 @@
+// SPDX-License-Identifier: GPL-2.0
+#include &lt;linux/swap.h&gt;
+#include &lt;linux/memcontrol.h&gt;
+#include &quot;memcontrol-v1.h&quot;
+#include &lt;linux/sysfs.h&gt;
+#include &lt;linux/plist.h&gt;
+
+#include &quot;swap.h&quot;
+#include &quot;swap_tier.h&quot;
+
+/*
+ * struct swap_tier - structure representing a swap tier.
+ *
+ * @name: name of the swap_tier.
+ * @prio: starting value of priority.
+ * @list: linked list of tiers.
+*/
+static struct swap_tier {
+	char name[MAX_TIERNAME];
+	short prio;
+	struct list_head list;
+} swap_tiers[MAX_SWAPTIER];
+
+DEFINE_SPINLOCK(swap_tier_lock);
+/* active swap priority list, sorted in descending order */
+static LIST_HEAD(swap_tier_active_list);
+/* unused swap_tier object */
+static LIST_HEAD(swap_tier_inactive_list);
+
+#define TIER_IDX(tier)	((tier) - swap_tiers)
+#define TIER_MASK(tier)	(1 &lt;&lt; TIER_IDX(tier))
+#define TIER_INVALID_PRIO (DEF_SWAP_PRIO - 1)
+#define TIER_END_PRIO(tier) \
+	(!list_is_first(&amp;(tier)-&gt;list, &amp;swap_tier_active_list) ? \
+	list_prev_entry((tier), list)-&gt;prio - 1 : SHRT_MAX)
+
+#define for_each_tier(tier, idx) \
+	for (idx = 0, tier = &amp;swap_tiers[0]; idx &lt; MAX_SWAPTIER; \
+		idx++, tier = &amp;swap_tiers[idx])
+
+#define for_each_active_tier(tier) \
+	list_for_each_entry(tier, &amp;swap_tier_active_list, list)
+
+#define for_each_inactive_tier(tier) \
+	list_for_each_entry(tier, &amp;swap_tier_inactive_list, list)
+
+/*
+ * Naming Convention:
+ *   swap_tiers_*() - Public/exported functions
+ *   swap_tier_*()  - Private/internal functions
+ */
+
+static bool swap_tier_is_active(void)
+{
+	return !list_empty(&amp;swap_tier_active_list) ? true : false;
+}
+
+static struct swap_tier *swap_tier_lookup(const char *name)
+{
+	struct swap_tier *tier;
+
+	for_each_active_tier(tier) {
+		if (!strcmp(tier-&gt;name, name))
+			return tier;
+	}
+
+	return NULL;
+}
+
+void swap_tiers_init(void)
+{
+	struct swap_tier *tier;
+	int idx;
+
+	BUILD_BUG_ON(BITS_PER_TYPE(int) &lt; MAX_SWAPTIER);
+
+	for_each_tier(tier, idx) {
+		INIT_LIST_HEAD(&amp;tier-&gt;list);
+		list_add_tail(&amp;tier-&gt;list, &amp;swap_tier_inactive_list);
+	}
+}
+
+ssize_t swap_tiers_sysfs_show(char *buf)
+{
+	struct swap_tier *tier;
+	ssize_t len = 0;
+
+	len += sysfs_emit_at(buf, len, &quot;%-16s %-5s %-11s %-11s\n&quot;,
+			 &quot;Name&quot;, &quot;Idx&quot;, &quot;PrioStart&quot;, &quot;PrioEnd&quot;);
+
+	spin_lock(&amp;swap_tier_lock);
+	for_each_active_tier(tier) {
+		len += sysfs_emit_at(buf, len, &quot;%-16s %-5ld %-11d %-11d\n&quot;,
+				     tier-&gt;name,
+				     TIER_IDX(tier),
+				     tier-&gt;prio,
+				     TIER_END_PRIO(tier));
+		if (len &gt;= PAGE_SIZE)
+			break;
+	}
+	spin_unlock(&amp;swap_tier_lock);
+
+	return len;
+}
+
+static void swap_tier_insert_by_prio(struct swap_tier *new)
+{
+	struct swap_tier *tier;
+
+	for_each_active_tier(tier) {
+		if (tier-&gt;prio &gt; new-&gt;prio)
+			continue;
+
+		list_add_tail(&amp;new-&gt;list, &amp;tier-&gt;list);
+		return;
+	}
+	/* First addition, or becomes the first tier */
+	list_add_tail(&amp;new-&gt;list, &amp;swap_tier_active_list);
+}
+
+static void __swap_tier_prepare(struct swap_tier *tier, const char *name,
+	short prio)
+{
+	list_del_init(&amp;tier-&gt;list);
+	strscpy(tier-&gt;name, name, MAX_TIERNAME);
+	tier-&gt;prio = prio;
+}
+
+static struct swap_tier *swap_tier_prepare(const char *name, short prio)
+{
+	struct swap_tier *tier;
+
+	lockdep_assert_held(&amp;swap_tier_lock);
+
+	if (prio &lt; DEF_SWAP_PRIO)
+		return NULL;
+
+	if (list_empty(&amp;swap_tier_inactive_list))
+		return ERR_PTR(-EPERM);
+
+	tier = list_first_entry(&amp;swap_tier_inactive_list,
+		struct swap_tier, list);
+
+	__swap_tier_prepare(tier, name, prio);
+	return tier;
+}
+
+static int swap_tier_check_range(short prio)
+{
+	struct swap_tier *tier;
+
+	lockdep_assert_held(&amp;swap_lock);
+	lockdep_assert_held(&amp;swap_tier_lock);
+
+	for_each_active_tier(tier) {
+		/* No overwrite */
+		if (tier-&gt;prio == prio)
+			return -EINVAL;
+	}
+
+	return 0;
+}
+
+int swap_tiers_add(const char *name, int prio)
+{
+	int ret;
+	struct swap_tier *tier;
+
+	lockdep_assert_held(&amp;swap_lock);
+	lockdep_assert_held(&amp;swap_tier_lock);
+
+	/* Duplicate check */
+	if (swap_tier_lookup(name))
+		return -EPERM;
+
+	ret = swap_tier_check_range(prio);
+	if (ret)
+		return ret;
+
+	tier = swap_tier_prepare(name, prio);
+	if (IS_ERR(tier)) {
+		ret = PTR_ERR(tier);
+		return ret;
+	}
+
+
+	swap_tier_insert_by_prio(tier);
+	return ret;
+}
+
+int swap_tiers_remove(const char *name)
+{
+	int ret = 0;
+	struct swap_tier *tier;
+
+	lockdep_assert_held(&amp;swap_lock);
+	lockdep_assert_held(&amp;swap_tier_lock);
+
+	tier = swap_tier_lookup(name);
+	if (!tier)
+		return -EINVAL;
+
+	list_move(&amp;tier-&gt;list, &amp;swap_tier_inactive_list);
+
+	/* Removing DEF_SWAP_PRIO merges into the higher tier. */
+	if (swap_tier_is_active() &amp;&amp; tier-&gt;prio == DEF_SWAP_PRIO)
+		list_prev_entry(tier, list)-&gt;prio = DEF_SWAP_PRIO;
+
+	return ret;
+}
+
+int swap_tiers_modify(const char *name, int prio)
+{
+	int ret;
+	struct swap_tier *tier;
+
+	lockdep_assert_held(&amp;swap_lock);
+	lockdep_assert_held(&amp;swap_tier_lock);
+
+	tier = swap_tier_lookup(name);
+	if (!tier)
+		return -EINVAL;
+
+	/* No need to modify */
+	if (tier-&gt;prio == prio)
+		return 0;
+
+	ret = swap_tier_check_range(prio);
+	if (ret)
+		return ret;
+
+	list_del_init(&amp;tier-&gt;list);
+	tier-&gt;prio = prio;
+	swap_tier_insert_by_prio(tier);
+
+	return ret;
+}
+
+/*
+ * XXX: Reverting individual operations becomes complex as the number of
+ * operations grows. Instead, we save the original state beforehand and
+ * fully restore it if any operation fails.
+ */
+void swap_tiers_save(struct swap_tier_save_ctx ctx[])
+{
+	struct swap_tier *tier;
+	int idx;
+
+	lockdep_assert_held(&amp;swap_lock);
+	lockdep_assert_held(&amp;swap_tier_lock);
+
+	for_each_active_tier(tier) {
+		idx = TIER_IDX(tier);
+		strcpy(ctx[idx].name, tier-&gt;name);
+		ctx[idx].prio = tier-&gt;prio;
+	}
+
+	for_each_inactive_tier(tier) {
+		idx = TIER_IDX(tier);
+		/* Indicator of inactive */
+		ctx[idx].prio = TIER_INVALID_PRIO;
+	}
+}
+
+void swap_tiers_restore(struct swap_tier_save_ctx ctx[])
+{
+	struct swap_tier *tier;
+	int idx;
+
+	lockdep_assert_held(&amp;swap_lock);
+	lockdep_assert_held(&amp;swap_tier_lock);
+
+	/* Invalidate active list */
+	list_splice_tail_init(&amp;swap_tier_active_list,
+			&amp;swap_tier_inactive_list);
+
+	for_each_tier(tier, idx) {
+		if (ctx[idx].prio != TIER_INVALID_PRIO) {
+			/* Preserve idx(mask) */
+			__swap_tier_prepare(tier, ctx[idx].name, ctx[idx].prio);
+			swap_tier_insert_by_prio(tier);
+		}
+	}
+}
+
+bool swap_tiers_validate(void)
+{
+	struct swap_tier *tier;
+
+	/*
+	 * Initial setting might not cover DEF_SWAP_PRIO.
+	 * Swap tier must cover the full range (DEF_SWAP_PRIO to SHRT_MAX).
+	 * Also, modify operation can change only one remaining priority.
+	 */
+	if (swap_tier_is_active()) {
+		tier = list_last_entry(&amp;swap_tier_active_list,
+			struct swap_tier, list);
+
+		if (tier-&gt;prio != DEF_SWAP_PRIO)
+			return false;
+	}
+
+	return true;
+}
diff --git a/mm/swap_tier.h b/mm/swap_tier.h
new file mode 100644
index 000000000000..4b1b0602d691
--- /dev/null
+++ b/mm/swap_tier.h
@@ -0,0 +1,38 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _SWAP_TIER_H
+#define _SWAP_TIER_H
+
+#include &lt;linux/types.h&gt;
+#include &lt;linux/spinlock.h&gt;
+
+#define MAX_TIERNAME		16
+
+/* Ensure MAX_SWAPTIER does not exceed MAX_SWAPFILES */
+#if 8 &gt; MAX_SWAPFILES
+#define MAX_SWAPTIER		MAX_SWAPFILES
+#else
+#define MAX_SWAPTIER		8
+#endif
+
+extern spinlock_t swap_tier_lock;
+
+struct swap_tier_save_ctx {
+	char name[MAX_TIERNAME];
+	short prio;
+};
+
+#define DEFINE_SWAP_TIER_SAVE_CTX(_name) \
+	struct swap_tier_save_ctx _name[MAX_SWAPTIER] = {0}
+
+/* Initialization and application */
+void swap_tiers_init(void);
+ssize_t swap_tiers_sysfs_show(char *buf);
+
+int swap_tiers_add(const char *name, int prio);
+int swap_tiers_remove(const char *name);
+int swap_tiers_modify(const char *name, int prio);
+
+void swap_tiers_save(struct swap_tier_save_ctx ctx[]);
+void swap_tiers_restore(struct swap_tier_save_ctx ctx[]);
+bool swap_tiers_validate(void);
+#endif /* _SWAP_TIER_H */
diff --git a/mm/swapfile.c b/mm/swapfile.c
index 7b055f15d705..c27952b41d4f 100644
--- a/mm/swapfile.c
+++ b/mm/swapfile.c
@@ -50,6 +50,7 @@
 #include &quot;internal.h&quot;
 #include &quot;swap_table.h&quot;
 #include &quot;swap.h&quot;
+#include &quot;swap_tier.h&quot;
 
 static bool swap_count_continued(struct swap_info_struct *, pgoff_t,
 				 unsigned char);
@@ -65,7 +66,7 @@ static void move_cluster(struct swap_info_struct *si,
 			 struct swap_cluster_info *ci, struct list_head *list,
 			 enum swap_cluster_flags new_flags);
 
-static DEFINE_SPINLOCK(swap_lock);
+DEFINE_SPINLOCK(swap_lock);
 static unsigned int nr_swapfiles;
 atomic_long_t nr_swap_pages;
 /*
@@ -76,7 +77,6 @@ atomic_long_t nr_swap_pages;
 EXPORT_SYMBOL_GPL(nr_swap_pages);
 /* protected with swap_lock. reading in vm_swap_full() doesn&#x27;t need lock */
 long total_swap_pages;
-#define DEF_SWAP_PRIO  -1
 unsigned long swapfile_maximum_size;
 #ifdef CONFIG_MIGRATION
 bool swap_migration_ad_supported;
@@ -89,7 +89,7 @@ static const char Bad_offset[] = &quot;Bad swap offset entry &quot;;
  * all active swap_info_structs
  * protected with swap_lock, and ordered by priority.
  */
-static PLIST_HEAD(swap_active_head);
+PLIST_HEAD(swap_active_head);
 
 /*
  * all available (active, not full) swap_info_structs
@@ -3977,6 +3977,7 @@ static int __init swapfile_init(void)
 		swap_migration_ad_supported = true;
 #endif	/* CONFIG_MIGRATION */
 
+	swap_tiers_init();
 	return 0;
 }
 subsys_initcall(swapfile_init);
-- 
2.34.1</pre>
</details>
<div class="review-comment-signals">Signals: acknowledged a fix is needed</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Youngjun Park (author)</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-01-26">2026-01-26</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author is addressing a concern about caching oscillation and priority inversion in swap devices due to the global percpu cluster. They agree that reverting commit 1b7e90020eb7 is necessary to use each swap device&#x27;s percpu cluster, which will prevent these issues.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">This reverts commit 1b7e90020eb7 (&quot;mm, swap: use percpu cluster as
allocation fast path&quot;).

Because in the newly introduced swap tiers, the global percpu cluster
will cause two issues:
1) it will cause caching oscillation in the same order of different si
   if two different memcg can only be allowed to access different si and
   both of them are swapping out.
2) It can cause priority inversion on swap devices. Imagine a case where
   there are two memcg, say memcg1 and memcg2. Memcg1 can access si A, B
   and A is higher priority device. While memcg2 can only access si B.
   Then memcg 2 could write the global percpu cluster with si B, then
   memcg1 take si B in fast path even though si A is not exhausted.

Hence in order to support swap tier, revert commit to use
each swap device&#x27;s percpu cluster.

Suggested-by: Kairui Song &lt;kasong@tencent.com&gt;
Co-developed-by: Baoquan He &lt;bhe@redhat.com&gt;
Signed-off-by: Baoquan He &lt;bhe@redhat.com&gt;
Signed-off-by: Youngjun Park &lt;youngjun.park@lge.com&gt;
---
 include/linux/swap.h |  17 ++++--
 mm/swapfile.c        | 142 ++++++++++++++-----------------------------
 2 files changed, 57 insertions(+), 102 deletions(-)

diff --git a/include/linux/swap.h b/include/linux/swap.h
index 1e68c220a0e7..6921e22b14d3 100644
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@ -247,11 +247,18 @@ enum {
 #define SWAP_NR_ORDERS		1
 #endif
 
-/*
- * We keep using same cluster for rotational device so IO will be sequential.
- * The purpose is to optimize SWAP throughput on these device.
- */
+ /*
+  * We assign a cluster to each CPU, so each CPU can allocate swap entry from
+  * its own cluster and swapout sequentially. The purpose is to optimize swapout
+  * throughput.
+  */
+struct percpu_cluster {
+	local_lock_t lock; /* Protect the percpu_cluster above */
+	unsigned int next[SWAP_NR_ORDERS]; /* Likely next allocation offset */
+};
+
 struct swap_sequential_cluster {
+	spinlock_t lock; /* Serialize usage of global cluster */
 	unsigned int next[SWAP_NR_ORDERS]; /* Likely next allocation offset */
 };
 
@@ -277,8 +284,8 @@ struct swap_info_struct {
 					/* list of cluster that are fragmented or contented */
 	unsigned int pages;		/* total of usable pages of swap */
 	atomic_long_t inuse_pages;	/* number of those currently in use */
+	struct percpu_cluster	__percpu *percpu_cluster; /* per cpu&#x27;s swap location */
 	struct swap_sequential_cluster *global_cluster; /* Use one global cluster for rotating device */
-	spinlock_t global_cluster_lock;	/* Serialize usage of global cluster */
 	struct rb_root swap_extent_root;/* root of the swap extent rbtree */
 	struct block_device *bdev;	/* swap device or bdev of swap file */
 	struct file *swap_file;		/* seldom referenced */
diff --git a/mm/swapfile.c b/mm/swapfile.c
index dd97e850ea2c..5e3b87799440 100644
--- a/mm/swapfile.c
+++ b/mm/swapfile.c
@@ -118,18 +118,6 @@ static atomic_t proc_poll_event = ATOMIC_INIT(0);
 
 atomic_t nr_rotate_swap = ATOMIC_INIT(0);
 
-struct percpu_swap_cluster {
-	struct swap_info_struct *si[SWAP_NR_ORDERS];
-	unsigned long offset[SWAP_NR_ORDERS];
-	local_lock_t lock;
-};
-
-static DEFINE_PER_CPU(struct percpu_swap_cluster, percpu_swap_cluster) = {
-	.si = { NULL },
-	.offset = { SWAP_ENTRY_INVALID },
-	.lock = INIT_LOCAL_LOCK(),
-};
-
 /* May return NULL on invalid type, caller must check for NULL return */
 static struct swap_info_struct *swap_type_to_info(int type)
 {
@@ -477,7 +465,7 @@ swap_cluster_alloc_table(struct swap_info_struct *si,
 	 * Swap allocator uses percpu clusters and holds the local lock.
 	 */
 	lockdep_assert_held(&amp;ci-&gt;lock);
-	lockdep_assert_held(&amp;this_cpu_ptr(&amp;percpu_swap_cluster)-&gt;lock);
+	lockdep_assert_held(this_cpu_ptr(&amp;si-&gt;percpu_cluster-&gt;lock));
 
 	/* The cluster must be free and was just isolated from the free list. */
 	VM_WARN_ON_ONCE(ci-&gt;flags || !cluster_is_empty(ci));
@@ -495,8 +483,8 @@ swap_cluster_alloc_table(struct swap_info_struct *si,
 	 */
 	spin_unlock(&amp;ci-&gt;lock);
 	if (!(si-&gt;flags &amp; SWP_SOLIDSTATE))
-		spin_unlock(&amp;si-&gt;global_cluster_lock);
-	local_unlock(&amp;percpu_swap_cluster.lock);
+		spin_unlock(&amp;si-&gt;global_cluster-&gt;lock);
+	local_unlock(&amp;si-&gt;percpu_cluster-&gt;lock);
 
 	table = swap_table_alloc(__GFP_HIGH | __GFP_NOMEMALLOC | GFP_KERNEL);
 
@@ -508,9 +496,9 @@ swap_cluster_alloc_table(struct swap_info_struct *si,
 	 * could happen with ignoring the percpu cluster is fragmentation,
 	 * which is acceptable since this fallback and race is rare.
 	 */
-	local_lock(&amp;percpu_swap_cluster.lock);
+	local_lock(&amp;si-&gt;percpu_cluster-&gt;lock);
 	if (!(si-&gt;flags &amp; SWP_SOLIDSTATE))
-		spin_lock(&amp;si-&gt;global_cluster_lock);
+		spin_lock(&amp;si-&gt;global_cluster-&gt;lock);
 	spin_lock(&amp;ci-&gt;lock);
 
 	/* Nothing except this helper should touch a dangling empty cluster. */
@@ -622,7 +610,7 @@ static bool swap_do_scheduled_discard(struct swap_info_struct *si)
 		ci = list_first_entry(&amp;si-&gt;discard_clusters, struct swap_cluster_info, list);
 		/*
 		 * Delete the cluster from list to prepare for discard, but keep
-		 * the CLUSTER_FLAG_DISCARD flag, percpu_swap_cluster could be
+		 * the CLUSTER_FLAG_DISCARD flag, there could be percpu_cluster
 		 * pointing to it, or ran into by relocate_cluster.
 		 */
 		list_del(&amp;ci-&gt;list);
@@ -953,12 +941,11 @@ static unsigned int alloc_swap_scan_cluster(struct swap_info_struct *si,
 out:
 	relocate_cluster(si, ci);
 	swap_cluster_unlock(ci);
-	if (si-&gt;flags &amp; SWP_SOLIDSTATE) {
-		this_cpu_write(percpu_swap_cluster.offset[order], next);
-		this_cpu_write(percpu_swap_cluster.si[order], si);
-	} else {
+	if (si-&gt;flags &amp; SWP_SOLIDSTATE)
+		this_cpu_write(si-&gt;percpu_cluster-&gt;next[order], next);
+	else
 		si-&gt;global_cluster-&gt;next[order] = next;
-	}
+
 	return found;
 }
 
@@ -1052,13 +1039,17 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,
 	if (order &amp;&amp; !(si-&gt;flags &amp; SWP_BLKDEV))
 		return 0;
 
-	if (!(si-&gt;flags &amp; SWP_SOLIDSTATE)) {
+	if (si-&gt;flags &amp; SWP_SOLIDSTATE) {
+		/* Fast path using per CPU cluster */
+		local_lock(&amp;si-&gt;percpu_cluster-&gt;lock);
+		offset = __this_cpu_read(si-&gt;percpu_cluster-&gt;next[order]);
+	} else {
 		/* Serialize HDD SWAP allocation for each device. */
-		spin_lock(&amp;si-&gt;global_cluster_lock);
+		spin_lock(&amp;si-&gt;global_cluster-&gt;lock);
 		offset = si-&gt;global_cluster-&gt;next[order];
-		if (offset == SWAP_ENTRY_INVALID)
-			goto new_cluster;
+	}
 
+	if (offset != SWAP_ENTRY_INVALID) {
 		ci = swap_cluster_lock(si, offset);
 		/* Cluster could have been used by another order */
 		if (cluster_is_usable(ci, order)) {
@@ -1072,7 +1063,6 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,
 			goto done;
 	}
 
-new_cluster:
 	/*
 	 * If the device need discard, prefer new cluster over nonfull
 	 * to spread out the writes.
@@ -1129,8 +1119,10 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,
 			goto done;
 	}
 done:
-	if (!(si-&gt;flags &amp; SWP_SOLIDSTATE))
-		spin_unlock(&amp;si-&gt;global_cluster_lock);
+	if (si-&gt;flags &amp; SWP_SOLIDSTATE)
+		local_unlock(&amp;si-&gt;percpu_cluster-&gt;lock);
+	else
+		spin_unlock(&amp;si-&gt;global_cluster-&gt;lock);
 
 	return found;
 }
@@ -1311,41 +1303,8 @@ static bool get_swap_device_info(struct swap_info_struct *si)
 	return true;
 }
 
-/*
- * Fast path try to get swap entries with specified order from current
- * CPU&#x27;s swap entry pool (a cluster).
- */
-static bool swap_alloc_fast(struct folio *folio)
-{
-	unsigned int order = folio_order(folio);
-	struct swap_cluster_info *ci;
-	struct swap_info_struct *si;
-	unsigned int offset;
-
-	/*
-	 * Once allocated, swap_info_struct will never be completely freed,
-	 * so checking it&#x27;s liveness by get_swap_device_info is enough.
-	 */
-	si = this_cpu_read(percpu_swap_cluster.si[order]);
-	offset = this_cpu_read(percpu_swap_cluster.offset[order]);
-	if (!si || !offset || !get_swap_device_info(si))
-		return false;
-
-	ci = swap_cluster_lock(si, offset);
-	if (cluster_is_usable(ci, order)) {
-		if (cluster_is_empty(ci))
-			offset = cluster_offset(si, ci);
-		alloc_swap_scan_cluster(si, ci, folio, offset);
-	} else {
-		swap_cluster_unlock(ci);
-	}
-
-	put_swap_device(si);
-	return folio_test_swapcache(folio);
-}
-
 /* Rotate the device and switch to a new cluster */
-static void swap_alloc_slow(struct folio *folio)
+static void swap_alloc_entry(struct folio *folio)
 {
 	struct swap_info_struct *si, *next;
 	int mask = folio_memcg(folio) ?
@@ -1363,6 +1322,7 @@ static void swap_alloc_slow(struct folio *folio)
 		if (get_swap_device_info(si)) {
 			cluster_alloc_swap_entry(si, folio);
 			put_swap_device(si);
+
 			if (folio_test_swapcache(folio))
 				return;
 			if (folio_test_large(folio))
@@ -1522,11 +1482,7 @@ int folio_alloc_swap(struct folio *folio)
 	}
 
 again:
-	local_lock(&amp;percpu_swap_cluster.lock);
-	if (!swap_alloc_fast(folio))
-		swap_alloc_slow(folio);
-	local_unlock(&amp;percpu_swap_cluster.lock);
-
+	swap_alloc_entry(folio);
 	if (!order &amp;&amp; unlikely(!folio_test_swapcache(folio))) {
 		if (swap_sync_discard())
 			goto again;
@@ -1945,9 +1901,7 @@ swp_entry_t swap_alloc_hibernation_slot(int type)
 			 * Grab the local lock to be compliant
 			 * with swap table allocation.
 			 */
-			local_lock(&amp;percpu_swap_cluster.lock);
 			offset = cluster_alloc_swap_entry(si, NULL);
-			local_unlock(&amp;percpu_swap_cluster.lock);
 			if (offset)
 				entry = swp_entry(si-&gt;type, offset);
 		}
@@ -2751,28 +2705,6 @@ static void free_cluster_info(struct swap_cluster_info *cluster_info,
 	kvfree(cluster_info);
 }
 
-/*
- * Called after swap device&#x27;s reference count is dead, so
- * neither scan nor allocation will use it.
- */
-static void flush_percpu_swap_cluster(struct swap_info_struct *si)
-{
-	int cpu, i;
-	struct swap_info_struct **pcp_si;
-
-	for_each_possible_cpu(cpu) {
-		pcp_si = per_cpu_ptr(percpu_swap_cluster.si, cpu);
-		/*
-		 * Invalidate the percpu swap cluster cache, si-&gt;users
-		 * is dead, so no new user will point to it, just flush
-		 * any existing user.
-		 */
-		for (i = 0; i &lt; SWAP_NR_ORDERS; i++)
-			cmpxchg(&amp;pcp_si[i], si, NULL);
-	}
-}
-
-
 SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)
 {
 	struct swap_info_struct *p = NULL;
@@ -2856,7 +2788,6 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)
 
 	flush_work(&amp;p-&gt;discard_work);
 	flush_work(&amp;p-&gt;reclaim_work);
-	flush_percpu_swap_cluster(p);
 
 	destroy_swap_extents(p);
 	if (p-&gt;flags &amp; SWP_CONTINUED)
@@ -2885,6 +2816,8 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)
 	arch_swap_invalidate_area(p-&gt;type);
 	zswap_swapoff(p-&gt;type);
 	mutex_unlock(&amp;swapon_mutex);
+	free_percpu(p-&gt;percpu_cluster);
+	p-&gt;percpu_cluster = NULL;
 	kfree(p-&gt;global_cluster);
 	p-&gt;global_cluster = NULL;
 	vfree(swap_map);
@@ -3268,7 +3201,7 @@ static struct swap_cluster_info *setup_clusters(struct swap_info_struct *si,
 {
 	unsigned long nr_clusters = DIV_ROUND_UP(maxpages, SWAPFILE_CLUSTER);
 	struct swap_cluster_info *cluster_info;
-	int err = -ENOMEM;
+	int cpu, err = -ENOMEM;
 	unsigned long i;
 
 	cluster_info = kvcalloc(nr_clusters, sizeof(*cluster_info), GFP_KERNEL);
@@ -3278,14 +3211,27 @@ static struct swap_cluster_info *setup_clusters(struct swap_info_struct *si,
 	for (i = 0; i &lt; nr_clusters; i++)
 		spin_lock_init(&amp;cluster_info[i].lock);
 
-	if (!(si-&gt;flags &amp; SWP_SOLIDSTATE)) {
+	if (si-&gt;flags &amp; SWP_SOLIDSTATE) {
+		si-&gt;percpu_cluster = alloc_percpu(struct percpu_cluster);
+		if (!si-&gt;percpu_cluster)
+			goto err;
+
+		for_each_possible_cpu(cpu) {
+			struct percpu_cluster *cluster;
+
+			cluster = per_cpu_ptr(si-&gt;percpu_cluster, cpu);
+			for (i = 0; i &lt; SWAP_NR_ORDERS; i++)
+				cluster-&gt;next[i] = SWAP_ENTRY_INVALID;
+			local_lock_init(&amp;cluster-&gt;lock);
+		}
+	} else {
 		si-&gt;global_cluster = kmalloc(sizeof(*si-&gt;global_cluster),
 				     GFP_KERNEL);
 		if (!si-&gt;global_cluster)
 			goto err;
 		for (i = 0; i &lt; SWAP_NR_ORDERS; i++)
 			si-&gt;global_cluster-&gt;next[i] = SWAP_ENTRY_INVALID;
-		spin_lock_init(&amp;si-&gt;global_cluster_lock);
+		spin_lock_init(&amp;si-&gt;global_cluster-&gt;lock);
 	}
 
 	/*
@@ -3566,6 +3512,8 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)
 bad_swap_unlock_inode:
 	inode_unlock(inode);
 bad_swap:
+	free_percpu(si-&gt;percpu_cluster);
+	si-&gt;percpu_cluster = NULL;
 	kfree(si-&gt;global_cluster);
 	si-&gt;global_cluster = NULL;
 	inode = NULL;
-- 
2.34.1</pre>
</details>
<div class="review-comment-signals">Signals: acknowledged a fix is needed, agreed with the approach</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Youngjun Park (author)</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-01-26">2026-01-26</a>
<span class="badge" style="color:#155724;background:#d4edda">Positive</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed a concern about swap device rotation on every allocation, which leads to severe fragmentation and performance regression. They introduced a per-cpu cache for the swap device, prioritizing the cached device within its cluster, effectively restoring the traditional fastpath and slowpath flow. This change minimizes side effects on the existing fastpath.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">When using per-device percpu clusters (instead of a global one),
a naive allocation logic triggers swap device rotation on every
allocation. This behavior leads to severe fragmentation and performance
regression.

To address this, this patch introduces a per-cpu cache for the swap
device. The allocation logic is updated to prioritize the per-cpu
cluster within the cached swap device, effectively restoring the
traditional fastpath and slowpath flow. This approach minimizes side
effects on the existing fastpath.

With this change, swap device rotation occurs only when the current
cached device is unable to satisfy the allocation, rather than on
every attempt.

Signed-off-by: Youngjun Park &lt;youngjun.park@lge.com&gt;
---
 include/linux/swap.h |  1 -
 mm/swapfile.c        | 78 +++++++++++++++++++++++++++++++++++++-------
 2 files changed, 66 insertions(+), 13 deletions(-)

diff --git a/include/linux/swap.h b/include/linux/swap.h
index 6921e22b14d3..ac634a21683a 100644
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@ -253,7 +253,6 @@ enum {
   * throughput.
   */
 struct percpu_cluster {
-	local_lock_t lock; /* Protect the percpu_cluster above */
 	unsigned int next[SWAP_NR_ORDERS]; /* Likely next allocation offset */
 };
 
diff --git a/mm/swapfile.c b/mm/swapfile.c
index 5e3b87799440..0dcd451afee5 100644
--- a/mm/swapfile.c
+++ b/mm/swapfile.c
@@ -106,6 +106,16 @@ PLIST_HEAD(swap_active_head);
 static PLIST_HEAD(swap_avail_head);
 static DEFINE_SPINLOCK(swap_avail_lock);
 
+struct percpu_swap_device {
+	struct swap_info_struct *si[SWAP_NR_ORDERS];
+	local_lock_t lock;
+};
+
+static DEFINE_PER_CPU(struct percpu_swap_device, percpu_swap_device) = {
+	.si = { NULL },
+	.lock = INIT_LOCAL_LOCK(),
+};
+
 struct swap_info_struct *swap_info[MAX_SWAPFILES];
 
 static struct kmem_cache *swap_table_cachep;
@@ -465,7 +475,7 @@ swap_cluster_alloc_table(struct swap_info_struct *si,
 	 * Swap allocator uses percpu clusters and holds the local lock.
 	 */
 	lockdep_assert_held(&amp;ci-&gt;lock);
-	lockdep_assert_held(this_cpu_ptr(&amp;si-&gt;percpu_cluster-&gt;lock));
+	lockdep_assert_held(this_cpu_ptr(&amp;percpu_swap_device.lock));
 
 	/* The cluster must be free and was just isolated from the free list. */
 	VM_WARN_ON_ONCE(ci-&gt;flags || !cluster_is_empty(ci));
@@ -484,7 +494,7 @@ swap_cluster_alloc_table(struct swap_info_struct *si,
 	spin_unlock(&amp;ci-&gt;lock);
 	if (!(si-&gt;flags &amp; SWP_SOLIDSTATE))
 		spin_unlock(&amp;si-&gt;global_cluster-&gt;lock);
-	local_unlock(&amp;si-&gt;percpu_cluster-&gt;lock);
+	local_unlock(&amp;percpu_swap_device.lock);
 
 	table = swap_table_alloc(__GFP_HIGH | __GFP_NOMEMALLOC | GFP_KERNEL);
 
@@ -496,7 +506,7 @@ swap_cluster_alloc_table(struct swap_info_struct *si,
 	 * could happen with ignoring the percpu cluster is fragmentation,
 	 * which is acceptable since this fallback and race is rare.
 	 */
-	local_lock(&amp;si-&gt;percpu_cluster-&gt;lock);
+	local_lock(&amp;percpu_swap_device.lock);
 	if (!(si-&gt;flags &amp; SWP_SOLIDSTATE))
 		spin_lock(&amp;si-&gt;global_cluster-&gt;lock);
 	spin_lock(&amp;ci-&gt;lock);
@@ -941,9 +951,10 @@ static unsigned int alloc_swap_scan_cluster(struct swap_info_struct *si,
 out:
 	relocate_cluster(si, ci);
 	swap_cluster_unlock(ci);
-	if (si-&gt;flags &amp; SWP_SOLIDSTATE)
+	if (si-&gt;flags &amp; SWP_SOLIDSTATE) {
 		this_cpu_write(si-&gt;percpu_cluster-&gt;next[order], next);
-	else
+		this_cpu_write(percpu_swap_device.si[order], si);
+	} else
 		si-&gt;global_cluster-&gt;next[order] = next;
 
 	return found;
@@ -1041,7 +1052,6 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,
 
 	if (si-&gt;flags &amp; SWP_SOLIDSTATE) {
 		/* Fast path using per CPU cluster */
-		local_lock(&amp;si-&gt;percpu_cluster-&gt;lock);
 		offset = __this_cpu_read(si-&gt;percpu_cluster-&gt;next[order]);
 	} else {
 		/* Serialize HDD SWAP allocation for each device. */
@@ -1119,9 +1129,7 @@ static unsigned long cluster_alloc_swap_entry(struct swap_info_struct *si,
 			goto done;
 	}
 done:
-	if (si-&gt;flags &amp; SWP_SOLIDSTATE)
-		local_unlock(&amp;si-&gt;percpu_cluster-&gt;lock);
-	else
+	if (!(si-&gt;flags &amp; SWP_SOLIDSTATE))
 		spin_unlock(&amp;si-&gt;global_cluster-&gt;lock);
 
 	return found;
@@ -1303,8 +1311,27 @@ static bool get_swap_device_info(struct swap_info_struct *si)
 	return true;
 }
 
+static bool swap_alloc_fast(struct folio *folio)
+{
+	unsigned int order = folio_order(folio);
+	struct swap_info_struct *si;
+
+	/*
+	 * Once allocated, swap_info_struct will never be completely freed,
+	 * so checking it&#x27;s liveness by get_swap_device_info is enough.
+	 */
+	si = this_cpu_read(percpu_swap_device.si[order]);
+	if (!si || !get_swap_device_info(si))
+		return false;
+
+	cluster_alloc_swap_entry(si, folio);
+	put_swap_device(si);
+
+	return folio_test_swapcache(folio);
+}
+
 /* Rotate the device and switch to a new cluster */
-static void swap_alloc_entry(struct folio *folio)
+static void swap_alloc_slow(struct folio *folio)
 {
 	struct swap_info_struct *si, *next;
 	int mask = folio_memcg(folio) ?
@@ -1482,7 +1509,11 @@ int folio_alloc_swap(struct folio *folio)
 	}
 
 again:
-	swap_alloc_entry(folio);
+	local_lock(&amp;percpu_swap_device.lock);
+	if (!swap_alloc_fast(folio))
+		swap_alloc_slow(folio);
+	local_unlock(&amp;percpu_swap_device.lock);
+
 	if (!order &amp;&amp; unlikely(!folio_test_swapcache(folio))) {
 		if (swap_sync_discard())
 			goto again;
@@ -1901,7 +1932,9 @@ swp_entry_t swap_alloc_hibernation_slot(int type)
 			 * Grab the local lock to be compliant
 			 * with swap table allocation.
 			 */
+			local_lock(&amp;percpu_swap_device.lock);
 			offset = cluster_alloc_swap_entry(si, NULL);
+			local_unlock(&amp;percpu_swap_device.lock);
 			if (offset)
 				entry = swp_entry(si-&gt;type, offset);
 		}
@@ -2705,6 +2738,27 @@ static void free_cluster_info(struct swap_cluster_info *cluster_info,
 	kvfree(cluster_info);
 }
 
+/*
+ * Called after swap device&#x27;s reference count is dead, so
+ * neither scan nor allocation will use it.
+ */
+static void flush_percpu_swap_device(struct swap_info_struct *si)
+{
+	int cpu, i;
+	struct swap_info_struct **pcp_si;
+
+	for_each_possible_cpu(cpu) {
+		pcp_si = per_cpu_ptr(percpu_swap_device.si, cpu);
+		/*
+		 * Invalidate the percpu swap device cache, si-&gt;users
+		 * is dead, so no new user will point to it, just flush
+		 * any existing user.
+		 */
+		for (i = 0; i &lt; SWAP_NR_ORDERS; i++)
+			cmpxchg(&amp;pcp_si[i], si, NULL);
+	}
+}
+
 SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)
 {
 	struct swap_info_struct *p = NULL;
@@ -2788,6 +2842,7 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)
 
 	flush_work(&amp;p-&gt;discard_work);
 	flush_work(&amp;p-&gt;reclaim_work);
+	flush_percpu_swap_device(p);
 
 	destroy_swap_extents(p);
 	if (p-&gt;flags &amp; SWP_CONTINUED)
@@ -3222,7 +3277,6 @@ static struct swap_cluster_info *setup_clusters(struct swap_info_struct *si,
 			cluster = per_cpu_ptr(si-&gt;percpu_cluster, cpu);
 			for (i = 0; i &lt; SWAP_NR_ORDERS; i++)
 				cluster-&gt;next[i] = SWAP_ENTRY_INVALID;
-			local_lock_init(&amp;cluster-&gt;lock);
 		}
 	} else {
 		si-&gt;global_cluster = kmalloc(sizeof(*si-&gt;global_cluster),
-- 
2.34.1</pre>
</details>
<div class="review-comment-signals">Signals: acknowledged a fix is needed, agreed to restructure in v2</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Chris Li</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-12">2026-02-12</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer Chris Li noted that the patch series is not too long and suggested reviewing it in its entirety, retracting his previous suggestion to shorten the series by only including tier names.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Just take a quick look at the series. I take that suggestion back.
This series is actually not too long. Adding the tiers name alone does
not add any real value. I actually need to look at the whole series
rather than just the tier name alone.

Chris</pre>
</details>
<div class="review-comment-signals">Signals: retraction of previous suggestion, need for further review</div>
</div>
</div>
</div>

    <footer>LKML Daily Activity Tracker</footer>
    <script>
    // When arriving via a date anchor (e.g. #2026-02-15 from a daily report),
    // scroll the anchor into view after a brief delay so layout is complete.
    (function () {
        var hash = window.location.hash;
        if (!hash) return;
        var target = document.getElementById(hash.slice(1));
        if (!target) return;
        setTimeout(function () {
            target.scrollIntoView({behavior: 'smooth', block: 'start'});
        }, 80);
    })();
    </script>
</body>
</html>