{
  "thread_id": "20260219003911.344478-1-joannelkoong@gmail.com",
  "subject": "[PATCH v1 0/1] iomap: don't mark folio uptodate if read IO has bytes pending",
  "url": "https://lore.kernel.org/all/20260219003911.344478-1-joannelkoong@gmail.com/",
  "dates": {
    "2026-02-19": {
      "report_file": "2026-02-19_ollama_llama3.1-8b.html",
      "developer": "Joanne Koong",
      "reviews": [
        {
          "author": "Matthew Wilcox",
          "summary": "reviewer expressed frustration that the iomap code has become overly complicated, making it difficult to understand or explain how to fix the issue of marking a folio uptodate when read IO has bytes pending",
          "sentiment": "neutral",
          "sentiment_signals": [
            "frustration",
            "difficulty understanding"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "This isn't \"the xor thing has come back to bite us\".  This is \"the iomap\ncode is now too complicated and I cannot figure out how to explain to\nJoanne that there's really a simple way to do this\".\n\nI'm going to have to set aside my current projects and redo the iomap\nreadahead/read_folio code myself, aren't I?",
          "reply_to": "Darrick Wong",
          "message_date": "2026-02-19"
        }
      ],
      "analysis_source": "llm-per-reviewer",
      "patch_summary": "This patch fixes a bug where a folio is incorrectly marked as uptodate if there are still bytes pending from a read IO operation. The issue occurs when the read_folio() function is called on a folio size that is larger than the actual file size, and the post-eof blocks are zeroed and marked uptodate before the pending bytes are subtracted. The patch prevents this by not marking the folio uptodate if there are still bytes pending from the read IO operation."
    },
    "2026-02-20": {
      "report_file": "2026-02-20_ollama_llama3.1-8b.html",
      "developer": "Joanne Koong",
      "reviews": [
        {
          "author": "Joanne Koong (author)",
          "summary": "Author acknowledged that the swapoff path needs to drop the per-vswap spinlock before calling try_to_unmap(), agreed to restructure in v2, but instead provided a link to prior discussion without addressing the specific feedback.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "lack of direct response",
            "link to prior discussion"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "This is the link to the prior discussion\nhttps://lore.kernel.org/linux-fsdevel/20251223223018.3295372-1-sashal@kernel.org/T/#mbd61eaa5fd1e8922caa479720232628e39b8c9da\n\nThanks,\nJoanne",
          "reply_to": "Darrick Wong",
          "message_date": "2026-02-20"
        },
        {
          "author": "Darrick Wong",
          "summary": "Reviewer Darrick Wong noted that the read_bytes_pending field has inconsistent behavior across different IO paths, and suggested consolidating the read code into a single function to simplify the logic.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "suggested improvements"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "<willy and I had a chat; this is a clumsy non-AI summary of it>\n\nI started looking at folio read state management in iomap, and made a\nfew observations that (I hope) match what willy's grumpy about.\n\nThere are three ways that iomap can be reading into the pagecache:\na) async ->readahead,\nb) synchronous ->read_folio (page faults), and\nc) synchronous ->read_folio_range (pagecache write).\n\n(Note that (b) can call a different ->read_folio_range than (c), though\nall implementations seem to have the same function)\n\nAll three of these IO paths share the behavior that they try to fill out\nthe folio's contents and set the corresponding folio/ifs uptodate bits\nif that succeeds.  Folio contents can come from anywhere, whether it's:\n\ni) zeroing memory,\nii) copying from an inlinedata buffer, or\niii) asynchronously fetching the contents from somewhere\n\nIn the case of (c) above, if the read fails then we fail the write, and\nif the read succeeds then we start copying to the pagecache.\n\nHowever, (a) and (b) have this additional read_bytes_pending field in\nthe ifs that implements some extra tracking.  AFAICT the purpose of this\nfield is to ensure that we don't call folio_end_read prematurely if\nthere's an async read in progress.  This can happen if iomap_iter\nreturns a negative errno on a partially processed folio, I think?\n\nread_bytes_pending is initialized to the folio_size() at the start of a\nread and subtracted from when parts of the folio are supplied, whether\nthat's synchronous zeroing or asynchronous read ioend completion.  When\nthe field reaches zero, we can then call folio_end_read().\n\nBut then there are twists, like the fact that we only call\niomap_read_init() to set read_bytes_pending if we decide to do an\nasynchronous read.  Or that iomap_read_end and iomap_finish_folio_read\nhave awfully similar code.  I think in the case of (i) and (ii) we also\ndon't touch read_pending_bytes at all, and merely set the uptodate bits?\n\nThis is confusing to me.  It would be more straightforward (I think) if\nwe just did it for all cases instead of adding more conditionals.  IOWs,\nhow hard would it be to consolidate the read code so that there's one\nfunction that iomap calls when it has filled out part of a folio.  Is\nthat possible, even though we shouldn't be calling folio_end_read during\na pagecache write?\n\nAt the end of the day, however, there's a bug in Linus' tree and we need\nto fix it, so Joanne's patch is a sufficient bandaid until we can go\nclean this up.\n\n--D",
          "reply_to": "Matthew Wilcox",
          "message_date": "2026-02-20"
        }
      ],
      "analysis_source": "llm-per-reviewer",
      "patch_summary": "This patch fixes a bug where a folio is incorrectly marked as uptodate if there are still bytes pending from a read IO operation. The issue occurs when the read_folio() function is called on a folio size that is larger than the actual file size, and the post-eof blocks are zeroed and marked uptodate before the pending bytes are subtracted. The patch prevents this by not marking the folio uptodate if there are still bytes pending from the read IO operation."
    },
    "2026-02-18": {
      "report_file": "2026-02-19_ollama_llama3.1-8b.html",
      "developer": "Joanne Koong",
      "reviews": [
        {
          "author": "Joanne Koong (author)",
          "summary": "The author addressed a concern that marking the folio uptodate in iomap_set_range_uptodate() can be cleared by the XOR semantics used in folio_end_read(). The author agreed to fix this issue by not marking the folio as uptodate if the read IO has bytes pending, and instead setting it uptodate through iomap_end_read()->folio_end_read() in the read completion path.",
          "sentiment": "needs_work",
          "sentiment_signals": [],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "If a folio has ifs metadata attached to it and the folio is partially\nread in through an async IO helper with the rest of it then being read\nin through post-EOF zeroing or as inline data, and the helper\nsuccessfully finishes the read first, then post-EOF zeroing / reading\ninline will mark the folio as uptodate in iomap_set_range_uptodate().\n\nThis is a problem because when the read completion path later calls\niomap_read_end(), it will call folio_end_read(), which sets the uptodate\nbit using XOR semantics. Calling folio_end_read() on a folio that was\nalready marked uptodate clears the uptodate bit.\n\nFix this by not marking the folio as uptodate if the read IO has bytes\npending. The folio uptodate state will be set in the read completion\npath through iomap_end_read() -> folio_end_read().\n\nReported-by: Wei Gao <wegao@suse.com>\nSuggested-by: Sasha Levin <sashal@kernel.org>\nTested-by: Wei Gao <wegao@suse.com>\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nFixes: b2f35ac4146d (\"iomap: add caller-provided callbacks for read and readahead\")\n---\n fs/iomap/buffered-io.c | 15 ++++++++++++---\n 1 file changed, 12 insertions(+), 3 deletions(-)\n\ndiff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\nindex 58887513b894..4fc5ce963feb 100644\n--- a/fs/iomap/buffered-io.c\n+++ b/fs/iomap/buffered-io.c\n@@ -80,18 +80,27 @@ static void iomap_set_range_uptodate(struct folio *folio, size_t off,\n {\n \tstruct iomap_folio_state *ifs = folio->private;\n \tunsigned long flags;\n-\tbool uptodate = true;\n+\tbool mark_uptodate = true;\n \n \tif (folio_test_uptodate(folio))\n \t\treturn;\n \n \tif (ifs) {\n \t\tspin_lock_irqsave(&ifs->state_lock, flags);\n-\t\tuptodate = ifs_set_range_uptodate(folio, ifs, off, len);\n+\t\t/*\n+\t\t * If a read with bytes pending is in progress, we must not call\n+\t\t * folio_mark_uptodate(). The read completion path\n+\t\t * (iomap_read_end()) will call folio_end_read(), which uses XOR\n+\t\t * semantics to set the uptodate bit. If we set it here, the XOR\n+\t\t * in folio_end_read() will clear it, leaving the folio not\n+\t\t * uptodate.\n+\t\t */\n+\t\tmark_uptodate = ifs_set_range_uptodate(folio, ifs, off, len) &&\n+\t\t\t\t!ifs->read_bytes_pending;\n \t\tspin_unlock_irqrestore(&ifs->state_lock, flags);\n \t}\n \n-\tif (uptodate)\n+\tif (mark_uptodate)\n \t\tfolio_mark_uptodate(folio);\n }\n \n-- \n2.47.3",
          "reply_to": "",
          "message_date": "2026-02-18"
        },
        {
          "author": "Darrick Wong",
          "summary": "Reviewer Darrick Wong suggested adding a link to the relevant discussion on linux-fsdevel and CC'ing stable@vger.kernel.org, as the original patch is now in a released kernel.\n\nReviewer Darrick Wong noted that the patch is easy to understand and implement, but requested a test be written for it",
          "sentiment": "neutral",
          "sentiment_signals": [
            "no clear signal"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "I would add:\n\nLink: https://lore.kernel.org/linux-fsdevel/aYbmy8JdgXwsGaPP@autotest-wegao.qe.prg2.suse.org/\nCc: <stable@vger.kernel.org> # v6.19\n\nsince the recent discussion around this was sort of buried in a\ndifferent thread, and the original patch is now in a released kernel.\n\n---\n\nYeah, that makes sense.  How difficult is this to write up as an fstest?\n\nReviewed-by: \"Darrick J. Wong\" <djwong@kernel.org>\n\n--D",
          "reply_to": "Joanne Koong",
          "message_date": "2026-02-18"
        },
        {
          "author": "Darrick Wong",
          "summary": "Reviewer Darrick Wong expressed confusion about an alternative approach mentioned in the patch description, suggesting that he missed a relevant discussion and requested clarification on what this simpler way is.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "confusion",
            "lack of context"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Well you could try explaining to me what that simpler way is?\n\n/me gets the sense he's missing a discussion somewhere...\n\n--D",
          "reply_to": "Matthew Wilcox",
          "message_date": "2026-02-18"
        }
      ],
      "analysis_source": "llm-per-reviewer",
      "patch_summary": "This patch fixes a bug where a folio is incorrectly marked as uptodate if there are still bytes pending from a read IO operation. The issue occurs when the read_folio() function is called on a folio size that is larger than the actual file size, and the post-eof blocks are zeroed and marked uptodate before the pending bytes are subtracted. The patch prevents this by not marking the folio uptodate if there are still bytes pending from the read IO operation."
    },
    "2026-02-23": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Joanne Koong",
      "reviews": [
        {
          "author": "Joanne Koong (author)",
          "summary": "Author acknowledged that the read IO has bytes pending issue also applies to async reads, confirming the problem is more widespread than initially thought.\n\nAuthor clarified that synchronous zeroing does not update read_bytes_pending, explaining the distinction between synchronous and asynchronous read completions.\n\nAuthor Joanne Koong addressed Darrick Wong's concern about consolidating synchronous ->read_folio_range() for buffered writes with the async read logic, explaining that it would add extra overhead and make handling more complicated. She agreed that there are edge cases to consider in the async read path but expressed reservations about manipulating read_bytes_pending from other paths like zeroing and inline reads due to potential race conditions.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledged a broader scope of the issue",
            "clarification",
            "explanation",
            "no clear resolution signal",
            "author expresses reservations"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "b) is async as well. The code for b) and a) are exactly the same (the\nlogic in iomap_read_folio_iter())\n\n---\n\nSynchronous zeroing does not update read_bytes_pending, only async\nread completions do.\n\n---\n\nimo, I don't think the synchronous ->read_folio_range() for buffered\nwrites should be consolidated with the async read logic. If we have\nthe synchronous write path setting read_bytes_pending, that adds extra\noverhead with having to acquire/release the spinlock for every range\nread in. It also makes the handling more complicated (eg now having to\ndifferentiate whether the folio was read in for a read vs. a write).\nSynchronous ->read_folio_range() for buffered writes is extremely\nsimple and self-contained right now and I think it should be kept that\nway.\n\nFor async reads, I agree that there are a bunch of different edge\ncases that arise from i) ii) and iii), and from the fact that a folio\ncould be composed of a mixture of i) ii) and iii).\n\nThe motivation for adding read_bytes_pending was so we could know\nwhich async read finishes last. eg this example scenario: read a 64k\nfolio where the first and last page are not uptodate but everything in\nbetween is\n* ->read_folio_range() for 0 to 4k\n* ->read_folio_range() for 60k to 64k\nThese two async read calls may be two different I/O requests that\ncomplete at different times but only the last finisher should call\nfolio_end_read().\n\nI don't think having the zeroing and inline read paths also\nmanipulating read_bytes_pending helps here. This was discussed a bit\nin [1] but I think it runs into other edge cases / race conditions [2]\nthat would need to be accounted for and makes some paths more\nsuboptimal (eg unnecessary ifs allocations and spinlock acquires). But\nmaybe I'm missing something here and there is a better approach for\ndoing this?\n\nThanks,\nJoanne\n\n[1] https://lore.kernel.org/linux-fsdevel/CAJnrk1YcuhKwbZLo-11=umcTzH_OJ+bdwZq5=XjeJo8gb9e5ig@mail.gmail.com/T/#md09648082a96122ec1e541993872e0c43da5105f\n[2] https://lore.kernel.org/linux-fsdevel/CAJnrk1YcuhKwbZLo-11=umcTzH_OJ+bdwZq5=XjeJo8gb9e5ig@mail.gmail.com/T/#mdc49b649378798fa9e850c9c6914c8c6af5e2895",
          "reply_to": "Darrick Wong",
          "message_date": "2026-02-23"
        }
      ],
      "analysis_source": "llm-per-reviewer",
      "patch_summary": "This patch fixes a bug where a folio is incorrectly marked as uptodate if there are still bytes pending from a read IO operation. The issue occurs when the read_folio() function is called on a folio size that is larger than the actual file size, and the post-eof blocks are zeroed and marked uptodate before the pending bytes are subtracted. The patch prevents this by not marking the folio uptodate if there are still bytes pending from the read IO operation."
    },
    "2026-02-24": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Joanne Koong",
      "reviews": [
        {
          "author": "Christoph Hellwig",
          "summary": "Christoph Hellwig noted that merging the code would be useful, but he hasn't found a good way to do it yet, and expressed concern about the range logic in ->read_folio",
          "sentiment": "neutral",
          "sentiment_signals": [
            "NEEDS_WORK"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Yes.  I've been thinking about that on and off, but unfortunately so far\nI've not come up with a good idea how to merge the code.  Doing so would\nbe very useful for many reasons.\n\nThe problem with that isn't really async vs sync; ->read_folio clearly\nshows you you turn underlying asynchronous logic into a synchronous call.\nIt's really about the range logic, where the writer preparation might\nwant to only read the head and the tail segments of a folio.\n\nBut if we can merge that into the main implementation and have a single\ncore implementation we'd be much better off.\n\nAnyone looking for a \"little\" project? :)",
          "reply_to": "Joanne Koong",
          "message_date": "2026-02-24"
        }
      ],
      "analysis_source": "llm-per-reviewer",
      "patch_summary": "This patch fixes a bug where a folio is incorrectly marked as uptodate if there are still bytes pending from a read IO operation. The issue occurs when the read_folio() function is called on a folio size that is larger than the actual file size, and the post-eof blocks are zeroed and marked uptodate before the pending bytes are subtracted. The patch prevents this by not marking the folio uptodate if there are still bytes pending from the read IO operation."
    }
  }
}