{
  "thread_id": "aZc-8dMBz1XCJI3n@cmpxchg.org",
  "subject": "Re: [LSF/MM/BPF TOPIC] Beyond 2MB: Why Terabyte-Scale Machines Need 1GB Transparent Huge Pages",
  "url": "https://lore.kernel.org/all/aZc-8dMBz1XCJI3n@cmpxchg.org/",
  "dates": {
    "2026-02-19": {
      "report_file": "2026-02-19_ollama_llama3.1-8b.html",
      "developer": "Johannes Weiner",
      "reviews": [
        {
          "author": "David (Arm)",
          "summary": "The reviewer, David from Arm, raised concerns about the proposed solution of migrating all memory away instead of splitting a THP when remapping it to be mapped by PMDs. He thinks this approach has limitations and may not work in certain scenarios.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "considered an interesting approach but with limitations"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "\n> \n> I see 1G THPs being opportunistically used ideally at the start of the application\n> or by the allocator (jemalloc/tcmalloc) when there is plenty of free memory\n> available and a greater chance of getting 1G THPs.\n> \n> Splitting strategy\n> ==================\n> \n> When PUD THP must be break -- for COW after fork, partial munmap, mprotect on\n> a subregion, or reclaim -- it splits directly from PUD to PTE level, converting\n> 1 PUD entry into 262,144 PTE entries. The ideal solution would be to split to\n> PMDs and only the necessary PMDs to PTEs. This is something that would hopefully\n> be possible with Davids proposal [3].\n\nThere once was this proposal where we would, instead of splitting a THP, \nmigrate all memory away instead. That means, instead of splitting the 1 \nGiB THP, you would instead return it to the page allocator where \nsomebody else could use it.\n\nHowever, we cannot easily do the same when remapping a 1 GiB THP to be \nmapped by PMDs etc. I think there are examples where that just doesn't \nwork or is not desired.\n\nBut I considered that in general (avoid folio_split()) an interesting \napproach. The remapping part is a bit different though.\n\n-- \nCheers,\n\nDavid\n\n",
          "reply_to": "Usama Arif"
        },
        {
          "author": "Johannes Weiner",
          "summary": "Johannes Weiner raised concerns about losing TLB coalescing benefits if 2M THPs are split into smaller pages, suggesting a possible lazy migration of huge page splits when larger pages are scarce.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "potential optimization"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Thu, Feb 19, 2026 at 05:00:19PM +0100, David Hildenbrand (Arm) wrote:\n> \n> > \n> > I see 1G THPs being opportunistically used ideally at the start of the application\n> > or by the allocator (jemalloc/tcmalloc) when there is plenty of free memory\n> > available and a greater chance of getting 1G THPs.\n> > \n> > Splitting strategy\n> > ==================\n> > \n> > When PUD THP must be break -- for COW after fork, partial munmap, mprotect on\n> > a subregion, or reclaim -- it splits directly from PUD to PTE level, converting\n> > 1 PUD entry into 262,144 PTE entries. The ideal solution would be to split to\n> > PMDs and only the necessary PMDs to PTEs. This is something that would hopefully\n> > be possible with Davids proposal [3].\n> \n> There once was this proposal where we would, instead of splitting a THP, \n> migrate all memory away instead. That means, instead of splitting the 1 \n> GiB THP, you would instead return it to the page allocator where \n> somebody else could use it.\n\nWith TLB coalescing, there is benefit in preserving contiguity. If you\nlop off the last 4k of a 2M-backed range, a split still gives you 511\ncontiguously mapped pfns that can be coalesced.\n\nIt would be unfortunate to lose that for pure virtual memory splits,\nwhile there is no demand or no shortage of huge pages. But it might be\npossible to do this lazily, e.g. when somebody has trouble getting a\nlarger page, scan the deferred split lists for candidates to migrate.\n\n",
          "reply_to": "David (Arm)"
        },
        {
          "author": "Zi Yan",
          "summary": "The reviewer suggests an alternative approach to implementing 1GB transparent huge pages by using non-uniform splitting of folios and remapping TLB entries, which could be a more reasonable solution than splitting the 1GB PUD itself.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On 19 Feb 2026, at 11:00, David Hildenbrand (Arm) wrote:\n\n>>\n>> I see 1G THPs being opportunistically used ideally at the start of the application\n>> or by the allocator (jemalloc/tcmalloc) when there is plenty of free memory\n>> available and a greater chance of getting 1G THPs.\n>>\n>> Splitting strategy\n>> ==================\n>>\n>> When PUD THP must be break -- for COW after fork, partial munmap, mprotect on\n>> a subregion, or reclaim -- it splits directly from PUD to PTE level, converting\n>> 1 PUD entry into 262,144 PTE entries. The ideal solution would be to split to\n>> PMDs and only the necessary PMDs to PTEs. This is something that would hopefully\n>> be possible with Davids proposal [3].\n\nWith mapping of folios > PMD with PMDs, you can use non uniform split to keep\nafter-split folios as large as possible.\n\n>\n> There once was this proposal where we would, instead of splitting a THP, migrate all memory away instead. That means, instead of splitting the 1 GiB THP, you would instead return it to the page allocator where somebody else could use it.\n\nThis sounds more reasonable than splitting 1GB itself.\n\n>\n> However, we cannot easily do the same when remapping a 1 GiB THP to be mapped by PMDs etc. I think there are examples where that just doesn't work or is not desired.\n>\n> But I considered that in general (avoid folio_split()) an interesting approach. The remapping part is a bit different though.\n\nIf HW can support multiple TLB entries translating to the same physical frame\nand allow translation priority of TLB entries, this remapping would be easy\nand we can still keep the 1GB PUD mapping. Basically, we can have 1GB TLB entry\npointing to the 1GB folio and another 4KB TLB entry pointing to the remapped\nregion and overriding the part in the original 1GB vaddr region.\n\nWithout that, SW will need to split the PUD into PMDs and PTEs.\n\n\nBest Regards,\nYan, Zi\n\n",
          "reply_to": "David (Arm)"
        },
        {
          "author": "Zi Yan",
          "summary": "Reviewer Zi Yan questioned the CPU architecture assumptions in the patch and asked about PMD level ARM contiguous bit support.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "questioning technical assumption",
            "request for clarification"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On 19 Feb 2026, at 11:48, Johannes Weiner wrote:\n\n> On Thu, Feb 19, 2026 at 05:00:19PM +0100, David Hildenbrand (Arm) wrote:\n>>\n>>>\n>>> I see 1G THPs being opportunistically used ideally at the start of the application\n>>> or by the allocator (jemalloc/tcmalloc) when there is plenty of free memory\n>>> available and a greater chance of getting 1G THPs.\n>>>\n>>> Splitting strategy\n>>> ==================\n>>>\n>>> When PUD THP must be break -- for COW after fork, partial munmap, mprotect on\n>>> a subregion, or reclaim -- it splits directly from PUD to PTE level, converting\n>>> 1 PUD entry into 262,144 PTE entries. The ideal solution would be to split to\n>>> PMDs and only the necessary PMDs to PTEs. This is something that would hopefully\n>>> be possible with Davids proposal [3].\n>>\n>> There once was this proposal where we would, instead of splitting a THP,\n>> migrate all memory away instead. That means, instead of splitting the 1\n>> GiB THP, you would instead return it to the page allocator where\n>> somebody else could use it.\n>\n> With TLB coalescing, there is benefit in preserving contiguity. If you\n> lop off the last 4k of a 2M-backed range, a split still gives you 511\n> contiguously mapped pfns that can be coalesced.\n\nWhich CPU are you referring to? AMD\\u2019s PTE coalescing works up to 32KB\nand ARM\\u2019s contig PTE supports larger sizes. BTW, do we have PMD level\nARM contiguous bit support?\n\n>\n> It would be unfortunate to lose that for pure virtual memory splits,\n> while there is no demand or no shortage of huge pages. But it might be\n> possible to do this lazily, e.g. when somebody has trouble getting a\n> larger page, scan the deferred split lists for candidates to migrate.\n\n\nBest Regards,\nYan, Zi\n\n",
          "reply_to": "Johannes Weiner"
        },
        {
          "author": "Johannes Weiner",
          "summary": "Johannes Weiner raised concerns that any coalescing benefits of 1GB THPs would be lost due to the need to scatter them into smaller pagelets, potentially negating their performance advantages.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Thu, Feb 19, 2026 at 11:52:57AM -0500, Zi Yan wrote:\n> On 19 Feb 2026, at 11:48, Johannes Weiner wrote:\n> \n> > On Thu, Feb 19, 2026 at 05:00:19PM +0100, David Hildenbrand (Arm) wrote:\n> >>\n> >>>\n> >>> I see 1G THPs being opportunistically used ideally at the start of the application\n> >>> or by the allocator (jemalloc/tcmalloc) when there is plenty of free memory\n> >>> available and a greater chance of getting 1G THPs.\n> >>>\n> >>> Splitting strategy\n> >>> ==================\n> >>>\n> >>> When PUD THP must be break -- for COW after fork, partial munmap, mprotect on\n> >>> a subregion, or reclaim -- it splits directly from PUD to PTE level, converting\n> >>> 1 PUD entry into 262,144 PTE entries. The ideal solution would be to split to\n> >>> PMDs and only the necessary PMDs to PTEs. This is something that would hopefully\n> >>> be possible with Davids proposal [3].\n> >>\n> >> There once was this proposal where we would, instead of splitting a THP,\n> >> migrate all memory away instead. That means, instead of splitting the 1\n> >> GiB THP, you would instead return it to the page allocator where\n> >> somebody else could use it.\n> >\n> > With TLB coalescing, there is benefit in preserving contiguity. If you\n> > lop off the last 4k of a 2M-backed range, a split still gives you 511\n> > contiguously mapped pfns that can be coalesced.\n> \n> Which CPU are you referring to? AMD\\u2019s PTE coalescing works up to 32KB\n> and ARM\\u2019s contig PTE supports larger sizes. BTW, do we have PMD level\n> ARM contiguous bit support?\n\nI'm not aware of a CPU that will coalesce the 511 entries into a\nsingle one. But *any* coalescing effects will be lost when the range\nis scattered into discontiguous 4k pagelets.\n\n",
          "reply_to": "Zi Yan"
        },
        {
          "author": "David (Arm)",
          "summary": "The reviewer David from Arm agrees that the current implementation of Transparent Huge Pages (THPs) is limited and supports the idea of introducing 1GB THPs, but notes that it's used for hugetlb only so far.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "agreement",
            "acknowledgment"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On 2/19/26 17:52, Zi Yan wrote:\n> On 19 Feb 2026, at 11:48, Johannes Weiner wrote:\n> \n>> On Thu, Feb 19, 2026 at 05:00:19PM +0100, David Hildenbrand (Arm) wrote:\n>>>\n>>>\n>>> There once was this proposal where we would, instead of splitting a THP,\n>>> migrate all memory away instead. That means, instead of splitting the 1\n>>> GiB THP, you would instead return it to the page allocator where\n>>> somebody else could use it.\n>>\n>> With TLB coalescing, there is benefit in preserving contiguity. If you\n>> lop off the last 4k of a 2M-backed range, a split still gives you 511\n>> contiguously mapped pfns that can be coalesced.\n> \n> Which CPU are you referring to? AMD\\u2019s PTE coalescing works up to 32KB\n> and ARM\\u2019s contig PTE supports larger sizes. BTW, do we have PMD level\n> ARM contiguous bit support?\n\nYes. It's used for hugetlb only so far, obviously (np THP > PMD).\n\n-- \nCheers,\n\nDavid\n\n",
          "reply_to": "Zi Yan"
        },
        {
          "author": "David (Arm)",
          "summary": "Reviewer David (Arm) suggested an alternative solution for large memory machines, proposing migration to larger folios instead of implementing 1GB transparent huge pages.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "alternative_solution"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On 2/19/26 18:08, Johannes Weiner wrote:\n> On Thu, Feb 19, 2026 at 11:52:57AM -0500, Zi Yan wrote:\n>> On 19 Feb 2026, at 11:48, Johannes Weiner wrote:\n>>\n>>>\n>>> With TLB coalescing, there is benefit in preserving contiguity. If you\n>>> lop off the last 4k of a 2M-backed range, a split still gives you 511\n>>> contiguously mapped pfns that can be coalesced.\n>>\n>> Which CPU are you referring to? AMD\\u2019s PTE coalescing works up to 32KB\n>> and ARM\\u2019s contig PTE supports larger sizes. BTW, do we have PMD level\n>> ARM contiguous bit support?\n> \n> I'm not aware of a CPU that will coalesce the 511 entries into a\n> single one. But *any* coalescing effects will be lost when the range\n> is scattered into discontiguous 4k pagelets.\n\nYou could of course migrate to larger folios, not necessarily 4k.\n\n-- \nCheers,\n\nDavid\n\n",
          "reply_to": "Johannes Weiner"
        },
        {
          "author": "Matthew Wilcox",
          "summary": "Matthew Wilcox expressed skepticism about the feasibility of 1GB Transparent Huge Pages due to potential hardware limitations.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "skepticism",
            "concern about hardware compatibility"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "On Thu, Feb 19, 2026 at 11:49:27AM -0500, Zi Yan wrote:\n> If HW can support multiple TLB entries translating to the same physical frame\n> and allow translation priority of TLB entries, this remapping would be easy\n> and we can still keep the 1GB PUD mapping. Basically, we can have 1GB TLB entry\n> pointing to the 1GB folio and another 4KB TLB entry pointing to the remapped\n> region and overriding the part in the original 1GB vaddr region.\n\nUh, do you know any hardware that supports that?  Every CPU I'm familiar\nwith has notes suggesting that trying to do this will cause you to Have\nA Very Bad Day.\n\n",
          "reply_to": "Zi Yan"
        },
        {
          "author": "Zi Yan",
          "summary": "The reviewer raised concerns about the performance implications of 1GB transparent huge pages on TLB hits, suggesting that additional translations would be needed for sub-ranges.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "On 19 Feb 2026, at 12:13, Matthew Wilcox wrote:\n\n> On Thu, Feb 19, 2026 at 11:49:27AM -0500, Zi Yan wrote:\n>> If HW can support multiple TLB entries translating to the same physical frame\n>> and allow translation priority of TLB entries, this remapping would be easy\n>> and we can still keep the 1GB PUD mapping. Basically, we can have 1GB TLB entry\n>> pointing to the 1GB folio and another 4KB TLB entry pointing to the remapped\n>> region and overriding the part in the original 1GB vaddr region.\n>\n> Uh, do you know any hardware that supports that?  Every CPU I'm familiar\n> with has notes suggesting that trying to do this will cause you to Have\n> A Very Bad Day.\n\nNo. I was imagining it. :)\n\nBut thinking about it more, that means for every >PTE TLB hit, HW needs to know\nwhether any sub-range has an additional translation. It is easy if all sub-range\ntranslations are present in the TLB. Otherwise, a per sub range bitmap or rewalks\nof each sub range is needed. Never mind, thank you for waking me up in my\ndaydream.\n\nBest Regards,\nYan, Zi\n\n",
          "reply_to": "Matthew Wilcox"
        },
        {
          "author": "Rik Riel",
          "summary": "Rik Riel raised concerns about the long-term implications of 1TB pages and suggested exploring a new approach to physical memory handling, potentially involving movable allocations and CMA balancing.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "suggested alternative direction"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Thu, 2026-02-19 at 15:53 +0000, Usama Arif wrote:\n> \n> Is CMA needed to make this work?\n> ================================\n> \n> The short answer is no. 1G THPs can be gotten without it. CMA can\n> help a lot\n> ofcourse, but we dont *need* it. For e.g. I can run the very simple\n> case of\n> trying to get 1G pages in the upstream kernel without CMA on my\n> server via\n> hugetlb and it works. The server has been up for more than 2 weeks\n> (so pretty\n> fragmented), is running a bunch of stuff in the background, uses 0\n> CMA memory,\n> and I tried to get 100x1G pages on it and it worked.\n> It uses folio_alloc_gigantic, which is exactly what this RFC uses:\n\nWhile I agree with the idea of starting simple, I think\nwe should ask the question of what we want physical memory\nhandling to look like if 1TB pages become more common,\nand applications start to rely on them to meet their\nperformance goals.\n\nWe have CMA balancing code today. It seems to work, but\nit likely is not the long term direction we want to go,\nmostly due to the way CMA does allocations.\n\nIt seems clear that in order to prevent memory fragmentation,\nwe need to split up system memory in some way between an area\nthat is used only for movable allocations, and an area where\nany kind of allocation can go.\n\nThis would need something similar to CMA balancing to prevent\nfalse OOMs for non-movable allocations.\n\nHowever, beyond that I really do not have any idea of what\nthings should look like.\n\nWhat do we want the kernel to do here?\n\n\n-- \nAll Rights Reversed.\n\n",
          "reply_to": "Usama Arif"
        },
        {
          "author": "David (Arm)",
          "summary": "Reviewer David (Arm) suggested exploring alternative approaches to implementing 1GB transparent huge pages, including modifying the buddy system and compaction algorithms, as well as considering sub-zones or separate memory management for large memory systems.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "alternative approaches"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On 2/19/26 20:02, Rik van Riel wrote:\n> On Thu, 2026-02-19 at 15:53 +0000, Usama Arif wrote:\n>>\n>> Is CMA needed to make this work?\n>> ================================\n>>\n>> The short answer is no. 1G THPs can be gotten without it. CMA can\n>> help a lot\n>> ofcourse, but we dont *need* it. For e.g. I can run the very simple\n>> case of\n>> trying to get 1G pages in the upstream kernel without CMA on my\n>> server via\n>> hugetlb and it works. The server has been up for more than 2 weeks\n>> (so pretty\n>> fragmented), is running a bunch of stuff in the background, uses 0\n>> CMA memory,\n>> and I tried to get 100x1G pages on it and it worked.\n>> It uses folio_alloc_gigantic, which is exactly what this RFC uses:\n> \n> While I agree with the idea of starting simple, I think\n> we should ask the question of what we want physical memory\n> handling to look like if 1TB pages become more common,\n> and applications start to rely on them to meet their\n> performance goals.\n> \n> We have CMA balancing code today. It seems to work, but\n> it likely is not the long term direction we want to go,\n> mostly due to the way CMA does allocations.\n> \n> It seems clear that in order to prevent memory fragmentation,\n> we need to split up system memory in some way between an area\n> that is used only for movable allocations, and an area where\n> any kind of allocation can go.\n> \n> This would need something similar to CMA balancing to prevent\n> false OOMs for non-movable allocations.\n> \n> However, beyond that I really do not have any idea of what\n> things should look like.\n> \n> What do we want the kernel to do here?\n\nThis subtopic is certainly worth a separate session as it's quite \ninvolved, but I assume the right (tm) thing to do will be\n\n(a) Teaching the buddy to manage pages larger than the current maximum\n     buddy order. There will certainly be some work required to get to\n     that point (and Zi Yan already did some work). It might also be\n     fair to say that order > current  buddy order might behave different\n     at least to some degree (thinking about relation to zone alignment,\n     section sizes etc).\n\n     If we require vmemmap for these larger orders, maybe the buddy order\n     could more easily exceed the section size; I don't remember all of\n     the details why that limitation was in place (but one of them was\n     memmap continuity within a high-order buddy page, which is only\n     guaranteed within a memory section with CONFIG_SPARSEMEM).\n\n(b) Teaching compaction etc. to *also* compact/group on a larger\n     granularity (in addition to current sized pageblocks). When we\n     discussed that in the past we used the term superblock, that\n     Zi Yan just brought up again in another thread [1].\n\n\n\nThere was a proposal a while ago to internally separate zones into \nchunks of memory (I think the proposal used DRAM banks, such that you \ncould more easily power down unused DRAM banks). I'm not saying we \nshould do that, but maybe something like sub-zones could be something to \nexplore. Maybe not.\n\nBig, more complex topic :)\n\n\n[1] \nhttps://lore.kernel.org/r/34730030-48F6-4D0C-91EA-998A5AF93F5F@nvidia.com\n\n-- \nCheers,\n\nDavid\n\n\n",
          "reply_to": "Rik Riel"
        }
      ],
      "analysis_source": "llm"
    }
  }
}