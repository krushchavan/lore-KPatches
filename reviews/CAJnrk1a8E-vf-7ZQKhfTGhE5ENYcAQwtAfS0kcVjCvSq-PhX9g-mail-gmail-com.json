{
  "thread_id": "CAJnrk1a8E-vf_7ZQKhfTGhE5ENYcAQwtAfS0kcVjCvSq=PhX9g@mail.gmail.com",
  "subject": "Re: [RFC PATCH 1/1] iomap: fix race between iomap_set_range_uptodate and folio_end_read",
  "url": "https://lore.kernel.org/all/CAJnrk1a8E-vf_7ZQKhfTGhE5ENYcAQwtAfS0kcVjCvSq=PhX9g@mail.gmail.com/",
  "dates": {
    "2026-02-13": {
      "report_file": "2026-02-13_ollama_llama3.1-8b.html",
      "developer": "Joanne Koong",
      "reviews": [
        {
          "author": "Joanne Koong (author)",
          "summary": "The reviewer Joanne Koong requested changes to the patch, specifically pointing out a race condition between iomap_set_range_uptodate and folio_end_read.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "race condition",
            "requested changes"
          ],
          "has_inline_review": true,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Move the iomap_readpage_iter() bio read logic into a separate helper\nfunction, iomap_bio_read_folio_range(). This is needed to make iomap\nread/readahead more generically usable, especially for filesystems that\ndo not require CONFIG_BLOCK.\n\nAdditionally rename buffered write's iomap_read_folio_range() function\nto iomap_bio_read_folio_range_sync() to better describe its synchronous\nbehavior.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nReviewed-by: \"Darrick J. Wong\" <djwong@kernel.org>\n---\n fs/iomap/buffered-io.c | 68 ++++++++++++++++++++++++------------------\n 1 file changed, 39 insertions(+), 29 deletions(-)\n\ndiff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\nindex 9535733ed07a..7e65075b6345 100644\n--- a/fs/iomap/buffered-io.c\n+++ b/fs/iomap/buffered-io.c\n@@ -367,36 +367,15 @@ struct iomap_readpage_ctx {\n \tstruct readahead_control *rac;\n };\n \n-static int iomap_readpage_iter(struct iomap_iter *iter,\n-\t\tstruct iomap_readpage_ctx *ctx)\n+static void iomap_bio_read_folio_range(const struct iomap_iter *iter,\n+\t\tstruct iomap_readpage_ctx *ctx, loff_t pos, size_t plen)\n {\n+\tstruct folio *folio = ctx->cur_folio;\n \tconst struct iomap *iomap = &iter->iomap;\n-\tloff_t pos = iter->pos;\n+\tstruct iomap_folio_state *ifs = folio->private;\n+\tsize_t poff = offset_in_folio(folio, pos);\n \tloff_t length = iomap_length(iter);\n-\tstruct folio *folio = ctx->cur_folio;\n-\tstruct iomap_folio_state *ifs;\n-\tsize_t poff, plen;\n \tsector_t sector;\n-\tint ret;\n-\n-\tif (iomap->type == IOMAP_INLINE) {\n-\t\tret = iomap_read_inline_data(iter, folio);\n-\t\tif (ret)\n-\t\t\treturn ret;\n-\t\treturn iomap_iter_advance(iter, length);\n-\t}\n-\n-\t/* zero post-eof blocks as the page may be mapped */\n-\tifs = ifs_alloc(iter->inode, folio, iter->flags);\n-\tiomap_adjust_read_range(iter->inode, folio, &pos, length, &poff, &plen);\n-\tif (plen == 0)\n-\t\tgoto done;\n-\n-\tif (iomap_block_needs_zeroing(iter, pos)) {\n-\t\tfolio_zero_range(folio, poff, plen);\n-\t\tiomap_set_range_uptodate(folio, poff, plen);\n-\t\tgoto done;\n-\t}\n \n \tctx->cur_folio_in_bio = true;\n \tif (ifs) {\n@@ -435,6 +414,37 @@ static int iomap_readpage_iter(struct iomap_iter *iter,\n \t\tctx->bio->bi_end_io = iomap_read_end_io;\n \t\tbio_add_folio_nofail(ctx->bio, folio, plen, poff);\n \t}\n+}\n+\n+static int iomap_readpage_iter(struct iomap_iter *iter,\n+\t\tstruct iomap_readpage_ctx *ctx)\n+{\n+\tconst struct iomap *iomap = &iter->iomap;\n+\tloff_t pos = iter->pos;\n+\tloff_t length = iomap_length(iter);\n+\tstruct folio *folio = ctx->cur_folio;\n+\tsize_t poff, plen;\n+\tint ret;\n+\n+\tif (iomap->type == IOMAP_INLINE) {\n+\t\tret = iomap_read_inline_data(iter, folio);\n+\t\tif (ret)\n+\t\t\treturn ret;\n+\t\treturn iomap_iter_advance(iter, length);\n+\t}\n+\n+\t/* zero post-eof blocks as the page may be mapped */\n+\tifs_alloc(iter->inode, folio, iter->flags);\n+\tiomap_adjust_read_range(iter->inode, folio, &pos, length, &poff, &plen);\n+\tif (plen == 0)\n+\t\tgoto done;\n+\n+\tif (iomap_block_needs_zeroing(iter, pos)) {\n+\t\tfolio_zero_range(folio, poff, plen);\n+\t\tiomap_set_range_uptodate(folio, poff, plen);\n+\t} else {\n+\t\tiomap_bio_read_folio_range(iter, ctx, pos, plen);\n+\t}\n \n done:\n \t/*\n@@ -559,7 +569,7 @@ void iomap_readahead(struct readahead_control *rac, const struct iomap_ops *ops)\n }\n EXPORT_SYMBOL_GPL(iomap_readahead);\n \n-static int iomap_read_folio_range(const struct iomap_iter *iter,\n+static int iomap_bio_read_folio_range_sync(const struct iomap_iter *iter,\n \t\tstruct folio *folio, loff_t pos, size_t len)\n {\n \tconst struct iomap *srcmap = iomap_iter_srcmap(iter);\n@@ -572,7 +582,7 @@ static int iomap_read_folio_range(const struct iomap_iter *iter,\n \treturn submit_bio_wait(&bio);\n }\n #else\n-static int iomap_read_folio_range(const struct iomap_iter *iter,\n+static int iomap_bio_read_folio_range_sync(const struct iomap_iter *iter,\n \t\tstruct folio *folio, loff_t pos, size_t len)\n {\n \tWARN_ON_ONCE(1);\n@@ -749,7 +759,7 @@ static int __iomap_write_begin(const struct iomap_iter *iter,\n \t\t\t\tstatus = write_ops->read_folio_range(iter,\n \t\t\t\t\t\tfolio, block_start, plen);\n \t\t\telse\n-\t\t\t\tstatus = iomap_read_folio_range(iter,\n+\t\t\t\tstatus = iomap_bio_read_folio_range_sync(iter,\n \t\t\t\t\t\tfolio, block_start, plen);\n \t\t\tif (status)\n \t\t\t\treturn status;\n-- \n2.47.3\n\n\n\n---\n\nMove the read/readahead bio submission logic into a separate helper.\nThis is needed to make iomap read/readahead more generically usable,\nespecially for filesystems that do not require CONFIG_BLOCK.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nReviewed-by: \"Darrick J. Wong\" <djwong@kernel.org>\n---\n fs/iomap/buffered-io.c | 30 ++++++++++++++++--------------\n 1 file changed, 16 insertions(+), 14 deletions(-)\n\ndiff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\nindex 7e65075b6345..f8b985bb5a6b 100644\n--- a/fs/iomap/buffered-io.c\n+++ b/fs/iomap/buffered-io.c\n@@ -367,6 +367,14 @@ struct iomap_readpage_ctx {\n \tstruct readahead_control *rac;\n };\n \n+static void iomap_bio_submit_read(struct iomap_readpage_ctx *ctx)\n+{\n+\tstruct bio *bio = ctx->bio;\n+\n+\tif (bio)\n+\t\tsubmit_bio(bio);\n+}\n+\n static void iomap_bio_read_folio_range(const struct iomap_iter *iter,\n \t\tstruct iomap_readpage_ctx *ctx, loff_t pos, size_t plen)\n {\n@@ -392,8 +400,7 @@ static void iomap_bio_read_folio_range(const struct iomap_iter *iter,\n \t\tgfp_t orig_gfp = gfp;\n \t\tunsigned int nr_vecs = DIV_ROUND_UP(length, PAGE_SIZE);\n \n-\t\tif (ctx->bio)\n-\t\t\tsubmit_bio(ctx->bio);\n+\t\tiomap_bio_submit_read(ctx);\n \n \t\tif (ctx->rac) /* same as readahead_gfp_mask */\n \t\t\tgfp |= __GFP_NORETRY | __GFP_NOWARN;\n@@ -488,13 +495,10 @@ int iomap_read_folio(struct folio *folio, const struct iomap_ops *ops)\n \twhile ((ret = iomap_iter(&iter, ops)) > 0)\n \t\titer.status = iomap_read_folio_iter(&iter, &ctx);\n \n-\tif (ctx.bio) {\n-\t\tsubmit_bio(ctx.bio);\n-\t\tWARN_ON_ONCE(!ctx.cur_folio_in_bio);\n-\t} else {\n-\t\tWARN_ON_ONCE(ctx.cur_folio_in_bio);\n+\tiomap_bio_submit_read(&ctx);\n+\n+\tif (!ctx.cur_folio_in_bio)\n \t\tfolio_unlock(folio);\n-\t}\n \n \t/*\n \t * Just like mpage_readahead and block_read_full_folio, we always\n@@ -560,12 +564,10 @@ void iomap_readahead(struct readahead_control *rac, const struct iomap_ops *ops)\n \twhile (iomap_iter(&iter, ops) > 0)\n \t\titer.status = iomap_readahead_iter(&iter, &ctx);\n \n-\tif (ctx.bio)\n-\t\tsubmit_bio(ctx.bio);\n-\tif (ctx.cur_folio) {\n-\t\tif (!ctx.cur_folio_in_bio)\n-\t\t\tfolio_unlock(ctx.cur_folio);\n-\t}\n+\tiomap_bio_submit_read(&ctx);\n+\n+\tif (ctx.cur_folio && !ctx.cur_folio_in_bio)\n+\t\tfolio_unlock(ctx.cur_folio);\n }\n EXPORT_SYMBOL_GPL(iomap_readahead);\n \n-- \n2.47.3\n\n\n\n---\n\nStore the iomap_readpage_ctx bio generically as a \"void *read_ctx\".\nThis makes the read/readahead interface more generic, which allows it to\nbe used by filesystems that may not be block-based and may not have\nCONFIG_BLOCK set.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nReviewed-by: \"Darrick J. Wong\" <djwong@kernel.org>\n---\n fs/iomap/buffered-io.c | 29 ++++++++++++++---------------\n 1 file changed, 14 insertions(+), 15 deletions(-)\n\ndiff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\nindex f8b985bb5a6b..b06b532033ad 100644\n--- a/fs/iomap/buffered-io.c\n+++ b/fs/iomap/buffered-io.c\n@@ -363,13 +363,13 @@ static void iomap_read_end_io(struct bio *bio)\n struct iomap_readpage_ctx {\n \tstruct folio\t\t*cur_folio;\n \tbool\t\t\tcur_folio_in_bio;\n-\tstruct bio\t\t*bio;\n+\tvoid\t\t\t*read_ctx;\n \tstruct readahead_control *rac;\n };\n \n static void iomap_bio_submit_read(struct iomap_readpage_ctx *ctx)\n {\n-\tstruct bio *bio = ctx->bio;\n+\tstruct bio *bio = ctx->read_ctx;\n \n \tif (bio)\n \t\tsubmit_bio(bio);\n@@ -384,6 +384,7 @@ static void iomap_bio_read_folio_range(const struct iomap_iter *iter,\n \tsize_t poff = offset_in_folio(folio, pos);\n \tloff_t length = iomap_length(iter);\n \tsector_t sector;\n+\tstruct bio *bio = ctx->read_ctx;\n \n \tctx->cur_folio_in_bio = true;\n \tif (ifs) {\n@@ -393,9 +394,8 @@ static void iomap_bio_read_folio_range(const struct iomap_iter *iter,\n \t}\n \n \tsector = iomap_sector(iomap, pos);\n-\tif (!ctx->bio ||\n-\t    bio_end_sector(ctx->bio) != sector ||\n-\t    !bio_add_folio(ctx->bio, folio, plen, poff)) {\n+\tif (!bio || bio_end_sector(bio) != sector ||\n+\t    !bio_add_folio(bio, folio, plen, poff)) {\n \t\tgfp_t gfp = mapping_gfp_constraint(folio->mapping, GFP_KERNEL);\n \t\tgfp_t orig_gfp = gfp;\n \t\tunsigned int nr_vecs = DIV_ROUND_UP(length, PAGE_SIZE);\n@@ -404,22 +404,21 @@ static void iomap_bio_read_folio_range(const struct iomap_iter *iter,\n \n \t\tif (ctx->rac) /* same as readahead_gfp_mask */\n \t\t\tgfp |= __GFP_NORETRY | __GFP_NOWARN;\n-\t\tctx->bio = bio_alloc(iomap->bdev, bio_max_segs(nr_vecs),\n-\t\t\t\t     REQ_OP_READ, gfp);\n+\t\tbio = bio_alloc(iomap->bdev, bio_max_segs(nr_vecs), REQ_OP_READ,\n+\t\t\t\t     gfp);\n \t\t/*\n \t\t * If the bio_alloc fails, try it again for a single page to\n \t\t * avoid having to deal with partial page reads.  This emulates\n \t\t * what do_mpage_read_folio does.\n \t\t */\n-\t\tif (!ctx->bio) {\n-\t\t\tctx->bio = bio_alloc(iomap->bdev, 1, REQ_OP_READ,\n-\t\t\t\t\t     orig_gfp);\n-\t\t}\n+\t\tif (!bio)\n+\t\t\tbio = bio_alloc(iomap->bdev, 1, REQ_OP_READ, orig_gfp);\n \t\tif (ctx->rac)\n-\t\t\tctx->bio->bi_opf |= REQ_RAHEAD;\n-\t\tctx->bio->bi_iter.bi_sector = sector;\n-\t\tctx->bio->bi_end_io = iomap_read_end_io;\n-\t\tbio_add_folio_nofail(ctx->bio, folio, plen, poff);\n+\t\t\tbio->bi_opf |= REQ_RAHEAD;\n+\t\tbio->bi_iter.bi_sector = sector;\n+\t\tbio->bi_end_io = iomap_read_end_io;\n+\t\tbio_add_folio_nofail(bio, folio, plen, poff);\n+\t\tctx->read_ctx = bio;\n \t}\n }\n \n-- \n2.47.3\n\n\n\n---\n\nIterate over all non-uptodate ranges of a folio mapping in a single call\nto iomap_readpage_iter() instead of leaving the partial iteration to the\ncaller.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nReviewed-by: Christoph Hellwig <hch@lst.de>\nReviewed-by: \"Darrick J. Wong\" <djwong@kernel.org>\n---\n fs/iomap/buffered-io.c | 53 ++++++++++++++++++++----------------------\n 1 file changed, 25 insertions(+), 28 deletions(-)\n\ndiff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\nindex b06b532033ad..dbe5783ee68c 100644\n--- a/fs/iomap/buffered-io.c\n+++ b/fs/iomap/buffered-io.c\n@@ -430,6 +430,7 @@ static int iomap_readpage_iter(struct iomap_iter *iter,\n \tloff_t length = iomap_length(iter);\n \tstruct folio *folio = ctx->cur_folio;\n \tsize_t poff, plen;\n+\tloff_t count;\n \tint ret;\n \n \tif (iomap->type == IOMAP_INLINE) {\n@@ -439,41 +440,35 @@ static int iomap_readpage_iter(struct iomap_iter *iter,\n \t\treturn iomap_iter_advance(iter, length);\n \t}\n \n-\t/* zero post-eof blocks as the page may be mapped */\n \tifs_alloc(iter->inode, folio, iter->flags);\n-\tiomap_adjust_read_range(iter->inode, folio, &pos, length, &poff, &plen);\n-\tif (plen == 0)\n-\t\tgoto done;\n \n-\tif (iomap_block_needs_zeroing(iter, pos)) {\n-\t\tfolio_zero_range(folio, poff, plen);\n-\t\tiomap_set_range_uptodate(folio, poff, plen);\n-\t} else {\n-\t\tiomap_bio_read_folio_range(iter, ctx, pos, plen);\n-\t}\n+\tlength = min_t(loff_t, length,\n+\t\t\tfolio_size(folio) - offset_in_folio(folio, pos));\n+\twhile (length) {\n+\t\tiomap_adjust_read_range(iter->inode, folio, &pos, length, &poff,\n+\t\t\t\t&plen);\n \n-done:\n-\t/*\n-\t * Move the caller beyond our range so that it keeps making progress.\n-\t * For that, we have to include any leading non-uptodate ranges, but\n-\t * we can skip trailing ones as they will be handled in the next\n-\t * iteration.\n-\t */\n-\tlength = pos - iter->pos + plen;\n-\treturn iomap_iter_advance(iter, length);\n-}\n+\t\tcount = pos - iter->pos + plen;\n+\t\tif (WARN_ON_ONCE(count > length))\n+\t\t\treturn -EIO;\n \n-static int iomap_read_folio_iter(struct iomap_iter *iter,\n-\t\tstruct iomap_readpage_ctx *ctx)\n-{\n-\tint ret;\n+\t\tif (plen == 0)\n+\t\t\treturn iomap_iter_advance(iter, count);\n \n-\twhile (iomap_length(iter)) {\n-\t\tret = iomap_readpage_iter(iter, ctx);\n+\t\t/* zero post-eof blocks as the page may be mapped */\n+\t\tif (iomap_block_needs_zeroing(iter, pos)) {\n+\t\t\tfolio_zero_range(folio, poff, plen);\n+\t\t\tiomap_set_range_uptodate(folio, poff, plen);\n+\t\t} else {\n+\t\t\tiomap_bio_read_folio_range(iter, ctx, pos, plen);\n+\t\t}\n+\n+\t\tret = iomap_iter_advance(iter, count);\n \t\tif (ret)\n \t\t\treturn ret;\n+\t\tlength -= count;\n+\t\tpos = iter->pos;\n \t}\n-\n \treturn 0;\n }\n \n@@ -492,7 +487,7 @@ int iomap_read_folio(struct folio *folio, const struct iomap_ops *ops)\n \ttrace_iomap_readpage(iter.inode, 1);\n \n \twhile ((ret = iomap_iter(&iter, ops)) > 0)\n-\t\titer.status = iomap_read_folio_iter(&iter, &ctx);\n+\t\titer.status = iomap_readpage_iter(&iter, &ctx);\n \n \tiomap_bio_submit_read(&ctx);\n \n@@ -522,6 +517,8 @@ static int iomap_readahead_iter(struct iomap_iter *iter,\n \t\t}\n \t\tif (!ctx->cur_folio) {\n \t\t\tctx->cur_folio = readahead_folio(ctx->rac);\n+\t\t\tif (WARN_ON_ONCE(!ctx->cur_folio))\n+\t\t\t\treturn -EINVAL;\n \t\t\tctx->cur_folio_in_bio = false;\n \t\t}\n \t\tret = iomap_readpage_iter(iter, ctx);\n-- \n2.47.3\n\n\n\n---\n\n->readpage was deprecated and reads are now on folios.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nReviewed-by: \"Darrick J. Wong\" <djwong@kernel.org>\nReviewed-by: Christoph Hellwig <hch@lst.de>\n---\n fs/iomap/buffered-io.c | 6 +++---\n 1 file changed, 3 insertions(+), 3 deletions(-)\n\ndiff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\nindex dbe5783ee68c..23601373573e 100644\n--- a/fs/iomap/buffered-io.c\n+++ b/fs/iomap/buffered-io.c\n@@ -422,7 +422,7 @@ static void iomap_bio_read_folio_range(const struct iomap_iter *iter,\n \t}\n }\n \n-static int iomap_readpage_iter(struct iomap_iter *iter,\n+static int iomap_read_folio_iter(struct iomap_iter *iter,\n \t\tstruct iomap_readpage_ctx *ctx)\n {\n \tconst struct iomap *iomap = &iter->iomap;\n@@ -487,7 +487,7 @@ int iomap_read_folio(struct folio *folio, const struct iomap_ops *ops)\n \ttrace_iomap_readpage(iter.inode, 1);\n \n \twhile ((ret = iomap_iter(&iter, ops)) > 0)\n-\t\titer.status = iomap_readpage_iter(&iter, &ctx);\n+\t\titer.status = iomap_read_folio_iter(&iter, &ctx);\n \n \tiomap_bio_submit_read(&ctx);\n \n@@ -521,7 +521,7 @@ static int iomap_readahead_iter(struct iomap_iter *iter,\n \t\t\t\treturn -EINVAL;\n \t\t\tctx->cur_folio_in_bio = false;\n \t\t}\n-\t\tret = iomap_readpage_iter(iter, ctx);\n+\t\tret = iomap_read_folio_iter(iter, ctx);\n \t\tif (ret)\n \t\t\treturn ret;\n \t}\n-- \n2.47.3\n\n\n\n---\n\n->readpage was deprecated and reads are now on folios.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nReviewed-by: \"Darrick J. Wong\" <djwong@kernel.org>\nReviewed-by: Christoph Hellwig <hch@lst.de>\n---\n fs/iomap/buffered-io.c | 14 +++++++-------\n 1 file changed, 7 insertions(+), 7 deletions(-)\n\ndiff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\nindex 23601373573e..09e65771a947 100644\n--- a/fs/iomap/buffered-io.c\n+++ b/fs/iomap/buffered-io.c\n@@ -360,14 +360,14 @@ static void iomap_read_end_io(struct bio *bio)\n \tbio_put(bio);\n }\n \n-struct iomap_readpage_ctx {\n+struct iomap_read_folio_ctx {\n \tstruct folio\t\t*cur_folio;\n \tbool\t\t\tcur_folio_in_bio;\n \tvoid\t\t\t*read_ctx;\n \tstruct readahead_control *rac;\n };\n \n-static void iomap_bio_submit_read(struct iomap_readpage_ctx *ctx)\n+static void iomap_bio_submit_read(struct iomap_read_folio_ctx *ctx)\n {\n \tstruct bio *bio = ctx->read_ctx;\n \n@@ -376,7 +376,7 @@ static void iomap_bio_submit_read(struct iomap_readpage_ctx *ctx)\n }\n \n static void iomap_bio_read_folio_range(const struct iomap_iter *iter,\n-\t\tstruct iomap_readpage_ctx *ctx, loff_t pos, size_t plen)\n+\t\tstruct iomap_read_folio_ctx *ctx, loff_t pos, size_t plen)\n {\n \tstruct folio *folio = ctx->cur_folio;\n \tconst struct iomap *iomap = &iter->iomap;\n@@ -423,7 +423,7 @@ static void iomap_bio_read_folio_range(const struct iomap_iter *iter,\n }\n \n static int iomap_read_folio_iter(struct iomap_iter *iter,\n-\t\tstruct iomap_readpage_ctx *ctx)\n+\t\tstruct iomap_read_folio_ctx *ctx)\n {\n \tconst struct iomap *iomap = &iter->iomap;\n \tloff_t pos = iter->pos;\n@@ -479,7 +479,7 @@ int iomap_read_folio(struct folio *folio, const struct iomap_ops *ops)\n \t\t.pos\t\t= folio_pos(folio),\n \t\t.len\t\t= folio_size(folio),\n \t};\n-\tstruct iomap_readpage_ctx ctx = {\n+\tstruct iomap_read_folio_ctx ctx = {\n \t\t.cur_folio\t= folio,\n \t};\n \tint ret;\n@@ -504,7 +504,7 @@ int iomap_read_folio(struct folio *folio, const struct iomap_ops *ops)\n EXPORT_SYMBOL_GPL(iomap_read_folio);\n \n static int iomap_readahead_iter(struct iomap_iter *iter,\n-\t\tstruct iomap_readpage_ctx *ctx)\n+\t\tstruct iomap_read_folio_ctx *ctx)\n {\n \tint ret;\n \n@@ -551,7 +551,7 @@ void iomap_readahead(struct readahead_control *rac, const struct iomap_ops *ops)\n \t\t.pos\t= readahead_pos(rac),\n \t\t.len\t= readahead_length(rac),\n \t};\n-\tstruct iomap_readpage_ctx ctx = {\n+\tstruct iomap_read_folio_ctx ctx = {\n \t\t.rac\t= rac,\n \t};\n \n-- \n2.47.3\n\n\n\n---\n\nInstead of incrementing read_bytes_pending for every folio range read in\n(which requires acquiring the spinlock to do so), set read_bytes_pending\nto the folio size when the first range is asynchronously read in, keep\ntrack of how many bytes total are asynchronously read in, and adjust\nread_bytes_pending accordingly after issuing requests to read in all the\nnecessary ranges.\n\niomap_read_folio_ctx->cur_folio_in_bio can be removed since a non-zero\nvalue for pending bytes necessarily indicates the folio is in the bio.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nSuggested-by: \"Darrick J. Wong\" <djwong@kernel.org>\n---\n fs/iomap/buffered-io.c | 87 ++++++++++++++++++++++++++++++++----------\n 1 file changed, 66 insertions(+), 21 deletions(-)\n\ndiff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\nindex 09e65771a947..4e6258fdb915 100644\n--- a/fs/iomap/buffered-io.c\n+++ b/fs/iomap/buffered-io.c\n@@ -362,7 +362,6 @@ static void iomap_read_end_io(struct bio *bio)\n \n struct iomap_read_folio_ctx {\n \tstruct folio\t\t*cur_folio;\n-\tbool\t\t\tcur_folio_in_bio;\n \tvoid\t\t\t*read_ctx;\n \tstruct readahead_control *rac;\n };\n@@ -380,19 +379,11 @@ static void iomap_bio_read_folio_range(const struct iomap_iter *iter,\n {\n \tstruct folio *folio = ctx->cur_folio;\n \tconst struct iomap *iomap = &iter->iomap;\n-\tstruct iomap_folio_state *ifs = folio->private;\n \tsize_t poff = offset_in_folio(folio, pos);\n \tloff_t length = iomap_length(iter);\n \tsector_t sector;\n \tstruct bio *bio = ctx->read_ctx;\n \n-\tctx->cur_folio_in_bio = true;\n-\tif (ifs) {\n-\t\tspin_lock_irq(&ifs->state_lock);\n-\t\tifs->read_bytes_pending += plen;\n-\t\tspin_unlock_irq(&ifs->state_lock);\n-\t}\n-\n \tsector = iomap_sector(iomap, pos);\n \tif (!bio || bio_end_sector(bio) != sector ||\n \t    !bio_add_folio(bio, folio, plen, poff)) {\n@@ -422,8 +413,57 @@ static void iomap_bio_read_folio_range(const struct iomap_iter *iter,\n \t}\n }\n \n+static void iomap_read_init(struct folio *folio)\n+{\n+\tstruct iomap_folio_state *ifs = folio->private;\n+\n+\tif (ifs) {\n+\t\tsize_t len = folio_size(folio);\n+\n+\t\tspin_lock_irq(&ifs->state_lock);\n+\t\tifs->read_bytes_pending += len;\n+\t\tspin_unlock_irq(&ifs->state_lock);\n+\t}\n+}\n+\n+static void iomap_read_end(struct folio *folio, size_t bytes_pending)\n+{\n+\tstruct iomap_folio_state *ifs;\n+\n+\t/*\n+\t * If there are no bytes pending, this means we are responsible for\n+\t * unlocking the folio here, since no IO helper has taken ownership of\n+\t * it.\n+\t */\n+\tif (!bytes_pending) {\n+\t\tfolio_unlock(folio);\n+\t\treturn;\n+\t}\n+\n+\tifs = folio->private;\n+\tif (ifs) {\n+\t\tbool end_read, uptodate;\n+\t\tsize_t bytes_accounted = folio_size(folio) - bytes_pending;\n+\n+\t\tspin_lock_irq(&ifs->state_lock);\n+\t\tifs->read_bytes_pending -= bytes_accounted;\n+\t\t/*\n+\t\t * If !ifs->read_bytes_pending, this means all pending reads\n+\t\t * by the IO helper have already completed, which means we need\n+\t\t * to end the folio read here. If ifs->read_bytes_pending != 0,\n+\t\t * the IO helper will end the folio read.\n+\t\t */\n+\t\tend_read = !ifs->read_bytes_pending;\n+\t\tif (end_read)\n+\t\t\tuptodate = ifs_is_fully_uptodate(folio, ifs);\n+\t\tspin_unlock_irq(&ifs->state_lock);\n+\t\tif (end_read)\n+\t\t\tfolio_end_read(folio, uptodate);\n+\t}\n+}\n+\n static int iomap_read_folio_iter(struct iomap_iter *iter,\n-\t\tstruct iomap_read_folio_ctx *ctx)\n+\t\tstruct iomap_read_folio_ctx *ctx, size_t *bytes_pending)\n {\n \tconst struct iomap *iomap = &iter->iomap;\n \tloff_t pos = iter->pos;\n@@ -460,6 +500,9 @@ static int iomap_read_folio_iter(struct iomap_iter *iter,\n \t\t\tfolio_zero_range(folio, poff, plen);\n \t\t\tiomap_set_range_uptodate(folio, poff, plen);\n \t\t} else {\n+\t\t\tif (!*bytes_pending)\n+\t\t\t\tiomap_read_init(folio);\n+\t\t\t*bytes_pending += plen;\n \t\t\tiomap_bio_read_folio_range(iter, ctx, pos, plen);\n \t\t}\n \n@@ -482,17 +525,18 @@ int iomap_read_folio(struct folio *folio, const struct iomap_ops *ops)\n \tstruct iomap_read_folio_ctx ctx = {\n \t\t.cur_folio\t= folio,\n \t};\n+\tsize_t bytes_pending = 0;\n \tint ret;\n \n \ttrace_iomap_readpage(iter.inode, 1);\n \n \twhile ((ret = iomap_iter(&iter, ops)) > 0)\n-\t\titer.status = iomap_read_folio_iter(&iter, &ctx);\n+\t\titer.status = iomap_read_folio_iter(&iter, &ctx,\n+\t\t\t\t&bytes_pending);\n \n \tiomap_bio_submit_read(&ctx);\n \n-\tif (!ctx.cur_folio_in_bio)\n-\t\tfolio_unlock(folio);\n+\tiomap_read_end(folio, bytes_pending);\n \n \t/*\n \t * Just like mpage_readahead and block_read_full_folio, we always\n@@ -504,24 +548,23 @@ int iomap_read_folio(struct folio *folio, const struct iomap_ops *ops)\n EXPORT_SYMBOL_GPL(iomap_read_folio);\n \n static int iomap_readahead_iter(struct iomap_iter *iter,\n-\t\tstruct iomap_read_folio_ctx *ctx)\n+\t\tstruct iomap_read_folio_ctx *ctx, size_t *cur_bytes_pending)\n {\n \tint ret;\n \n \twhile (iomap_length(iter)) {\n \t\tif (ctx->cur_folio &&\n \t\t    offset_in_folio(ctx->cur_folio, iter->pos) == 0) {\n-\t\t\tif (!ctx->cur_folio_in_bio)\n-\t\t\t\tfolio_unlock(ctx->cur_folio);\n+\t\t\tiomap_read_end(ctx->cur_folio, *cur_bytes_pending);\n \t\t\tctx->cur_folio = NULL;\n \t\t}\n \t\tif (!ctx->cur_folio) {\n \t\t\tctx->cur_folio = readahead_folio(ctx->rac);\n \t\t\tif (WARN_ON_ONCE(!ctx->cur_folio))\n \t\t\t\treturn -EINVAL;\n-\t\t\tctx->cur_folio_in_bio = false;\n+\t\t\t*cur_bytes_pending = 0;\n \t\t}\n-\t\tret = iomap_read_folio_iter(iter, ctx);\n+\t\tret = iomap_read_folio_iter(iter, ctx, cur_bytes_pending);\n \t\tif (ret)\n \t\t\treturn ret;\n \t}\n@@ -554,16 +597,18 @@ void iomap_readahead(struct readahead_control *rac, const struct iomap_ops *ops)\n \tstruct iomap_read_folio_ctx ctx = {\n \t\t.rac\t= rac,\n \t};\n+\tsize_t cur_bytes_pending;\n \n \ttrace_iomap_readahead(rac->mapping->host, readahead_count(rac));\n \n \twhile (iomap_iter(&iter, ops) > 0)\n-\t\titer.status = iomap_readahead_iter(&iter, &ctx);\n+\t\titer.status = iomap_readahead_iter(&iter, &ctx,\n+\t\t\t\t\t&cur_bytes_pending);\n \n \tiomap_bio_submit_read(&ctx);\n \n-\tif (ctx.cur_folio && !ctx.cur_folio_in_bio)\n-\t\tfolio_unlock(ctx.cur_folio);\n+\tif (ctx.cur_folio)\n+\t\tiomap_read_end(ctx.cur_folio, cur_bytes_pending);\n }\n EXPORT_SYMBOL_GPL(iomap_readahead);\n \n-- \n2.47.3\n\n\n\n---\n\nAdvance iter to the correct position before calling an IO helper to read\nin a folio range. This allows the helper to reliably use iter->pos to\ndetermine the starting offset for reading.\n\nThis will simplify the interface for reading in folio ranges when iomap\nread/readahead supports caller-provided callbacks.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nSuggested-by: Christoph Hellwig <hch@lst.de>\n---\n fs/iomap/buffered-io.c | 21 +++++++++++++--------\n 1 file changed, 13 insertions(+), 8 deletions(-)\n\ndiff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\nindex 4e6258fdb915..82bdf7c5e03c 100644\n--- a/fs/iomap/buffered-io.c\n+++ b/fs/iomap/buffered-io.c\n@@ -375,10 +375,11 @@ static void iomap_bio_submit_read(struct iomap_read_folio_ctx *ctx)\n }\n \n static void iomap_bio_read_folio_range(const struct iomap_iter *iter,\n-\t\tstruct iomap_read_folio_ctx *ctx, loff_t pos, size_t plen)\n+\t\tstruct iomap_read_folio_ctx *ctx, size_t plen)\n {\n \tstruct folio *folio = ctx->cur_folio;\n \tconst struct iomap *iomap = &iter->iomap;\n+\tloff_t pos = iter->pos;\n \tsize_t poff = offset_in_folio(folio, pos);\n \tloff_t length = iomap_length(iter);\n \tsector_t sector;\n@@ -470,7 +471,7 @@ static int iomap_read_folio_iter(struct iomap_iter *iter,\n \tloff_t length = iomap_length(iter);\n \tstruct folio *folio = ctx->cur_folio;\n \tsize_t poff, plen;\n-\tloff_t count;\n+\tloff_t pos_diff;\n \tint ret;\n \n \tif (iomap->type == IOMAP_INLINE) {\n@@ -488,12 +489,16 @@ static int iomap_read_folio_iter(struct iomap_iter *iter,\n \t\tiomap_adjust_read_range(iter->inode, folio, &pos, length, &poff,\n \t\t\t\t&plen);\n \n-\t\tcount = pos - iter->pos + plen;\n-\t\tif (WARN_ON_ONCE(count > length))\n+\t\tpos_diff = pos - iter->pos;\n+\t\tif (WARN_ON_ONCE(pos_diff + plen > length))\n \t\t\treturn -EIO;\n \n+\t\tret = iomap_iter_advance(iter, pos_diff);\n+\t\tif (ret)\n+\t\t\treturn ret;\n+\n \t\tif (plen == 0)\n-\t\t\treturn iomap_iter_advance(iter, count);\n+\t\t\treturn 0;\n \n \t\t/* zero post-eof blocks as the page may be mapped */\n \t\tif (iomap_block_needs_zeroing(iter, pos)) {\n@@ -503,13 +508,13 @@ static int iomap_read_folio_iter(struct iomap_iter *iter,\n \t\t\tif (!*bytes_pending)\n \t\t\t\tiomap_read_init(folio);\n \t\t\t*bytes_pending += plen;\n-\t\t\tiomap_bio_read_folio_range(iter, ctx, pos, plen);\n+\t\t\tiomap_bio_read_folio_range(iter, ctx, plen);\n \t\t}\n \n-\t\tret = iomap_iter_advance(iter, count);\n+\t\tret = iomap_iter_advance(iter, plen);\n \t\tif (ret)\n \t\t\treturn ret;\n-\t\tlength -= count;\n+\t\tlength -= pos_diff + plen;\n \t\tpos = iter->pos;\n \t}\n \treturn 0;\n-- \n2.47.3\n\n\n\n---\n\nAdd caller-provided callbacks for read and readahead so that it can be\nused generically, especially by filesystems that are not block-based.\n\nIn particular, this:\n* Modifies the read and readahead interface to take in a\n  struct iomap_read_folio_ctx that is publicly defined as:\n\n  struct iomap_read_folio_ctx {\n\tconst struct iomap_read_ops *ops;\n\tstruct folio *cur_folio;\n\tstruct readahead_control *rac;\n\tvoid *read_ctx;\n  };\n\n  where struct iomap_read_ops is defined as:\n\n  struct iomap_read_ops {\n      int (*read_folio_range)(const struct iomap_iter *iter,\n                             struct iomap_read_folio_ctx *ctx,\n                             size_t len);\n      void (*read_submit)(struct iomap_read_folio_ctx *ctx);\n  };\n\n  read_folio_range() reads in the folio range and is required by the\n  caller to provide. read_submit() is optional and is used for\n  submitting any pending read requests.\n\n* Modifies existing filesystems that use iomap for read and readahead to\n  use the new API, through the new statically inlined helpers\n  iomap_bio_read_folio() and iomap_bio_readahead(). There is no change\n  in functionality for those filesystems.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\n---\n .../filesystems/iomap/operations.rst          | 44 +++++++++++++\n block/fops.c                                  |  5 +-\n fs/erofs/data.c                               |  5 +-\n fs/gfs2/aops.c                                |  6 +-\n fs/iomap/buffered-io.c                        | 55 ++++++++--------\n fs/xfs/xfs_aops.c                             |  5 +-\n fs/zonefs/file.c                              |  5 +-\n include/linux/iomap.h                         | 63 ++++++++++++++++++-\n 8 files changed, 149 insertions(+), 39 deletions(-)\n\ndiff --git a/Documentation/filesystems/iomap/operations.rst b/Documentation/filesystems/iomap/operations.rst\nindex 067ed8e14ef3..cef3c3e76e9e 100644\n--- a/Documentation/filesystems/iomap/operations.rst\n+++ b/Documentation/filesystems/iomap/operations.rst\n@@ -135,6 +135,28 @@ These ``struct kiocb`` flags are significant for buffered I/O with iomap:\n \n  * ``IOCB_DONTCACHE``: Turns on ``IOMAP_DONTCACHE``.\n \n+``struct iomap_read_ops``\n+--------------------------\n+\n+.. code-block:: c\n+\n+ struct iomap_read_ops {\n+     int (*read_folio_range)(const struct iomap_iter *iter,\n+                             struct iomap_read_folio_ctx *ctx, size_t len);\n+     void (*submit_read)(struct iomap_read_folio_ctx *ctx);\n+ };\n+\n+iomap calls these functions:\n+\n+  - ``read_folio_range``: Called to read in the range. This must be provided\n+    by the caller. The caller is responsible for calling\n+    iomap_finish_folio_read() after reading in the folio range. This should be\n+    done even if an error is encountered during the read. This returns 0 on\n+    success or a negative error on failure.\n+\n+  - ``submit_read``: Submit any pending read requests. This function is\n+    optional.\n+\n Internal per-Folio State\n ------------------------\n \n@@ -182,6 +204,28 @@ The ``flags`` argument to ``->iomap_begin`` will be set to zero.\n The pagecache takes whatever locks it needs before calling the\n filesystem.\n \n+Both ``iomap_readahead`` and ``iomap_read_folio`` pass in a ``struct\n+iomap_read_folio_ctx``:\n+\n+.. code-block:: c\n+\n+ struct iomap_read_folio_ctx {\n+    const struct iomap_read_ops *ops;\n+    struct folio *cur_folio;\n+    struct readahead_control *rac;\n+    void *read_ctx;\n+ };\n+\n+``iomap_readahead`` must set:\n+ * ``ops->read_folio_range()`` and ``rac``\n+\n+``iomap_read_folio`` must set:\n+ * ``ops->read_folio_range()`` and ``cur_folio``\n+\n+``ops->submit_read()`` and ``read_ctx`` are optional. ``read_ctx`` is used to\n+pass in any custom data the caller needs accessible in the ops callbacks for\n+fulfilling reads.\n+\n Buffered Writes\n ---------------\n \ndiff --git a/block/fops.c b/block/fops.c\nindex ddbc69c0922b..a2c2391d8dfa 100644\n--- a/block/fops.c\n+++ b/block/fops.c\n@@ -533,12 +533,13 @@ const struct address_space_operations def_blk_aops = {\n #else /* CONFIG_BUFFER_HEAD */\n static int blkdev_read_folio(struct file *file, struct folio *folio)\n {\n-\treturn iomap_read_folio(folio, &blkdev_iomap_ops);\n+\tiomap_bio_read_folio(folio, &blkdev_iomap_ops);\n+\treturn 0;\n }\n \n static void blkdev_readahead(struct readahead_control *rac)\n {\n-\tiomap_readahead(rac, &blkdev_iomap_ops);\n+\tiomap_bio_readahead(rac, &blkdev_iomap_ops);\n }\n \n static ssize_t blkdev_writeback_range(struct iomap_writepage_ctx *wpc,\ndiff --git a/fs/erofs/data.c b/fs/erofs/data.c\nindex 3b1ba571c728..be4191b33321 100644\n--- a/fs/erofs/data.c\n+++ b/fs/erofs/data.c\n@@ -371,7 +371,8 @@ static int erofs_read_folio(struct file *file, struct folio *folio)\n {\n \ttrace_erofs_read_folio(folio, true);\n \n-\treturn iomap_read_folio(folio, &erofs_iomap_ops);\n+\tiomap_bio_read_folio(folio, &erofs_iomap_ops);\n+\treturn 0;\n }\n \n static void erofs_readahead(struct readahead_control *rac)\n@@ -379,7 +380,7 @@ static void erofs_readahead(struct readahead_control *rac)\n \ttrace_erofs_readahead(rac->mapping->host, readahead_index(rac),\n \t\t\t\t\treadahead_count(rac), true);\n \n-\treturn iomap_readahead(rac, &erofs_iomap_ops);\n+\tiomap_bio_readahead(rac, &erofs_iomap_ops);\n }\n \n static sector_t erofs_bmap(struct address_space *mapping, sector_t block)\ndiff --git a/fs/gfs2/aops.c b/fs/gfs2/aops.c\nindex 47d74afd63ac..38d4f343187a 100644\n--- a/fs/gfs2/aops.c\n+++ b/fs/gfs2/aops.c\n@@ -424,11 +424,11 @@ static int gfs2_read_folio(struct file *file, struct folio *folio)\n \tstruct inode *inode = folio->mapping->host;\n \tstruct gfs2_inode *ip = GFS2_I(inode);\n \tstruct gfs2_sbd *sdp = GFS2_SB(inode);\n-\tint error;\n+\tint error = 0;\n \n \tif (!gfs2_is_jdata(ip) ||\n \t    (i_blocksize(inode) == PAGE_SIZE && !folio_buffers(folio))) {\n-\t\terror = iomap_read_folio(folio, &gfs2_iomap_ops);\n+\t\tiomap_bio_read_folio(folio, &gfs2_iomap_ops);\n \t} else if (gfs2_is_stuffed(ip)) {\n \t\terror = stuffed_read_folio(ip, folio);\n \t} else {\n@@ -503,7 +503,7 @@ static void gfs2_readahead(struct readahead_control *rac)\n \telse if (gfs2_is_jdata(ip))\n \t\tmpage_readahead(rac, gfs2_block_map);\n \telse\n-\t\tiomap_readahead(rac, &gfs2_iomap_ops);\n+\t\tiomap_bio_readahead(rac, &gfs2_iomap_ops);\n }\n \n /**\ndiff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\nindex 82bdf7c5e03c..9e1f1f0f8bf1 100644\n--- a/fs/iomap/buffered-io.c\n+++ b/fs/iomap/buffered-io.c\n@@ -328,8 +328,8 @@ static int iomap_read_inline_data(const struct iomap_iter *iter,\n }\n \n #ifdef CONFIG_BLOCK\n-static void iomap_finish_folio_read(struct folio *folio, size_t off,\n-\t\tsize_t len, int error)\n+void iomap_finish_folio_read(struct folio *folio, size_t off, size_t len,\n+\t\tint error)\n {\n \tstruct iomap_folio_state *ifs = folio->private;\n \tbool uptodate = !error;\n@@ -349,6 +349,7 @@ static void iomap_finish_folio_read(struct folio *folio, size_t off,\n \tif (finished)\n \t\tfolio_end_read(folio, uptodate);\n }\n+EXPORT_SYMBOL_GPL(iomap_finish_folio_read);\n \n static void iomap_read_end_io(struct bio *bio)\n {\n@@ -360,12 +361,6 @@ static void iomap_read_end_io(struct bio *bio)\n \tbio_put(bio);\n }\n \n-struct iomap_read_folio_ctx {\n-\tstruct folio\t\t*cur_folio;\n-\tvoid\t\t\t*read_ctx;\n-\tstruct readahead_control *rac;\n-};\n-\n static void iomap_bio_submit_read(struct iomap_read_folio_ctx *ctx)\n {\n \tstruct bio *bio = ctx->read_ctx;\n@@ -374,7 +369,7 @@ static void iomap_bio_submit_read(struct iomap_read_folio_ctx *ctx)\n \t\tsubmit_bio(bio);\n }\n \n-static void iomap_bio_read_folio_range(const struct iomap_iter *iter,\n+static int iomap_bio_read_folio_range(const struct iomap_iter *iter,\n \t\tstruct iomap_read_folio_ctx *ctx, size_t plen)\n {\n \tstruct folio *folio = ctx->cur_folio;\n@@ -412,8 +407,15 @@ static void iomap_bio_read_folio_range(const struct iomap_iter *iter,\n \t\tbio_add_folio_nofail(bio, folio, plen, poff);\n \t\tctx->read_ctx = bio;\n \t}\n+\treturn 0;\n }\n \n+const struct iomap_read_ops iomap_bio_read_ops = {\n+\t.read_folio_range\t= iomap_bio_read_folio_range,\n+\t.submit_read\t\t= iomap_bio_submit_read,\n+};\n+EXPORT_SYMBOL_GPL(iomap_bio_read_ops);\n+\n static void iomap_read_init(struct folio *folio)\n {\n \tstruct iomap_folio_state *ifs = folio->private;\n@@ -508,7 +510,9 @@ static int iomap_read_folio_iter(struct iomap_iter *iter,\n \t\t\tif (!*bytes_pending)\n \t\t\t\tiomap_read_init(folio);\n \t\t\t*bytes_pending += plen;\n-\t\t\tiomap_bio_read_folio_range(iter, ctx, plen);\n+\t\t\tret = ctx->ops->read_folio_range(iter, ctx, plen);\n+\t\t\tif (ret)\n+\t\t\t\treturn ret;\n \t\t}\n \n \t\tret = iomap_iter_advance(iter, plen);\n@@ -520,26 +524,25 @@ static int iomap_read_folio_iter(struct iomap_iter *iter,\n \treturn 0;\n }\n \n-int iomap_read_folio(struct folio *folio, const struct iomap_ops *ops)\n+int iomap_read_folio(const struct iomap_ops *ops,\n+\t\tstruct iomap_read_folio_ctx *ctx)\n {\n+\tstruct folio *folio = ctx->cur_folio;\n \tstruct iomap_iter iter = {\n \t\t.inode\t\t= folio->mapping->host,\n \t\t.pos\t\t= folio_pos(folio),\n \t\t.len\t\t= folio_size(folio),\n \t};\n-\tstruct iomap_read_folio_ctx ctx = {\n-\t\t.cur_folio\t= folio,\n-\t};\n \tsize_t bytes_pending = 0;\n \tint ret;\n \n \ttrace_iomap_readpage(iter.inode, 1);\n \n \twhile ((ret = iomap_iter(&iter, ops)) > 0)\n-\t\titer.status = iomap_read_folio_iter(&iter, &ctx,\n-\t\t\t\t&bytes_pending);\n+\t\titer.status = iomap_read_folio_iter(&iter, ctx, &bytes_pending);\n \n-\tiomap_bio_submit_read(&ctx);\n+\tif (ctx->ops->submit_read)\n+\t\tctx->ops->submit_read(ctx);\n \n \tiomap_read_end(folio, bytes_pending);\n \n@@ -579,8 +582,8 @@ static int iomap_readahead_iter(struct iomap_iter *iter,\n \n /**\n  * iomap_readahead - Attempt to read pages from a file.\n- * @rac: Describes the pages to be read.\n  * @ops: The operations vector for the filesystem.\n+ * @ctx: The ctx used for issuing readahead.\n  *\n  * This function is for filesystems to call to implement their readahead\n  * address_space operation.\n@@ -592,28 +595,28 @@ static int iomap_readahead_iter(struct iomap_iter *iter,\n  * function is called with memalloc_nofs set, so allocations will not cause\n  * the filesystem to be reentered.\n  */\n-void iomap_readahead(struct readahead_control *rac, const struct iomap_ops *ops)\n+void iomap_readahead(const struct iomap_ops *ops,\n+\t\tstruct iomap_read_folio_ctx *ctx)\n {\n+\tstruct readahead_control *rac = ctx->rac;\n \tstruct iomap_iter iter = {\n \t\t.inode\t= rac->mapping->host,\n \t\t.pos\t= readahead_pos(rac),\n \t\t.len\t= readahead_length(rac),\n \t};\n-\tstruct iomap_read_folio_ctx ctx = {\n-\t\t.rac\t= rac,\n-\t};\n \tsize_t cur_bytes_pending;\n \n \ttrace_iomap_readahead(rac->mapping->host, readahead_count(rac));\n \n \twhile (iomap_iter(&iter, ops) > 0)\n-\t\titer.status = iomap_readahead_iter(&iter, &ctx,\n+\t\titer.status = iomap_readahead_iter(&iter, ctx,\n \t\t\t\t\t&cur_bytes_pending);\n \n-\tiomap_bio_submit_read(&ctx);\n+\tif (ctx->ops->submit_read)\n+\t\tctx->ops->submit_read(ctx);\n \n-\tif (ctx.cur_folio)\n-\t\tiomap_read_end(ctx.cur_folio, cur_bytes_pending);\n+\tif (ctx->cur_folio)\n+\t\tiomap_read_end(ctx->cur_folio, cur_bytes_pending);\n }\n EXPORT_SYMBOL_GPL(iomap_readahead);\n \ndiff --git a/fs/xfs/xfs_aops.c b/fs/xfs/xfs_aops.c\nindex a26f79815533..0c2ed00733f2 100644\n--- a/fs/xfs/xfs_aops.c\n+++ b/fs/xfs/xfs_aops.c\n@@ -742,14 +742,15 @@ xfs_vm_read_folio(\n \tstruct file\t\t*unused,\n \tstruct folio\t\t*folio)\n {\n-\treturn iomap_read_folio(folio, &xfs_read_iomap_ops);\n+\tiomap_bio_read_folio(folio, &xfs_read_iomap_ops);\n+\treturn 0;\n }\n \n STATIC void\n xfs_vm_readahead(\n \tstruct readahead_control\t*rac)\n {\n-\tiomap_readahead(rac, &xfs_read_iomap_ops);\n+\tiomap_bio_readahead(rac, &xfs_read_iomap_ops);\n }\n \n static int\ndiff --git a/fs/zonefs/file.c b/fs/zonefs/file.c\nindex fd3a5922f6c3..4d6e7eb52966 100644\n--- a/fs/zonefs/file.c\n+++ b/fs/zonefs/file.c\n@@ -112,12 +112,13 @@ static const struct iomap_ops zonefs_write_iomap_ops = {\n \n static int zonefs_read_folio(struct file *unused, struct folio *folio)\n {\n-\treturn iomap_read_folio(folio, &zonefs_read_iomap_ops);\n+\tiomap_bio_read_folio(folio, &zonefs_read_iomap_ops);\n+\treturn 0;\n }\n \n static void zonefs_readahead(struct readahead_control *rac)\n {\n-\tiomap_readahead(rac, &zonefs_read_iomap_ops);\n+\tiomap_bio_readahead(rac, &zonefs_read_iomap_ops);\n }\n \n /*\ndiff --git a/include/linux/iomap.h b/include/linux/iomap.h\nindex 4469b2318b08..37435b912755 100644\n--- a/include/linux/iomap.h\n+++ b/include/linux/iomap.h\n@@ -16,6 +16,7 @@ struct inode;\n struct iomap_iter;\n struct iomap_dio;\n struct iomap_writepage_ctx;\n+struct iomap_read_folio_ctx;\n struct iov_iter;\n struct kiocb;\n struct page;\n@@ -337,8 +338,10 @@ static inline bool iomap_want_unshare_iter(const struct iomap_iter *iter)\n ssize_t iomap_file_buffered_write(struct kiocb *iocb, struct iov_iter *from,\n \t\tconst struct iomap_ops *ops,\n \t\tconst struct iomap_write_ops *write_ops, void *private);\n-int iomap_read_folio(struct folio *folio, const struct iomap_ops *ops);\n-void iomap_readahead(struct readahead_control *, const struct iomap_ops *ops);\n+int iomap_read_folio(const struct iomap_ops *ops,\n+\t\tstruct iomap_read_folio_ctx *ctx);\n+void iomap_readahead(const struct iomap_ops *ops,\n+\t\tstruct iomap_read_folio_ctx *ctx);\n bool iomap_is_partially_uptodate(struct folio *, size_t from, size_t count);\n struct folio *iomap_get_folio(struct iomap_iter *iter, loff_t pos, size_t len);\n bool iomap_release_folio(struct folio *folio, gfp_t gfp_flags);\n@@ -465,6 +468,8 @@ ssize_t iomap_add_to_ioend(struct iomap_writepage_ctx *wpc, struct folio *folio,\n \t\tloff_t pos, loff_t end_pos, unsigned int dirty_len);\n int iomap_ioend_writeback_submit(struct iomap_writepage_ctx *wpc, int error);\n \n+void iomap_finish_folio_read(struct folio *folio, size_t off, size_t len,\n+\t\tint error);\n void iomap_start_folio_write(struct inode *inode, struct folio *folio,\n \t\tsize_t len);\n void iomap_finish_folio_write(struct inode *inode, struct folio *folio,\n@@ -473,6 +478,34 @@ void iomap_finish_folio_write(struct inode *inode, struct folio *folio,\n int iomap_writeback_folio(struct iomap_writepage_ctx *wpc, struct folio *folio);\n int iomap_writepages(struct iomap_writepage_ctx *wpc);\n \n+struct iomap_read_folio_ctx {\n+\tconst struct iomap_read_ops *ops;\n+\tstruct folio\t\t*cur_folio;\n+\tstruct readahead_control *rac;\n+\tvoid\t\t\t*read_ctx;\n+};\n+\n+struct iomap_read_ops {\n+\t/*\n+\t * Read in a folio range.\n+\t *\n+\t * The caller is responsible for calling iomap_finish_folio_read() after\n+\t * reading in the folio range. This should be done even if an error is\n+\t * encountered during the read.\n+\t *\n+\t * Returns 0 on success or a negative error on failure.\n+\t */\n+\tint (*read_folio_range)(const struct iomap_iter *iter,\n+\t\t\tstruct iomap_read_folio_ctx *ctx, size_t len);\n+\n+\t/*\n+\t * Submit any pending read requests.\n+\t *\n+\t * This is optional.\n+\t */\n+\tvoid (*submit_read)(struct iomap_read_folio_ctx *ctx);\n+};\n+\n /*\n  * Flags for direct I/O ->end_io:\n  */\n@@ -538,4 +571,30 @@ int iomap_swapfile_activate(struct swap_info_struct *sis,\n \n extern struct bio_set iomap_ioend_bioset;\n \n+#ifdef CONFIG_BLOCK\n+extern const struct iomap_read_ops iomap_bio_read_ops;\n+\n+static inline void iomap_bio_read_folio(struct folio *folio,\n+\t\tconst struct iomap_ops *ops)\n+{\n+\tstruct iomap_read_folio_ctx ctx = {\n+\t\t.ops\t\t= &iomap_bio_read_ops,\n+\t\t.cur_folio\t= folio,\n+\t};\n+\n+\tiomap_read_folio(ops, &ctx);\n+}\n+\n+static inline void iomap_bio_readahead(struct readahead_control *rac,\n+\t\tconst struct iomap_ops *ops)\n+{\n+\tstruct iomap_read_folio_ctx ctx = {\n+\t\t.ops\t\t= &iomap_bio_read_ops,\n+\t\t.rac\t\t= rac,\n+\t};\n+\n+\tiomap_readahead(ops, &ctx);\n+}\n+#endif /* CONFIG_BLOCK */\n+\n #endif /* LINUX_IOMAP_H */\n-- \n2.47.3\n\n\n\n---\n\nFrom: Christoph Hellwig <hch@lst.de> [1]\n\nMove bio logic in the buffered io code into its own file and remove\nCONFIG_BLOCK gating for iomap read/readahead.\n\n[1] https://lore.kernel.org/linux-fsdevel/aMK2GuumUf93ep99@infradead.org/\n\nSigned-off-by: Christoph Hellwig <hch@lst.de>\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nReviewed-by: \"Darrick J. Wong\" <djwong@kernel.org>\n---\n fs/iomap/Makefile      |  3 +-\n fs/iomap/bio.c         | 88 ++++++++++++++++++++++++++++++++++++++++++\n fs/iomap/buffered-io.c | 88 +-----------------------------------------\n fs/iomap/internal.h    | 12 ++++++\n 4 files changed, 103 insertions(+), 88 deletions(-)\n create mode 100644 fs/iomap/bio.c\n\ndiff --git a/fs/iomap/Makefile b/fs/iomap/Makefile\nindex f7e1c8534c46..a572b8808524 100644\n--- a/fs/iomap/Makefile\n+++ b/fs/iomap/Makefile\n@@ -14,5 +14,6 @@ iomap-y\t\t\t\t+= trace.o \\\n iomap-$(CONFIG_BLOCK)\t\t+= direct-io.o \\\n \t\t\t\t   ioend.o \\\n \t\t\t\t   fiemap.o \\\n-\t\t\t\t   seek.o\n+\t\t\t\t   seek.o \\\n+\t\t\t\t   bio.o\n iomap-$(CONFIG_SWAP)\t\t+= swapfile.o\ndiff --git a/fs/iomap/bio.c b/fs/iomap/bio.c\nnew file mode 100644\nindex 000000000000..fc045f2e4c45\n--- /dev/null\n+++ b/fs/iomap/bio.c\n@@ -0,0 +1,88 @@\n+// SPDX-License-Identifier: GPL-2.0\n+/*\n+ * Copyright (C) 2010 Red Hat, Inc.\n+ * Copyright (C) 2016-2023 Christoph Hellwig.\n+ */\n+#include <linux/iomap.h>\n+#include <linux/pagemap.h>\n+#include \"internal.h\"\n+#include \"trace.h\"\n+\n+static void iomap_read_end_io(struct bio *bio)\n+{\n+\tint error = blk_status_to_errno(bio->bi_status);\n+\tstruct folio_iter fi;\n+\n+\tbio_for_each_folio_all(fi, bio)\n+\t\tiomap_finish_folio_read(fi.folio, fi.offset, fi.length, error);\n+\tbio_put(bio);\n+}\n+\n+static void iomap_bio_submit_read(struct iomap_read_folio_ctx *ctx)\n+{\n+\tstruct bio *bio = ctx->read_ctx;\n+\n+\tif (bio)\n+\t\tsubmit_bio(bio);\n+}\n+\n+static int iomap_bio_read_folio_range(const struct iomap_iter *iter,\n+\t\tstruct iomap_read_folio_ctx *ctx, size_t plen)\n+{\n+\tstruct folio *folio = ctx->cur_folio;\n+\tconst struct iomap *iomap = &iter->iomap;\n+\tloff_t pos = iter->pos;\n+\tsize_t poff = offset_in_folio(folio, pos);\n+\tloff_t length = iomap_length(iter);\n+\tsector_t sector;\n+\tstruct bio *bio = ctx->read_ctx;\n+\n+\tsector = iomap_sector(iomap, pos);\n+\tif (!bio || bio_end_sector(bio) != sector ||\n+\t    !bio_add_folio(bio, folio, plen, poff)) {\n+\t\tgfp_t gfp = mapping_gfp_constraint(folio->mapping, GFP_KERNEL);\n+\t\tgfp_t orig_gfp = gfp;\n+\t\tunsigned int nr_vecs = DIV_ROUND_UP(length, PAGE_SIZE);\n+\n+\t\tif (bio)\n+\t\t\tsubmit_bio(bio);\n+\n+\t\tif (ctx->rac) /* same as readahead_gfp_mask */\n+\t\t\tgfp |= __GFP_NORETRY | __GFP_NOWARN;\n+\t\tbio = bio_alloc(iomap->bdev, bio_max_segs(nr_vecs), REQ_OP_READ,\n+\t\t\t\t     gfp);\n+\t\t/*\n+\t\t * If the bio_alloc fails, try it again for a single page to\n+\t\t * avoid having to deal with partial page reads.  This emulates\n+\t\t * what do_mpage_read_folio does.\n+\t\t */\n+\t\tif (!bio)\n+\t\t\tbio = bio_alloc(iomap->bdev, 1, REQ_OP_READ, orig_gfp);\n+\t\tif (ctx->rac)\n+\t\t\tbio->bi_opf |= REQ_RAHEAD;\n+\t\tbio->bi_iter.bi_sector = sector;\n+\t\tbio->bi_end_io = iomap_read_end_io;\n+\t\tbio_add_folio_nofail(bio, folio, plen, poff);\n+\t\tctx->read_ctx = bio;\n+\t}\n+\treturn 0;\n+}\n+\n+const struct iomap_read_ops iomap_bio_read_ops = {\n+\t.read_folio_range = iomap_bio_read_folio_range,\n+\t.submit_read = iomap_bio_submit_read,\n+};\n+EXPORT_SYMBOL_GPL(iomap_bio_read_ops);\n+\n+int iomap_bio_read_folio_range_sync(const struct iomap_iter *iter,\n+\t\tstruct folio *folio, loff_t pos, size_t len)\n+{\n+\tconst struct iomap *srcmap = iomap_iter_srcmap(iter);\n+\tstruct bio_vec bvec;\n+\tstruct bio bio;\n+\n+\tbio_init(&bio, srcmap->bdev, &bvec, 1, REQ_OP_READ);\n+\tbio.bi_iter.bi_sector = iomap_sector(srcmap, pos);\n+\tbio_add_folio_nofail(&bio, folio, len, offset_in_folio(folio, pos));\n+\treturn submit_bio_wait(&bio);\n+}\ndiff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\nindex 9e1f1f0f8bf1..86c8094e5cc8 100644\n--- a/fs/iomap/buffered-io.c\n+++ b/fs/iomap/buffered-io.c\n@@ -8,6 +8,7 @@\n #include <linux/writeback.h>\n #include <linux/swap.h>\n #include <linux/migrate.h>\n+#include \"internal.h\"\n #include \"trace.h\"\n \n #include \"../internal.h\"\n@@ -327,7 +328,6 @@ static int iomap_read_inline_data(const struct iomap_iter *iter,\n \treturn 0;\n }\n \n-#ifdef CONFIG_BLOCK\n void iomap_finish_folio_read(struct folio *folio, size_t off, size_t len,\n \t\tint error)\n {\n@@ -351,71 +351,6 @@ void iomap_finish_folio_read(struct folio *folio, size_t off, size_t len,\n }\n EXPORT_SYMBOL_GPL(iomap_finish_folio_read);\n \n-static void iomap_read_end_io(struct bio *bio)\n-{\n-\tint error = blk_status_to_errno(bio->bi_status);\n-\tstruct folio_iter fi;\n-\n-\tbio_for_each_folio_all(fi, bio)\n-\t\tiomap_finish_folio_read(fi.folio, fi.offset, fi.length, error);\n-\tbio_put(bio);\n-}\n-\n-static void iomap_bio_submit_read(struct iomap_read_folio_ctx *ctx)\n-{\n-\tstruct bio *bio = ctx->read_ctx;\n-\n-\tif (bio)\n-\t\tsubmit_bio(bio);\n-}\n-\n-static int iomap_bio_read_folio_range(const struct iomap_iter *iter,\n-\t\tstruct iomap_read_folio_ctx *ctx, size_t plen)\n-{\n-\tstruct folio *folio = ctx->cur_folio;\n-\tconst struct iomap *iomap = &iter->iomap;\n-\tloff_t pos = iter->pos;\n-\tsize_t poff = offset_in_folio(folio, pos);\n-\tloff_t length = iomap_length(iter);\n-\tsector_t sector;\n-\tstruct bio *bio = ctx->read_ctx;\n-\n-\tsector = iomap_sector(iomap, pos);\n-\tif (!bio || bio_end_sector(bio) != sector ||\n-\t    !bio_add_folio(bio, folio, plen, poff)) {\n-\t\tgfp_t gfp = mapping_gfp_constraint(folio->mapping, GFP_KERNEL);\n-\t\tgfp_t orig_gfp = gfp;\n-\t\tunsigned int nr_vecs = DIV_ROUND_UP(length, PAGE_SIZE);\n-\n-\t\tiomap_bio_submit_read(ctx);\n-\n-\t\tif (ctx->rac) /* same as readahead_gfp_mask */\n-\t\t\tgfp |= __GFP_NORETRY | __GFP_NOWARN;\n-\t\tbio = bio_alloc(iomap->bdev, bio_max_segs(nr_vecs), REQ_OP_READ,\n-\t\t\t\t     gfp);\n-\t\t/*\n-\t\t * If the bio_alloc fails, try it again for a single page to\n-\t\t * avoid having to deal with partial page reads.  This emulates\n-\t\t * what do_mpage_read_folio does.\n-\t\t */\n-\t\tif (!bio)\n-\t\t\tbio = bio_alloc(iomap->bdev, 1, REQ_OP_READ, orig_gfp);\n-\t\tif (ctx->rac)\n-\t\t\tbio->bi_opf |= REQ_RAHEAD;\n-\t\tbio->bi_iter.bi_sector = sector;\n-\t\tbio->bi_end_io = iomap_read_end_io;\n-\t\tbio_add_folio_nofail(bio, folio, plen, poff);\n-\t\tctx->read_ctx = bio;\n-\t}\n-\treturn 0;\n-}\n-\n-const struct iomap_read_ops iomap_bio_read_ops = {\n-\t.read_folio_range\t= iomap_bio_read_folio_range,\n-\t.submit_read\t\t= iomap_bio_submit_read,\n-};\n-EXPORT_SYMBOL_GPL(iomap_bio_read_ops);\n-\n static void iomap_read_init(struct folio *folio)\n {\n \tstruct iomap_folio_state *ifs = folio->private;\n@@ -620,27 +555,6 @@ void iomap_readahead(const struct iomap_ops *ops,\n }\n EXPORT_SYMBOL_GPL(iomap_readahead);\n \n-static int iomap_bio_read_folio_range_sync(const struct iomap_iter *iter,\n-\t\tstruct folio *folio, loff_t pos, size_t len)\n-{\n-\tconst struct iomap *srcmap = iomap_iter_srcmap(iter);\n-\tstruct bio_vec bvec;\n-\tstruct bio bio;\n-\n-\tbio_init(&bio, srcmap->bdev, &bvec, 1, REQ_OP_READ);\n-\tbio.bi_iter.bi_sector = iomap_sector(srcmap, pos);\n-\tbio_add_folio_nofail(&bio, folio, len, offset_in_folio(folio, pos));\n-\treturn submit_bio_wait(&bio);\n-}\n-#else\n-static int iomap_bio_read_folio_range_sync(const struct iomap_iter *iter,\n-\t\tstruct folio *folio, loff_t pos, size_t len)\n-{\n-\tWARN_ON_ONCE(1);\n-\treturn -EIO;\n-}\n-#endif /* CONFIG_BLOCK */\n-\n /*\n  * iomap_is_partially_uptodate checks whether blocks within a folio are\n  * uptodate or not.\ndiff --git a/fs/iomap/internal.h b/fs/iomap/internal.h\nindex d05cb3aed96e..3a4e4aad2bd1 100644\n--- a/fs/iomap/internal.h\n+++ b/fs/iomap/internal.h\n@@ -6,4 +6,16 @@\n \n u32 iomap_finish_ioend_direct(struct iomap_ioend *ioend);\n \n+#ifdef CONFIG_BLOCK\n+int iomap_bio_read_folio_range_sync(const struct iomap_iter *iter,\n+\t\tstruct folio *folio, loff_t pos, size_t len);\n+#else\n+static inline int iomap_bio_read_folio_range_sync(const struct iomap_iter *iter,\n+\t\tstruct folio *folio, loff_t pos, size_t len)\n+{\n+\tWARN_ON_ONCE(1);\n+\treturn -EIO;\n+}\n+#endif /* CONFIG_BLOCK */\n+\n #endif /* _IOMAP_INTERNAL_H */\n-- \n2.47.3\n\n\n\n---\n\nNo errors are propagated in iomap_read_folio(). Change\niomap_read_folio() to a void return to make this clearer to callers.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nReviewed-by: \"Darrick J. Wong\" <djwong@kernel.org>\n---\n fs/iomap/buffered-io.c | 9 +--------\n include/linux/iomap.h  | 2 +-\n 2 files changed, 2 insertions(+), 9 deletions(-)\n\ndiff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\nindex 86c8094e5cc8..f9ae72713f74 100644\n--- a/fs/iomap/buffered-io.c\n+++ b/fs/iomap/buffered-io.c\n@@ -459,7 +459,7 @@ static int iomap_read_folio_iter(struct iomap_iter *iter,\n \treturn 0;\n }\n \n-int iomap_read_folio(const struct iomap_ops *ops,\n+void iomap_read_folio(const struct iomap_ops *ops,\n \t\tstruct iomap_read_folio_ctx *ctx)\n {\n \tstruct folio *folio = ctx->cur_folio;\n@@ -480,13 +480,6 @@ int iomap_read_folio(const struct iomap_ops *ops,\n \t\tctx->ops->submit_read(ctx);\n \n \tiomap_read_end(folio, bytes_pending);\n-\n-\t/*\n-\t * Just like mpage_readahead and block_read_full_folio, we always\n-\t * return 0 and just set the folio error flag on errors.  This\n-\t * should be cleaned up throughout the stack eventually.\n-\t */\n-\treturn 0;\n }\n EXPORT_SYMBOL_GPL(iomap_read_folio);\n \ndiff --git a/include/linux/iomap.h b/include/linux/iomap.h\nindex 37435b912755..6d864b446b6e 100644\n--- a/include/linux/iomap.h\n+++ b/include/linux/iomap.h\n@@ -338,7 +338,7 @@ static inline bool iomap_want_unshare_iter(const struct iomap_iter *iter)\n ssize_t iomap_file_buffered_write(struct kiocb *iocb, struct iov_iter *from,\n \t\tconst struct iomap_ops *ops,\n \t\tconst struct iomap_write_ops *write_ops, void *private);\n-int iomap_read_folio(const struct iomap_ops *ops,\n+void iomap_read_folio(const struct iomap_ops *ops,\n \t\tstruct iomap_read_folio_ctx *ctx);\n void iomap_readahead(const struct iomap_ops *ops,\n \t\tstruct iomap_read_folio_ctx *ctx);\n-- \n2.47.3\n\n\n\n---\n\nRead folio data into the page cache using iomap. This gives us granular\nuptodate tracking for large folios, which optimizes how much data needs\nto be read in. If some portions of the folio are already uptodate (eg\nthrough a prior write), we only need to read in the non-uptodate\nportions.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nReviewed-by: \"Darrick J. Wong\" <djwong@kernel.org>\n---\n fs/fuse/file.c | 80 +++++++++++++++++++++++++++++++++++---------------\n 1 file changed, 56 insertions(+), 24 deletions(-)\n\ndiff --git a/fs/fuse/file.c b/fs/fuse/file.c\nindex 4adcf09d4b01..db93c83ee4a3 100644\n--- a/fs/fuse/file.c\n+++ b/fs/fuse/file.c\n@@ -828,23 +828,69 @@ static int fuse_do_readfolio(struct file *file, struct folio *folio,\n \treturn 0;\n }\n \n+static int fuse_iomap_begin(struct inode *inode, loff_t offset, loff_t length,\n+\t\t\t    unsigned int flags, struct iomap *iomap,\n+\t\t\t    struct iomap *srcmap)\n+{\n+\tiomap->type = IOMAP_MAPPED;\n+\tiomap->length = length;\n+\tiomap->offset = offset;\n+\treturn 0;\n+}\n+\n+static const struct iomap_ops fuse_iomap_ops = {\n+\t.iomap_begin\t= fuse_iomap_begin,\n+};\n+\n+struct fuse_fill_read_data {\n+\tstruct file *file;\n+};\n+\n+static int fuse_iomap_read_folio_range_async(const struct iomap_iter *iter,\n+\t\t\t\t\t     struct iomap_read_folio_ctx *ctx,\n+\t\t\t\t\t     size_t len)\n+{\n+\tstruct fuse_fill_read_data *data = ctx->read_ctx;\n+\tstruct folio *folio = ctx->cur_folio;\n+\tloff_t pos =  iter->pos;\n+\tsize_t off = offset_in_folio(folio, pos);\n+\tstruct file *file = data->file;\n+\tint ret;\n+\n+\t/*\n+\t *  for non-readahead read requests, do reads synchronously since\n+\t *  it's not guaranteed that the server can handle out-of-order reads\n+\t */\n+\tret = fuse_do_readfolio(file, folio, off, len);\n+\tiomap_finish_folio_read(folio, off, len, ret);\n+\treturn ret;\n+}\n+\n+static const struct iomap_read_ops fuse_iomap_read_ops = {\n+\t.read_folio_range = fuse_iomap_read_folio_range_async,\n+};\n+\n static int fuse_read_folio(struct file *file, struct folio *folio)\n {\n \tstruct inode *inode = folio->mapping->host;\n-\tint err;\n+\tstruct fuse_fill_read_data data = {\n+\t\t.file = file,\n+\t};\n+\tstruct iomap_read_folio_ctx ctx = {\n+\t\t.cur_folio = folio,\n+\t\t.ops = &fuse_iomap_read_ops,\n+\t\t.read_ctx = &data,\n \n-\terr = -EIO;\n-\tif (fuse_is_bad(inode))\n-\t\tgoto out;\n+\t};\n \n-\terr = fuse_do_readfolio(file, folio, 0, folio_size(folio));\n-\tif (!err)\n-\t\tfolio_mark_uptodate(folio);\n+\tif (fuse_is_bad(inode)) {\n+\t\tfolio_unlock(folio);\n+\t\treturn -EIO;\n+\t}\n \n+\tiomap_read_folio(&fuse_iomap_ops, &ctx);\n \tfuse_invalidate_atime(inode);\n- out:\n-\tfolio_unlock(folio);\n-\treturn err;\n+\treturn 0;\n }\n \n static int fuse_iomap_read_folio_range(const struct iomap_iter *iter,\n@@ -1394,20 +1440,6 @@ static const struct iomap_write_ops fuse_iomap_write_ops = {\n \t.read_folio_range = fuse_iomap_read_folio_range,\n };\n \n-static int fuse_iomap_begin(struct inode *inode, loff_t offset, loff_t length,\n-\t\t\t    unsigned int flags, struct iomap *iomap,\n-\t\t\t    struct iomap *srcmap)\n-{\n-\tiomap->type = IOMAP_MAPPED;\n-\tiomap->length = length;\n-\tiomap->offset = offset;\n-\treturn 0;\n-}\n-\n-static const struct iomap_ops fuse_iomap_ops = {\n-\t.iomap_begin\t= fuse_iomap_begin,\n-};\n-\n static ssize_t fuse_cache_write_iter(struct kiocb *iocb, struct iov_iter *from)\n {\n \tstruct file *file = iocb->ki_filp;\n-- \n2.47.3\n\n\n\n---\n\nDo readahead in fuse using iomap. This gives us granular uptodate\ntracking for large folios, which optimizes how much data needs to be\nread in. If some portions of the folio are already uptodate (eg through\na prior write), we only need to read in the non-uptodate portions.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nReviewed-by: \"Darrick J. Wong\" <djwong@kernel.org>\n---\n fs/fuse/file.c | 220 ++++++++++++++++++++++++++++---------------------\n 1 file changed, 124 insertions(+), 96 deletions(-)\n\ndiff --git a/fs/fuse/file.c b/fs/fuse/file.c\nindex db93c83ee4a3..7c9c00784e33 100644\n--- a/fs/fuse/file.c\n+++ b/fs/fuse/file.c\n@@ -844,8 +844,65 @@ static const struct iomap_ops fuse_iomap_ops = {\n \n struct fuse_fill_read_data {\n \tstruct file *file;\n+\n+\t/* Fields below are used if sending the read request asynchronously */\n+\tstruct fuse_conn *fc;\n+\tstruct fuse_io_args *ia;\n+\tunsigned int nr_bytes;\n };\n \n+/* forward declarations */\n+static bool fuse_folios_need_send(struct fuse_conn *fc, loff_t pos,\n+\t\t\t\t  unsigned len, struct fuse_args_pages *ap,\n+\t\t\t\t  unsigned cur_bytes, bool write);\n+static void fuse_send_readpages(struct fuse_io_args *ia, struct file *file,\n+\t\t\t\tunsigned int count, bool async);\n+\n+static int fuse_handle_readahead(struct folio *folio,\n+\t\t\t\t struct readahead_control *rac,\n+\t\t\t\t struct fuse_fill_read_data *data, loff_t pos,\n+\t\t\t\t size_t len)\n+{\n+\tstruct fuse_io_args *ia = data->ia;\n+\tsize_t off = offset_in_folio(folio, pos);\n+\tstruct fuse_conn *fc = data->fc;\n+\tstruct fuse_args_pages *ap;\n+\tunsigned int nr_pages;\n+\n+\tif (ia && fuse_folios_need_send(fc, pos, len, &ia->ap, data->nr_bytes,\n+\t\t\t\t\tfalse)) {\n+\t\tfuse_send_readpages(ia, data->file, data->nr_bytes,\n+\t\t\t\t    fc->async_read);\n+\t\tdata->nr_bytes = 0;\n+\t\tdata->ia = NULL;\n+\t\tia = NULL;\n+\t}\n+\tif (!ia) {\n+\t\tif (fc->num_background >= fc->congestion_threshold &&\n+\t\t    rac->ra->async_size >= readahead_count(rac))\n+\t\t\t/*\n+\t\t\t * Congested and only async pages left, so skip the\n+\t\t\t * rest.\n+\t\t\t */\n+\t\t\treturn -EAGAIN;\n+\n+\t\tnr_pages = min(fc->max_pages, readahead_count(rac));\n+\t\tdata->ia = fuse_io_alloc(NULL, nr_pages);\n+\t\tif (!data->ia)\n+\t\t\treturn -ENOMEM;\n+\t\tia = data->ia;\n+\t}\n+\tfolio_get(folio);\n+\tap = &ia->ap;\n+\tap->folios[ap->num_folios] = folio;\n+\tap->descs[ap->num_folios].offset = off;\n+\tap->descs[ap->num_folios].length = len;\n+\tdata->nr_bytes += len;\n+\tap->num_folios++;\n+\n+\treturn 0;\n+}\n+\n static int fuse_iomap_read_folio_range_async(const struct iomap_iter *iter,\n \t\t\t\t\t     struct iomap_read_folio_ctx *ctx,\n \t\t\t\t\t     size_t len)\n@@ -857,17 +914,39 @@ static int fuse_iomap_read_folio_range_async(const struct iomap_iter *iter,\n \tstruct file *file = data->file;\n \tint ret;\n \n-\t/*\n-\t *  for non-readahead read requests, do reads synchronously since\n-\t *  it's not guaranteed that the server can handle out-of-order reads\n-\t */\n-\tret = fuse_do_readfolio(file, folio, off, len);\n-\tiomap_finish_folio_read(folio, off, len, ret);\n+\tif (ctx->rac) {\n+\t\tret = fuse_handle_readahead(folio, ctx->rac, data, pos, len);\n+\t\t/*\n+\t\t * If fuse_handle_readahead was successful, fuse_readpages_end\n+\t\t * will do the iomap_finish_folio_read, else we need to call it\n+\t\t * here\n+\t\t */\n+\t\tif (ret)\n+\t\t\tiomap_finish_folio_read(folio, off, len, ret);\n+\t} else {\n+\t\t/*\n+\t\t *  for non-readahead read requests, do reads synchronously\n+\t\t *  since it's not guaranteed that the server can handle\n+\t\t *  out-of-order reads\n+\t\t */\n+\t\tret = fuse_do_readfolio(file, folio, off, len);\n+\t\tiomap_finish_folio_read(folio, off, len, ret);\n+\t}\n \treturn ret;\n }\n \n+static void fuse_iomap_read_submit(struct iomap_read_folio_ctx *ctx)\n+{\n+\tstruct fuse_fill_read_data *data = ctx->read_ctx;\n+\n+\tif (data->ia)\n+\t\tfuse_send_readpages(data->ia, data->file, data->nr_bytes,\n+\t\t\t\t    data->fc->async_read);\n+}\n+\n static const struct iomap_read_ops fuse_iomap_read_ops = {\n \t.read_folio_range = fuse_iomap_read_folio_range_async,\n+\t.submit_read = fuse_iomap_read_submit,\n };\n \n static int fuse_read_folio(struct file *file, struct folio *folio)\n@@ -929,7 +1008,8 @@ static void fuse_readpages_end(struct fuse_mount *fm, struct fuse_args *args,\n \t}\n \n \tfor (i = 0; i < ap->num_folios; i++) {\n-\t\tfolio_end_read(ap->folios[i], !err);\n+\t\tiomap_finish_folio_read(ap->folios[i], ap->descs[i].offset,\n+\t\t\t\t\tap->descs[i].length, err);\n \t\tfolio_put(ap->folios[i]);\n \t}\n \tif (ia->ff)\n@@ -939,7 +1019,7 @@ static void fuse_readpages_end(struct fuse_mount *fm, struct fuse_args *args,\n }\n \n static void fuse_send_readpages(struct fuse_io_args *ia, struct file *file,\n-\t\t\t\tunsigned int count)\n+\t\t\t\tunsigned int count, bool async)\n {\n \tstruct fuse_file *ff = file->private_data;\n \tstruct fuse_mount *fm = ff->fm;\n@@ -961,7 +1041,7 @@ static void fuse_send_readpages(struct fuse_io_args *ia, struct file *file,\n \n \tfuse_read_args_fill(ia, file, pos, count, FUSE_READ);\n \tia->read.attr_ver = fuse_get_attr_version(fm->fc);\n-\tif (fm->fc->async_read) {\n+\tif (async) {\n \t\tia->ff = fuse_file_get(ff);\n \t\tap->args.end = fuse_readpages_end;\n \t\terr = fuse_simple_background(fm, &ap->args, GFP_KERNEL);\n@@ -978,81 +1058,20 @@ static void fuse_readahead(struct readahead_control *rac)\n {\n \tstruct inode *inode = rac->mapping->host;\n \tstruct fuse_conn *fc = get_fuse_conn(inode);\n-\tunsigned int max_pages, nr_pages;\n-\tstruct folio *folio = NULL;\n+\tstruct fuse_fill_read_data data = {\n+\t\t.file = rac->file,\n+\t\t.fc = fc,\n+\t};\n+\tstruct iomap_read_folio_ctx ctx = {\n+\t\t.ops = &fuse_iomap_read_ops,\n+\t\t.rac = rac,\n+\t\t.read_ctx = &data\n+\t};\n \n \tif (fuse_is_bad(inode))\n \t\treturn;\n \n-\tmax_pages = min_t(unsigned int, fc->max_pages,\n-\t\t\tfc->max_read / PAGE_SIZE);\n-\n-\t/*\n-\t * This is only accurate the first time through, since readahead_folio()\n-\t * doesn't update readahead_count() from the previous folio until the\n-\t * next call.  Grab nr_pages here so we know how many pages we're going\n-\t * to have to process.  This means that we will exit here with\n-\t * readahead_count() == folio_nr_pages(last_folio), but we will have\n-\t * consumed all of the folios, and read_pages() will call\n-\t * readahead_folio() again which will clean up the rac.\n-\t */\n-\tnr_pages = readahead_count(rac);\n-\n-\twhile (nr_pages) {\n-\t\tstruct fuse_io_args *ia;\n-\t\tstruct fuse_args_pages *ap;\n-\t\tunsigned cur_pages = min(max_pages, nr_pages);\n-\t\tunsigned int pages = 0;\n-\n-\t\tif (fc->num_background >= fc->congestion_threshold &&\n-\t\t    rac->ra->async_size >= readahead_count(rac))\n-\t\t\t/*\n-\t\t\t * Congested and only async pages left, so skip the\n-\t\t\t * rest.\n-\t\t\t */\n-\t\t\tbreak;\n-\n-\t\tia = fuse_io_alloc(NULL, cur_pages);\n-\t\tif (!ia)\n-\t\t\tbreak;\n-\t\tap = &ia->ap;\n-\n-\t\twhile (pages < cur_pages) {\n-\t\t\tunsigned int folio_pages;\n-\n-\t\t\t/*\n-\t\t\t * This returns a folio with a ref held on it.\n-\t\t\t * The ref needs to be held until the request is\n-\t\t\t * completed, since the splice case (see\n-\t\t\t * fuse_try_move_page()) drops the ref after it's\n-\t\t\t * replaced in the page cache.\n-\t\t\t */\n-\t\t\tif (!folio)\n-\t\t\t\tfolio =  __readahead_folio(rac);\n-\n-\t\t\tfolio_pages = folio_nr_pages(folio);\n-\t\t\tif (folio_pages > cur_pages - pages) {\n-\t\t\t\t/*\n-\t\t\t\t * Large folios belonging to fuse will never\n-\t\t\t\t * have more pages than max_pages.\n-\t\t\t\t */\n-\t\t\t\tWARN_ON(!pages);\n-\t\t\t\tbreak;\n-\t\t\t}\n-\n-\t\t\tap->folios[ap->num_folios] = folio;\n-\t\t\tap->descs[ap->num_folios].length = folio_size(folio);\n-\t\t\tap->num_folios++;\n-\t\t\tpages += folio_pages;\n-\t\t\tfolio = NULL;\n-\t\t}\n-\t\tfuse_send_readpages(ia, rac->file, pages << PAGE_SHIFT);\n-\t\tnr_pages -= pages;\n-\t}\n-\tif (folio) {\n-\t\tfolio_end_read(folio, false);\n-\t\tfolio_put(folio);\n-\t}\n+\tiomap_readahead(&fuse_iomap_ops, &ctx);\n }\n \n static ssize_t fuse_cache_read_iter(struct kiocb *iocb, struct iov_iter *to)\n@@ -2083,7 +2102,7 @@ struct fuse_fill_wb_data {\n \tstruct fuse_file *ff;\n \tunsigned int max_folios;\n \t/*\n-\t * nr_bytes won't overflow since fuse_writepage_need_send() caps\n+\t * nr_bytes won't overflow since fuse_folios_need_send() caps\n \t * wb requests to never exceed fc->max_pages (which has an upper bound\n \t * of U16_MAX).\n \t */\n@@ -2128,14 +2147,15 @@ static void fuse_writepages_send(struct inode *inode,\n \tspin_unlock(&fi->lock);\n }\n \n-static bool fuse_writepage_need_send(struct fuse_conn *fc, loff_t pos,\n-\t\t\t\t     unsigned len, struct fuse_args_pages *ap,\n-\t\t\t\t     struct fuse_fill_wb_data *data)\n+static bool fuse_folios_need_send(struct fuse_conn *fc, loff_t pos,\n+\t\t\t\t  unsigned len, struct fuse_args_pages *ap,\n+\t\t\t\t  unsigned cur_bytes, bool write)\n {\n \tstruct folio *prev_folio;\n \tstruct fuse_folio_desc prev_desc;\n-\tunsigned bytes = data->nr_bytes + len;\n+\tunsigned bytes = cur_bytes + len;\n \tloff_t prev_pos;\n+\tsize_t max_bytes = write ? fc->max_write : fc->max_read;\n \n \tWARN_ON(!ap->num_folios);\n \n@@ -2143,8 +2163,7 @@ static bool fuse_writepage_need_send(struct fuse_conn *fc, loff_t pos,\n \tif ((bytes + PAGE_SIZE - 1) >> PAGE_SHIFT > fc->max_pages)\n \t\treturn true;\n \n-\t/* Reached max write bytes */\n-\tif (bytes > fc->max_write)\n+\tif (bytes > max_bytes)\n \t\treturn true;\n \n \t/* Discontinuity */\n@@ -2154,11 +2173,6 @@ static bool fuse_writepage_need_send(struct fuse_conn *fc, loff_t pos,\n \tif (prev_pos != pos)\n \t\treturn true;\n \n-\t/* Need to grow the pages array?  If so, did the expansion fail? */\n-\tif (ap->num_folios == data->max_folios &&\n-\t    !fuse_pages_realloc(data, fc->max_pages))\n-\t\treturn true;\n-\n \treturn false;\n }\n \n@@ -2182,10 +2196,24 @@ static ssize_t fuse_iomap_writeback_range(struct iomap_writepage_ctx *wpc,\n \t\t\treturn -EIO;\n \t}\n \n-\tif (wpa && fuse_writepage_need_send(fc, pos, len, ap, data)) {\n-\t\tfuse_writepages_send(inode, data);\n-\t\tdata->wpa = NULL;\n-\t\tdata->nr_bytes = 0;\n+\tif (wpa) {\n+\t\tbool send = fuse_folios_need_send(fc, pos, len, ap,\n+\t\t\t\t\t\t  data->nr_bytes, true);\n+\n+\t\tif (!send) {\n+\t\t\t/*\n+\t\t\t * Need to grow the pages array?  If so, did the\n+\t\t\t * expansion fail?\n+\t\t\t */\n+\t\t\tsend = (ap->num_folios == data->max_folios) &&\n+\t\t\t\t!fuse_pages_realloc(data, fc->max_pages);\n+\t\t}\n+\n+\t\tif (send) {\n+\t\t\tfuse_writepages_send(inode, data);\n+\t\t\tdata->wpa = NULL;\n+\t\t\tdata->nr_bytes = 0;\n+\t\t}\n \t}\n \n \tif (data->wpa == NULL) {\n-- \n2.47.3\n\n\n\n---\n\nNow that fuse is integrated with iomap for read/readahead, we can remove\nthe workaround that was added in commit bd24d2108e9c (\"fuse: fix fuseblk\ni_blkbits for iomap partial writes\"), which was previously needed to\navoid a race condition where an iomap partial write may be overwritten\nby a read if blocksize < PAGE_SIZE. Now that fuse does iomap\nread/readahead, this is protected against since there is granular\nuptodate tracking of blocks, which means this workaround can be removed.\n\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nReviewed-by: \"Darrick J. Wong\" <djwong@kernel.org>\n---\n fs/fuse/dir.c    |  2 +-\n fs/fuse/fuse_i.h |  8 --------\n fs/fuse/inode.c  | 13 +------------\n 3 files changed, 2 insertions(+), 21 deletions(-)\n\ndiff --git a/fs/fuse/dir.c b/fs/fuse/dir.c\nindex 5c569c3cb53f..ebee7e0b1cd3 100644\n--- a/fs/fuse/dir.c\n+++ b/fs/fuse/dir.c\n@@ -1199,7 +1199,7 @@ static void fuse_fillattr(struct mnt_idmap *idmap, struct inode *inode,\n \tif (attr->blksize != 0)\n \t\tblkbits = ilog2(attr->blksize);\n \telse\n-\t\tblkbits = fc->blkbits;\n+\t\tblkbits = inode->i_sb->s_blocksize_bits;\n \n \tstat->blksize = 1 << blkbits;\n }\ndiff --git a/fs/fuse/fuse_i.h b/fs/fuse/fuse_i.h\nindex cc428d04be3e..1647eb7ca6fa 100644\n--- a/fs/fuse/fuse_i.h\n+++ b/fs/fuse/fuse_i.h\n@@ -975,14 +975,6 @@ struct fuse_conn {\n \t\t/* Request timeout (in jiffies). 0 = no timeout */\n \t\tunsigned int req_timeout;\n \t} timeout;\n-\n-\t/*\n-\t * This is a workaround until fuse uses iomap for reads.\n-\t * For fuseblk servers, this represents the blocksize passed in at\n-\t * mount time and for regular fuse servers, this is equivalent to\n-\t * inode->i_blkbits.\n-\t */\n-\tu8 blkbits;\n };\n \n /*\ndiff --git a/fs/fuse/inode.c b/fs/fuse/inode.c\nindex 7485a41af892..a1b9e8587155 100644\n--- a/fs/fuse/inode.c\n+++ b/fs/fuse/inode.c\n@@ -292,7 +292,7 @@ void fuse_change_attributes_common(struct inode *inode, struct fuse_attr *attr,\n \tif (attr->blksize)\n \t\tfi->cached_i_blkbits = ilog2(attr->blksize);\n \telse\n-\t\tfi->cached_i_blkbits = fc->blkbits;\n+\t\tfi->cached_i_blkbits = inode->i_sb->s_blocksize_bits;\n \n \t/*\n \t * Don't set the sticky bit in i_mode, unless we want the VFS\n@@ -1810,21 +1810,10 @@ int fuse_fill_super_common(struct super_block *sb, struct fuse_fs_context *ctx)\n \t\terr = -EINVAL;\n \t\tif (!sb_set_blocksize(sb, ctx->blksize))\n \t\t\tgoto err;\n-\t\t/*\n-\t\t * This is a workaround until fuse hooks into iomap for reads.\n-\t\t * Use PAGE_SIZE for the blocksize else if the writeback cache\n-\t\t * is enabled, buffered writes go through iomap and a read may\n-\t\t * overwrite partially written data if blocksize < PAGE_SIZE\n-\t\t */\n-\t\tfc->blkbits = sb->s_blocksize_bits;\n-\t\tif (ctx->blksize != PAGE_SIZE &&\n-\t\t    !sb_set_blocksize(sb, PAGE_SIZE))\n-\t\t\tgoto err;\n #endif\n \t} else {\n \t\tsb->s_blocksize = PAGE_SIZE;\n \t\tsb->s_blocksize_bits = PAGE_SHIFT;\n-\t\tfc->blkbits = sb->s_blocksize_bits;\n \t}\n \n \tsb->s_subtype = ctx->subtype;\n-- \n2.47.3\n\n\n\n---\n\nOn Thu, Oct 23, 2025 at 12:30PM Brian Foster <bfoster@redhat.com> wrote:\n>\n> On Thu, Sep 25, 2025 at 05:26:02PM -0700, Joanne Koong wrote:\n> > Instead of incrementing read_bytes_pending for every folio range read in\n> > (which requires acquiring the spinlock to do so), set read_bytes_pending\n> > to the folio size when the first range is asynchronously read in, keep\n> > track of how many bytes total are asynchronously read in, and adjust\n> > read_bytes_pending accordingly after issuing requests to read in all the\n> > necessary ranges.\n> >\n> > iomap_read_folio_ctx->cur_folio_in_bio can be removed since a non-zero\n> > value for pending bytes necessarily indicates the folio is in the bio.\n> >\n> > Signed-off-by: Joanne Koong <joannelkoong@gmail.com>\n> > Suggested-by: \"Darrick J. Wong\" <djwong@kernel.org>\n> > ---\n>\n> Hi Joanne,\n>\n> I was throwing some extra testing at the vfs-6.19.iomap branch since the\n> little merge conflict thing with iomap_iter_advance(). I end up hitting\n> what appears to be a lockup on XFS with 1k FSB (-bsize=1k) running\n> generic/051. It reproduces fairly reliably within a few iterations or so\n> and seems to always stall during a read for a dedupe operation:\n>\n> task:fsstress        state:D stack:0     pid:12094 tgid:12094 ppid:12091  task_flags:0x400140 flags:0x00080003\n> Call Trace:\n>  <TASK>\n>  __schedule+0x2fc/0x7a0\n>  schedule+0x27/0x80\n>  io_schedule+0x46/0x70\n>  folio_wait_bit_common+0x12b/0x310\n>  ? __pfx_wake_page_function+0x10/0x10\n>  ? __pfx_xfs_vm_read_folio+0x10/0x10 [xfs]\n>  filemap_read_folio+0x85/0xd0\n>  ? __pfx_xfs_vm_read_folio+0x10/0x10 [xfs]\n>  do_read_cache_folio+0x7c/0x1b0\n>  vfs_dedupe_file_range_compare.constprop.0+0xaf/0x2d0\n>  __generic_remap_file_range_prep+0x276/0x2a0\n>  generic_remap_file_range_prep+0x10/0x20\n>  xfs_reflink_remap_prep+0x22c/0x300 [xfs]\n>  xfs_file_remap_range+0x84/0x360 [xfs]\n>  vfs_dedupe_file_range_one+0x1b2/0x1d0\n>  ? remap_verify_area+0x46/0x140\n>  vfs_dedupe_file_range+0x162/0x220\n>  do_vfs_ioctl+0x4d1/0x940\n>  __x64_sys_ioctl+0x75/0xe0\n>  do_syscall_64+0x84/0x800\n>  ? do_syscall_64+0xbb/0x800\n>  ? avc_has_perm_noaudit+0x6b/0xf0\n>  ? _copy_to_user+0x31/0x40\n>  ? cp_new_stat+0x130/0x170\n>  ? __do_sys_newfstat+0x44/0x70\n>  ? do_syscall_64+0xbb/0x800\n>  ? do_syscall_64+0xbb/0x800\n>  ? clear_bhb_loop+0x30/0x80\n>  ? clear_bhb_loop+0x30/0x80\n>  entry_SYSCALL_64_after_hwframe+0x76/0x7e\n> RIP: 0033:0x7fe6bbd9a14d\n> RSP: 002b:00007ffde72cd4e0 EFLAGS: 00000246 ORIG_RAX: 0000000000000010\n> RAX: ffffffffffffffda RBX: 0000000000000068 RCX: 00007fe6bbd9a14d\n> RDX: 000000000a1394b0 RSI: 00000000c0189436 RDI: 0000000000000004\n> RBP: 00007ffde72cd530 R08: 0000000000001000 R09: 000000000a11a3fc\n> R10: 000000000001d6c0 R11: 0000000000000246 R12: 000000000a12cfb0\n> R13: 000000000a12ba10 R14: 000000000a14e610 R15: 0000000000019000\n>  </TASK>\n>\n> It wasn't immediately clear to me what the issue was so I bisected and\n> it landed on this patch. It kind of looks like we're failing to unlock a\n> folio at some point and then tripping over it later..? I can kill the\n> fsstress process but then the umount ultimately gets stuck tossing\n> pagecache [1], so the mount still ends up stuck indefinitely. Anyways,\n> I'll poke at it some more but I figure you might be able to make sense\n> of this faster than I can.\n>\n> Brian\n\nHi Brian,\n\nThanks for your report and the repro instructions. I will look into\nthis and report back what I find.\n\nThanks,\nJoanne\n>\n> [1] umount stack trace:\n>\n> task:umount          state:D stack:0     pid:12216 tgid:12216 ppid:2514   task_flags:0x400100 flags:0x00080001\n> Call Trace:\n>  <TASK>\n>  __schedule+0x2fc/0x7a0\n>  schedule+0x27/0x80\n>  io_schedule+0x46/0x70\n>  folio_wait_bit_common+0x12b/0x310\n>  ? __pfx_wake_page_function+0x10/0x10\n>  truncate_inode_pages_range+0x42a/0x4d0\n>  xfs_fs_evict_inode+0x1f/0x30 [xfs]\n>  evict+0x112/0x290\n>  evict_inodes+0x209/0x230\n>  generic_shutdown_super+0x42/0x100\n>  kill_block_super+0x1a/0x40\n>  xfs_kill_sb+0x12/0x20 [xfs]\n>  deactivate_locked_super+0x33/0xb0\n>  cleanup_mnt+0xba/0x150\n>  task_work_run+0x5c/0x90\n>  exit_to_user_mode_loop+0x12f/0x170\n>  do_syscall_64+0x1af/0x800\n>  ? vfs_statx+0x80/0x160\n>  ? do_statx+0x62/0xa0\n>  ? __x64_sys_statx+0xaf/0x100\n>  ? do_syscall_64+0xbb/0x800\n>  ? __x64_sys_statx+0xaf/0x100\n>  ? do_syscall_64+0xbb/0x800\n>  ? count_memcg_events+0xdd/0x1b0\n>  ? handle_mm_fault+0x220/0x340\n>  ? do_user_addr_fault+0x2c3/0x7f0\n>  ? clear_bhb_loop+0x30/0x80\n>  ? clear_bhb_loop+0x30/0x80\n>  entry_SYSCALL_64_after_hwframe+0x76/0x7e\n> RIP: 0033:0x7fdd641ed5ab\n> RSP: 002b:00007ffd671182e8 EFLAGS: 00000246 ORIG_RAX: 00000000000000a6\n> RAX: 0000000000000000 RBX: 0000559b3e2056b0 RCX: 00007fdd641ed5ab\n> RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000559b3e205ac0\n> RBP: 00007ffd671183c0 R08: 0000000000000000 R09: 0000000000000000\n> R10: 0000000000000103 R11: 0000000000000246 R12: 0000559b3e2057b8\n> R13: 0000000000000000 R14: 0000559b3e205ac0 R15: 0000000000000000\n>  </TASK>\n>\n\n\n---\n\nOn Thu, Oct 23, 2025 at 5:01PM Joanne Koong <joannelkoong@gmail.com> wrote:\n>\n> On Thu, Oct 23, 2025 at 12:30PM Brian Foster <bfoster@redhat.com> wrote:\n> >\n> > On Thu, Sep 25, 2025 at 05:26:02PM -0700, Joanne Koong wrote:\n> > > Instead of incrementing read_bytes_pending for every folio range read in\n> > > (which requires acquiring the spinlock to do so), set read_bytes_pending\n> > > to the folio size when the first range is asynchronously read in, keep\n> > > track of how many bytes total are asynchronously read in, and adjust\n> > > read_bytes_pending accordingly after issuing requests to read in all the\n> > > necessary ranges.\n> > >\n> > > iomap_read_folio_ctx->cur_folio_in_bio can be removed since a non-zero\n> > > value for pending bytes necessarily indicates the folio is in the bio.\n> > >\n> > > Signed-off-by: Joanne Koong <joannelkoong@gmail.com>\n> > > Suggested-by: \"Darrick J. Wong\" <djwong@kernel.org>\n> > > ---\n> >\n> > Hi Joanne,\n> >\n> > I was throwing some extra testing at the vfs-6.19.iomap branch since the\n> > little merge conflict thing with iomap_iter_advance(). I end up hitting\n> > what appears to be a lockup on XFS with 1k FSB (-bsize=1k) running\n> > generic/051. It reproduces fairly reliably within a few iterations or so\n> > and seems to always stall during a read for a dedupe operation:\n> >\n> > task:fsstress        state:D stack:0     pid:12094 tgid:12094 ppid:12091  task_flags:0x400140 flags:0x00080003\n> > Call Trace:\n> >  <TASK>\n> >  __schedule+0x2fc/0x7a0\n> >  schedule+0x27/0x80\n> >  io_schedule+0x46/0x70\n> >  folio_wait_bit_common+0x12b/0x310\n> >  ? __pfx_wake_page_function+0x10/0x10\n> >  ? __pfx_xfs_vm_read_folio+0x10/0x10 [xfs]\n> >  filemap_read_folio+0x85/0xd0\n> >  ? __pfx_xfs_vm_read_folio+0x10/0x10 [xfs]\n> >  do_read_cache_folio+0x7c/0x1b0\n> >  vfs_dedupe_file_range_compare.constprop.0+0xaf/0x2d0\n> >  __generic_remap_file_range_prep+0x276/0x2a0\n> >  generic_remap_file_range_prep+0x10/0x20\n> >  xfs_reflink_remap_prep+0x22c/0x300 [xfs]\n> >  xfs_file_remap_range+0x84/0x360 [xfs]\n> >  vfs_dedupe_file_range_one+0x1b2/0x1d0\n> >  ? remap_verify_area+0x46/0x140\n> >  vfs_dedupe_file_range+0x162/0x220\n> >  do_vfs_ioctl+0x4d1/0x940\n> >  __x64_sys_ioctl+0x75/0xe0\n> >  do_syscall_64+0x84/0x800\n> >  ? do_syscall_64+0xbb/0x800\n> >  ? avc_has_perm_noaudit+0x6b/0xf0\n> >  ? _copy_to_user+0x31/0x40\n> >  ? cp_new_stat+0x130/0x170\n> >  ? __do_sys_newfstat+0x44/0x70\n> >  ? do_syscall_64+0xbb/0x800\n> >  ? do_syscall_64+0xbb/0x800\n> >  ? clear_bhb_loop+0x30/0x80\n> >  ? clear_bhb_loop+0x30/0x80\n> >  entry_SYSCALL_64_after_hwframe+0x76/0x7e\n> > RIP: 0033:0x7fe6bbd9a14d\n> > RSP: 002b:00007ffde72cd4e0 EFLAGS: 00000246 ORIG_RAX: 0000000000000010\n> > RAX: ffffffffffffffda RBX: 0000000000000068 RCX: 00007fe6bbd9a14d\n> > RDX: 000000000a1394b0 RSI: 00000000c0189436 RDI: 0000000000000004\n> > RBP: 00007ffde72cd530 R08: 0000000000001000 R09: 000000000a11a3fc\n> > R10: 000000000001d6c0 R11: 0000000000000246 R12: 000000000a12cfb0\n> > R13: 000000000a12ba10 R14: 000000000a14e610 R15: 0000000000019000\n> >  </TASK>\n> >\n> > It wasn't immediately clear to me what the issue was so I bisected and\n> > it landed on this patch. It kind of looks like we're failing to unlock a\n> > folio at some point and then tripping over it later..? I can kill the\n> > fsstress process but then the umount ultimately gets stuck tossing\n> > pagecache [1], so the mount still ends up stuck indefinitely. Anyways,\n> > I'll poke at it some more but I figure you might be able to make sense\n> > of this faster than I can.\n> >\n> > Brian\n>\n> Hi Brian,\n>\n> Thanks for your report and the repro instructions. I will look into\n> this and report back what I find.\n\nThis is the fix:\n\ndiff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\nindex 4e6258fdb915..aa46fec8362d 100644\n--- a/fs/iomap/buffered-io.c\n+++ b/fs/iomap/buffered-io.c\n@@ -445,6 +445,9 @@ static void iomap_read_end(struct folio *folio,\nsize_t bytes_pending)\n                bool end_read, uptodate;\n                size_t bytes_accounted = folio_size(folio) - bytes_pending;\n\n+               if (!bytes_accounted)\n+                       return;\n+\n                spin_lock_irq(&ifs->state_lock);\n\n\nWhat I missed was that if all the bytes in the folio are non-uptodate\nand need to read in by the filesystem, then there's a bug where the\nread will be ended on the folio twice (in iomap_read_end() and when\nthe filesystem calls iomap_finish_folio_write(), when only the\nfilesystem should end the read), which does 2 folio unlocks which ends\nup locking the folio. Looking at the writeback patch that does a\nsimilar optimization [1], I miss the same thing there.\n\nI'll fix up both. Thanks for catching this and bisecting it down to\nthis patch. Sorry for the trouble.\n\nThanks,\nJoanne\n\n[1] https://lore.kernel.org/linux-fsdevel/20251009225611.3744728-4-joannelkoong@gmail.com/\n>\n> Thanks,\n> Joanne\n> >\n\n\n---\n\nOn Fri, Oct 24, 2025 at 10:21AM Matthew Wilcox <willy@infradead.org> wrote:\n>\n> On Fri, Oct 24, 2025 at 09:25:13AM -0700, Joanne Koong wrote:\n> > What I missed was that if all the bytes in the folio are non-uptodate\n> > and need to read in by the filesystem, then there's a bug where the\n> > read will be ended on the folio twice (in iomap_read_end() and when\n> > the filesystem calls iomap_finish_folio_write(), when only the\n> > filesystem should end the read), which does 2 folio unlocks which ends\n> > up locking the folio. Looking at the writeback patch that does a\n> > similar optimization [1], I miss the same thing there.\n>\n> folio_unlock() contains:\n>         VM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n>\n> Feels like more filesystem people should be enabling CONFIG_DEBUG_VM\n> when testing (excluding performance testing of course; it'll do ugly\n> things to your performance numbers).\n\nPoint taken. It looks like there's a bunch of other memory debugging\nconfigs as well. Do you recommend enabling all of these when testing?\nDo you have a particular .config you use for when you run tests?\n\nThanks,\nJoanne\n\n\n---\n\nOn Fri, Oct 24, 2025 at 10:10AM Brian Foster <bfoster@redhat.com> wrote:\n>\n> On Fri, Oct 24, 2025 at 09:25:13AM -0700, Joanne Koong wrote:\n> > On Thu, Oct 23, 2025 at 5:01PM Joanne Koong <joannelkoong@gmail.com> wrote:\n> > >\n> > > On Thu, Oct 23, 2025 at 12:30PM Brian Foster <bfoster@redhat.com> wrote:\n> > > >\n> > > > On Thu, Sep 25, 2025 at 05:26:02PM -0700, Joanne Koong wrote:\n> > > > > Instead of incrementing read_bytes_pending for every folio range read in\n> > > > > (which requires acquiring the spinlock to do so), set read_bytes_pending\n> > > > > to the folio size when the first range is asynchronously read in, keep\n> > > > > track of how many bytes total are asynchronously read in, and adjust\n> > > > > read_bytes_pending accordingly after issuing requests to read in all the\n> > > > > necessary ranges.\n> > > > >\n> > > > > iomap_read_folio_ctx->cur_folio_in_bio can be removed since a non-zero\n> > > > > value for pending bytes necessarily indicates the folio is in the bio.\n> > > > >\n> > > > > Signed-off-by: Joanne Koong <joannelkoong@gmail.com>\n> > > > > Suggested-by: \"Darrick J. Wong\" <djwong@kernel.org>\n> > > > > ---\n> > > >\n> > > > Hi Joanne,\n> > > >\n> > > > I was throwing some extra testing at the vfs-6.19.iomap branch since the\n> > > > little merge conflict thing with iomap_iter_advance(). I end up hitting\n> > > > what appears to be a lockup on XFS with 1k FSB (-bsize=1k) running\n> > > > generic/051. It reproduces fairly reliably within a few iterations or so\n> > > > and seems to always stall during a read for a dedupe operation:\n> > > >\n> > > > task:fsstress        state:D stack:0     pid:12094 tgid:12094 ppid:12091  task_flags:0x400140 flags:0x00080003\n> > > > Call Trace:\n> > > >  <TASK>\n> > > >  __schedule+0x2fc/0x7a0\n> > > >  schedule+0x27/0x80\n> > > >  io_schedule+0x46/0x70\n> > > >  folio_wait_bit_common+0x12b/0x310\n> > > >  ? __pfx_wake_page_function+0x10/0x10\n> > > >  ? __pfx_xfs_vm_read_folio+0x10/0x10 [xfs]\n> > > >  filemap_read_folio+0x85/0xd0\n> > > >  ? __pfx_xfs_vm_read_folio+0x10/0x10 [xfs]\n> > > >  do_read_cache_folio+0x7c/0x1b0\n> > > >  vfs_dedupe_file_range_compare.constprop.0+0xaf/0x2d0\n> > > >  __generic_remap_file_range_prep+0x276/0x2a0\n> > > >  generic_remap_file_range_prep+0x10/0x20\n> > > >  xfs_reflink_remap_prep+0x22c/0x300 [xfs]\n> > > >  xfs_file_remap_range+0x84/0x360 [xfs]\n> > > >  vfs_dedupe_file_range_one+0x1b2/0x1d0\n> > > >  ? remap_verify_area+0x46/0x140\n> > > >  vfs_dedupe_file_range+0x162/0x220\n> > > >  do_vfs_ioctl+0x4d1/0x940\n> > > >  __x64_sys_ioctl+0x75/0xe0\n> > > >  do_syscall_64+0x84/0x800\n> > > >  ? do_syscall_64+0xbb/0x800\n> > > >  ? avc_has_perm_noaudit+0x6b/0xf0\n> > > >  ? _copy_to_user+0x31/0x40\n> > > >  ? cp_new_stat+0x130/0x170\n> > > >  ? __do_sys_newfstat+0x44/0x70\n> > > >  ? do_syscall_64+0xbb/0x800\n> > > >  ? do_syscall_64+0xbb/0x800\n> > > >  ? clear_bhb_loop+0x30/0x80\n> > > >  ? clear_bhb_loop+0x30/0x80\n> > > >  entry_SYSCALL_64_after_hwframe+0x76/0x7e\n> > > > RIP: 0033:0x7fe6bbd9a14d\n> > > > RSP: 002b:00007ffde72cd4e0 EFLAGS: 00000246 ORIG_RAX: 0000000000000010\n> > > > RAX: ffffffffffffffda RBX: 0000000000000068 RCX: 00007fe6bbd9a14d\n> > > > RDX: 000000000a1394b0 RSI: 00000000c0189436 RDI: 0000000000000004\n> > > > RBP: 00007ffde72cd530 R08: 0000000000001000 R09: 000000000a11a3fc\n> > > > R10: 000000000001d6c0 R11: 0000000000000246 R12: 000000000a12cfb0\n> > > > R13: 000000000a12ba10 R14: 000000000a14e610 R15: 0000000000019000\n> > > >  </TASK>\n> > > >\n> > > > It wasn't immediately clear to me what the issue was so I bisected and\n> > > > it landed on this patch. It kind of looks like we're failing to unlock a\n> > > > folio at some point and then tripping over it later..? I can kill the\n> > > > fsstress process but then the umount ultimately gets stuck tossing\n> > > > pagecache [1], so the mount still ends up stuck indefinitely. Anyways,\n> > > > I'll poke at it some more but I figure you might be able to make sense\n> > > > of this faster than I can.\n> > > >\n> > > > Brian\n> > >\n> > > Hi Brian,\n> > >\n> > > Thanks for your report and the repro instructions. I will look into\n> > > this and report back what I find.\n> >\n> > This is the fix:\n> >\n> > diff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\n> > index 4e6258fdb915..aa46fec8362d 100644\n> > --- a/fs/iomap/buffered-io.c\n> > +++ b/fs/iomap/buffered-io.c\n> > @@ -445,6 +445,9 @@ static void iomap_read_end(struct folio *folio,\n> > size_t bytes_pending)\n> >                 bool end_read, uptodate;\n> >                 size_t bytes_accounted = folio_size(folio) - bytes_pending;\n> >\n> > +               if (!bytes_accounted)\n> > +                       return;\n> > +\n> >                 spin_lock_irq(&ifs->state_lock);\n> >\n> >\n> > What I missed was that if all the bytes in the folio are non-uptodate\n> > and need to read in by the filesystem, then there's a bug where the\n> > read will be ended on the folio twice (in iomap_read_end() and when\n> > the filesystem calls iomap_finish_folio_write(), when only the\n> > filesystem should end the read), which does 2 folio unlocks which ends\n> > up locking the folio. Looking at the writeback patch that does a\n> > similar optimization [1], I miss the same thing there.\n> >\n>\n> Makes sense.. though a short comment wouldn't hurt in there. ;) I found\n> myself a little confused by the accounted vs. pending naming when\n> reading through that code. If I follow correctly, the intent is to refer\n> to the additional bytes accounted to read_bytes_pending via the init\n> (where it just accounts the whole folio up front) and pending refers to\n> submitted I/O.\n>\n> Presumably that extra accounting doubly serves as the typical \"don't\n> complete the op before the submitter is done processing\" extra\n> reference, except in this full submit case of course. If so, that's\n> subtle enough in my mind that a sentence or two on it wouldn't hurt..\n\nI will add some a comment about this :) That's a good point about the\nnaming, maybe \"bytes_submitted\" and \"bytes_unsubmitted\" is a lot less\nconfusing than \"bytes_pending\" and \"bytes_accounted\".\n\nThanks,\nJoanne\n\n>\n> > I'll fix up both. Thanks for catching this and bisecting it down to\n> > this patch. Sorry for the trouble.\n> >\n>\n> No prob. Thanks for the fix!\n>\n> Brian\n>\n> > Thanks,\n> > Joanne\n> >\n> > [1] https://lore.kernel.org/linux-fsdevel/20251009225611.3744728-4-joannelkoong@gmail.com/\n> > >\n> > > Thanks,\n> > > Joanne\n> > > >\n> >\n>\n\n\n---\n\nOn Fri, Oct 24, 2025 at 12:48PM Joanne Koong <joannelkoong@gmail.com> wrote:\n>\n> On Fri, Oct 24, 2025 at 10:10AM Brian Foster <bfoster@redhat.com> wrote:\n> >\n> > On Fri, Oct 24, 2025 at 09:25:13AM -0700, Joanne Koong wrote:\n> > > On Thu, Oct 23, 2025 at 5:01PM Joanne Koong <joannelkoong@gmail.com> wrote:\n> > > >\n> > > > On Thu, Oct 23, 2025 at 12:30PM Brian Foster <bfoster@redhat.com> wrote:\n> > > > >\n> > > > > On Thu, Sep 25, 2025 at 05:26:02PM -0700, Joanne Koong wrote:\n> > > > > > Instead of incrementing read_bytes_pending for every folio range read in\n> > > > > > (which requires acquiring the spinlock to do so), set read_bytes_pending\n> > > > > > to the folio size when the first range is asynchronously read in, keep\n> > > > > > track of how many bytes total are asynchronously read in, and adjust\n> > > > > > read_bytes_pending accordingly after issuing requests to read in all the\n> > > > > > necessary ranges.\n> > > > > >\n> > > > > > iomap_read_folio_ctx->cur_folio_in_bio can be removed since a non-zero\n> > > > > > value for pending bytes necessarily indicates the folio is in the bio.\n> > > > > >\n> > > > > > Signed-off-by: Joanne Koong <joannelkoong@gmail.com>\n> > > > > > Suggested-by: \"Darrick J. Wong\" <djwong@kernel.org>\n> > > > > > ---\n> > > > >\n> > > > > Hi Joanne,\n> > > > >\n> > > > > I was throwing some extra testing at the vfs-6.19.iomap branch since the\n> > > > > little merge conflict thing with iomap_iter_advance(). I end up hitting\n> > > > > what appears to be a lockup on XFS with 1k FSB (-bsize=1k) running\n> > > > > generic/051. It reproduces fairly reliably within a few iterations or so\n> > > > > and seems to always stall during a read for a dedupe operation:\n> > > > >\n> > > > > task:fsstress        state:D stack:0     pid:12094 tgid:12094 ppid:12091  task_flags:0x400140 flags:0x00080003\n> > > > > Call Trace:\n> > > > >  <TASK>\n> > > > >  __schedule+0x2fc/0x7a0\n> > > > >  schedule+0x27/0x80\n> > > > >  io_schedule+0x46/0x70\n> > > > >  folio_wait_bit_common+0x12b/0x310\n> > > > >  ? __pfx_wake_page_function+0x10/0x10\n> > > > >  ? __pfx_xfs_vm_read_folio+0x10/0x10 [xfs]\n> > > > >  filemap_read_folio+0x85/0xd0\n> > > > >  ? __pfx_xfs_vm_read_folio+0x10/0x10 [xfs]\n> > > > >  do_read_cache_folio+0x7c/0x1b0\n> > > > >  vfs_dedupe_file_range_compare.constprop.0+0xaf/0x2d0\n> > > > >  __generic_remap_file_range_prep+0x276/0x2a0\n> > > > >  generic_remap_file_range_prep+0x10/0x20\n> > > > >  xfs_reflink_remap_prep+0x22c/0x300 [xfs]\n> > > > >  xfs_file_remap_range+0x84/0x360 [xfs]\n> > > > >  vfs_dedupe_file_range_one+0x1b2/0x1d0\n> > > > >  ? remap_verify_area+0x46/0x140\n> > > > >  vfs_dedupe_file_range+0x162/0x220\n> > > > >  do_vfs_ioctl+0x4d1/0x940\n> > > > >  __x64_sys_ioctl+0x75/0xe0\n> > > > >  do_syscall_64+0x84/0x800\n> > > > >  ? do_syscall_64+0xbb/0x800\n> > > > >  ? avc_has_perm_noaudit+0x6b/0xf0\n> > > > >  ? _copy_to_user+0x31/0x40\n> > > > >  ? cp_new_stat+0x130/0x170\n> > > > >  ? __do_sys_newfstat+0x44/0x70\n> > > > >  ? do_syscall_64+0xbb/0x800\n> > > > >  ? do_syscall_64+0xbb/0x800\n> > > > >  ? clear_bhb_loop+0x30/0x80\n> > > > >  ? clear_bhb_loop+0x30/0x80\n> > > > >  entry_SYSCALL_64_after_hwframe+0x76/0x7e\n> > > > > RIP: 0033:0x7fe6bbd9a14d\n> > > > > RSP: 002b:00007ffde72cd4e0 EFLAGS: 00000246 ORIG_RAX: 0000000000000010\n> > > > > RAX: ffffffffffffffda RBX: 0000000000000068 RCX: 00007fe6bbd9a14d\n> > > > > RDX: 000000000a1394b0 RSI: 00000000c0189436 RDI: 0000000000000004\n> > > > > RBP: 00007ffde72cd530 R08: 0000000000001000 R09: 000000000a11a3fc\n> > > > > R10: 000000000001d6c0 R11: 0000000000000246 R12: 000000000a12cfb0\n> > > > > R13: 000000000a12ba10 R14: 000000000a14e610 R15: 0000000000019000\n> > > > >  </TASK>\n> > > > >\n> > > > > It wasn't immediately clear to me what the issue was so I bisected and\n> > > > > it landed on this patch. It kind of looks like we're failing to unlock a\n> > > > > folio at some point and then tripping over it later..? I can kill the\n> > > > > fsstress process but then the umount ultimately gets stuck tossing\n> > > > > pagecache [1], so the mount still ends up stuck indefinitely. Anyways,\n> > > > > I'll poke at it some more but I figure you might be able to make sense\n> > > > > of this faster than I can.\n> > > > >\n> > > > > Brian\n> > > >\n> > > > Hi Brian,\n> > > >\n> > > > Thanks for your report and the repro instructions. I will look into\n> > > > this and report back what I find.\n> > >\n> > > This is the fix:\n> > >\n> > > diff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\n> > > index 4e6258fdb915..aa46fec8362d 100644\n> > > --- a/fs/iomap/buffered-io.c\n> > > +++ b/fs/iomap/buffered-io.c\n> > > @@ -445,6 +445,9 @@ static void iomap_read_end(struct folio *folio,\n> > > size_t bytes_pending)\n> > >                 bool end_read, uptodate;\n> > >                 size_t bytes_accounted = folio_size(folio) - bytes_pending;\n> > >\n> > > +               if (!bytes_accounted)\n> > > +                       return;\n> > > +\n> > >                 spin_lock_irq(&ifs->state_lock);\n> > >\n> > >\n> > > What I missed was that if all the bytes in the folio are non-uptodate\n> > > and need to read in by the filesystem, then there's a bug where the\n> > > read will be ended on the folio twice (in iomap_read_end() and when\n> > > the filesystem calls iomap_finish_folio_write(), when only the\n> > > filesystem should end the read), which does 2 folio unlocks which ends\n> > > up locking the folio. Looking at the writeback patch that does a\n> > > similar optimization [1], I miss the same thing there.\n> > >\n> >\n> > Makes sense.. though a short comment wouldn't hurt in there. ;) I found\n> > myself a little confused by the accounted vs. pending naming when\n> > reading through that code. If I follow correctly, the intent is to refer\n> > to the additional bytes accounted to read_bytes_pending via the init\n> > (where it just accounts the whole folio up front) and pending refers to\n> > submitted I/O.\n> >\n> > Presumably that extra accounting doubly serves as the typical \"don't\n> > complete the op before the submitter is done processing\" extra\n> > reference, except in this full submit case of course. If so, that's\n> > subtle enough in my mind that a sentence or two on it wouldn't hurt..\n>\n> I will add some a comment about this :) That's a good point about the\n> naming, maybe \"bytes_submitted\" and \"bytes_unsubmitted\" is a lot less\n> confusing than \"bytes_pending\" and \"bytes_accounted\".\n\nThinking about this some more, bytes_unsubmitted sounds even more\nconfusing, so maybe bytes_nonsubmitted or bytes_not_submitted. I'll\nthink about this some more but kept it as pending/accounted for now.\n\nThe fix for this bug is here [1].\n\nThanks,\nJoanne\n\n[1] https://lore.kernel.org/linux-fsdevel/20251024215008.3844068-1-joannelkoong@gmail.com/\n\n>\n> Thanks,\n> Joanne\n>\n> >\n> > > I'll fix up both. Thanks for catching this and bisecting it down to\n> > > this patch. Sorry for the trouble.\n> > >\n> >\n> > No prob. Thanks for the fix!\n> >\n> > Brian\n> >\n> > > Thanks,\n> > > Joanne\n> > >\n> > > [1] https://lore.kernel.org/linux-fsdevel/20251009225611.3744728-4-joannelkoong@gmail.com/\n> > > >\n> > > > Thanks,\n> > > > Joanne\n> > > > >\n> > >\n> >\n\n\n---\n\nOn Fri, Oct 24, 2025 at 1:59PM Matthew Wilcox <willy@infradead.org> wrote:\n>\n> On Fri, Oct 24, 2025 at 12:22:32PM -0700, Joanne Koong wrote:\n> > > Feels like more filesystem people should be enabling CONFIG_DEBUG_VM\n> > > when testing (excluding performance testing of course; it'll do ugly\n> > > things to your performance numbers).\n> >\n> > Point taken. It looks like there's a bunch of other memory debugging\n> > configs as well. Do you recommend enabling all of these when testing?\n> > Do you have a particular .config you use for when you run tests?\n>\n> Our Kconfig is far too ornate.  We could do with a \"recommended for\n> kernel developers\" profile.  Here's what I'm currently using, though I\n> know it's changed over time:\n>\n> CONFIG_X86_DEBUGCTLMSR=y\n> CONFIG_PM_DEBUG=y\n> CONFIG_PM_SLEEP_DEBUG=y\n> CONFIG_ARCH_SUPPORTS_DEBUG_PAGEALLOC=y\n> CONFIG_BLK_DEBUG_FS=y\n> CONFIG_PNP_DEBUG_MESSAGES=y\n> CONFIG_SCSI_DEBUG=m\n> CONFIG_EXT4_DEBUG=y\n> CONFIG_JFS_DEBUG=y\n> CONFIG_XFS_DEBUG=y\n> CONFIG_BTRFS_DEBUG=y\n> CONFIG_UFS_DEBUG=y\n> CONFIG_DEBUG_BUGVERBOSE=y\n> CONFIG_DEBUG_KERNEL=y\n> CONFIG_DEBUG_MISC=y\n> CONFIG_DEBUG_INFO=y\n> CONFIG_DEBUG_INFO_DWARF4=y\n> CONFIG_DEBUG_INFO_COMPRESSED_NONE=y\n> CONFIG_DEBUG_FS=y\n> CONFIG_DEBUG_FS_ALLOW_ALL=y\n> CONFIG_ARCH_HAS_EARLY_DEBUG=y\n> CONFIG_SLUB_DEBUG=y\n> CONFIG_ARCH_HAS_DEBUG_WX=y\n> CONFIG_HAVE_DEBUG_KMEMLEAK=y\n> CONFIG_SHRINKER_DEBUG=y\n> CONFIG_ARCH_HAS_DEBUG_VM_PGTABLE=y\n> CONFIG_DEBUG_VM_IRQSOFF=y\n> CONFIG_DEBUG_VM=y\n> CONFIG_ARCH_HAS_DEBUG_VIRTUAL=y\n> CONFIG_DEBUG_MEMORY_INIT=y\n> CONFIG_LOCK_DEBUGGING_SUPPORT=y\n> CONFIG_DEBUG_RT_MUTEXES=y\n> CONFIG_DEBUG_SPINLOCK=y\n> CONFIG_DEBUG_MUTEXES=y\n> CONFIG_DEBUG_WW_MUTEX_SLOWPATH=y\n> CONFIG_DEBUG_RWSEMS=y\n> CONFIG_DEBUG_LOCK_ALLOC=y\n> CONFIG_DEBUG_LIST=y\n> CONFIG_X86_DEBUG_FPU=y\n> CONFIG_FAULT_INJECTION_DEBUG_FS=y\n>\n> (output from grep DEBUG .build/.config |grep -v ^#)\n\nThank you, I'll copy this.\n>\n\n\n---\n\nOn Tue, Dec 23, 2025 at 2:30PM Sasha Levin <sashal@kernel.org> wrote:\n>\n\nHi Sasha,\n\nThanks for your patch and for the detailed writeup.\n\n> When iomap uses large folios, per-block uptodate tracking is managed via\n> iomap_folio_state (ifs). A race condition can cause the ifs uptodate bits\n> to become inconsistent with the folio's uptodate flag.\n>\n> The race occurs because folio_end_read() uses XOR semantics to atomically\n> set the uptodate bit and clear the locked bit:\n>\n>   Thread A (read completion):          Thread B (concurrent write):\n>   --------------------------------     --------------------------------\n>   iomap_finish_folio_read()\n>     spin_lock(state_lock)\n>     ifs_set_range_uptodate() -> true\n>     spin_unlock(state_lock)\n>                                        iomap_set_range_uptodate()\n>                                          spin_lock(state_lock)\n>                                          ifs_set_range_uptodate() -> true\n>                                          spin_unlock(state_lock)\n>                                          folio_mark_uptodate(folio)\n>     folio_end_read(folio, true)\n>       folio_xor_flags()  // XOR CLEARS uptodate!\n\nThe part I'm confused about here is how this can happen between a\nconcurrent read and write. My understanding is that the folio is\nlocked when the read occurs and locked when the write occurs and both\nlocks get dropped only when the read or write finishes. Looking at\niomap code, I see iomap_set_range_uptodate() getting called in\n__iomap_write_begin() and __iomap_write_end() for the writes, but in\nboth those places the folio lock is held while this is called. I'm not\nseeing how the read and write race in the diagram can happen, but\nmaybe I'm missing something here?\n\n>\n> Result: folio is NOT uptodate, but ifs says all blocks ARE uptodate.\n\nAh I see the WARN_ON_ONCE() in ifs_free:\n        WARN_ON_ONCE(ifs_is_fully_uptodate(folio, ifs) !=\n                        folio_test_uptodate(folio));\n\nJust to confirm, are you seeing that the folio is not marked uptodate\nbut the ifs blocks are? Or are the ifs blocks not uptodate but the\nfolio is?\n\n>\n> Fix by checking read_bytes_pending in iomap_set_range_uptodate() under the\n> lock. If a read is in progress, skip calling folio_mark_uptodate() - the\n> read completion path will handle it via folio_end_read().\n>\n> The warning was triggered during FUSE-based filesystem (e.g., NTFS-3G)\n> unmount when the LTP writev03 test was run:\n>\n>   WARNING: fs/iomap/buffered-io.c at ifs_free\n>   Call trace:\n>    ifs_free\n>    iomap_invalidate_folio\n>    truncate_cleanup_folio\n>    truncate_inode_pages_range\n>    truncate_inode_pages_final\n>    fuse_evict_inode\n>    ...\n>    fuse_kill_sb_blk\n>\n> Fixes: 7a4847e54cc1 (\"iomap: use folio_end_read()\")\n> Assisted-by: claude-opus-4-5-20251101\n> Signed-off-by: Sasha Levin <sashal@kernel.org>\n> ---\n>  fs/fuse/dev.c          |  3 +-\n>  fs/fuse/file.c         |  6 ++--\n>  fs/iomap/buffered-io.c | 65 +++++++++++++++++++++++++++++++++++++++---\n>  include/linux/iomap.h  |  2 ++\n>  4 files changed, 68 insertions(+), 8 deletions(-)\n>\n> diff --git a/fs/fuse/dev.c b/fs/fuse/dev.c\n> index 6d59cbc877c6..50e84e913589 100644\n> --- a/fs/fuse/dev.c\n> +++ b/fs/fuse/dev.c\n> @@ -11,6 +11,7 @@\n>  #include \"fuse_dev_i.h\"\n>\n>  #include <linux/init.h>\n> +#include <linux/iomap.h>\n>  #include <linux/module.h>\n>  #include <linux/poll.h>\n>  #include <linux/sched/signal.h>\n> @@ -1820,7 +1821,7 @@ static int fuse_notify_store(struct fuse_conn *fc, unsigned int size,\n>                 if (!folio_test_uptodate(folio) && !err && offset == 0 &&\n>                     (nr_bytes == folio_size(folio) || file_size == end)) {\n>                         folio_zero_segment(folio, nr_bytes, folio_size(folio));\n> -                       folio_mark_uptodate(folio);\n> +                       iomap_set_range_uptodate(folio, 0, folio_size(folio));\n>                 }\n>                 folio_unlock(folio);\n>                 folio_put(folio);\n> diff --git a/fs/fuse/file.c b/fs/fuse/file.c\n> index 01bc894e9c2b..3abe38416199 100644\n> --- a/fs/fuse/file.c\n> +++ b/fs/fuse/file.c\n> @@ -1216,13 +1216,13 @@ static ssize_t fuse_send_write_pages(struct fuse_io_args *ia,\n>                 struct folio *folio = ap->folios[i];\n>\n>                 if (err) {\n> -                       folio_clear_uptodate(folio);\n> +                       iomap_clear_folio_uptodate(folio);\n>                 } else {\n>                         if (count >= folio_size(folio) - offset)\n>                                 count -= folio_size(folio) - offset;\n>                         else {\n>                                 if (short_write)\n> -                                       folio_clear_uptodate(folio);\n> +                                       iomap_clear_folio_uptodate(folio);\n>                                 count = 0;\n>                         }\n>                         offset = 0;\n> @@ -1305,7 +1305,7 @@ static ssize_t fuse_fill_write_pages(struct fuse_io_args *ia,\n>\n>                 /* If we copied full folio, mark it uptodate */\n>                 if (tmp == folio_size(folio))\n> -                       folio_mark_uptodate(folio);\n> +                       iomap_set_range_uptodate(folio, 0, folio_size(folio));\n>\n>                 if (folio_test_uptodate(folio)) {\n>                         folio_unlock(folio);\n> diff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\n> index e5c1ca440d93..7ceda24cf6a7 100644\n> --- a/fs/iomap/buffered-io.c\n> +++ b/fs/iomap/buffered-io.c\n> @@ -74,8 +74,7 @@ static bool ifs_set_range_uptodate(struct folio *folio,\n>         return ifs_is_fully_uptodate(folio, ifs);\n>  }\n>\n> -static void iomap_set_range_uptodate(struct folio *folio, size_t off,\n> -               size_t len)\n> +void iomap_set_range_uptodate(struct folio *folio, size_t off, size_t len)\n>  {\n>         struct iomap_folio_state *ifs = folio->private;\n>         unsigned long flags;\n> @@ -87,12 +86,50 @@ static void iomap_set_range_uptodate(struct folio *folio, size_t off,\n>         if (ifs) {\n>                 spin_lock_irqsave(&ifs->state_lock, flags);\n>                 uptodate = ifs_set_range_uptodate(folio, ifs, off, len);\n> +               /*\n> +                * If a read is in progress, we must NOT call folio_mark_uptodate\n> +                * here. The read completion path (iomap_finish_folio_read or\n> +                * iomap_read_end) will call folio_end_read() which uses XOR\n> +                * semantics to set the uptodate bit. If we set it here, the XOR\n> +                * in folio_end_read() will clear it, leaving the folio not\n> +                * uptodate while the ifs says all blocks are uptodate.\n> +                */\n> +               if (uptodate && ifs->read_bytes_pending)\n> +                       uptodate = false;\n\nDoes the warning you saw in ifs_free() still go away without the\nchanges here to iomap_set_range_uptodate() or is this change here\nnecessary?  I'm asking mostly because I'm not seeing how\niomap_set_range_uptodate() can be called while the read is in\nprogress, as the logic should be already protected by the folio locks.\n\n>                 spin_unlock_irqrestore(&ifs->state_lock, flags);\n>         }\n>\n>         if (uptodate)\n>                 folio_mark_uptodate(folio);\n>  }\n> +EXPORT_SYMBOL_GPL(iomap_set_range_uptodate);\n> +\n> +void iomap_clear_folio_uptodate(struct folio *folio)\n> +{\n> +       struct iomap_folio_state *ifs = folio->private;\n> +\n> +       if (ifs) {\n> +               struct inode *inode = folio->mapping->host;\n> +               unsigned int nr_blocks = i_blocks_per_folio(inode, folio);\n> +               unsigned long flags;\n> +\n> +               spin_lock_irqsave(&ifs->state_lock, flags);\n> +               /*\n> +                * If a read is in progress, don't clear the uptodate state.\n> +                * The read completion path will handle the folio state, and\n> +                * clearing here would race with iomap_finish_folio_read()\n> +                * potentially causing ifs/folio uptodate state mismatch.\n> +                */\n> +               if (ifs->read_bytes_pending) {\n> +                       spin_unlock_irqrestore(&ifs->state_lock, flags);\n> +                       return;\n> +               }\n> +               bitmap_clear(ifs->state, 0, nr_blocks);\n> +               spin_unlock_irqrestore(&ifs->state_lock, flags);\n> +       }\n> +       folio_clear_uptodate(folio);\n> +}\n> +EXPORT_SYMBOL_GPL(iomap_clear_folio_uptodate);\n>\n>  /*\n>   * Find the next dirty block in the folio. end_blk is inclusive.\n> @@ -399,8 +436,17 @@ void iomap_finish_folio_read(struct folio *folio, size_t off, size_t len,\n>                 spin_unlock_irqrestore(&ifs->state_lock, flags);\n>         }\n>\n> -       if (finished)\n> +       if (finished) {\n> +               /*\n> +                * If uptodate is true but the folio is already marked uptodate,\n> +                * folio_end_read's XOR semantics would clear the uptodate bit.\n> +                * This should never happen because iomap_set_range_uptodate()\n> +                * skips calling folio_mark_uptodate() when read_bytes_pending\n> +                * is non-zero, ensuring only the read completion path sets it.\n> +                */\n> +               WARN_ON_ONCE(uptodate && folio_test_uptodate(folio));\n\nMatthew pointed out in another thread [1] that folio_end_read() has\nalready the warnings against double-unlocks or double-uptodates\nin-built:\n\n        VM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n        VM_BUG_ON_FOLIO(success && folio_test_uptodate(folio), folio);\n\nbut imo the WARN_ON_ONCE() here is nice to have too, as I don't think\nmost builds enable CONFIG_DEBUG_VM.\n\n[1] https://lore.kernel.org/linux-fsdevel/aPu1ilw6Tq6tKPrf@casper.infradead.org/\n\nThanks,\nJoanne\n>                 folio_end_read(folio, uptodate);\n> +       }\n>  }\n>  EXPORT_SYMBOL_GPL(iomap_finish_folio_read);\n>\n> @@ -481,8 +527,19 @@ static void iomap_read_end(struct folio *folio, size_t bytes_submitted)\n>                 if (end_read)\n>                         uptodate = ifs_is_fully_uptodate(folio, ifs);\n>                 spin_unlock_irq(&ifs->state_lock);\n> -               if (end_read)\n> +               if (end_read) {\n> +                       /*\n> +                        * If uptodate is true but the folio is already marked\n> +                        * uptodate, folio_end_read's XOR semantics would clear\n> +                        * the uptodate bit. This should never happen because\n> +                        * iomap_set_range_uptodate() skips calling\n> +                        * folio_mark_uptodate() when read_bytes_pending is\n> +                        * non-zero, ensuring only the read completion path\n> +                        * sets it.\n> +                        */\n> +                       WARN_ON_ONCE(uptodate && folio_test_uptodate(folio));\n>                         folio_end_read(folio, uptodate);\n> +               }\n>         } else if (!bytes_submitted) {\n>                 /*\n>                  * If there were no bytes submitted, this means we are\n> diff --git a/include/linux/iomap.h b/include/linux/iomap.h\n> index 520e967cb501..3c2ad88d16b6 100644\n> --- a/include/linux/iomap.h\n> +++ b/include/linux/iomap.h\n> @@ -345,6 +345,8 @@ void iomap_read_folio(const struct iomap_ops *ops,\n>  void iomap_readahead(const struct iomap_ops *ops,\n>                 struct iomap_read_folio_ctx *ctx);\n>  bool iomap_is_partially_uptodate(struct folio *, size_t from, size_t count);\n> +void iomap_set_range_uptodate(struct folio *folio, size_t off, size_t len);\n> +void iomap_clear_folio_uptodate(struct folio *folio);\n>  struct folio *iomap_get_folio(struct iomap_iter *iter, loff_t pos, size_t len);\n>  bool iomap_release_folio(struct folio *folio, gfp_t gfp_flags);\n>  void iomap_invalidate_folio(struct folio *folio, size_t offset, size_t len);\n> --\n> 2.51.0\n>\n\n\n---\n\nOn Wed, Dec 24, 2025 at 1:21PM Sasha Levin <sashal@kernel.org> wrote:\n>\n> On Wed, Dec 24, 2025 at 05:27:03PM +0000, Matthew Wilcox wrote:\n> >\n> > WARNING: fs/iomap/buffered-io.c:254 at ifs_free+0x130/0x148, CPU#0: msync04/406\n> >\n> >That's this one:\n> >\n> >        WARN_ON_ONCE(ifs_is_fully_uptodate(folio, ifs) !=\n> >                        folio_test_uptodate(folio));\n> >\n> >which would be fully explained by fuse calling folio_clear_uptodate()\n> >in fuse_send_write_pages().  I have come to believe that allowing\n> >filesystems to call folio_clear_uptodate() is just dangerous.  It\n> >causes assertions to fire all over the place (eg if the page is mapped\n> >into memory, the MM contains assertions that it must be uptodate).\n> >\n> >So I think the first step is simply to delete the folio_clear_uptodate()\n> >calls in fuse:\n\nHmm... this fuse_perform_write() call path is for writethrough. In\nwritethrough, fuse first writes the data to the page cache and then to\nthe server. I think because we're doing the writes in that order (eg\nfirst to the page cache, then the server), the clear uptodate is\nneeded if the server write is a short write or an error since we can't\nrevert the page cache data back to its original content (eg we want to\nwrite 2 KB starting at offset 0, the folio representing that in the\npage cache is uptodate, we retrieve that folio and write 2 KB to it,\nthen when we try writing it to the server, the server can only write\nout 1 KB, where now there's a discrepancy between the page cache\ncontents and the disk contents, where we're unable to make these\nconsistent by undoing the page cache write for the chunk between 1 KB\nand 2 KB). If we could switch the ordering and write it to the server\nfirst and then to the page cache, then we could get rid of the clear\nuptodates, but to switch this ordering requires a bigger change where\nwe'd need to add support for copying out data from a userspace iter to\nthe server (currently, only copying out data from folios are\nsupported). I'm happy to work on this though if you think we should\ntry our best to fully eradicate folio_clear_uptodate() from fuse.\n\nThere's also another folio_clear_uptodate() call in\nfuse_try_move_folio() in fuse/dev.c when the server gifts pages to the\nkernel through vmsplice. This one I think is needed else\nfolio_end_read() will xor uptodate state of an already uptodate folio\n(commit 76a51ac (\"fuse: clear PG_uptodate when using a stolen page\")\nsays a bit more about this).\n\n> [snip]\n>\n> Here's the log of a run with the change you've provided applied: https://qa-reports.linaro.org/lkft/sashal-linus-next/build/v6.18-rc7-13807-g26a15474eb13/testrun/30620754/suite/log-parser-test/test/exception-warning-fsiomapbuffered-io-at-ifs_free/log\n\nHmm, I think this WARN_ON_ONCE is getting triggered from the\nfolio_mark_uptodate() call in fuse_fill_write_pages().\n\nThis is happening because iomap integration hasn't (yet) been added to\nthe fuse writethrough path, as it's not necessary / urgent (whereas\nfor buffered writes, it is in order for fuse to use large folios). imo\nupdating the folio uptodate/dirty state but not the bitmap is\nlogically fine as the worst outcome from this is that we miss being\nable to skip some extra read calls that we could saved if we did add\nthe iomap bitmap integration. However, I didn't realize there's a\nWARN_ON_ONCE checking the ifs uptodate bitmap state (but curiously no\nWARN_ON_ONCE checking the ifs dirty bitmap state).\n\nWith that said, I think it makes sense to either a) do the\niomap_set_range_uptodate() / iomap_clear_folio_uptodate() bitmap\nupdating you proposed as a fix for this WARN_ON_ONCE for now to\nunblock things, until iomap integration gets added to the fuse\nwritethrough path, which I'll now prioritize, or b) remove that\nwarning. The warning does seem otherwise useful though so it seems\nlike we should probably just go with a).\n\nThanks,\nJoanne\n\n>\n> --\n> Thanks,\n> Sasha\n\n\n---\n\nOn Fri, Feb 6, 2026 at 11:16PM Wei Gao <wegao@suse.com> wrote:\n>\n> On Tue, Dec 23, 2025 at 08:31:57PM -0500, Sasha Levin wrote:\n> > On Tue, Dec 23, 2025 at 05:12:09PM -0800, Joanne Koong wrote:\n> > > On Tue, Dec 23, 2025 at 2:30PM Sasha Levin <sashal@kernel.org> wrote:\n> > > >\n> > >\n> > > Hi Sasha,\n> > >\n> > > Thanks for your patch and for the detailed writeup.\n> >\n> > Thanks for looking into this!\n> >\n> > > > When iomap uses large folios, per-block uptodate tracking is managed via\n> > > > iomap_folio_state (ifs). A race condition can cause the ifs uptodate bits\n> > > > to become inconsistent with the folio's uptodate flag.\n> > > >\n> > > > The race occurs because folio_end_read() uses XOR semantics to atomically\n> > > > set the uptodate bit and clear the locked bit:\n> > > >\n> > > >   Thread A (read completion):          Thread B (concurrent write):\n> > > >   --------------------------------     --------------------------------\n> > > >   iomap_finish_folio_read()\n> > > >     spin_lock(state_lock)\n> > > >     ifs_set_range_uptodate() -> true\n> > > >     spin_unlock(state_lock)\n> > > >                                        iomap_set_range_uptodate()\n> > > >                                          spin_lock(state_lock)\n> > > >                                          ifs_set_range_uptodate() -> true\n> > > >                                          spin_unlock(state_lock)\n> > > >                                          folio_mark_uptodate(folio)\n> > > >     folio_end_read(folio, true)\n> > > >       folio_xor_flags()  // XOR CLEARS uptodate!\n> > >\n> > > The part I'm confused about here is how this can happen between a\n> > > concurrent read and write. My understanding is that the folio is\n> > > locked when the read occurs and locked when the write occurs and both\n> > > locks get dropped only when the read or write finishes. Looking at\n> > > iomap code, I see iomap_set_range_uptodate() getting called in\n> > > __iomap_write_begin() and __iomap_write_end() for the writes, but in\n> > > both those places the folio lock is held while this is called. I'm not\n> > > seeing how the read and write race in the diagram can happen, but\n> > > maybe I'm missing something here?\n> >\n> > Hmm, you're right... The folio lock should prevent concurrent read/write\n> > access. Looking at this again, I suspect that FUSE was calling\n> > folio_clear_uptodate() and folio_mark_uptodate() directly without updating the\n> > ifs bits. For example, in fuse_send_write_pages() on write error, it calls\n> > folio_clear_uptodate(folio) which clears the folio flag but leaves ifs still\n> > showing all blocks uptodate?\n>\n> Hi Sasha\n> On PowerPC with 64KB page size, msync04 fails with SIGBUS on NTFS-FUSE. The issue stems from a state inconsistency between\n> the iomap_folio_state (ifs) bitmap and the folio's Uptodate flag.\n> tst_test.c:1985: TINFO: === Testing on ntfs ===\n> tst_test.c:1290: TINFO: Formatting /dev/loop0 with ntfs opts='' extra opts=''\n> Failed to set locale, using default 'C'.\n> The partition start sector was not specified for /dev/loop0 and it could not be obtained automatically.  It has been set to 0.\n> The number of sectors per track was not specified for /dev/loop0 and it could not be obtained automatically.  It has been set to 0.\n> The number of heads was not specified for /dev/loop0 and it could not be obtained automatically.  It has been set to 0.\n> To boot from a device, Windows needs the 'partition start sector', the 'sectors per track' and the 'number of heads' to be set.\n> Windows will not be able to boot from this device.\n> tst_test.c:1302: TINFO: Mounting /dev/loop0 to /tmp/LTP_msy3ljVxi/msync04 fstyp=ntfs flags=0\n> tst_test.c:1302: TINFO: Trying FUSE...\n> tst_test.c:1953: TBROK: Test killed by SIGBUS!\n>\n> Root Cause Analysis: When a page fault triggers fuse_read_folio, the iomap_read_folio_iter handles the request. For a 64KB page,\n> after fetching 4KB via fuse_iomap_read_folio_range_async, the remaining 60KB (61440 bytes) is zero-filled via iomap_block_needs_zeroing,\n> then iomap_set_range_uptodate marks the folio as Uptodate globally, after folio_xor_flags folio's uptodate become 0 again, finally trigger\n> an SIGBUS issue in filemap_fault.\n\nHi Wei,\n\nThanks for your report. afaict, this scenario occurs only if the\nserver is a fuseblk server with a block size different from the memory\npage size and if the file size is less than the size of the folio\nbeing read in.\n\nCould you verify that this snippet from Sasha's patch fixes the issue?:\n\ndiff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\nindex e5c1ca440d93..7ceda24cf6a7 100644\n--- a/fs/iomap/buffered-io.c\n+++ b/fs/iomap/buffered-io.c\n@@ -87,12 +86,50 @@ static void iomap_set_range_uptodate(struct folio\n*folio, size_t off,\n  if (ifs) {\n          spin_lock_irqsave(&ifs->state_lock, flags);\n          uptodate = ifs_set_range_uptodate(folio, ifs, off, len);\n          + /*\n          + * If a read is in progress, we must NOT call folio_mark_uptodate\n          + * here. The read completion path (iomap_finish_folio_read or\n          + * iomap_read_end) will call folio_end_read() which uses XOR\n          + * semantics to set the uptodate bit. If we set it here, the XOR\n          + * in folio_end_read() will clear it, leaving the folio not\n          + * uptodate while the ifs says all blocks are uptodate.\n          + */\n         + if (uptodate && ifs->read_bytes_pending)\n                   + uptodate = false;\n        spin_unlock_irqrestore(&ifs->state_lock, flags);\n  }\n\nThanks,\nJoanne\n\n>\n> So your iomap_set_range_uptodate patch can fix above failed case since it block mark folio's uptodate to 1.\n> Hope my findings are helpful.\n>\n> >\n> > --\n> > Thanks,\n> > Sasha\n> >\n\n\n---\n\nOn Mon, Feb 9, 2026 at 4:12PM Wei Gao <wegao@suse.com> wrote:\n>\n> On Mon, Feb 09, 2026 at 11:08:50AM -0800, Joanne Koong wrote:\n> > On Fri, Feb 6, 2026 at 11:16PM Wei Gao <wegao@suse.com> wrote:\n> > >\n> > > On Tue, Dec 23, 2025 at 08:31:57PM -0500, Sasha Levin wrote:\n> > > > On Tue, Dec 23, 2025 at 05:12:09PM -0800, Joanne Koong wrote:\n> > > > > On Tue, Dec 23, 2025 at 2:30PM Sasha Levin <sashal@kernel.org> wrote:\n> > > > > >\n> > > > >\n> > > > > Hi Sasha,\n> > > > >\n> > > > > Thanks for your patch and for the detailed writeup.\n> > > >\n> > > > Thanks for looking into this!\n> > > >\n> > > > > > When iomap uses large folios, per-block uptodate tracking is managed via\n> > > > > > iomap_folio_state (ifs). A race condition can cause the ifs uptodate bits\n> > > > > > to become inconsistent with the folio's uptodate flag.\n> > > > > >\n> > > > > > The race occurs because folio_end_read() uses XOR semantics to atomically\n> > > > > > set the uptodate bit and clear the locked bit:\n> > > > > >\n> > > > > >   Thread A (read completion):          Thread B (concurrent write):\n> > > > > >   --------------------------------     --------------------------------\n> > > > > >   iomap_finish_folio_read()\n> > > > > >     spin_lock(state_lock)\n> > > > > >     ifs_set_range_uptodate() -> true\n> > > > > >     spin_unlock(state_lock)\n> > > > > >                                        iomap_set_range_uptodate()\n> > > > > >                                          spin_lock(state_lock)\n> > > > > >                                          ifs_set_range_uptodate() -> true\n> > > > > >                                          spin_unlock(state_lock)\n> > > > > >                                          folio_mark_uptodate(folio)\n> > > > > >     folio_end_read(folio, true)\n> > > > > >       folio_xor_flags()  // XOR CLEARS uptodate!\n> > > > >\n> > > > > The part I'm confused about here is how this can happen between a\n> > > > > concurrent read and write. My understanding is that the folio is\n> > > > > locked when the read occurs and locked when the write occurs and both\n> > > > > locks get dropped only when the read or write finishes. Looking at\n> > > > > iomap code, I see iomap_set_range_uptodate() getting called in\n> > > > > __iomap_write_begin() and __iomap_write_end() for the writes, but in\n> > > > > both those places the folio lock is held while this is called. I'm not\n> > > > > seeing how the read and write race in the diagram can happen, but\n> > > > > maybe I'm missing something here?\n> > > >\n> > > > Hmm, you're right... The folio lock should prevent concurrent read/write\n> > > > access. Looking at this again, I suspect that FUSE was calling\n> > > > folio_clear_uptodate() and folio_mark_uptodate() directly without updating the\n> > > > ifs bits. For example, in fuse_send_write_pages() on write error, it calls\n> > > > folio_clear_uptodate(folio) which clears the folio flag but leaves ifs still\n> > > > showing all blocks uptodate?\n> > >\n> > > Hi Sasha\n> > > On PowerPC with 64KB page size, msync04 fails with SIGBUS on NTFS-FUSE. The issue stems from a state inconsistency between\n> > > the iomap_folio_state (ifs) bitmap and the folio's Uptodate flag.\n> > > tst_test.c:1985: TINFO: === Testing on ntfs ===\n> > > tst_test.c:1290: TINFO: Formatting /dev/loop0 with ntfs opts='' extra opts=''\n> > > Failed to set locale, using default 'C'.\n> > > The partition start sector was not specified for /dev/loop0 and it could not be obtained automatically.  It has been set to 0.\n> > > The number of sectors per track was not specified for /dev/loop0 and it could not be obtained automatically.  It has been set to 0.\n> > > The number of heads was not specified for /dev/loop0 and it could not be obtained automatically.  It has been set to 0.\n> > > To boot from a device, Windows needs the 'partition start sector', the 'sectors per track' and the 'number of heads' to be set.\n> > > Windows will not be able to boot from this device.\n> > > tst_test.c:1302: TINFO: Mounting /dev/loop0 to /tmp/LTP_msy3ljVxi/msync04 fstyp=ntfs flags=0\n> > > tst_test.c:1302: TINFO: Trying FUSE...\n> > > tst_test.c:1953: TBROK: Test killed by SIGBUS!\n> > >\n> > > Root Cause Analysis: When a page fault triggers fuse_read_folio, the iomap_read_folio_iter handles the request. For a 64KB page,\n> > > after fetching 4KB via fuse_iomap_read_folio_range_async, the remaining 60KB (61440 bytes) is zero-filled via iomap_block_needs_zeroing,\n> > > then iomap_set_range_uptodate marks the folio as Uptodate globally, after folio_xor_flags folio's uptodate become 0 again, finally trigger\n> > > an SIGBUS issue in filemap_fault.\n> >\n> > Hi Wei,\n> >\n> > Thanks for your report. afaict, this scenario occurs only if the\n> > server is a fuseblk server with a block size different from the memory\n> > page size and if the file size is less than the size of the folio\n> > being read in.\n> Thanks for checking this and give quick feedback :)\n> >\n> > Could you verify that this snippet from Sasha's patch fixes the issue?:\n> Yes, Sasha's patch can fixes the issue.\n\nI think just those lines I pasted from Sasha's patch is the relevant\nfix. Could you verify that just those lines (without the changes\nfrom the rest of his patch) fixes the issue?\n\nThanks,\nJoanne\n\n\n> >\n> > diff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\n> > index e5c1ca440d93..7ceda24cf6a7 100644\n> > --- a/fs/iomap/buffered-io.c\n> > +++ b/fs/iomap/buffered-io.c\n> > @@ -87,12 +86,50 @@ static void iomap_set_range_uptodate(struct folio\n> > *folio, size_t off,\n> >   if (ifs) {\n> >           spin_lock_irqsave(&ifs->state_lock, flags);\n> >           uptodate = ifs_set_range_uptodate(folio, ifs, off, len);\n> >           + /*\n> >           + * If a read is in progress, we must NOT call folio_mark_uptodate\n> >           + * here. The read completion path (iomap_finish_folio_read or\n> >           + * iomap_read_end) will call folio_end_read() which uses XOR\n> >           + * semantics to set the uptodate bit. If we set it here, the XOR\n> >           + * in folio_end_read() will clear it, leaving the folio not\n> >           + * uptodate while the ifs says all blocks are uptodate.\n> >           + */\n> >          + if (uptodate && ifs->read_bytes_pending)\n> >                    + uptodate = false;\n> >         spin_unlock_irqrestore(&ifs->state_lock, flags);\n> >   }\n> >\n> > Thanks,\n> > Joanne\n> >\n> > >\n> > > So your iomap_set_range_uptodate patch can fix above failed case since it block mark folio's uptodate to 1.\n> > > Hope my findings are helpful.\n> > >\n> > > >\n> > > > --\n> > > > Thanks,\n> > > > Sasha\n> > > >\n\n\n---\n\nOn Mon, Feb 9, 2026 at 4:40PM Wei Gao <wegao@suse.com> wrote:\n>\n> On Mon, Feb 09, 2026 at 04:20:01PM -0800, Joanne Koong wrote:\n> > On Mon, Feb 9, 2026 at 4:12PM Wei Gao <wegao@suse.com> wrote:\n> > >\n> > > On Mon, Feb 09, 2026 at 11:08:50AM -0800, Joanne Koong wrote:\n> > > > On Fri, Feb 6, 2026 at 11:16PM Wei Gao <wegao@suse.com> wrote:\n> > > > >\n> > > > > On Tue, Dec 23, 2025 at 08:31:57PM -0500, Sasha Levin wrote:\n> > > > > > On Tue, Dec 23, 2025 at 05:12:09PM -0800, Joanne Koong wrote:\n> > > > > > > On Tue, Dec 23, 2025 at 2:30PM Sasha Levin <sashal@kernel.org> wrote:\n> > > > > > > >\n> > > > > > >\n> > > > > > > Hi Sasha,\n> > > > > > >\n> > > > > > > Thanks for your patch and for the detailed writeup.\n> > > > > >\n> > > > > > Thanks for looking into this!\n> > > > > >\n> > > > > > > > When iomap uses large folios, per-block uptodate tracking is managed via\n> > > > > > > > iomap_folio_state (ifs). A race condition can cause the ifs uptodate bits\n> > > > > > > > to become inconsistent with the folio's uptodate flag.\n> > > > > > > >\n> > > > > > > > The race occurs because folio_end_read() uses XOR semantics to atomically\n> > > > > > > > set the uptodate bit and clear the locked bit:\n> > > > > > > >\n> > > > > > > >   Thread A (read completion):          Thread B (concurrent write):\n> > > > > > > >   --------------------------------     --------------------------------\n> > > > > > > >   iomap_finish_folio_read()\n> > > > > > > >     spin_lock(state_lock)\n> > > > > > > >     ifs_set_range_uptodate() -> true\n> > > > > > > >     spin_unlock(state_lock)\n> > > > > > > >                                        iomap_set_range_uptodate()\n> > > > > > > >                                          spin_lock(state_lock)\n> > > > > > > >                                          ifs_set_range_uptodate() -> true\n> > > > > > > >                                          spin_unlock(state_lock)\n> > > > > > > >                                          folio_mark_uptodate(folio)\n> > > > > > > >     folio_end_read(folio, true)\n> > > > > > > >       folio_xor_flags()  // XOR CLEARS uptodate!\n> > > > > > >\n> > > > > > > The part I'm confused about here is how this can happen between a\n> > > > > > > concurrent read and write. My understanding is that the folio is\n> > > > > > > locked when the read occurs and locked when the write occurs and both\n> > > > > > > locks get dropped only when the read or write finishes. Looking at\n> > > > > > > iomap code, I see iomap_set_range_uptodate() getting called in\n> > > > > > > __iomap_write_begin() and __iomap_write_end() for the writes, but in\n> > > > > > > both those places the folio lock is held while this is called. I'm not\n> > > > > > > seeing how the read and write race in the diagram can happen, but\n> > > > > > > maybe I'm missing something here?\n> > > > > >\n> > > > > > Hmm, you're right... The folio lock should prevent concurrent read/write\n> > > > > > access. Looking at this again, I suspect that FUSE was calling\n> > > > > > folio_clear_uptodate() and folio_mark_uptodate() directly without updating the\n> > > > > > ifs bits. For example, in fuse_send_write_pages() on write error, it calls\n> > > > > > folio_clear_uptodate(folio) which clears the folio flag but leaves ifs still\n> > > > > > showing all blocks uptodate?\n> > > > >\n> > > > > Hi Sasha\n> > > > > On PowerPC with 64KB page size, msync04 fails with SIGBUS on NTFS-FUSE. The issue stems from a state inconsistency between\n> > > > > the iomap_folio_state (ifs) bitmap and the folio's Uptodate flag.\n> > > > > tst_test.c:1985: TINFO: === Testing on ntfs ===\n> > > > > tst_test.c:1290: TINFO: Formatting /dev/loop0 with ntfs opts='' extra opts=''\n> > > > > Failed to set locale, using default 'C'.\n> > > > > The partition start sector was not specified for /dev/loop0 and it could not be obtained automatically.  It has been set to 0.\n> > > > > The number of sectors per track was not specified for /dev/loop0 and it could not be obtained automatically.  It has been set to 0.\n> > > > > The number of heads was not specified for /dev/loop0 and it could not be obtained automatically.  It has been set to 0.\n> > > > > To boot from a device, Windows needs the 'partition start sector', the 'sectors per track' and the 'number of heads' to be set.\n> > > > > Windows will not be able to boot from this device.\n> > > > > tst_test.c:1302: TINFO: Mounting /dev/loop0 to /tmp/LTP_msy3ljVxi/msync04 fstyp=ntfs flags=0\n> > > > > tst_test.c:1302: TINFO: Trying FUSE...\n> > > > > tst_test.c:1953: TBROK: Test killed by SIGBUS!\n> > > > >\n> > > > > Root Cause Analysis: When a page fault triggers fuse_read_folio, the iomap_read_folio_iter handles the request. For a 64KB page,\n> > > > > after fetching 4KB via fuse_iomap_read_folio_range_async, the remaining 60KB (61440 bytes) is zero-filled via iomap_block_needs_zeroing,\n> > > > > then iomap_set_range_uptodate marks the folio as Uptodate globally, after folio_xor_flags folio's uptodate become 0 again, finally trigger\n> > > > > an SIGBUS issue in filemap_fault.\n> > > >\n> > > > Hi Wei,\n> > > >\n> > > > Thanks for your report. afaict, this scenario occurs only if the\n> > > > server is a fuseblk server with a block size different from the memory\n> > > > page size and if the file size is less than the size of the folio\n> > > > being read in.\n> > > Thanks for checking this and give quick feedback :)\n> > > >\n> > > > Could you verify that this snippet from Sasha's patch fixes the issue?:\n> > > Yes, Sasha's patch can fixes the issue.\n> >\n> > I think just those lines I pasted from Sasha's patch is the relevant\n> > fix. Could you verify that just those lines (without the changes\n> > from the rest of his patch) fixes the issue?\n> Yes, i just add two lines change in iomap_set_range_uptodate can fixes\n> the issue.\n\nGreat, thank you for confirming.\n\nSasha, would you mind submitting this snippet of your patch as the fix\nfor the EOF zeroing issue? I think it could be restructured to\n\ndiff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\nindex 1fe19b4ee2f4..412e661871f8 100644\n--- a/fs/iomap/buffered-io.c\n+++ b/fs/iomap/buffered-io.c\n@@ -87,7 +87,16 @@ static void iomap_set_range_uptodate(struct folio\n*folio, size_t off,\n\n        if (ifs) {\n                spin_lock_irqsave(&ifs->state_lock, flags);\n-               uptodate = ifs_set_range_uptodate(folio, ifs, off, len);\n+               /*\n+                * If a read is in progress, we must NOT call\nfolio_mark_uptodate.\n+                * The read completion path (iomap_finish_folio_read or\n+                * iomap_read_end) will call folio_end_read() which uses XOR\n+                * semantics to set the uptodate bit. If we set it here, the XOR\n+                * in folio_end_read() will clear it, leaving the folio not\n+                * uptodate.\n+                */\n+               uptodate = ifs_set_range_uptodate(folio, ifs, off, len) &&\n+                       !ifs->read_bytes_pending;\n                spin_unlock_irqrestore(&ifs->state_lock, flags);\n        }\n\nto be a bit more concise.\n\nIf you're busy and don't have the bandwidth, I'm happy to forward the\npatch on your behalf with your Signed-off-by / authorship.\n\nThanks,\nJoanne\n> +               if (uptodate && ifs->read_bytes_pending)\n> +                       uptodate = false;\n> >\n> > Thanks,\n> > Joanne\n> >\n> >\n> > > >\n> > > > diff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\n> > > > index e5c1ca440d93..7ceda24cf6a7 100644\n> > > > --- a/fs/iomap/buffered-io.c\n> > > > +++ b/fs/iomap/buffered-io.c\n> > > > @@ -87,12 +86,50 @@ static void iomap_set_range_uptodate(struct folio\n> > > > *folio, size_t off,\n> > > >   if (ifs) {\n> > > >           spin_lock_irqsave(&ifs->state_lock, flags);\n> > > >           uptodate = ifs_set_range_uptodate(folio, ifs, off, len);\n> > > >           + /*\n> > > >           + * If a read is in progress, we must NOT call folio_mark_uptodate\n> > > >           + * here. The read completion path (iomap_finish_folio_read or\n> > > >           + * iomap_read_end) will call folio_end_read() which uses XOR\n> > > >           + * semantics to set the uptodate bit. If we set it here, the XOR\n> > > >           + * in folio_end_read() will clear it, leaving the folio not\n> > > >           + * uptodate while the ifs says all blocks are uptodate.\n> > > >           + */\n> > > >          + if (uptodate && ifs->read_bytes_pending)\n> > > >                    + uptodate = false;\n> > > >         spin_unlock_irqrestore(&ifs->state_lock, flags);\n> > > >   }\n> > > >\n> > > > Thanks,\n> > > > Joanne\n> > > >\n> > > > >\n> > > > > So your iomap_set_range_uptodate patch can fix above failed case since it block mark folio's uptodate to 1.\n> > > > > Hope my findings are helpful.\n> > > > >\n> > > > > >\n> > > > > > --\n> > > > > > Thanks,\n> > > > > > Sasha\n> > > > > >\n\n\n---\n\nOn Tue, Feb 10, 2026 at 7:11PM Matthew Wilcox <willy@infradead.org> wrote:\n>\n> On Tue, Feb 10, 2026 at 02:18:06PM -0800, Joanne Koong wrote:\n> >                 spin_lock_irqsave(&ifs->state_lock, flags);\n> > -               uptodate = ifs_set_range_uptodate(folio, ifs, off, len);\n> > +               /*\n> > +                * If a read is in progress, we must NOT call\n> > folio_mark_uptodate.\n> > +                * The read completion path (iomap_finish_folio_read or\n> > +                * iomap_read_end) will call folio_end_read() which uses XOR\n> > +                * semantics to set the uptodate bit. If we set it here, the XOR\n> > +                * in folio_end_read() will clear it, leaving the folio not\n> > +                * uptodate.\n> > +                */\n> > +               uptodate = ifs_set_range_uptodate(folio, ifs, off, len) &&\n> > +                       !ifs->read_bytes_pending;\n> >                 spin_unlock_irqrestore(&ifs->state_lock, flags);\n>\n> This can't possibly be the right fix.  There's some horrible confusion\n> here.  It should not be possible to have read bytes pending _and_ the\n> entire folio be uptodate.  That's an invariant that should always be\n> maintained.\n\nifs->read_bytes_pending gets initialized to the folio size, but if the\nfile being read in is smaller than the size of the folio, then we\nreach this scenario because the file has been read in but\nifs->read_bytes_pending is still a positive value because it\nrepresents the bytes between the end of the file and the end of the\nfolio. If the folio size is 16k and the file size is 4k:\n  a) ifs->read_bytes_pending gets initialized to 16k\n  b) ->read_folio_range() is called for the 4k read\n  c) the 4k read succeeds, ifs->read_bytes_pending is now 12k and the\n0 to 4k range is marked uptodate\n  d) the post-eof blocks are zeroed and marked uptodate in the call to\niomap_set_range_uptodate()\n  e) iomap_set_range_uptodate() sees all the ranges are marked\nuptodate and it marks the folio uptodate\n  f) iomap_read_end() gets called to subtract the 12k from\nifs->read_bytes_pending. it too sees all the ranges are marked\nuptodate and marks the folio uptodate\n\nThe same scenario could happen for IOMAP_INLINE mappings if part of\nthe folio is read in through ->read_folio_range() and then the rest is\nread in as inline data.\n\nAn alternative solution is to not have zeroed-out / inlined mappings\ncall iomap_read_end(), eg something like this [1], but this adds\nadditional complexity and doesn't work if there's additional mappings\nfor the folio after a non-IOMAP_MAPPED mapping.\n\nIs there a better approach that I'm missing?\n\nThanks,\nJoanne\n\n[1] https://github.com/joannekoong/linux/commit/de48d3c29db8ae654300341e3eec12497df54673\n\n\n---\n\nOn Wed, Feb 11, 2026 at 1:03PM Matthew Wilcox <willy@infradead.org> wrote:\n>\n> On Wed, Feb 11, 2026 at 11:33:05AM -0800, Joanne Koong wrote:\n> > ifs->read_bytes_pending gets initialized to the folio size, but if the\n> > file being read in is smaller than the size of the folio, then we\n> > reach this scenario because the file has been read in but\n> > ifs->read_bytes_pending is still a positive value because it\n> > represents the bytes between the end of the file and the end of the\n> > folio. If the folio size is 16k and the file size is 4k:\n> >   a) ifs->read_bytes_pending gets initialized to 16k\n> >   b) ->read_folio_range() is called for the 4k read\n> >   c) the 4k read succeeds, ifs->read_bytes_pending is now 12k and the\n> > 0 to 4k range is marked uptodate\n> >   d) the post-eof blocks are zeroed and marked uptodate in the call to\n> > iomap_set_range_uptodate()\n>\n> This is the bug then.  If they're marked uptodate, read_bytes_pending\n> should be decremented at the same time.  Now, I appreciate that\n> iomap_set_range_uptodate() is called both from iomap_read_folio_iter()\n> and __iomap_write_begin(), and it can't decrement read_bytes_pending\n> in the latter case.  Perhaps a flag or a second length parameter is\n> the solution?\n\nI don't think it's enough to decrement read_bytes_pending by the\nzeroed/read-inline length because there's these two edge cases:\na) some blocks in the folio were already uptodate from the very\nbeginning and skipped for IO but not decremented yet from\nifs->read_bytes_pending, which means in iomap_read_end(),\nifs->read_bytes_pending would be > 0 and the uptodate flag could get\nXORed again. This means we need to also decrement read_bytes_pending\nby bytes_submitted as well for this case\nb) the async ->read_folio_range() callback finishes after the\nzeroing's read_bytes_pending decrement and calls folio_end_read(), so\nwe need to assign ctx->cur_folio to NULL\n\nI think the code would have to look something like [1] (this is\nsimilar to the alternative approach I mentioned in my previous reply\nbut fixed up to cover some more edge cases).\n\nThanks,\nJoanne\n\n[1] https://github.com/joannekoong/linux/commit/b42f47726433a8130e8c27d1b43b16e27dfd6960\n\n>\n> >   e) iomap_set_range_uptodate() sees all the ranges are marked\n> > uptodate and it marks the folio uptodate\n> >   f) iomap_read_end() gets called to subtract the 12k from\n> > ifs->read_bytes_pending. it too sees all the ranges are marked\n> > uptodate and marks the folio uptodate\n> >\n> > The same scenario could happen for IOMAP_INLINE mappings if part of\n> > the folio is read in through ->read_folio_range() and then the rest is\n> > read in as inline data.\n>\n> This is basically the same case as post-eof.\n>\n> > An alternative solution is to not have zeroed-out / inlined mappings\n> > call iomap_read_end(), eg something like this [1], but this adds\n> > additional complexity and doesn't work if there's additional mappings\n> > for the folio after a non-IOMAP_MAPPED mapping.\n\n(I was wrong about it not working for cases where's additional\nmappings after a non-IOMAP_MAPPED mapping, since both\ninline-read/zeroing are no-ops if the entire folio is already\nuptodate)\n\n > >\n> > Is there a better approach that I'm missing?\n> >\n> > Thanks,\n> > Joanne\n> >\n> > [1] https://github.com/joannekoong/linux/commit/de48d3c29db8ae654300341e3eec12497df54673\n\n\n---\n\nOn Thu, Feb 12, 2026 at 11:31AM Matthew Wilcox <willy@infradead.org> wrote:\n>\n> On Wed, Feb 11, 2026 at 03:13:48PM -0800, Joanne Koong wrote:\n> > On Wed, Feb 11, 2026 at 1:03PM Matthew Wilcox <willy@infradead.org> wrote:\n> > >\n> > > On Wed, Feb 11, 2026 at 11:33:05AM -0800, Joanne Koong wrote:\n> > > > ifs->read_bytes_pending gets initialized to the folio size, but if the\n> > > > file being read in is smaller than the size of the folio, then we\n> > > > reach this scenario because the file has been read in but\n> > > > ifs->read_bytes_pending is still a positive value because it\n> > > > represents the bytes between the end of the file and the end of the\n> > > > folio. If the folio size is 16k and the file size is 4k:\n> > > >   a) ifs->read_bytes_pending gets initialized to 16k\n> > > >   b) ->read_folio_range() is called for the 4k read\n> > > >   c) the 4k read succeeds, ifs->read_bytes_pending is now 12k and the\n> > > > 0 to 4k range is marked uptodate\n> > > >   d) the post-eof blocks are zeroed and marked uptodate in the call to\n> > > > iomap_set_range_uptodate()\n> > >\n> > > This is the bug then.  If they're marked uptodate, read_bytes_pending\n> > > should be decremented at the same time.  Now, I appreciate that\n> > > iomap_set_range_uptodate() is called both from iomap_read_folio_iter()\n> > > and __iomap_write_begin(), and it can't decrement read_bytes_pending\n> > > in the latter case.  Perhaps a flag or a second length parameter is\n> > > the solution?\n> >\n> > I don't think it's enough to decrement read_bytes_pending by the\n> > zeroed/read-inline length because there's these two edge cases:\n> > a) some blocks in the folio were already uptodate from the very\n> > beginning and skipped for IO but not decremented yet from\n> > ifs->read_bytes_pending, which means in iomap_read_end(),\n> > ifs->read_bytes_pending would be > 0 and the uptodate flag could get\n> > XORed again. This means we need to also decrement read_bytes_pending\n> > by bytes_submitted as well for this case\n>\n> Hm, that's a good one.  It can't happen for readahead, but it can happen\n> if we start out by writing to some blocks of a folio, then call\n> read_folio to get the remaining blocks uptodate.  We could avoid it\n> happening by initialising read_bytes_pending to folio_size() -\n> bitmap_weight(ifs->uptodate) * block_size.\n\nThis is an interesting idea but if we do this then I think this adds\nsome more edge cases. For example, the range being inlined or zeroed\nmay have some already uptodate blocks (eg from a prior buffered write)\nso we'll need to calculate how many already-existing uptodate bytes\nthere are in that range to avoid over-decrementing\nifs->read_bytes_pending. I think we would also have to move the\nifs_alloc() and iomap_read_init() calls to the very beginning of\niomap_read_folio_iter() before any iomap_read_inline_data() call\nbecause there could be the case where a folio has an ifs that was\nallocated from a prior write, so if we call iomap_finish_folio_read()\nafter iomap_read_inline_data(), the folio's ifs->read_bytes_pending\nnow must be initialized before the inline read. Whereas before, we had\nsome more optimal behavior with being able to entirely skip the ifs\nallocation and read initialization if the entire folio gets read\ninline.\n\n>\n> > b) the async ->read_folio_range() callback finishes after the\n> > zeroing's read_bytes_pending decrement and calls folio_end_read(), so\n> > we need to assign ctx->cur_folio to NULL\n>\n> If we return 'finished' from iomap_finish_folio_read(), we can handle\n> this?\n\nI think there is still this scenario:\n- ->read_folio gets called on an 8k-size folio for a 4k-size file\n- iomap_read_init() is called, ifs->read_bytes_pending is now 8k\n- make async ->read_folio_range() call to read in 4k\n- iomap zeroes out folio from 4k to 8k, then calls\niomap_finish_folio_read() with off = 4k and len = 4k\n- in iomap_finish_folio_read(), decrement ifs->read_bytes_pending by\nlen. ifs->read_bytes_pending is now 4k\n- async ->read_folio_range() completes read, calls\niomap_finish_folio_read() with off=0 and len = 4k, which now\ndecrements ifs->read_bytes_pending by 4k. read_bytes_pending is now 0,\nso folio_end_read() gets called. folio should now not be touched by\niomap\n- iomap still has valid ctx->cur_folio, and calls iomap_read_end on\nctx->cur_folio\n\nThis is the same issue as the one in\nhttps://lore.kernel.org/linux-fsdevel/20260126224107.2182262-2-joannelkoong@gmail.com/\n\nWe could always set ctx->cur_folio to NULL after inline/zeroing calls\niomap_finish_folio_read() regardless of whether it actually ended the\nread or not, but then this runs into issues for zeroing. The zeroing\ncan be triggered by non-EOF cases, eg if the first mapping is an\nIOMAP_HOLE and then the rest of hte folio is mapped. We may still need\nto read in the rest of the folio, so we can't just set ctx->cur_folio\nto NULL. i guess one workaround is to explicitly check if the zeroing\nis for IOMAP_MAPPED types and if so then always set ctx->cur_folio to\nNULL, but I think this just gets uglier / more complex to understand\nand I'm not sure if there's other edge cases I'm missing that we'd\nneed to account for. One other idea is to try avoiding the\niomap_end_read() call for non-error cases if we use your\nbitmap_weight() idea above, then it wouldn't matter in that scenario\nabove if ctx->cur_folio points to a folio that already had read ended\non it. But I think that also just makes the code harder to\nread/understand.\n\nThe original patch seemed cleanest to me, maybe if we renamed uptodate\nto mark_uptodate, it'd be more appetible?  eg\n\n@@ -80,18 +80,19 @@ static void iomap_set_range_uptodate(struct folio\n*folio, size_t off,\n {\n        struct iomap_folio_state *ifs = folio->private;\n        unsigned long flags;\n-       bool uptodate = true;\n+       bool mark_uptodate = true;\n\n        if (folio_test_uptodate(folio))\n                return;\n\n        if (ifs) {\n                spin_lock_irqsave(&ifs->state_lock, flags);\n-               uptodate = ifs_set_range_uptodate(folio, ifs, off, len);\n+               mark_uptodate = ifs_set_range_uptodate(folio, ifs, off, len) &&\n+                       !ifs->read_bytes_pending;\n                spin_unlock_irqrestore(&ifs->state_lock, flags);\n        }\n\n-       if (uptodate)\n+       if (mark_uptodate)\n                folio_mark_uptodate(folio);\n }\n\n\nThanks,\nJoanne\n\n>\n> > I think the code would have to look something like [1] (this is\n> > similar to the alternative approach I mentioned in my previous reply\n> > but fixed up to cover some more edge cases).\n> >\n> > Thanks,\n> > Joanne\n> >\n> > [1] https://github.com/joannekoong/linux/commit/b42f47726433a8130e8c27d1b43b16e27dfd6960\n>\n> I think we can do everything we need with a suitably modified\n> iomap_finish_folio_read() rather than the new iomap_finish_read_range().\n\n",
          "reply_to": ""
        },
        {
          "author": "Christian Brauner",
          "summary": "Christian Brauner applied the patch series to his vfs-6.19.iomap branch and suggested that reviewers report any remaining bugs in a new review, allowing the original patch series to be dropped.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "no specific technical concerns mentioned"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Thu, 25 Sep 2025 17:25:55 -0700, Joanne Koong wrote:\n> This series adds fuse iomap support for buffered reads and readahead.\n> This is needed so that granular uptodate tracking can be used in fuse when\n> large folios are enabled so that only the non-uptodate portions of the folio\n> need to be read in instead of having to read in the entire folio. It also is\n> needed in order to turn on large folios for servers that use the writeback\n> cache since otherwise there is a race condition that may lead to data\n> corruption if there is a partial write, then a read and the read happens\n> before the write has undergone writeback, since otherwise the folio will not\n> be marked uptodate from the partial write so the read will read in the entire\n> folio from disk, which will overwrite the partial write.\n> \n> [...]\n\nApplied to the vfs-6.19.iomap branch of the vfs/vfs.git tree.\nPatches in the vfs-6.19.iomap branch should appear in linux-next soon.\n\nPlease report any outstanding bugs that were missed during review in a\nnew review to the original patch series allowing us to drop it.\n\nIt's encouraged to provide Acked-bys and Reviewed-bys even though the\npatch has now been applied. If possible patch trailers will be updated.\n\nNote that commit hashes shown below are subject to change due to rebase,\ntrailer updates or similar. If in doubt, please check the listed branch.\n\ntree:   https://git.kernel.org/pub/scm/linux/kernel/git/vfs/vfs.git\nbranch: vfs-6.19.iomap\n\n[01/14] iomap: move bio read logic into helper function\n        https://git.kernel.org/vfs/vfs/c/4b1f54633425\n[02/14] iomap: move read/readahead bio submission logic into helper function\n        https://git.kernel.org/vfs/vfs/c/22159441469a\n[03/14] iomap: store read/readahead bio generically\n        https://git.kernel.org/vfs/vfs/c/7c732b99c04f\n[04/14] iomap: iterate over folio mapping in iomap_readpage_iter()\n        https://git.kernel.org/vfs/vfs/c/3b404627d3e2\n[05/14] iomap: rename iomap_readpage_iter() to iomap_read_folio_iter()\n        https://git.kernel.org/vfs/vfs/c/bf8b9f4ce6a9\n[06/14] iomap: rename iomap_readpage_ctx struct to iomap_read_folio_ctx\n        https://git.kernel.org/vfs/vfs/c/abea60c60330\n[07/14] iomap: track pending read bytes more optimally\n        https://git.kernel.org/vfs/vfs/c/13cc90f6c38e\n[08/14] iomap: set accurate iter->pos when reading folio ranges\n        https://git.kernel.org/vfs/vfs/c/63adb033604e\n[09/14] iomap: add caller-provided callbacks for read and readahead\n        https://git.kernel.org/vfs/vfs/c/56b6f5d3792b\n[10/14] iomap: move buffered io bio logic into new file\n        https://git.kernel.org/vfs/vfs/c/80cd9857c47f\n[11/14] iomap: make iomap_read_folio() a void return\n        https://git.kernel.org/vfs/vfs/c/434651f1a9b7\n[12/14] fuse: use iomap for read_folio\n        https://git.kernel.org/vfs/vfs/c/12cae30dc565\n[13/14] fuse: use iomap for readahead\n        https://git.kernel.org/vfs/vfs/c/0853f58ed0b4\n[14/14] fuse: remove fc->blkbits workaround for partial writes\n        https://git.kernel.org/vfs/vfs/c/bb944dc82db1\n",
          "reply_to": ""
        },
        {
          "author": "Brian Foster",
          "summary": "Reviewer Brian Foster reported a potential lockup issue on XFS with 1k FSB (-bsize=1k) running generic/051, which landed on patch 07/14 after bisecting. He also suggested adding comments to clarify the naming conventions for accounted and pending read bytes.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "potential bug",
            "requested changes"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Thu, Sep 25, 2025 at 05:26:02PM -0700, Joanne Koong wrote:\n> Instead of incrementing read_bytes_pending for every folio range read in\n> (which requires acquiring the spinlock to do so), set read_bytes_pending\n> to the folio size when the first range is asynchronously read in, keep\n> track of how many bytes total are asynchronously read in, and adjust\n> read_bytes_pending accordingly after issuing requests to read in all the\n> necessary ranges.\n> \n> iomap_read_folio_ctx->cur_folio_in_bio can be removed since a non-zero\n> value for pending bytes necessarily indicates the folio is in the bio.\n> \n> Signed-off-by: Joanne Koong <joannelkoong@gmail.com>\n> Suggested-by: \"Darrick J. Wong\" <djwong@kernel.org>\n> ---\n\nHi Joanne,\n\nI was throwing some extra testing at the vfs-6.19.iomap branch since the\nlittle merge conflict thing with iomap_iter_advance(). I end up hitting\nwhat appears to be a lockup on XFS with 1k FSB (-bsize=1k) running\ngeneric/051. It reproduces fairly reliably within a few iterations or so\nand seems to always stall during a read for a dedupe operation:\n\ntask:fsstress        state:D stack:0     pid:12094 tgid:12094 ppid:12091  task_flags:0x400140 flags:0x00080003\nCall Trace:\n <TASK>\n __schedule+0x2fc/0x7a0\n schedule+0x27/0x80\n io_schedule+0x46/0x70\n folio_wait_bit_common+0x12b/0x310\n ? __pfx_wake_page_function+0x10/0x10\n ? __pfx_xfs_vm_read_folio+0x10/0x10 [xfs]\n filemap_read_folio+0x85/0xd0\n ? __pfx_xfs_vm_read_folio+0x10/0x10 [xfs]\n do_read_cache_folio+0x7c/0x1b0\n vfs_dedupe_file_range_compare.constprop.0+0xaf/0x2d0\n __generic_remap_file_range_prep+0x276/0x2a0\n generic_remap_file_range_prep+0x10/0x20\n xfs_reflink_remap_prep+0x22c/0x300 [xfs]\n xfs_file_remap_range+0x84/0x360 [xfs]\n vfs_dedupe_file_range_one+0x1b2/0x1d0\n ? remap_verify_area+0x46/0x140\n vfs_dedupe_file_range+0x162/0x220\n do_vfs_ioctl+0x4d1/0x940\n __x64_sys_ioctl+0x75/0xe0\n do_syscall_64+0x84/0x800\n ? do_syscall_64+0xbb/0x800\n ? avc_has_perm_noaudit+0x6b/0xf0\n ? _copy_to_user+0x31/0x40\n ? cp_new_stat+0x130/0x170\n ? __do_sys_newfstat+0x44/0x70\n ? do_syscall_64+0xbb/0x800\n ? do_syscall_64+0xbb/0x800\n ? clear_bhb_loop+0x30/0x80\n ? clear_bhb_loop+0x30/0x80\n entry_SYSCALL_64_after_hwframe+0x76/0x7e\nRIP: 0033:0x7fe6bbd9a14d\nRSP: 002b:00007ffde72cd4e0 EFLAGS: 00000246 ORIG_RAX: 0000000000000010\nRAX: ffffffffffffffda RBX: 0000000000000068 RCX: 00007fe6bbd9a14d\nRDX: 000000000a1394b0 RSI: 00000000c0189436 RDI: 0000000000000004\nRBP: 00007ffde72cd530 R08: 0000000000001000 R09: 000000000a11a3fc\nR10: 000000000001d6c0 R11: 0000000000000246 R12: 000000000a12cfb0\nR13: 000000000a12ba10 R14: 000000000a14e610 R15: 0000000000019000\n </TASK>\n\nIt wasn't immediately clear to me what the issue was so I bisected and\nit landed on this patch. It kind of looks like we're failing to unlock a\nfolio at some point and then tripping over it later..? I can kill the\nfsstress process but then the umount ultimately gets stuck tossing\npagecache [1], so the mount still ends up stuck indefinitely. Anyways,\nI'll poke at it some more but I figure you might be able to make sense\nof this faster than I can.\n\nBrian\n\n[1] umount stack trace: \n\ntask:umount          state:D stack:0     pid:12216 tgid:12216 ppid:2514   task_flags:0x400100 flags:0x00080001\nCall Trace:\n <TASK>\n __schedule+0x2fc/0x7a0\n schedule+0x27/0x80\n io_schedule+0x46/0x70\n folio_wait_bit_common+0x12b/0x310\n ? __pfx_wake_page_function+0x10/0x10\n truncate_inode_pages_range+0x42a/0x4d0\n xfs_fs_evict_inode+0x1f/0x30 [xfs]\n evict+0x112/0x290\n evict_inodes+0x209/0x230\n generic_shutdown_super+0x42/0x100\n kill_block_super+0x1a/0x40\n xfs_kill_sb+0x12/0x20 [xfs]\n deactivate_locked_super+0x33/0xb0\n cleanup_mnt+0xba/0x150\n task_work_run+0x5c/0x90\n exit_to_user_mode_loop+0x12f/0x170\n do_syscall_64+0x1af/0x800\n ? vfs_statx+0x80/0x160\n ? do_statx+0x62/0xa0\n ? __x64_sys_statx+0xaf/0x100\n ? do_syscall_64+0xbb/0x800\n ? __x64_sys_statx+0xaf/0x100\n ? do_syscall_64+0xbb/0x800\n ? count_memcg_events+0xdd/0x1b0\n ? handle_mm_fault+0x220/0x340\n ? do_user_addr_fault+0x2c3/0x7f0\n ? clear_bhb_loop+0x30/0x80\n ? clear_bhb_loop+0x30/0x80\n entry_SYSCALL_64_after_hwframe+0x76/0x7e\nRIP: 0033:0x7fdd641ed5ab\nRSP: 002b:00007ffd671182e8 EFLAGS: 00000246 ORIG_RAX: 00000000000000a6\nRAX: 0000000000000000 RBX: 0000559b3e2056b0 RCX: 00007fdd641ed5ab\nRDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000559b3e205ac0\nRBP: 00007ffd671183c0 R08: 0000000000000000 R09: 0000000000000000\nR10: 0000000000000103 R11: 0000000000000246 R12: 0000559b3e2057b8\nR13: 0000000000000000 R14: 0000559b3e205ac0 R15: 0000000000000000\n </TASK>\n\n>  fs/iomap/buffered-io.c | 87 ++++++++++++++++++++++++++++++++----------\n>  1 file changed, 66 insertions(+), 21 deletions(-)\n> \n> diff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\n> index 09e65771a947..4e6258fdb915 100644\n> --- a/fs/iomap/buffered-io.c\n> +++ b/fs/iomap/buffered-io.c\n> @@ -362,7 +362,6 @@ static void iomap_read_end_io(struct bio *bio)\n>  \n>  struct iomap_read_folio_ctx {\n>  \tstruct folio\t\t*cur_folio;\n> -\tbool\t\t\tcur_folio_in_bio;\n>  \tvoid\t\t\t*read_ctx;\n>  \tstruct readahead_control *rac;\n>  };\n> @@ -380,19 +379,11 @@ static void iomap_bio_read_folio_range(const struct iomap_iter *iter,\n>  {\n>  \tstruct folio *folio = ctx->cur_folio;\n>  \tconst struct iomap *iomap = &iter->iomap;\n> -\tstruct iomap_folio_state *ifs = folio->private;\n>  \tsize_t poff = offset_in_folio(folio, pos);\n>  \tloff_t length = iomap_length(iter);\n>  \tsector_t sector;\n>  \tstruct bio *bio = ctx->read_ctx;\n>  \n> -\tctx->cur_folio_in_bio = true;\n> -\tif (ifs) {\n> -\t\tspin_lock_irq(&ifs->state_lock);\n> -\t\tifs->read_bytes_pending += plen;\n> -\t\tspin_unlock_irq(&ifs->state_lock);\n> -\t}\n> -\n>  \tsector = iomap_sector(iomap, pos);\n>  \tif (!bio || bio_end_sector(bio) != sector ||\n>  \t    !bio_add_folio(bio, folio, plen, poff)) {\n> @@ -422,8 +413,57 @@ static void iomap_bio_read_folio_range(const struct iomap_iter *iter,\n>  \t}\n>  }\n>  \n> +static void iomap_read_init(struct folio *folio)\n> +{\n> +\tstruct iomap_folio_state *ifs = folio->private;\n> +\n> +\tif (ifs) {\n> +\t\tsize_t len = folio_size(folio);\n> +\n> +\t\tspin_lock_irq(&ifs->state_lock);\n> +\t\tifs->read_bytes_pending += len;\n> +\t\tspin_unlock_irq(&ifs->state_lock);\n> +\t}\n> +}\n> +\n> +static void iomap_read_end(struct folio *folio, size_t bytes_pending)\n> +{\n> +\tstruct iomap_folio_state *ifs;\n> +\n> +\t/*\n> +\t * If there are no bytes pending, this means we are responsible for\n> +\t * unlocking the folio here, since no IO helper has taken ownership of\n> +\t * it.\n> +\t */\n> +\tif (!bytes_pending) {\n> +\t\tfolio_unlock(folio);\n> +\t\treturn;\n> +\t}\n> +\n> +\tifs = folio->private;\n> +\tif (ifs) {\n> +\t\tbool end_read, uptodate;\n> +\t\tsize_t bytes_accounted = folio_size(folio) - bytes_pending;\n> +\n> +\t\tspin_lock_irq(&ifs->state_lock);\n> +\t\tifs->read_bytes_pending -= bytes_accounted;\n> +\t\t/*\n> +\t\t * If !ifs->read_bytes_pending, this means all pending reads\n> +\t\t * by the IO helper have already completed, which means we need\n> +\t\t * to end the folio read here. If ifs->read_bytes_pending != 0,\n> +\t\t * the IO helper will end the folio read.\n> +\t\t */\n> +\t\tend_read = !ifs->read_bytes_pending;\n> +\t\tif (end_read)\n> +\t\t\tuptodate = ifs_is_fully_uptodate(folio, ifs);\n> +\t\tspin_unlock_irq(&ifs->state_lock);\n> +\t\tif (end_read)\n> +\t\t\tfolio_end_read(folio, uptodate);\n> +\t}\n> +}\n> +\n>  static int iomap_read_folio_iter(struct iomap_iter *iter,\n> -\t\tstruct iomap_read_folio_ctx *ctx)\n> +\t\tstruct iomap_read_folio_ctx *ctx, size_t *bytes_pending)\n>  {\n>  \tconst struct iomap *iomap = &iter->iomap;\n>  \tloff_t pos = iter->pos;\n> @@ -460,6 +500,9 @@ static int iomap_read_folio_iter(struct iomap_iter *iter,\n>  \t\t\tfolio_zero_range(folio, poff, plen);\n>  \t\t\tiomap_set_range_uptodate(folio, poff, plen);\n>  \t\t} else {\n> +\t\t\tif (!*bytes_pending)\n> +\t\t\t\tiomap_read_init(folio);\n> +\t\t\t*bytes_pending += plen;\n>  \t\t\tiomap_bio_read_folio_range(iter, ctx, pos, plen);\n>  \t\t}\n>  \n> @@ -482,17 +525,18 @@ int iomap_read_folio(struct folio *folio, const struct iomap_ops *ops)\n>  \tstruct iomap_read_folio_ctx ctx = {\n>  \t\t.cur_folio\t= folio,\n>  \t};\n> +\tsize_t bytes_pending = 0;\n>  \tint ret;\n>  \n>  \ttrace_iomap_readpage(iter.inode, 1);\n>  \n>  \twhile ((ret = iomap_iter(&iter, ops)) > 0)\n> -\t\titer.status = iomap_read_folio_iter(&iter, &ctx);\n> +\t\titer.status = iomap_read_folio_iter(&iter, &ctx,\n> +\t\t\t\t&bytes_pending);\n>  \n>  \tiomap_bio_submit_read(&ctx);\n>  \n> -\tif (!ctx.cur_folio_in_bio)\n> -\t\tfolio_unlock(folio);\n> +\tiomap_read_end(folio, bytes_pending);\n>  \n>  \t/*\n>  \t * Just like mpage_readahead and block_read_full_folio, we always\n> @@ -504,24 +548,23 @@ int iomap_read_folio(struct folio *folio, const struct iomap_ops *ops)\n>  EXPORT_SYMBOL_GPL(iomap_read_folio);\n>  \n>  static int iomap_readahead_iter(struct iomap_iter *iter,\n> -\t\tstruct iomap_read_folio_ctx *ctx)\n> +\t\tstruct iomap_read_folio_ctx *ctx, size_t *cur_bytes_pending)\n>  {\n>  \tint ret;\n>  \n>  \twhile (iomap_length(iter)) {\n>  \t\tif (ctx->cur_folio &&\n>  \t\t    offset_in_folio(ctx->cur_folio, iter->pos) == 0) {\n> -\t\t\tif (!ctx->cur_folio_in_bio)\n> -\t\t\t\tfolio_unlock(ctx->cur_folio);\n> +\t\t\tiomap_read_end(ctx->cur_folio, *cur_bytes_pending);\n>  \t\t\tctx->cur_folio = NULL;\n>  \t\t}\n>  \t\tif (!ctx->cur_folio) {\n>  \t\t\tctx->cur_folio = readahead_folio(ctx->rac);\n>  \t\t\tif (WARN_ON_ONCE(!ctx->cur_folio))\n>  \t\t\t\treturn -EINVAL;\n> -\t\t\tctx->cur_folio_in_bio = false;\n> +\t\t\t*cur_bytes_pending = 0;\n>  \t\t}\n> -\t\tret = iomap_read_folio_iter(iter, ctx);\n> +\t\tret = iomap_read_folio_iter(iter, ctx, cur_bytes_pending);\n>  \t\tif (ret)\n>  \t\t\treturn ret;\n>  \t}\n> @@ -554,16 +597,18 @@ void iomap_readahead(struct readahead_control *rac, const struct iomap_ops *ops)\n>  \tstruct iomap_read_folio_ctx ctx = {\n>  \t\t.rac\t= rac,\n>  \t};\n> +\tsize_t cur_bytes_pending;\n>  \n>  \ttrace_iomap_readahead(rac->mapping->host, readahead_count(rac));\n>  \n>  \twhile (iomap_iter(&iter, ops) > 0)\n> -\t\titer.status = iomap_readahead_iter(&iter, &ctx);\n> +\t\titer.status = iomap_readahead_iter(&iter, &ctx,\n> +\t\t\t\t\t&cur_bytes_pending);\n>  \n>  \tiomap_bio_submit_read(&ctx);\n>  \n> -\tif (ctx.cur_folio && !ctx.cur_folio_in_bio)\n> -\t\tfolio_unlock(ctx.cur_folio);\n> +\tif (ctx.cur_folio)\n> +\t\tiomap_read_end(ctx.cur_folio, cur_bytes_pending);\n>  }\n>  EXPORT_SYMBOL_GPL(iomap_readahead);\n>  \n> -- \n> 2.47.3\n> \n> \n\n\n\n---\n\nOn Fri, Oct 24, 2025 at 09:25:13AM -0700, Joanne Koong wrote:\n> On Thu, Oct 23, 2025 at 5:01\\u202fPM Joanne Koong <joannelkoong@gmail.com> wrote:\n> >\n> > On Thu, Oct 23, 2025 at 12:30\\u202fPM Brian Foster <bfoster@redhat.com> wrote:\n> > >\n> > > On Thu, Sep 25, 2025 at 05:26:02PM -0700, Joanne Koong wrote:\n> > > > Instead of incrementing read_bytes_pending for every folio range read in\n> > > > (which requires acquiring the spinlock to do so), set read_bytes_pending\n> > > > to the folio size when the first range is asynchronously read in, keep\n> > > > track of how many bytes total are asynchronously read in, and adjust\n> > > > read_bytes_pending accordingly after issuing requests to read in all the\n> > > > necessary ranges.\n> > > >\n> > > > iomap_read_folio_ctx->cur_folio_in_bio can be removed since a non-zero\n> > > > value for pending bytes necessarily indicates the folio is in the bio.\n> > > >\n> > > > Signed-off-by: Joanne Koong <joannelkoong@gmail.com>\n> > > > Suggested-by: \"Darrick J. Wong\" <djwong@kernel.org>\n> > > > ---\n> > >\n> > > Hi Joanne,\n> > >\n> > > I was throwing some extra testing at the vfs-6.19.iomap branch since the\n> > > little merge conflict thing with iomap_iter_advance(). I end up hitting\n> > > what appears to be a lockup on XFS with 1k FSB (-bsize=1k) running\n> > > generic/051. It reproduces fairly reliably within a few iterations or so\n> > > and seems to always stall during a read for a dedupe operation:\n> > >\n> > > task:fsstress        state:D stack:0     pid:12094 tgid:12094 ppid:12091  task_flags:0x400140 flags:0x00080003\n> > > Call Trace:\n> > >  <TASK>\n> > >  __schedule+0x2fc/0x7a0\n> > >  schedule+0x27/0x80\n> > >  io_schedule+0x46/0x70\n> > >  folio_wait_bit_common+0x12b/0x310\n> > >  ? __pfx_wake_page_function+0x10/0x10\n> > >  ? __pfx_xfs_vm_read_folio+0x10/0x10 [xfs]\n> > >  filemap_read_folio+0x85/0xd0\n> > >  ? __pfx_xfs_vm_read_folio+0x10/0x10 [xfs]\n> > >  do_read_cache_folio+0x7c/0x1b0\n> > >  vfs_dedupe_file_range_compare.constprop.0+0xaf/0x2d0\n> > >  __generic_remap_file_range_prep+0x276/0x2a0\n> > >  generic_remap_file_range_prep+0x10/0x20\n> > >  xfs_reflink_remap_prep+0x22c/0x300 [xfs]\n> > >  xfs_file_remap_range+0x84/0x360 [xfs]\n> > >  vfs_dedupe_file_range_one+0x1b2/0x1d0\n> > >  ? remap_verify_area+0x46/0x140\n> > >  vfs_dedupe_file_range+0x162/0x220\n> > >  do_vfs_ioctl+0x4d1/0x940\n> > >  __x64_sys_ioctl+0x75/0xe0\n> > >  do_syscall_64+0x84/0x800\n> > >  ? do_syscall_64+0xbb/0x800\n> > >  ? avc_has_perm_noaudit+0x6b/0xf0\n> > >  ? _copy_to_user+0x31/0x40\n> > >  ? cp_new_stat+0x130/0x170\n> > >  ? __do_sys_newfstat+0x44/0x70\n> > >  ? do_syscall_64+0xbb/0x800\n> > >  ? do_syscall_64+0xbb/0x800\n> > >  ? clear_bhb_loop+0x30/0x80\n> > >  ? clear_bhb_loop+0x30/0x80\n> > >  entry_SYSCALL_64_after_hwframe+0x76/0x7e\n> > > RIP: 0033:0x7fe6bbd9a14d\n> > > RSP: 002b:00007ffde72cd4e0 EFLAGS: 00000246 ORIG_RAX: 0000000000000010\n> > > RAX: ffffffffffffffda RBX: 0000000000000068 RCX: 00007fe6bbd9a14d\n> > > RDX: 000000000a1394b0 RSI: 00000000c0189436 RDI: 0000000000000004\n> > > RBP: 00007ffde72cd530 R08: 0000000000001000 R09: 000000000a11a3fc\n> > > R10: 000000000001d6c0 R11: 0000000000000246 R12: 000000000a12cfb0\n> > > R13: 000000000a12ba10 R14: 000000000a14e610 R15: 0000000000019000\n> > >  </TASK>\n> > >\n> > > It wasn't immediately clear to me what the issue was so I bisected and\n> > > it landed on this patch. It kind of looks like we're failing to unlock a\n> > > folio at some point and then tripping over it later..? I can kill the\n> > > fsstress process but then the umount ultimately gets stuck tossing\n> > > pagecache [1], so the mount still ends up stuck indefinitely. Anyways,\n> > > I'll poke at it some more but I figure you might be able to make sense\n> > > of this faster than I can.\n> > >\n> > > Brian\n> >\n> > Hi Brian,\n> >\n> > Thanks for your report and the repro instructions. I will look into\n> > this and report back what I find.\n> \n> This is the fix:\n> \n> diff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\n> index 4e6258fdb915..aa46fec8362d 100644\n> --- a/fs/iomap/buffered-io.c\n> +++ b/fs/iomap/buffered-io.c\n> @@ -445,6 +445,9 @@ static void iomap_read_end(struct folio *folio,\n> size_t bytes_pending)\n>                 bool end_read, uptodate;\n>                 size_t bytes_accounted = folio_size(folio) - bytes_pending;\n> \n> +               if (!bytes_accounted)\n> +                       return;\n> +\n>                 spin_lock_irq(&ifs->state_lock);\n> \n> \n> What I missed was that if all the bytes in the folio are non-uptodate\n> and need to read in by the filesystem, then there's a bug where the\n> read will be ended on the folio twice (in iomap_read_end() and when\n> the filesystem calls iomap_finish_folio_write(), when only the\n> filesystem should end the read), which does 2 folio unlocks which ends\n> up locking the folio. Looking at the writeback patch that does a\n> similar optimization [1], I miss the same thing there.\n> \n\nMakes sense.. though a short comment wouldn't hurt in there. ;) I found\nmyself a little confused by the accounted vs. pending naming when\nreading through that code. If I follow correctly, the intent is to refer\nto the additional bytes accounted to read_bytes_pending via the init\n(where it just accounts the whole folio up front) and pending refers to\nsubmitted I/O.\n\nPresumably that extra accounting doubly serves as the typical \"don't\ncomplete the op before the submitter is done processing\" extra\nreference, except in this full submit case of course. If so, that's\nsubtle enough in my mind that a sentence or two on it wouldn't hurt..\n\n> I'll fix up both. Thanks for catching this and bisecting it down to\n> this patch. Sorry for the trouble.\n> \n\nNo prob. Thanks for the fix!\n\nBrian\n\n> Thanks,\n> Joanne\n> \n> [1] https://lore.kernel.org/linux-fsdevel/20251009225611.3744728-4-joannelkoong@gmail.com/\n> >\n> > Thanks,\n> > Joanne\n> > >\n> \n\n\n\n---\n\nOn Fri, Oct 24, 2025 at 02:55:20PM -0700, Joanne Koong wrote:\n> On Fri, Oct 24, 2025 at 12:48\\u202fPM Joanne Koong <joannelkoong@gmail.com> wrote:\n> >\n> > On Fri, Oct 24, 2025 at 10:10\\u202fAM Brian Foster <bfoster@redhat.com> wrote:\n> > >\n> > > On Fri, Oct 24, 2025 at 09:25:13AM -0700, Joanne Koong wrote:\n> > > > On Thu, Oct 23, 2025 at 5:01\\u202fPM Joanne Koong <joannelkoong@gmail.com> wrote:\n> > > > >\n> > > > > On Thu, Oct 23, 2025 at 12:30\\u202fPM Brian Foster <bfoster@redhat.com> wrote:\n> > > > > >\n> > > > > > On Thu, Sep 25, 2025 at 05:26:02PM -0700, Joanne Koong wrote:\n> > > > > > > Instead of incrementing read_bytes_pending for every folio range read in\n> > > > > > > (which requires acquiring the spinlock to do so), set read_bytes_pending\n> > > > > > > to the folio size when the first range is asynchronously read in, keep\n> > > > > > > track of how many bytes total are asynchronously read in, and adjust\n> > > > > > > read_bytes_pending accordingly after issuing requests to read in all the\n> > > > > > > necessary ranges.\n> > > > > > >\n> > > > > > > iomap_read_folio_ctx->cur_folio_in_bio can be removed since a non-zero\n> > > > > > > value for pending bytes necessarily indicates the folio is in the bio.\n> > > > > > >\n> > > > > > > Signed-off-by: Joanne Koong <joannelkoong@gmail.com>\n> > > > > > > Suggested-by: \"Darrick J. Wong\" <djwong@kernel.org>\n> > > > > > > ---\n> > > > > >\n> > > > > > Hi Joanne,\n> > > > > >\n> > > > > > I was throwing some extra testing at the vfs-6.19.iomap branch since the\n> > > > > > little merge conflict thing with iomap_iter_advance(). I end up hitting\n> > > > > > what appears to be a lockup on XFS with 1k FSB (-bsize=1k) running\n> > > > > > generic/051. It reproduces fairly reliably within a few iterations or so\n> > > > > > and seems to always stall during a read for a dedupe operation:\n> > > > > >\n> > > > > > task:fsstress        state:D stack:0     pid:12094 tgid:12094 ppid:12091  task_flags:0x400140 flags:0x00080003\n> > > > > > Call Trace:\n> > > > > >  <TASK>\n> > > > > >  __schedule+0x2fc/0x7a0\n> > > > > >  schedule+0x27/0x80\n> > > > > >  io_schedule+0x46/0x70\n> > > > > >  folio_wait_bit_common+0x12b/0x310\n> > > > > >  ? __pfx_wake_page_function+0x10/0x10\n> > > > > >  ? __pfx_xfs_vm_read_folio+0x10/0x10 [xfs]\n> > > > > >  filemap_read_folio+0x85/0xd0\n> > > > > >  ? __pfx_xfs_vm_read_folio+0x10/0x10 [xfs]\n> > > > > >  do_read_cache_folio+0x7c/0x1b0\n> > > > > >  vfs_dedupe_file_range_compare.constprop.0+0xaf/0x2d0\n> > > > > >  __generic_remap_file_range_prep+0x276/0x2a0\n> > > > > >  generic_remap_file_range_prep+0x10/0x20\n> > > > > >  xfs_reflink_remap_prep+0x22c/0x300 [xfs]\n> > > > > >  xfs_file_remap_range+0x84/0x360 [xfs]\n> > > > > >  vfs_dedupe_file_range_one+0x1b2/0x1d0\n> > > > > >  ? remap_verify_area+0x46/0x140\n> > > > > >  vfs_dedupe_file_range+0x162/0x220\n> > > > > >  do_vfs_ioctl+0x4d1/0x940\n> > > > > >  __x64_sys_ioctl+0x75/0xe0\n> > > > > >  do_syscall_64+0x84/0x800\n> > > > > >  ? do_syscall_64+0xbb/0x800\n> > > > > >  ? avc_has_perm_noaudit+0x6b/0xf0\n> > > > > >  ? _copy_to_user+0x31/0x40\n> > > > > >  ? cp_new_stat+0x130/0x170\n> > > > > >  ? __do_sys_newfstat+0x44/0x70\n> > > > > >  ? do_syscall_64+0xbb/0x800\n> > > > > >  ? do_syscall_64+0xbb/0x800\n> > > > > >  ? clear_bhb_loop+0x30/0x80\n> > > > > >  ? clear_bhb_loop+0x30/0x80\n> > > > > >  entry_SYSCALL_64_after_hwframe+0x76/0x7e\n> > > > > > RIP: 0033:0x7fe6bbd9a14d\n> > > > > > RSP: 002b:00007ffde72cd4e0 EFLAGS: 00000246 ORIG_RAX: 0000000000000010\n> > > > > > RAX: ffffffffffffffda RBX: 0000000000000068 RCX: 00007fe6bbd9a14d\n> > > > > > RDX: 000000000a1394b0 RSI: 00000000c0189436 RDI: 0000000000000004\n> > > > > > RBP: 00007ffde72cd530 R08: 0000000000001000 R09: 000000000a11a3fc\n> > > > > > R10: 000000000001d6c0 R11: 0000000000000246 R12: 000000000a12cfb0\n> > > > > > R13: 000000000a12ba10 R14: 000000000a14e610 R15: 0000000000019000\n> > > > > >  </TASK>\n> > > > > >\n> > > > > > It wasn't immediately clear to me what the issue was so I bisected and\n> > > > > > it landed on this patch. It kind of looks like we're failing to unlock a\n> > > > > > folio at some point and then tripping over it later..? I can kill the\n> > > > > > fsstress process but then the umount ultimately gets stuck tossing\n> > > > > > pagecache [1], so the mount still ends up stuck indefinitely. Anyways,\n> > > > > > I'll poke at it some more but I figure you might be able to make sense\n> > > > > > of this faster than I can.\n> > > > > >\n> > > > > > Brian\n> > > > >\n> > > > > Hi Brian,\n> > > > >\n> > > > > Thanks for your report and the repro instructions. I will look into\n> > > > > this and report back what I find.\n> > > >\n> > > > This is the fix:\n> > > >\n> > > > diff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\n> > > > index 4e6258fdb915..aa46fec8362d 100644\n> > > > --- a/fs/iomap/buffered-io.c\n> > > > +++ b/fs/iomap/buffered-io.c\n> > > > @@ -445,6 +445,9 @@ static void iomap_read_end(struct folio *folio,\n> > > > size_t bytes_pending)\n> > > >                 bool end_read, uptodate;\n> > > >                 size_t bytes_accounted = folio_size(folio) - bytes_pending;\n> > > >\n> > > > +               if (!bytes_accounted)\n> > > > +                       return;\n> > > > +\n> > > >                 spin_lock_irq(&ifs->state_lock);\n> > > >\n> > > >\n> > > > What I missed was that if all the bytes in the folio are non-uptodate\n> > > > and need to read in by the filesystem, then there's a bug where the\n> > > > read will be ended on the folio twice (in iomap_read_end() and when\n> > > > the filesystem calls iomap_finish_folio_write(), when only the\n> > > > filesystem should end the read), which does 2 folio unlocks which ends\n> > > > up locking the folio. Looking at the writeback patch that does a\n> > > > similar optimization [1], I miss the same thing there.\n> > > >\n> > >\n> > > Makes sense.. though a short comment wouldn't hurt in there. ;) I found\n> > > myself a little confused by the accounted vs. pending naming when\n> > > reading through that code. If I follow correctly, the intent is to refer\n> > > to the additional bytes accounted to read_bytes_pending via the init\n> > > (where it just accounts the whole folio up front) and pending refers to\n> > > submitted I/O.\n> > >\n> > > Presumably that extra accounting doubly serves as the typical \"don't\n> > > complete the op before the submitter is done processing\" extra\n> > > reference, except in this full submit case of course. If so, that's\n> > > subtle enough in my mind that a sentence or two on it wouldn't hurt..\n> >\n> > I will add some a comment about this :) That's a good point about the\n> > naming, maybe \"bytes_submitted\" and \"bytes_unsubmitted\" is a lot less\n> > confusing than \"bytes_pending\" and \"bytes_accounted\".\n> \n> Thinking about this some more, bytes_unsubmitted sounds even more\n> confusing, so maybe bytes_nonsubmitted or bytes_not_submitted. I'll\n> think about this some more but kept it as pending/accounted for now.\n> \n\nbytes_submitted sounds better than pending to me, not sure about\nunsubmitted or whatever. As long as there's a sentence or two that\nexplains what accounted means in the end helper, though, that seems\nreasonable enough to me.\n\nBrian\n\n> The fix for this bug is here [1].\n> \n> Thanks,\n> Joanne\n> \n> [1] https://lore.kernel.org/linux-fsdevel/20251024215008.3844068-1-joannelkoong@gmail.com/\n> \n> >\n> > Thanks,\n> > Joanne\n> >\n> > >\n> > > > I'll fix up both. Thanks for catching this and bisecting it down to\n> > > > this patch. Sorry for the trouble.\n> > > >\n> > >\n> > > No prob. Thanks for the fix!\n> > >\n> > > Brian\n> > >\n> > > > Thanks,\n> > > > Joanne\n> > > >\n> > > > [1] https://lore.kernel.org/linux-fsdevel/20251009225611.3744728-4-joannelkoong@gmail.com/\n> > > > >\n> > > > > Thanks,\n> > > > > Joanne\n> > > > > >\n> > > >\n> > >\n> \n\n",
          "reply_to": ""
        },
        {
          "author": "Matthew Wilcox",
          "summary": "Matthew Wilcox pointed out that the patch does not correctly fix the race between iomap_set_range_uptodate and folio_end_read, suggesting that read_bytes_pending should be decremented at the same time as marking a range uptodate. He also proposed several alternative solutions to address this issue.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "alternative solutions"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Fri, Oct 24, 2025 at 09:25:13AM -0700, Joanne Koong wrote:\n> What I missed was that if all the bytes in the folio are non-uptodate\n> and need to read in by the filesystem, then there's a bug where the\n> read will be ended on the folio twice (in iomap_read_end() and when\n> the filesystem calls iomap_finish_folio_write(), when only the\n> filesystem should end the read), which does 2 folio unlocks which ends\n> up locking the folio. Looking at the writeback patch that does a\n> similar optimization [1], I miss the same thing there.\n\nfolio_unlock() contains:\n        VM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n\nFeels like more filesystem people should be enabling CONFIG_DEBUG_VM\nwhen testing (excluding performance testing of course; it'll do ugly\nthings to your performance numbers).\n\n\n---\n\nOn Fri, Oct 24, 2025 at 12:22:32PM -0700, Joanne Koong wrote:\n> > Feels like more filesystem people should be enabling CONFIG_DEBUG_VM\n> > when testing (excluding performance testing of course; it'll do ugly\n> > things to your performance numbers).\n> \n> Point taken. It looks like there's a bunch of other memory debugging\n> configs as well. Do you recommend enabling all of these when testing?\n> Do you have a particular .config you use for when you run tests?\n\nOur Kconfig is far too ornate.  We could do with a \"recommended for\nkernel developers\" profile.  Here's what I'm currently using, though I\nknow it's changed over time:\n\nCONFIG_X86_DEBUGCTLMSR=y\nCONFIG_PM_DEBUG=y\nCONFIG_PM_SLEEP_DEBUG=y\nCONFIG_ARCH_SUPPORTS_DEBUG_PAGEALLOC=y\nCONFIG_BLK_DEBUG_FS=y\nCONFIG_PNP_DEBUG_MESSAGES=y\nCONFIG_SCSI_DEBUG=m\nCONFIG_EXT4_DEBUG=y\nCONFIG_JFS_DEBUG=y\nCONFIG_XFS_DEBUG=y\nCONFIG_BTRFS_DEBUG=y\nCONFIG_UFS_DEBUG=y\nCONFIG_DEBUG_BUGVERBOSE=y\nCONFIG_DEBUG_KERNEL=y\nCONFIG_DEBUG_MISC=y\nCONFIG_DEBUG_INFO=y\nCONFIG_DEBUG_INFO_DWARF4=y\nCONFIG_DEBUG_INFO_COMPRESSED_NONE=y\nCONFIG_DEBUG_FS=y\nCONFIG_DEBUG_FS_ALLOW_ALL=y\nCONFIG_ARCH_HAS_EARLY_DEBUG=y\nCONFIG_SLUB_DEBUG=y\nCONFIG_ARCH_HAS_DEBUG_WX=y\nCONFIG_HAVE_DEBUG_KMEMLEAK=y\nCONFIG_SHRINKER_DEBUG=y\nCONFIG_ARCH_HAS_DEBUG_VM_PGTABLE=y\nCONFIG_DEBUG_VM_IRQSOFF=y\nCONFIG_DEBUG_VM=y\nCONFIG_ARCH_HAS_DEBUG_VIRTUAL=y\nCONFIG_DEBUG_MEMORY_INIT=y\nCONFIG_LOCK_DEBUGGING_SUPPORT=y\nCONFIG_DEBUG_RT_MUTEXES=y\nCONFIG_DEBUG_SPINLOCK=y\nCONFIG_DEBUG_MUTEXES=y\nCONFIG_DEBUG_WW_MUTEX_SLOWPATH=y\nCONFIG_DEBUG_RWSEMS=y\nCONFIG_DEBUG_LOCK_ALLOC=y\nCONFIG_DEBUG_LIST=y\nCONFIG_X86_DEBUG_FPU=y\nCONFIG_FAULT_INJECTION_DEBUG_FS=y\n\n(output from grep DEBUG .build/.config |grep -v ^#)\n\n\n---\n\nOn Tue, Dec 23, 2025 at 05:12:09PM -0800, Joanne Koong wrote:\n> On Tue, Dec 23, 2025 at 2:30\\u202fPM Sasha Levin <sashal@kernel.org> wrote:\n> >\n> \n> Hi Sasha,\n> \n> Thanks for your patch and for the detailed writeup.\n\nThe important line to note is:\n\nAssisted-by: claude-opus-4-5-20251101\n\nSo Sasha has produced a very convincingly worded writeup that's\nhallucinated.\n\n\n\n---\n\nOn Wed, Dec 24, 2025 at 10:43:58AM -0500, Sasha Levin wrote:\n> On Wed, Dec 24, 2025 at 02:10:19AM +0000, Matthew Wilcox wrote:\n> > So Sasha has produced a very convincingly worded writeup that's\n> > hallucinated.\n> \n> And spent a few hours trying to figure it out so I could unblock testing, but\n> sure - thanks.\n\nWhen you produce a convincingly worded writeup that's utterly wrong,\nand have a reputation for using AI, that's the kind of reaction you're\ngoing to get.\n\n> Here's the full log:\n> https://qa-reports.linaro.org/lkft/sashal-linus-next/build/v6.18-rc7-13806-gb927546677c8/testrun/30618654/suite/log-parser-test/test/exception-warning-fsiomapbuffered-io-at-ifs_free/log\n> , happy to test any patches you might have.\n\nThat's actually much more helpful because it removes your incorrect\nassumptions about what's going on.\n\n WARNING: fs/iomap/buffered-io.c:254 at ifs_free+0x130/0x148, CPU#0: msync04/406\n\nThat's this one:\n\n        WARN_ON_ONCE(ifs_is_fully_uptodate(folio, ifs) !=\n                        folio_test_uptodate(folio));\n\nwhich would be fully explained by fuse calling folio_clear_uptodate()\nin fuse_send_write_pages().  I have come to believe that allowing\nfilesystems to call folio_clear_uptodate() is just dangerous.  It\ncauses assertions to fire all over the place (eg if the page is mapped\ninto memory, the MM contains assertions that it must be uptodate).\n\nSo I think the first step is simply to delete the folio_clear_uptodate()\ncalls in fuse:\n\ndiff --git a/fs/fuse/file.c b/fs/fuse/file.c\nindex 01bc894e9c2b..b819ede407d5 100644\n--- a/fs/fuse/file.c\n+++ b/fs/fuse/file.c\n@@ -1194,7 +1194,6 @@ static ssize_t fuse_send_write_pages(struct fuse_io_args *ia,\n \tstruct fuse_file *ff = file->private_data;\n \tstruct fuse_mount *fm = ff->fm;\n \tunsigned int offset, i;\n-\tbool short_write;\n \tint err;\n \n \tfor (i = 0; i < ap->num_folios; i++)\n@@ -1209,22 +1208,16 @@ static ssize_t fuse_send_write_pages(struct fuse_io_args *ia,\n \tif (!err && ia->write.out.size > count)\n \t\terr = -EIO;\n \n-\tshort_write = ia->write.out.size < count;\n \toffset = ap->descs[0].offset;\n \tcount = ia->write.out.size;\n \tfor (i = 0; i < ap->num_folios; i++) {\n \t\tstruct folio *folio = ap->folios[i];\n \n-\t\tif (err) {\n-\t\t\tfolio_clear_uptodate(folio);\n-\t\t} else {\n+\t\tif (!err) {\n \t\t\tif (count >= folio_size(folio) - offset)\n \t\t\t\tcount -= folio_size(folio) - offset;\n-\t\t\telse {\n-\t\t\t\tif (short_write)\n-\t\t\t\t\tfolio_clear_uptodate(folio);\n+\t\t\telse\n \t\t\t\tcount = 0;\n-\t\t\t}\n \t\t\toffset = 0;\n \t\t}\n \t\tif (ia->write.folio_locked && (i == ap->num_folios - 1))\n\n\n---\n\nOn Tue, Feb 10, 2026 at 02:18:06PM -0800, Joanne Koong wrote:\n>                 spin_lock_irqsave(&ifs->state_lock, flags);\n> -               uptodate = ifs_set_range_uptodate(folio, ifs, off, len);\n> +               /*\n> +                * If a read is in progress, we must NOT call\n> folio_mark_uptodate.\n> +                * The read completion path (iomap_finish_folio_read or\n> +                * iomap_read_end) will call folio_end_read() which uses XOR\n> +                * semantics to set the uptodate bit. If we set it here, the XOR\n> +                * in folio_end_read() will clear it, leaving the folio not\n> +                * uptodate.\n> +                */\n> +               uptodate = ifs_set_range_uptodate(folio, ifs, off, len) &&\n> +                       !ifs->read_bytes_pending;\n>                 spin_unlock_irqrestore(&ifs->state_lock, flags);\n\nThis can't possibly be the right fix.  There's some horrible confusion\nhere.  It should not be possible to have read bytes pending _and_ the\nentire folio be uptodate.  That's an invariant that should always be\nmaintained.\n\n\n---\n\nOn Wed, Feb 11, 2026 at 11:33:05AM -0800, Joanne Koong wrote:\n> ifs->read_bytes_pending gets initialized to the folio size, but if the\n> file being read in is smaller than the size of the folio, then we\n> reach this scenario because the file has been read in but\n> ifs->read_bytes_pending is still a positive value because it\n> represents the bytes between the end of the file and the end of the\n> folio. If the folio size is 16k and the file size is 4k:\n>   a) ifs->read_bytes_pending gets initialized to 16k\n>   b) ->read_folio_range() is called for the 4k read\n>   c) the 4k read succeeds, ifs->read_bytes_pending is now 12k and the\n> 0 to 4k range is marked uptodate\n>   d) the post-eof blocks are zeroed and marked uptodate in the call to\n> iomap_set_range_uptodate()\n\nThis is the bug then.  If they're marked uptodate, read_bytes_pending\nshould be decremented at the same time.  Now, I appreciate that\niomap_set_range_uptodate() is called both from iomap_read_folio_iter()\nand __iomap_write_begin(), and it can't decrement read_bytes_pending\nin the latter case.  Perhaps a flag or a second length parameter is\nthe solution?\n\n>   e) iomap_set_range_uptodate() sees all the ranges are marked\n> uptodate and it marks the folio uptodate\n>   f) iomap_read_end() gets called to subtract the 12k from\n> ifs->read_bytes_pending. it too sees all the ranges are marked\n> uptodate and marks the folio uptodate\n> \n> The same scenario could happen for IOMAP_INLINE mappings if part of\n> the folio is read in through ->read_folio_range() and then the rest is\n> read in as inline data.\n\nThis is basically the same case as post-eof.\n\n> An alternative solution is to not have zeroed-out / inlined mappings\n> call iomap_read_end(), eg something like this [1], but this adds\n> additional complexity and doesn't work if there's additional mappings\n> for the folio after a non-IOMAP_MAPPED mapping.\n> \n> Is there a better approach that I'm missing?\n> \n> Thanks,\n> Joanne\n> \n> [1] https://github.com/joannekoong/linux/commit/de48d3c29db8ae654300341e3eec12497df54673\n\n\n---\n\nOn Wed, Feb 11, 2026 at 03:13:48PM -0800, Joanne Koong wrote:\n> On Wed, Feb 11, 2026 at 1:03\\u202fPM Matthew Wilcox <willy@infradead.org> wrote:\n> >\n> > On Wed, Feb 11, 2026 at 11:33:05AM -0800, Joanne Koong wrote:\n> > > ifs->read_bytes_pending gets initialized to the folio size, but if the\n> > > file being read in is smaller than the size of the folio, then we\n> > > reach this scenario because the file has been read in but\n> > > ifs->read_bytes_pending is still a positive value because it\n> > > represents the bytes between the end of the file and the end of the\n> > > folio. If the folio size is 16k and the file size is 4k:\n> > >   a) ifs->read_bytes_pending gets initialized to 16k\n> > >   b) ->read_folio_range() is called for the 4k read\n> > >   c) the 4k read succeeds, ifs->read_bytes_pending is now 12k and the\n> > > 0 to 4k range is marked uptodate\n> > >   d) the post-eof blocks are zeroed and marked uptodate in the call to\n> > > iomap_set_range_uptodate()\n> >\n> > This is the bug then.  If they're marked uptodate, read_bytes_pending\n> > should be decremented at the same time.  Now, I appreciate that\n> > iomap_set_range_uptodate() is called both from iomap_read_folio_iter()\n> > and __iomap_write_begin(), and it can't decrement read_bytes_pending\n> > in the latter case.  Perhaps a flag or a second length parameter is\n> > the solution?\n> \n> I don't think it's enough to decrement read_bytes_pending by the\n> zeroed/read-inline length because there's these two edge cases:\n> a) some blocks in the folio were already uptodate from the very\n> beginning and skipped for IO but not decremented yet from\n> ifs->read_bytes_pending, which means in iomap_read_end(),\n> ifs->read_bytes_pending would be > 0 and the uptodate flag could get\n> XORed again. This means we need to also decrement read_bytes_pending\n> by bytes_submitted as well for this case\n\nHm, that's a good one.  It can't happen for readahead, but it can happen\nif we start out by writing to some blocks of a folio, then call\nread_folio to get the remaining blocks uptodate.  We could avoid it\nhappening by initialising read_bytes_pending to folio_size() -\nbitmap_weight(ifs->uptodate) * block_size.\n\n> b) the async ->read_folio_range() callback finishes after the\n> zeroing's read_bytes_pending decrement and calls folio_end_read(), so\n> we need to assign ctx->cur_folio to NULL\n\nIf we return 'finished' from iomap_finish_folio_read(), we can handle\nthis?\n\n> I think the code would have to look something like [1] (this is\n> similar to the alternative approach I mentioned in my previous reply\n> but fixed up to cover some more edge cases).\n> \n> Thanks,\n> Joanne\n> \n> [1] https://github.com/joannekoong/linux/commit/b42f47726433a8130e8c27d1b43b16e27dfd6960\n\nI think we can do everything we need with a suitably modified\niomap_finish_folio_read() rather than the new iomap_finish_read_range().\n",
          "reply_to": ""
        },
        {
          "author": "Darrick Wong",
          "summary": "The reviewer suggested splitting VM debug checks into cheap and expensive ones, allowing for a kconfig option to enable only the cheap checks without significantly increasing fstests time.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "On Fri, Oct 24, 2025 at 09:59:01PM +0100, Matthew Wilcox wrote:\n> On Fri, Oct 24, 2025 at 12:22:32PM -0700, Joanne Koong wrote:\n> > > Feels like more filesystem people should be enabling CONFIG_DEBUG_VM\n> > > when testing (excluding performance testing of course; it'll do ugly\n> > > things to your performance numbers).\n> > \n> > Point taken. It looks like there's a bunch of other memory debugging\n> > configs as well. Do you recommend enabling all of these when testing?\n> > Do you have a particular .config you use for when you run tests?\n> \n> Our Kconfig is far too ornate.  We could do with a \"recommended for\n> kernel developers\" profile.  Here's what I'm currently using, though I\n> know it's changed over time:\n\nIs there any chance you could split the VM debug checks into cheap and\nexpensive ones, and create another kconfig option so that we could do\nthe cheap checks without having fstests take a lot longer?\n\nYou could also implement shenanigans like the following:\nhttps://git.kernel.org/pub/scm/linux/kernel/git/djwong/xfs-linux.git/commit/?h=djwong-wtf&id=b739fff870384fd239abfd99ecee6bc47640794d\n\nTo enable the expensive checks at runtime.\n\n(Yeah, I know, this is probably a 2 year project + bikeshed score of at\nleast 30...)\n\n--D\n\n> CONFIG_X86_DEBUGCTLMSR=y\n> CONFIG_PM_DEBUG=y\n> CONFIG_PM_SLEEP_DEBUG=y\n> CONFIG_ARCH_SUPPORTS_DEBUG_PAGEALLOC=y\n> CONFIG_BLK_DEBUG_FS=y\n> CONFIG_PNP_DEBUG_MESSAGES=y\n> CONFIG_SCSI_DEBUG=m\n> CONFIG_EXT4_DEBUG=y\n> CONFIG_JFS_DEBUG=y\n> CONFIG_XFS_DEBUG=y\n> CONFIG_BTRFS_DEBUG=y\n> CONFIG_UFS_DEBUG=y\n> CONFIG_DEBUG_BUGVERBOSE=y\n> CONFIG_DEBUG_KERNEL=y\n> CONFIG_DEBUG_MISC=y\n> CONFIG_DEBUG_INFO=y\n> CONFIG_DEBUG_INFO_DWARF4=y\n> CONFIG_DEBUG_INFO_COMPRESSED_NONE=y\n> CONFIG_DEBUG_FS=y\n> CONFIG_DEBUG_FS_ALLOW_ALL=y\n> CONFIG_ARCH_HAS_EARLY_DEBUG=y\n> CONFIG_SLUB_DEBUG=y\n> CONFIG_ARCH_HAS_DEBUG_WX=y\n> CONFIG_HAVE_DEBUG_KMEMLEAK=y\n> CONFIG_SHRINKER_DEBUG=y\n> CONFIG_ARCH_HAS_DEBUG_VM_PGTABLE=y\n> CONFIG_DEBUG_VM_IRQSOFF=y\n> CONFIG_DEBUG_VM=y\n> CONFIG_ARCH_HAS_DEBUG_VIRTUAL=y\n> CONFIG_DEBUG_MEMORY_INIT=y\n> CONFIG_LOCK_DEBUGGING_SUPPORT=y\n> CONFIG_DEBUG_RT_MUTEXES=y\n> CONFIG_DEBUG_SPINLOCK=y\n> CONFIG_DEBUG_MUTEXES=y\n> CONFIG_DEBUG_WW_MUTEX_SLOWPATH=y\n> CONFIG_DEBUG_RWSEMS=y\n> CONFIG_DEBUG_LOCK_ALLOC=y\n> CONFIG_DEBUG_LIST=y\n> CONFIG_X86_DEBUG_FPU=y\n> CONFIG_FAULT_INJECTION_DEBUG_FS=y\n> \n> (output from grep DEBUG .build/.config |grep -v ^#)\n",
          "reply_to": ""
        },
        {
          "author": "Sasha Levin",
          "summary": "Reviewer Sasha Levin raised concerns about a potential race condition between iomap_set_range_uptodate and folio_end_read, which was triggered by the FUSE iomap patchset. He provided a fix that checks read_bytes_pending under the state_lock in iomap_set_range_uptodate to prevent concurrent access.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "race condition",
            "potential bug"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Hi Joanne,\n\nWhile testing with your FUSE iomap patchset that recently landed upstream,\nI ran into a warning in ifs_free() where the folio's uptodate flag didn't\nmatch the ifs per-block uptodate bitmap. The warning was triggered during\nFUSE-based filesystem unmount when running the LTP writev03 test.\n\nAfter some investigation, I believe the root cause is a race condition\nthat has existed since commit 7a4847e54cc1 (\"iomap: use folio_end_read()\")\nbut was difficult to trigger until now. The issue is that folio_end_read()\nuses XOR semantics to set the uptodate bit, so if iomap_set_range_uptodate()\ncalls folio_mark_uptodate() while a read is in progress, the subsequent\nfolio_end_read() will XOR and clear the uptodate bit.\n\nThe FUSE iomap enablement seems to have created the right conditions to\nexpose this race - likely due to different file extent patterns in\nFUSE-based filesystems (like NTFS-3G) compared to native filesystems\nlike XFS/ext4.\n\nThe fix checks read_bytes_pending under the state_lock in\niomap_set_range_uptodate() and skips calling folio_mark_uptodate() if a\nread is in progress, letting the read completion path handle it.\n\nI'm not very familiar with the iomap internals, so I'd really appreciate\nyour review and feedback on whether this approach is correct.\n\nThanks,\nSasha\n\nSasha Levin (1):\n  iomap: fix race between iomap_set_range_uptodate and folio_end_read\n\n fs/fuse/dev.c          |  3 +-\n fs/fuse/file.c         |  6 ++--\n fs/iomap/buffered-io.c | 65 +++++++++++++++++++++++++++++++++++++++---\n include/linux/iomap.h  |  2 ++\n 4 files changed, 68 insertions(+), 8 deletions(-)\n\n-- \n2.51.0\n\n\n\n---\n\nWhen iomap uses large folios, per-block uptodate tracking is managed via\niomap_folio_state (ifs). A race condition can cause the ifs uptodate bits\nto become inconsistent with the folio's uptodate flag.\n\nThe race occurs because folio_end_read() uses XOR semantics to atomically\nset the uptodate bit and clear the locked bit:\n\n  Thread A (read completion):          Thread B (concurrent write):\n  --------------------------------     --------------------------------\n  iomap_finish_folio_read()\n    spin_lock(state_lock)\n    ifs_set_range_uptodate() -> true\n    spin_unlock(state_lock)\n                                       iomap_set_range_uptodate()\n                                         spin_lock(state_lock)\n                                         ifs_set_range_uptodate() -> true\n                                         spin_unlock(state_lock)\n                                         folio_mark_uptodate(folio)\n    folio_end_read(folio, true)\n      folio_xor_flags()  // XOR CLEARS uptodate!\n\nResult: folio is NOT uptodate, but ifs says all blocks ARE uptodate.\n\nFix by checking read_bytes_pending in iomap_set_range_uptodate() under the\nlock. If a read is in progress, skip calling folio_mark_uptodate() - the\nread completion path will handle it via folio_end_read().\n\nThe warning was triggered during FUSE-based filesystem (e.g., NTFS-3G)\nunmount when the LTP writev03 test was run:\n\n  WARNING: fs/iomap/buffered-io.c at ifs_free\n  Call trace:\n   ifs_free\n   iomap_invalidate_folio\n   truncate_cleanup_folio\n   truncate_inode_pages_range\n   truncate_inode_pages_final\n   fuse_evict_inode\n   ...\n   fuse_kill_sb_blk\n\nFixes: 7a4847e54cc1 (\"iomap: use folio_end_read()\")\nAssisted-by: claude-opus-4-5-20251101\nSigned-off-by: Sasha Levin <sashal@kernel.org>\n---\n fs/fuse/dev.c          |  3 +-\n fs/fuse/file.c         |  6 ++--\n fs/iomap/buffered-io.c | 65 +++++++++++++++++++++++++++++++++++++++---\n include/linux/iomap.h  |  2 ++\n 4 files changed, 68 insertions(+), 8 deletions(-)\n\ndiff --git a/fs/fuse/dev.c b/fs/fuse/dev.c\nindex 6d59cbc877c6..50e84e913589 100644\n--- a/fs/fuse/dev.c\n+++ b/fs/fuse/dev.c\n@@ -11,6 +11,7 @@\n #include \"fuse_dev_i.h\"\n \n #include <linux/init.h>\n+#include <linux/iomap.h>\n #include <linux/module.h>\n #include <linux/poll.h>\n #include <linux/sched/signal.h>\n@@ -1820,7 +1821,7 @@ static int fuse_notify_store(struct fuse_conn *fc, unsigned int size,\n \t\tif (!folio_test_uptodate(folio) && !err && offset == 0 &&\n \t\t    (nr_bytes == folio_size(folio) || file_size == end)) {\n \t\t\tfolio_zero_segment(folio, nr_bytes, folio_size(folio));\n-\t\t\tfolio_mark_uptodate(folio);\n+\t\t\tiomap_set_range_uptodate(folio, 0, folio_size(folio));\n \t\t}\n \t\tfolio_unlock(folio);\n \t\tfolio_put(folio);\ndiff --git a/fs/fuse/file.c b/fs/fuse/file.c\nindex 01bc894e9c2b..3abe38416199 100644\n--- a/fs/fuse/file.c\n+++ b/fs/fuse/file.c\n@@ -1216,13 +1216,13 @@ static ssize_t fuse_send_write_pages(struct fuse_io_args *ia,\n \t\tstruct folio *folio = ap->folios[i];\n \n \t\tif (err) {\n-\t\t\tfolio_clear_uptodate(folio);\n+\t\t\tiomap_clear_folio_uptodate(folio);\n \t\t} else {\n \t\t\tif (count >= folio_size(folio) - offset)\n \t\t\t\tcount -= folio_size(folio) - offset;\n \t\t\telse {\n \t\t\t\tif (short_write)\n-\t\t\t\t\tfolio_clear_uptodate(folio);\n+\t\t\t\t\tiomap_clear_folio_uptodate(folio);\n \t\t\t\tcount = 0;\n \t\t\t}\n \t\t\toffset = 0;\n@@ -1305,7 +1305,7 @@ static ssize_t fuse_fill_write_pages(struct fuse_io_args *ia,\n \n \t\t/* If we copied full folio, mark it uptodate */\n \t\tif (tmp == folio_size(folio))\n-\t\t\tfolio_mark_uptodate(folio);\n+\t\t\tiomap_set_range_uptodate(folio, 0, folio_size(folio));\n \n \t\tif (folio_test_uptodate(folio)) {\n \t\t\tfolio_unlock(folio);\ndiff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\nindex e5c1ca440d93..7ceda24cf6a7 100644\n--- a/fs/iomap/buffered-io.c\n+++ b/fs/iomap/buffered-io.c\n@@ -74,8 +74,7 @@ static bool ifs_set_range_uptodate(struct folio *folio,\n \treturn ifs_is_fully_uptodate(folio, ifs);\n }\n \n-static void iomap_set_range_uptodate(struct folio *folio, size_t off,\n-\t\tsize_t len)\n+void iomap_set_range_uptodate(struct folio *folio, size_t off, size_t len)\n {\n \tstruct iomap_folio_state *ifs = folio->private;\n \tunsigned long flags;\n@@ -87,12 +86,50 @@ static void iomap_set_range_uptodate(struct folio *folio, size_t off,\n \tif (ifs) {\n \t\tspin_lock_irqsave(&ifs->state_lock, flags);\n \t\tuptodate = ifs_set_range_uptodate(folio, ifs, off, len);\n+\t\t/*\n+\t\t * If a read is in progress, we must NOT call folio_mark_uptodate\n+\t\t * here. The read completion path (iomap_finish_folio_read or\n+\t\t * iomap_read_end) will call folio_end_read() which uses XOR\n+\t\t * semantics to set the uptodate bit. If we set it here, the XOR\n+\t\t * in folio_end_read() will clear it, leaving the folio not\n+\t\t * uptodate while the ifs says all blocks are uptodate.\n+\t\t */\n+\t\tif (uptodate && ifs->read_bytes_pending)\n+\t\t\tuptodate = false;\n \t\tspin_unlock_irqrestore(&ifs->state_lock, flags);\n \t}\n \n \tif (uptodate)\n \t\tfolio_mark_uptodate(folio);\n }\n+EXPORT_SYMBOL_GPL(iomap_set_range_uptodate);\n+\n+void iomap_clear_folio_uptodate(struct folio *folio)\n+{\n+\tstruct iomap_folio_state *ifs = folio->private;\n+\n+\tif (ifs) {\n+\t\tstruct inode *inode = folio->mapping->host;\n+\t\tunsigned int nr_blocks = i_blocks_per_folio(inode, folio);\n+\t\tunsigned long flags;\n+\n+\t\tspin_lock_irqsave(&ifs->state_lock, flags);\n+\t\t/*\n+\t\t * If a read is in progress, don't clear the uptodate state.\n+\t\t * The read completion path will handle the folio state, and\n+\t\t * clearing here would race with iomap_finish_folio_read()\n+\t\t * potentially causing ifs/folio uptodate state mismatch.\n+\t\t */\n+\t\tif (ifs->read_bytes_pending) {\n+\t\t\tspin_unlock_irqrestore(&ifs->state_lock, flags);\n+\t\t\treturn;\n+\t\t}\n+\t\tbitmap_clear(ifs->state, 0, nr_blocks);\n+\t\tspin_unlock_irqrestore(&ifs->state_lock, flags);\n+\t}\n+\tfolio_clear_uptodate(folio);\n+}\n+EXPORT_SYMBOL_GPL(iomap_clear_folio_uptodate);\n \n /*\n  * Find the next dirty block in the folio. end_blk is inclusive.\n@@ -399,8 +436,17 @@ void iomap_finish_folio_read(struct folio *folio, size_t off, size_t len,\n \t\tspin_unlock_irqrestore(&ifs->state_lock, flags);\n \t}\n \n-\tif (finished)\n+\tif (finished) {\n+\t\t/*\n+\t\t * If uptodate is true but the folio is already marked uptodate,\n+\t\t * folio_end_read's XOR semantics would clear the uptodate bit.\n+\t\t * This should never happen because iomap_set_range_uptodate()\n+\t\t * skips calling folio_mark_uptodate() when read_bytes_pending\n+\t\t * is non-zero, ensuring only the read completion path sets it.\n+\t\t */\n+\t\tWARN_ON_ONCE(uptodate && folio_test_uptodate(folio));\n \t\tfolio_end_read(folio, uptodate);\n+\t}\n }\n EXPORT_SYMBOL_GPL(iomap_finish_folio_read);\n \n@@ -481,8 +527,19 @@ static void iomap_read_end(struct folio *folio, size_t bytes_submitted)\n \t\tif (end_read)\n \t\t\tuptodate = ifs_is_fully_uptodate(folio, ifs);\n \t\tspin_unlock_irq(&ifs->state_lock);\n-\t\tif (end_read)\n+\t\tif (end_read) {\n+\t\t\t/*\n+\t\t\t * If uptodate is true but the folio is already marked\n+\t\t\t * uptodate, folio_end_read's XOR semantics would clear\n+\t\t\t * the uptodate bit. This should never happen because\n+\t\t\t * iomap_set_range_uptodate() skips calling\n+\t\t\t * folio_mark_uptodate() when read_bytes_pending is\n+\t\t\t * non-zero, ensuring only the read completion path\n+\t\t\t * sets it.\n+\t\t\t */\n+\t\t\tWARN_ON_ONCE(uptodate && folio_test_uptodate(folio));\n \t\t\tfolio_end_read(folio, uptodate);\n+\t\t}\n \t} else if (!bytes_submitted) {\n \t\t/*\n \t\t * If there were no bytes submitted, this means we are\ndiff --git a/include/linux/iomap.h b/include/linux/iomap.h\nindex 520e967cb501..3c2ad88d16b6 100644\n--- a/include/linux/iomap.h\n+++ b/include/linux/iomap.h\n@@ -345,6 +345,8 @@ void iomap_read_folio(const struct iomap_ops *ops,\n void iomap_readahead(const struct iomap_ops *ops,\n \t\tstruct iomap_read_folio_ctx *ctx);\n bool iomap_is_partially_uptodate(struct folio *, size_t from, size_t count);\n+void iomap_set_range_uptodate(struct folio *folio, size_t off, size_t len);\n+void iomap_clear_folio_uptodate(struct folio *folio);\n struct folio *iomap_get_folio(struct iomap_iter *iter, loff_t pos, size_t len);\n bool iomap_release_folio(struct folio *folio, gfp_t gfp_flags);\n void iomap_invalidate_folio(struct folio *folio, size_t offset, size_t len);\n-- \n2.51.0\n\n\n\n---\n\nOn Tue, Dec 23, 2025 at 05:12:09PM -0800, Joanne Koong wrote:\n>On Tue, Dec 23, 2025 at 2:30\\u202fPM Sasha Levin <sashal@kernel.org> wrote:\n>>\n>\n>Hi Sasha,\n>\n>Thanks for your patch and for the detailed writeup.\n\nThanks for looking into this!\n\n>> When iomap uses large folios, per-block uptodate tracking is managed via\n>> iomap_folio_state (ifs). A race condition can cause the ifs uptodate bits\n>> to become inconsistent with the folio's uptodate flag.\n>>\n>> The race occurs because folio_end_read() uses XOR semantics to atomically\n>> set the uptodate bit and clear the locked bit:\n>>\n>>   Thread A (read completion):          Thread B (concurrent write):\n>>   --------------------------------     --------------------------------\n>>   iomap_finish_folio_read()\n>>     spin_lock(state_lock)\n>>     ifs_set_range_uptodate() -> true\n>>     spin_unlock(state_lock)\n>>                                        iomap_set_range_uptodate()\n>>                                          spin_lock(state_lock)\n>>                                          ifs_set_range_uptodate() -> true\n>>                                          spin_unlock(state_lock)\n>>                                          folio_mark_uptodate(folio)\n>>     folio_end_read(folio, true)\n>>       folio_xor_flags()  // XOR CLEARS uptodate!\n>\n>The part I'm confused about here is how this can happen between a\n>concurrent read and write. My understanding is that the folio is\n>locked when the read occurs and locked when the write occurs and both\n>locks get dropped only when the read or write finishes. Looking at\n>iomap code, I see iomap_set_range_uptodate() getting called in\n>__iomap_write_begin() and __iomap_write_end() for the writes, but in\n>both those places the folio lock is held while this is called. I'm not\n>seeing how the read and write race in the diagram can happen, but\n>maybe I'm missing something here?\n\nHmm, you're right... The folio lock should prevent concurrent read/write\naccess. Looking at this again, I suspect that FUSE was calling\nfolio_clear_uptodate() and folio_mark_uptodate() directly without updating the\nifs bits. For example, in fuse_send_write_pages() on write error, it calls\nfolio_clear_uptodate(folio) which clears the folio flag but leaves ifs still\nshowing all blocks uptodate?\n\n>>\n>> Result: folio is NOT uptodate, but ifs says all blocks ARE uptodate.\n>\n>Ah I see the WARN_ON_ONCE() in ifs_free:\n>        WARN_ON_ONCE(ifs_is_fully_uptodate(folio, ifs) !=\n>                        folio_test_uptodate(folio));\n>\n>Just to confirm, are you seeing that the folio is not marked uptodate\n>but the ifs blocks are? Or are the ifs blocks not uptodate but the\n>folio is?\n\nThe former: folio is NOT uptodate but ifs shows all blocks ARE uptodate\n(state=0xffff with 16 blocks)\n\n>>\n>> Fix by checking read_bytes_pending in iomap_set_range_uptodate() under the\n>> lock. If a read is in progress, skip calling folio_mark_uptodate() - the\n>> read completion path will handle it via folio_end_read().\n>>\n>> The warning was triggered during FUSE-based filesystem (e.g., NTFS-3G)\n>> unmount when the LTP writev03 test was run:\n>>\n>>   WARNING: fs/iomap/buffered-io.c at ifs_free\n>>   Call trace:\n>>    ifs_free\n>>    iomap_invalidate_folio\n>>    truncate_cleanup_folio\n>>    truncate_inode_pages_range\n>>    truncate_inode_pages_final\n>>    fuse_evict_inode\n>>    ...\n>>    fuse_kill_sb_blk\n>>\n>> Fixes: 7a4847e54cc1 (\"iomap: use folio_end_read()\")\n>> Assisted-by: claude-opus-4-5-20251101\n>> Signed-off-by: Sasha Levin <sashal@kernel.org>\n>> ---\n>>  fs/fuse/dev.c          |  3 +-\n>>  fs/fuse/file.c         |  6 ++--\n>>  fs/iomap/buffered-io.c | 65 +++++++++++++++++++++++++++++++++++++++---\n>>  include/linux/iomap.h  |  2 ++\n>>  4 files changed, 68 insertions(+), 8 deletions(-)\n>>\n>> diff --git a/fs/fuse/dev.c b/fs/fuse/dev.c\n>> index 6d59cbc877c6..50e84e913589 100644\n>> --- a/fs/fuse/dev.c\n>> +++ b/fs/fuse/dev.c\n>> @@ -11,6 +11,7 @@\n>>  #include \"fuse_dev_i.h\"\n>>\n>>  #include <linux/init.h>\n>> +#include <linux/iomap.h>\n>>  #include <linux/module.h>\n>>  #include <linux/poll.h>\n>>  #include <linux/sched/signal.h>\n>> @@ -1820,7 +1821,7 @@ static int fuse_notify_store(struct fuse_conn *fc, unsigned int size,\n>>                 if (!folio_test_uptodate(folio) && !err && offset == 0 &&\n>>                     (nr_bytes == folio_size(folio) || file_size == end)) {\n>>                         folio_zero_segment(folio, nr_bytes, folio_size(folio));\n>> -                       folio_mark_uptodate(folio);\n>> +                       iomap_set_range_uptodate(folio, 0, folio_size(folio));\n>>                 }\n>>                 folio_unlock(folio);\n>>                 folio_put(folio);\n>> diff --git a/fs/fuse/file.c b/fs/fuse/file.c\n>> index 01bc894e9c2b..3abe38416199 100644\n>> --- a/fs/fuse/file.c\n>> +++ b/fs/fuse/file.c\n>> @@ -1216,13 +1216,13 @@ static ssize_t fuse_send_write_pages(struct fuse_io_args *ia,\n>>                 struct folio *folio = ap->folios[i];\n>>\n>>                 if (err) {\n>> -                       folio_clear_uptodate(folio);\n>> +                       iomap_clear_folio_uptodate(folio);\n>>                 } else {\n>>                         if (count >= folio_size(folio) - offset)\n>>                                 count -= folio_size(folio) - offset;\n>>                         else {\n>>                                 if (short_write)\n>> -                                       folio_clear_uptodate(folio);\n>> +                                       iomap_clear_folio_uptodate(folio);\n>>                                 count = 0;\n>>                         }\n>>                         offset = 0;\n>> @@ -1305,7 +1305,7 @@ static ssize_t fuse_fill_write_pages(struct fuse_io_args *ia,\n>>\n>>                 /* If we copied full folio, mark it uptodate */\n>>                 if (tmp == folio_size(folio))\n>> -                       folio_mark_uptodate(folio);\n>> +                       iomap_set_range_uptodate(folio, 0, folio_size(folio));\n>>\n>>                 if (folio_test_uptodate(folio)) {\n>>                         folio_unlock(folio);\n>> diff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\n>> index e5c1ca440d93..7ceda24cf6a7 100644\n>> --- a/fs/iomap/buffered-io.c\n>> +++ b/fs/iomap/buffered-io.c\n>> @@ -74,8 +74,7 @@ static bool ifs_set_range_uptodate(struct folio *folio,\n>>         return ifs_is_fully_uptodate(folio, ifs);\n>>  }\n>>\n>> -static void iomap_set_range_uptodate(struct folio *folio, size_t off,\n>> -               size_t len)\n>> +void iomap_set_range_uptodate(struct folio *folio, size_t off, size_t len)\n>>  {\n>>         struct iomap_folio_state *ifs = folio->private;\n>>         unsigned long flags;\n>> @@ -87,12 +86,50 @@ static void iomap_set_range_uptodate(struct folio *folio, size_t off,\n>>         if (ifs) {\n>>                 spin_lock_irqsave(&ifs->state_lock, flags);\n>>                 uptodate = ifs_set_range_uptodate(folio, ifs, off, len);\n>> +               /*\n>> +                * If a read is in progress, we must NOT call folio_mark_uptodate\n>> +                * here. The read completion path (iomap_finish_folio_read or\n>> +                * iomap_read_end) will call folio_end_read() which uses XOR\n>> +                * semantics to set the uptodate bit. If we set it here, the XOR\n>> +                * in folio_end_read() will clear it, leaving the folio not\n>> +                * uptodate while the ifs says all blocks are uptodate.\n>> +                */\n>> +               if (uptodate && ifs->read_bytes_pending)\n>> +                       uptodate = false;\n>\n>Does the warning you saw in ifs_free() still go away without the\n>changes here to iomap_set_range_uptodate() or is this change here\n>necessary?  I'm asking mostly because I'm not seeing how\n>iomap_set_range_uptodate() can be called while the read is in\n>progress, as the logic should be already protected by the folio locks.\n\nYes, the warning goes away even without this part. I don't think that this is\nnecessary - I just kept it while figuring out the race.\n\n>>                 spin_unlock_irqrestore(&ifs->state_lock, flags);\n>>         }\n>>\n>>         if (uptodate)\n>>                 folio_mark_uptodate(folio);\n>>  }\n>> +EXPORT_SYMBOL_GPL(iomap_set_range_uptodate);\n>> +\n>> +void iomap_clear_folio_uptodate(struct folio *folio)\n>> +{\n>> +       struct iomap_folio_state *ifs = folio->private;\n>> +\n>> +       if (ifs) {\n>> +               struct inode *inode = folio->mapping->host;\n>> +               unsigned int nr_blocks = i_blocks_per_folio(inode, folio);\n>> +               unsigned long flags;\n>> +\n>> +               spin_lock_irqsave(&ifs->state_lock, flags);\n>> +               /*\n>> +                * If a read is in progress, don't clear the uptodate state.\n>> +                * The read completion path will handle the folio state, and\n>> +                * clearing here would race with iomap_finish_folio_read()\n>> +                * potentially causing ifs/folio uptodate state mismatch.\n>> +                */\n>> +               if (ifs->read_bytes_pending) {\n>> +                       spin_unlock_irqrestore(&ifs->state_lock, flags);\n>> +                       return;\n>> +               }\n>> +               bitmap_clear(ifs->state, 0, nr_blocks);\n>> +               spin_unlock_irqrestore(&ifs->state_lock, flags);\n>> +       }\n>> +       folio_clear_uptodate(folio);\n>> +}\n>> +EXPORT_SYMBOL_GPL(iomap_clear_folio_uptodate);\n>>\n>>  /*\n>>   * Find the next dirty block in the folio. end_blk is inclusive.\n>> @@ -399,8 +436,17 @@ void iomap_finish_folio_read(struct folio *folio, size_t off, size_t len,\n>>                 spin_unlock_irqrestore(&ifs->state_lock, flags);\n>>         }\n>>\n>> -       if (finished)\n>> +       if (finished) {\n>> +               /*\n>> +                * If uptodate is true but the folio is already marked uptodate,\n>> +                * folio_end_read's XOR semantics would clear the uptodate bit.\n>> +                * This should never happen because iomap_set_range_uptodate()\n>> +                * skips calling folio_mark_uptodate() when read_bytes_pending\n>> +                * is non-zero, ensuring only the read completion path sets it.\n>> +                */\n>> +               WARN_ON_ONCE(uptodate && folio_test_uptodate(folio));\n>\n>Matthew pointed out in another thread [1] that folio_end_read() has\n>already the warnings against double-unlocks or double-uptodates\n>in-built:\n>\n>        VM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n>        VM_BUG_ON_FOLIO(success && folio_test_uptodate(folio), folio);\n>\n>but imo the WARN_ON_ONCE() here is nice to have too, as I don't think\n>most builds enable CONFIG_DEBUG_VM.\n>\n>[1] https://lore.kernel.org/linux-fsdevel/aPu1ilw6Tq6tKPrf@casper.infradead.org/\n>\n>Thanks,\n>Joanne\n>>                 folio_end_read(folio, uptodate);\n>> +       }\n>>  }\n>>  EXPORT_SYMBOL_GPL(iomap_finish_folio_read);\n>>\n>> @@ -481,8 +527,19 @@ static void iomap_read_end(struct folio *folio, size_t bytes_submitted)\n>>                 if (end_read)\n>>                         uptodate = ifs_is_fully_uptodate(folio, ifs);\n>>                 spin_unlock_irq(&ifs->state_lock);\n>> -               if (end_read)\n>> +               if (end_read) {\n>> +                       /*\n>> +                        * If uptodate is true but the folio is already marked\n>> +                        * uptodate, folio_end_read's XOR semantics would clear\n>> +                        * the uptodate bit. This should never happen because\n>> +                        * iomap_set_range_uptodate() skips calling\n>> +                        * folio_mark_uptodate() when read_bytes_pending is\n>> +                        * non-zero, ensuring only the read completion path\n>> +                        * sets it.\n>> +                        */\n>> +                       WARN_ON_ONCE(uptodate && folio_test_uptodate(folio));\n>>                         folio_end_read(folio, uptodate);\n>> +               }\n>>         } else if (!bytes_submitted) {\n>>                 /*\n>>                  * If there were no bytes submitted, this means we are\n>> diff --git a/include/linux/iomap.h b/include/linux/iomap.h\n>> index 520e967cb501..3c2ad88d16b6 100644\n>> --- a/include/linux/iomap.h\n>> +++ b/include/linux/iomap.h\n>> @@ -345,6 +345,8 @@ void iomap_read_folio(const struct iomap_ops *ops,\n>>  void iomap_readahead(const struct iomap_ops *ops,\n>>                 struct iomap_read_folio_ctx *ctx);\n>>  bool iomap_is_partially_uptodate(struct folio *, size_t from, size_t count);\n>> +void iomap_set_range_uptodate(struct folio *folio, size_t off, size_t len);\n>> +void iomap_clear_folio_uptodate(struct folio *folio);\n>>  struct folio *iomap_get_folio(struct iomap_iter *iter, loff_t pos, size_t len);\n>>  bool iomap_release_folio(struct folio *folio, gfp_t gfp_flags);\n>>  void iomap_invalidate_folio(struct folio *folio, size_t offset, size_t len);\n>> --\n>> 2.51.0\n>>\n\n-- \nThanks,\nSasha\n\n\n---\n\nOn Wed, Dec 24, 2025 at 02:10:19AM +0000, Matthew Wilcox wrote:\n>On Tue, Dec 23, 2025 at 05:12:09PM -0800, Joanne Koong wrote:\n>> On Tue, Dec 23, 2025 at 2:30\\u202fPM Sasha Levin <sashal@kernel.org> wrote:\n>> >\n>>\n>> Hi Sasha,\n>>\n>> Thanks for your patch and for the detailed writeup.\n>\n>The important line to note is:\n>\n>Assisted-by: claude-opus-4-5-20251101\n>\n>So Sasha has produced a very convincingly worded writeup that's\n>hallucinated.\n\nAnd spent a few hours trying to figure it out so I could unblock testing, but\nsure - thanks.\n\nHere's the full log:\nhttps://qa-reports.linaro.org/lkft/sashal-linus-next/build/v6.18-rc7-13806-gb927546677c8/testrun/30618654/suite/log-parser-test/test/exception-warning-fsiomapbuffered-io-at-ifs_free/log\n, happy to test any patches you might have.\n\n-- \nThanks,\nSasha\n\n\n---\n\nOn Wed, Dec 24, 2025 at 05:27:03PM +0000, Matthew Wilcox wrote:\n>On Wed, Dec 24, 2025 at 10:43:58AM -0500, Sasha Levin wrote:\n>> On Wed, Dec 24, 2025 at 02:10:19AM +0000, Matthew Wilcox wrote:\n>> > So Sasha has produced a very convincingly worded writeup that's\n>> > hallucinated.\n>>\n>> And spent a few hours trying to figure it out so I could unblock testing, but\n>> sure - thanks.\n>\n>When you produce a convincingly worded writeup that's utterly wrong,\n>and have a reputation for using AI, that's the kind of reaction you're\n>going to get.\n\nA rude and unprofessional one?\n\n>> Here's the full log:\n>> https://qa-reports.linaro.org/lkft/sashal-linus-next/build/v6.18-rc7-13806-gb927546677c8/testrun/30618654/suite/log-parser-test/test/exception-warning-fsiomapbuffered-io-at-ifs_free/log\n>> , happy to test any patches you might have.\n>\n>That's actually much more helpful because it removes your incorrect\n>assumptions about what's going on.\n>\n> WARNING: fs/iomap/buffered-io.c:254 at ifs_free+0x130/0x148, CPU#0: msync04/406\n>\n>That's this one:\n>\n>        WARN_ON_ONCE(ifs_is_fully_uptodate(folio, ifs) !=\n>                        folio_test_uptodate(folio));\n>\n>which would be fully explained by fuse calling folio_clear_uptodate()\n>in fuse_send_write_pages().  I have come to believe that allowing\n>filesystems to call folio_clear_uptodate() is just dangerous.  It\n>causes assertions to fire all over the place (eg if the page is mapped\n>into memory, the MM contains assertions that it must be uptodate).\n>\n>So I think the first step is simply to delete the folio_clear_uptodate()\n>calls in fuse:\n\n[snip]\n\nHere's the log of a run with the change you've provided applied: https://qa-reports.linaro.org/lkft/sashal-linus-next/build/v6.18-rc7-13807-g26a15474eb13/testrun/30620754/suite/log-parser-test/test/exception-warning-fsiomapbuffered-io-at-ifs_free/log\n\n-- \nThanks,\nSasha\n\n\n---\n\nOn Tue, Feb 10, 2026 at 02:18:06PM -0800, Joanne Koong wrote:\n>On Mon, Feb 9, 2026 at 4:40\\u202fPM Wei Gao <wegao@suse.com> wrote:\n>>\n>> On Mon, Feb 09, 2026 at 04:20:01PM -0800, Joanne Koong wrote:\n>> > On Mon, Feb 9, 2026 at 4:12\\u202fPM Wei Gao <wegao@suse.com> wrote:\n>> > >\n>> > > On Mon, Feb 09, 2026 at 11:08:50AM -0800, Joanne Koong wrote:\n>> > > > On Fri, Feb 6, 2026 at 11:16\\u202fPM Wei Gao <wegao@suse.com> wrote:\n>> > > > >\n>> > > > > On Tue, Dec 23, 2025 at 08:31:57PM -0500, Sasha Levin wrote:\n>> > > > > > On Tue, Dec 23, 2025 at 05:12:09PM -0800, Joanne Koong wrote:\n>> > > > > > > On Tue, Dec 23, 2025 at 2:30\\u202fPM Sasha Levin <sashal@kernel.org> wrote:\n>> > > > > > > >\n>> > > > > > >\n>> > > > > > > Hi Sasha,\n>> > > > > > >\n>> > > > > > > Thanks for your patch and for the detailed writeup.\n>> > > > > >\n>> > > > > > Thanks for looking into this!\n>> > > > > >\n>> > > > > > > > When iomap uses large folios, per-block uptodate tracking is managed via\n>> > > > > > > > iomap_folio_state (ifs). A race condition can cause the ifs uptodate bits\n>> > > > > > > > to become inconsistent with the folio's uptodate flag.\n>> > > > > > > >\n>> > > > > > > > The race occurs because folio_end_read() uses XOR semantics to atomically\n>> > > > > > > > set the uptodate bit and clear the locked bit:\n>> > > > > > > >\n>> > > > > > > >   Thread A (read completion):          Thread B (concurrent write):\n>> > > > > > > >   --------------------------------     --------------------------------\n>> > > > > > > >   iomap_finish_folio_read()\n>> > > > > > > >     spin_lock(state_lock)\n>> > > > > > > >     ifs_set_range_uptodate() -> true\n>> > > > > > > >     spin_unlock(state_lock)\n>> > > > > > > >                                        iomap_set_range_uptodate()\n>> > > > > > > >                                          spin_lock(state_lock)\n>> > > > > > > >                                          ifs_set_range_uptodate() -> true\n>> > > > > > > >                                          spin_unlock(state_lock)\n>> > > > > > > >                                          folio_mark_uptodate(folio)\n>> > > > > > > >     folio_end_read(folio, true)\n>> > > > > > > >       folio_xor_flags()  // XOR CLEARS uptodate!\n>> > > > > > >\n>> > > > > > > The part I'm confused about here is how this can happen between a\n>> > > > > > > concurrent read and write. My understanding is that the folio is\n>> > > > > > > locked when the read occurs and locked when the write occurs and both\n>> > > > > > > locks get dropped only when the read or write finishes. Looking at\n>> > > > > > > iomap code, I see iomap_set_range_uptodate() getting called in\n>> > > > > > > __iomap_write_begin() and __iomap_write_end() for the writes, but in\n>> > > > > > > both those places the folio lock is held while this is called. I'm not\n>> > > > > > > seeing how the read and write race in the diagram can happen, but\n>> > > > > > > maybe I'm missing something here?\n>> > > > > >\n>> > > > > > Hmm, you're right... The folio lock should prevent concurrent read/write\n>> > > > > > access. Looking at this again, I suspect that FUSE was calling\n>> > > > > > folio_clear_uptodate() and folio_mark_uptodate() directly without updating the\n>> > > > > > ifs bits. For example, in fuse_send_write_pages() on write error, it calls\n>> > > > > > folio_clear_uptodate(folio) which clears the folio flag but leaves ifs still\n>> > > > > > showing all blocks uptodate?\n>> > > > >\n>> > > > > Hi Sasha\n>> > > > > On PowerPC with 64KB page size, msync04 fails with SIGBUS on NTFS-FUSE. The issue stems from a state inconsistency between\n>> > > > > the iomap_folio_state (ifs) bitmap and the folio's Uptodate flag.\n>> > > > > tst_test.c:1985: TINFO: === Testing on ntfs ===\n>> > > > > tst_test.c:1290: TINFO: Formatting /dev/loop0 with ntfs opts='' extra opts=''\n>> > > > > Failed to set locale, using default 'C'.\n>> > > > > The partition start sector was not specified for /dev/loop0 and it could not be obtained automatically.  It has been set to 0.\n>> > > > > The number of sectors per track was not specified for /dev/loop0 and it could not be obtained automatically.  It has been set to 0.\n>> > > > > The number of heads was not specified for /dev/loop0 and it could not be obtained automatically.  It has been set to 0.\n>> > > > > To boot from a device, Windows needs the 'partition start sector', the 'sectors per track' and the 'number of heads' to be set.\n>> > > > > Windows will not be able to boot from this device.\n>> > > > > tst_test.c:1302: TINFO: Mounting /dev/loop0 to /tmp/LTP_msy3ljVxi/msync04 fstyp=ntfs flags=0\n>> > > > > tst_test.c:1302: TINFO: Trying FUSE...\n>> > > > > tst_test.c:1953: TBROK: Test killed by SIGBUS!\n>> > > > >\n>> > > > > Root Cause Analysis: When a page fault triggers fuse_read_folio, the iomap_read_folio_iter handles the request. For a 64KB page,\n>> > > > > after fetching 4KB via fuse_iomap_read_folio_range_async, the remaining 60KB (61440 bytes) is zero-filled via iomap_block_needs_zeroing,\n>> > > > > then iomap_set_range_uptodate marks the folio as Uptodate globally, after folio_xor_flags folio's uptodate become 0 again, finally trigger\n>> > > > > an SIGBUS issue in filemap_fault.\n>> > > >\n>> > > > Hi Wei,\n>> > > >\n>> > > > Thanks for your report. afaict, this scenario occurs only if the\n>> > > > server is a fuseblk server with a block size different from the memory\n>> > > > page size and if the file size is less than the size of the folio\n>> > > > being read in.\n>> > > Thanks for checking this and give quick feedback :)\n>> > > >\n>> > > > Could you verify that this snippet from Sasha's patch fixes the issue?:\n>> > > Yes, Sasha's patch can fixes the issue.\n>> >\n>> > I think just those lines I pasted from Sasha's patch is the relevant\n>> > fix. Could you verify that just those lines (without the changes\n>> > from the rest of his patch) fixes the issue?\n>> Yes, i just add two lines change in iomap_set_range_uptodate can fixes\n>> the issue.\n>\n>Great, thank you for confirming.\n>\n>Sasha, would you mind submitting this snippet of your patch as the fix\n>for the EOF zeroing issue? I think it could be restructured to\n>\n>diff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\n>index 1fe19b4ee2f4..412e661871f8 100644\n>--- a/fs/iomap/buffered-io.c\n>+++ b/fs/iomap/buffered-io.c\n>@@ -87,7 +87,16 @@ static void iomap_set_range_uptodate(struct folio\n>*folio, size_t off,\n>\n>        if (ifs) {\n>                spin_lock_irqsave(&ifs->state_lock, flags);\n>-               uptodate = ifs_set_range_uptodate(folio, ifs, off, len);\n>+               /*\n>+                * If a read is in progress, we must NOT call\n>folio_mark_uptodate.\n>+                * The read completion path (iomap_finish_folio_read or\n>+                * iomap_read_end) will call folio_end_read() which uses XOR\n>+                * semantics to set the uptodate bit. If we set it here, the XOR\n>+                * in folio_end_read() will clear it, leaving the folio not\n>+                * uptodate.\n>+                */\n>+               uptodate = ifs_set_range_uptodate(folio, ifs, off, len) &&\n>+                       !ifs->read_bytes_pending;\n>                spin_unlock_irqrestore(&ifs->state_lock, flags);\n>        }\n>\n>to be a bit more concise.\n>\n>If you're busy and don't have the bandwidth, I'm happy to forward the\n>patch on your behalf with your Signed-off-by / authorship.\n\nThanks for the offer Joanna!\n\nSince you've done all the triaging work here, please go ahead and submit it -\nsomething like a Suggested-by would be more than enought for me :)\n\n-- \nThanks,\nSasha\n",
          "reply_to": ""
        },
        {
          "author": "Wei Gao",
          "summary": "Wei Gao reported a race condition between iomap_set_range_uptodate and folio_end_read on PowerPC with 64KB page size, causing an SIGBUS issue in filemap_fault. He found that Sasha's patch can fix the issue by blocking mark folio's uptodate to 1.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "race condition",
            "SIGBUS issue"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Tue, Dec 23, 2025 at 08:31:57PM -0500, Sasha Levin wrote:\n> On Tue, Dec 23, 2025 at 05:12:09PM -0800, Joanne Koong wrote:\n> > On Tue, Dec 23, 2025 at 2:30\\u202fPM Sasha Levin <sashal@kernel.org> wrote:\n> > > \n> > \n> > Hi Sasha,\n> > \n> > Thanks for your patch and for the detailed writeup.\n> \n> Thanks for looking into this!\n> \n> > > When iomap uses large folios, per-block uptodate tracking is managed via\n> > > iomap_folio_state (ifs). A race condition can cause the ifs uptodate bits\n> > > to become inconsistent with the folio's uptodate flag.\n> > > \n> > > The race occurs because folio_end_read() uses XOR semantics to atomically\n> > > set the uptodate bit and clear the locked bit:\n> > > \n> > >   Thread A (read completion):          Thread B (concurrent write):\n> > >   --------------------------------     --------------------------------\n> > >   iomap_finish_folio_read()\n> > >     spin_lock(state_lock)\n> > >     ifs_set_range_uptodate() -> true\n> > >     spin_unlock(state_lock)\n> > >                                        iomap_set_range_uptodate()\n> > >                                          spin_lock(state_lock)\n> > >                                          ifs_set_range_uptodate() -> true\n> > >                                          spin_unlock(state_lock)\n> > >                                          folio_mark_uptodate(folio)\n> > >     folio_end_read(folio, true)\n> > >       folio_xor_flags()  // XOR CLEARS uptodate!\n> > \n> > The part I'm confused about here is how this can happen between a\n> > concurrent read and write. My understanding is that the folio is\n> > locked when the read occurs and locked when the write occurs and both\n> > locks get dropped only when the read or write finishes. Looking at\n> > iomap code, I see iomap_set_range_uptodate() getting called in\n> > __iomap_write_begin() and __iomap_write_end() for the writes, but in\n> > both those places the folio lock is held while this is called. I'm not\n> > seeing how the read and write race in the diagram can happen, but\n> > maybe I'm missing something here?\n> \n> Hmm, you're right... The folio lock should prevent concurrent read/write\n> access. Looking at this again, I suspect that FUSE was calling\n> folio_clear_uptodate() and folio_mark_uptodate() directly without updating the\n> ifs bits. For example, in fuse_send_write_pages() on write error, it calls\n> folio_clear_uptodate(folio) which clears the folio flag but leaves ifs still\n> showing all blocks uptodate?\n\nHi Sasha\nOn PowerPC with 64KB page size, msync04 fails with SIGBUS on NTFS-FUSE. The issue stems from a state inconsistency between\nthe iomap_folio_state (ifs) bitmap and the folio's Uptodate flag.\ntst_test.c:1985: TINFO: === Testing on ntfs ===\ntst_test.c:1290: TINFO: Formatting /dev/loop0 with ntfs opts='' extra opts=''\nFailed to set locale, using default 'C'.\nThe partition start sector was not specified for /dev/loop0 and it could not be obtained automatically.  It has been set to 0.\nThe number of sectors per track was not specified for /dev/loop0 and it could not be obtained automatically.  It has been set to 0.\nThe number of heads was not specified for /dev/loop0 and it could not be obtained automatically.  It has been set to 0.\nTo boot from a device, Windows needs the 'partition start sector', the 'sectors per track' and the 'number of heads' to be set.\nWindows will not be able to boot from this device.\ntst_test.c:1302: TINFO: Mounting /dev/loop0 to /tmp/LTP_msy3ljVxi/msync04 fstyp=ntfs flags=0\ntst_test.c:1302: TINFO: Trying FUSE...\ntst_test.c:1953: TBROK: Test killed by SIGBUS!\n\nRoot Cause Analysis: When a page fault triggers fuse_read_folio, the iomap_read_folio_iter handles the request. For a 64KB page, \nafter fetching 4KB via fuse_iomap_read_folio_range_async, the remaining 60KB (61440 bytes) is zero-filled via iomap_block_needs_zeroing, \nthen iomap_set_range_uptodate marks the folio as Uptodate globally, after folio_xor_flags folio's uptodate become 0 again, finally trigger \nan SIGBUS issue in filemap_fault.\n\nSo your iomap_set_range_uptodate patch can fix above failed case since it block mark folio's uptodate to 1.\nHope my findings are helpful.\n\n> \n> -- \n> Thanks,\n> Sasha\n> \n\n\n---\n\nOn Mon, Feb 09, 2026 at 11:08:50AM -0800, Joanne Koong wrote:\n> On Fri, Feb 6, 2026 at 11:16\\u202fPM Wei Gao <wegao@suse.com> wrote:\n> >\n> > On Tue, Dec 23, 2025 at 08:31:57PM -0500, Sasha Levin wrote:\n> > > On Tue, Dec 23, 2025 at 05:12:09PM -0800, Joanne Koong wrote:\n> > > > On Tue, Dec 23, 2025 at 2:30\\u202fPM Sasha Levin <sashal@kernel.org> wrote:\n> > > > >\n> > > >\n> > > > Hi Sasha,\n> > > >\n> > > > Thanks for your patch and for the detailed writeup.\n> > >\n> > > Thanks for looking into this!\n> > >\n> > > > > When iomap uses large folios, per-block uptodate tracking is managed via\n> > > > > iomap_folio_state (ifs). A race condition can cause the ifs uptodate bits\n> > > > > to become inconsistent with the folio's uptodate flag.\n> > > > >\n> > > > > The race occurs because folio_end_read() uses XOR semantics to atomically\n> > > > > set the uptodate bit and clear the locked bit:\n> > > > >\n> > > > >   Thread A (read completion):          Thread B (concurrent write):\n> > > > >   --------------------------------     --------------------------------\n> > > > >   iomap_finish_folio_read()\n> > > > >     spin_lock(state_lock)\n> > > > >     ifs_set_range_uptodate() -> true\n> > > > >     spin_unlock(state_lock)\n> > > > >                                        iomap_set_range_uptodate()\n> > > > >                                          spin_lock(state_lock)\n> > > > >                                          ifs_set_range_uptodate() -> true\n> > > > >                                          spin_unlock(state_lock)\n> > > > >                                          folio_mark_uptodate(folio)\n> > > > >     folio_end_read(folio, true)\n> > > > >       folio_xor_flags()  // XOR CLEARS uptodate!\n> > > >\n> > > > The part I'm confused about here is how this can happen between a\n> > > > concurrent read and write. My understanding is that the folio is\n> > > > locked when the read occurs and locked when the write occurs and both\n> > > > locks get dropped only when the read or write finishes. Looking at\n> > > > iomap code, I see iomap_set_range_uptodate() getting called in\n> > > > __iomap_write_begin() and __iomap_write_end() for the writes, but in\n> > > > both those places the folio lock is held while this is called. I'm not\n> > > > seeing how the read and write race in the diagram can happen, but\n> > > > maybe I'm missing something here?\n> > >\n> > > Hmm, you're right... The folio lock should prevent concurrent read/write\n> > > access. Looking at this again, I suspect that FUSE was calling\n> > > folio_clear_uptodate() and folio_mark_uptodate() directly without updating the\n> > > ifs bits. For example, in fuse_send_write_pages() on write error, it calls\n> > > folio_clear_uptodate(folio) which clears the folio flag but leaves ifs still\n> > > showing all blocks uptodate?\n> >\n> > Hi Sasha\n> > On PowerPC with 64KB page size, msync04 fails with SIGBUS on NTFS-FUSE. The issue stems from a state inconsistency between\n> > the iomap_folio_state (ifs) bitmap and the folio's Uptodate flag.\n> > tst_test.c:1985: TINFO: === Testing on ntfs ===\n> > tst_test.c:1290: TINFO: Formatting /dev/loop0 with ntfs opts='' extra opts=''\n> > Failed to set locale, using default 'C'.\n> > The partition start sector was not specified for /dev/loop0 and it could not be obtained automatically.  It has been set to 0.\n> > The number of sectors per track was not specified for /dev/loop0 and it could not be obtained automatically.  It has been set to 0.\n> > The number of heads was not specified for /dev/loop0 and it could not be obtained automatically.  It has been set to 0.\n> > To boot from a device, Windows needs the 'partition start sector', the 'sectors per track' and the 'number of heads' to be set.\n> > Windows will not be able to boot from this device.\n> > tst_test.c:1302: TINFO: Mounting /dev/loop0 to /tmp/LTP_msy3ljVxi/msync04 fstyp=ntfs flags=0\n> > tst_test.c:1302: TINFO: Trying FUSE...\n> > tst_test.c:1953: TBROK: Test killed by SIGBUS!\n> >\n> > Root Cause Analysis: When a page fault triggers fuse_read_folio, the iomap_read_folio_iter handles the request. For a 64KB page,\n> > after fetching 4KB via fuse_iomap_read_folio_range_async, the remaining 60KB (61440 bytes) is zero-filled via iomap_block_needs_zeroing,\n> > then iomap_set_range_uptodate marks the folio as Uptodate globally, after folio_xor_flags folio's uptodate become 0 again, finally trigger\n> > an SIGBUS issue in filemap_fault.\n> \n> Hi Wei,\n> \n> Thanks for your report. afaict, this scenario occurs only if the\n> server is a fuseblk server with a block size different from the memory\n> page size and if the file size is less than the size of the folio\n> being read in.\nThanks for checking this and give quick feedback :)\n> \n> Could you verify that this snippet from Sasha's patch fixes the issue?:\nYes, Sasha's patch can fixes the issue.\n> \n> diff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\n> index e5c1ca440d93..7ceda24cf6a7 100644\n> --- a/fs/iomap/buffered-io.c\n> +++ b/fs/iomap/buffered-io.c\n> @@ -87,12 +86,50 @@ static void iomap_set_range_uptodate(struct folio\n> *folio, size_t off,\n>   if (ifs) {\n>           spin_lock_irqsave(&ifs->state_lock, flags);\n>           uptodate = ifs_set_range_uptodate(folio, ifs, off, len);\n>           + /*\n>           + * If a read is in progress, we must NOT call folio_mark_uptodate\n>           + * here. The read completion path (iomap_finish_folio_read or\n>           + * iomap_read_end) will call folio_end_read() which uses XOR\n>           + * semantics to set the uptodate bit. If we set it here, the XOR\n>           + * in folio_end_read() will clear it, leaving the folio not\n>           + * uptodate while the ifs says all blocks are uptodate.\n>           + */\n>          + if (uptodate && ifs->read_bytes_pending)\n>                    + uptodate = false;\n>         spin_unlock_irqrestore(&ifs->state_lock, flags);\n>   }\n> \n> Thanks,\n> Joanne\n> \n> >\n> > So your iomap_set_range_uptodate patch can fix above failed case since it block mark folio's uptodate to 1.\n> > Hope my findings are helpful.\n> >\n> > >\n> > > --\n> > > Thanks,\n> > > Sasha\n> > >\n\n\n---\n\nOn Mon, Feb 09, 2026 at 04:20:01PM -0800, Joanne Koong wrote:\n> On Mon, Feb 9, 2026 at 4:12\\u202fPM Wei Gao <wegao@suse.com> wrote:\n> >\n> > On Mon, Feb 09, 2026 at 11:08:50AM -0800, Joanne Koong wrote:\n> > > On Fri, Feb 6, 2026 at 11:16\\u202fPM Wei Gao <wegao@suse.com> wrote:\n> > > >\n> > > > On Tue, Dec 23, 2025 at 08:31:57PM -0500, Sasha Levin wrote:\n> > > > > On Tue, Dec 23, 2025 at 05:12:09PM -0800, Joanne Koong wrote:\n> > > > > > On Tue, Dec 23, 2025 at 2:30\\u202fPM Sasha Levin <sashal@kernel.org> wrote:\n> > > > > > >\n> > > > > >\n> > > > > > Hi Sasha,\n> > > > > >\n> > > > > > Thanks for your patch and for the detailed writeup.\n> > > > >\n> > > > > Thanks for looking into this!\n> > > > >\n> > > > > > > When iomap uses large folios, per-block uptodate tracking is managed via\n> > > > > > > iomap_folio_state (ifs). A race condition can cause the ifs uptodate bits\n> > > > > > > to become inconsistent with the folio's uptodate flag.\n> > > > > > >\n> > > > > > > The race occurs because folio_end_read() uses XOR semantics to atomically\n> > > > > > > set the uptodate bit and clear the locked bit:\n> > > > > > >\n> > > > > > >   Thread A (read completion):          Thread B (concurrent write):\n> > > > > > >   --------------------------------     --------------------------------\n> > > > > > >   iomap_finish_folio_read()\n> > > > > > >     spin_lock(state_lock)\n> > > > > > >     ifs_set_range_uptodate() -> true\n> > > > > > >     spin_unlock(state_lock)\n> > > > > > >                                        iomap_set_range_uptodate()\n> > > > > > >                                          spin_lock(state_lock)\n> > > > > > >                                          ifs_set_range_uptodate() -> true\n> > > > > > >                                          spin_unlock(state_lock)\n> > > > > > >                                          folio_mark_uptodate(folio)\n> > > > > > >     folio_end_read(folio, true)\n> > > > > > >       folio_xor_flags()  // XOR CLEARS uptodate!\n> > > > > >\n> > > > > > The part I'm confused about here is how this can happen between a\n> > > > > > concurrent read and write. My understanding is that the folio is\n> > > > > > locked when the read occurs and locked when the write occurs and both\n> > > > > > locks get dropped only when the read or write finishes. Looking at\n> > > > > > iomap code, I see iomap_set_range_uptodate() getting called in\n> > > > > > __iomap_write_begin() and __iomap_write_end() for the writes, but in\n> > > > > > both those places the folio lock is held while this is called. I'm not\n> > > > > > seeing how the read and write race in the diagram can happen, but\n> > > > > > maybe I'm missing something here?\n> > > > >\n> > > > > Hmm, you're right... The folio lock should prevent concurrent read/write\n> > > > > access. Looking at this again, I suspect that FUSE was calling\n> > > > > folio_clear_uptodate() and folio_mark_uptodate() directly without updating the\n> > > > > ifs bits. For example, in fuse_send_write_pages() on write error, it calls\n> > > > > folio_clear_uptodate(folio) which clears the folio flag but leaves ifs still\n> > > > > showing all blocks uptodate?\n> > > >\n> > > > Hi Sasha\n> > > > On PowerPC with 64KB page size, msync04 fails with SIGBUS on NTFS-FUSE. The issue stems from a state inconsistency between\n> > > > the iomap_folio_state (ifs) bitmap and the folio's Uptodate flag.\n> > > > tst_test.c:1985: TINFO: === Testing on ntfs ===\n> > > > tst_test.c:1290: TINFO: Formatting /dev/loop0 with ntfs opts='' extra opts=''\n> > > > Failed to set locale, using default 'C'.\n> > > > The partition start sector was not specified for /dev/loop0 and it could not be obtained automatically.  It has been set to 0.\n> > > > The number of sectors per track was not specified for /dev/loop0 and it could not be obtained automatically.  It has been set to 0.\n> > > > The number of heads was not specified for /dev/loop0 and it could not be obtained automatically.  It has been set to 0.\n> > > > To boot from a device, Windows needs the 'partition start sector', the 'sectors per track' and the 'number of heads' to be set.\n> > > > Windows will not be able to boot from this device.\n> > > > tst_test.c:1302: TINFO: Mounting /dev/loop0 to /tmp/LTP_msy3ljVxi/msync04 fstyp=ntfs flags=0\n> > > > tst_test.c:1302: TINFO: Trying FUSE...\n> > > > tst_test.c:1953: TBROK: Test killed by SIGBUS!\n> > > >\n> > > > Root Cause Analysis: When a page fault triggers fuse_read_folio, the iomap_read_folio_iter handles the request. For a 64KB page,\n> > > > after fetching 4KB via fuse_iomap_read_folio_range_async, the remaining 60KB (61440 bytes) is zero-filled via iomap_block_needs_zeroing,\n> > > > then iomap_set_range_uptodate marks the folio as Uptodate globally, after folio_xor_flags folio's uptodate become 0 again, finally trigger\n> > > > an SIGBUS issue in filemap_fault.\n> > >\n> > > Hi Wei,\n> > >\n> > > Thanks for your report. afaict, this scenario occurs only if the\n> > > server is a fuseblk server with a block size different from the memory\n> > > page size and if the file size is less than the size of the folio\n> > > being read in.\n> > Thanks for checking this and give quick feedback :)\n> > >\n> > > Could you verify that this snippet from Sasha's patch fixes the issue?:\n> > Yes, Sasha's patch can fixes the issue.\n> \n> I think just those lines I pasted from Sasha's patch is the relevant\n> fix. Could you verify that just those lines (without the changes\n> from the rest of his patch) fixes the issue?\nYes, i just add two lines change in iomap_set_range_uptodate can fixes\nthe issue.\n+\t\tif (uptodate && ifs->read_bytes_pending)\n+\t\t\tuptodate = false;\n> \n> Thanks,\n> Joanne\n> \n> \n> > >\n> > > diff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\n> > > index e5c1ca440d93..7ceda24cf6a7 100644\n> > > --- a/fs/iomap/buffered-io.c\n> > > +++ b/fs/iomap/buffered-io.c\n> > > @@ -87,12 +86,50 @@ static void iomap_set_range_uptodate(struct folio\n> > > *folio, size_t off,\n> > >   if (ifs) {\n> > >           spin_lock_irqsave(&ifs->state_lock, flags);\n> > >           uptodate = ifs_set_range_uptodate(folio, ifs, off, len);\n> > >           + /*\n> > >           + * If a read is in progress, we must NOT call folio_mark_uptodate\n> > >           + * here. The read completion path (iomap_finish_folio_read or\n> > >           + * iomap_read_end) will call folio_end_read() which uses XOR\n> > >           + * semantics to set the uptodate bit. If we set it here, the XOR\n> > >           + * in folio_end_read() will clear it, leaving the folio not\n> > >           + * uptodate while the ifs says all blocks are uptodate.\n> > >           + */\n> > >          + if (uptodate && ifs->read_bytes_pending)\n> > >                    + uptodate = false;\n> > >         spin_unlock_irqrestore(&ifs->state_lock, flags);\n> > >   }\n> > >\n> > > Thanks,\n> > > Joanne\n> > >\n> > > >\n> > > > So your iomap_set_range_uptodate patch can fix above failed case since it block mark folio's uptodate to 1.\n> > > > Hope my findings are helpful.\n> > > >\n> > > > >\n> > > > > --\n> > > > > Thanks,\n> > > > > Sasha\n> > > > >\n",
          "reply_to": ""
        }
      ],
      "analysis_source": "llm"
    }
  }
}