{
  "thread_id": "20260217214815.658944-1-loemra.dev@gmail.com",
  "subject": "Re: [PATCH v2 1/3] btrfs: skip COW for written extent buffers allocated in current transaction",
  "url": "https://lore.kernel.org/all/20260217214815.658944-1-loemra.dev@gmail.com/",
  "dates": {
    "2026-02-17": {
      "report_file": "2026-02-17_ollama_llama3.1-8b.html",
      "developer": "Leo Martins",
      "reviews": [
        {
          "author": "Leo Martins (author)",
          "summary": "Reviewer Leo Martins pointed out the need for caution when overwriting in place, as it could lead to corruption of committed log blocks. He also suggested adding a comment to clarify that should_cow_block() may be called from btrfs_search_slot() with only a read lock on the buffer.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "clarification needed"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "When memory pressure causes writeback of a recently COW'd buffer,\nbtrfs sets BTRFS_HEADER_FLAG_WRITTEN on it. Subsequent\nbtrfs_search_slot() restarts then see the WRITTEN flag and re-COW\nthe buffer unnecessarily, causing COW amplification that can exhaust\nblock reservations and degrade throughput.\n\nOverwriting in place is crash-safe because the committed superblock\ndoes not reference buffers allocated in the current (uncommitted)\ntransaction, so no on-disk tree points to this block yet.\n\nWhen should_cow_block() encounters a WRITTEN buffer whose generation\nmatches the current transaction, instead of requesting a COW, re-dirty\nthe buffer and re-register its range in the transaction's dirty_pages.\n\nBoth are necessary because btrfs tracks dirty metadata through two\nindependent mechanisms. set_extent_buffer_dirty() sets the\nEXTENT_BUFFER_DIRTY flag and the buffer_tree xarray PAGECACHE_TAG_DIRTY\nmark, which is what background writeback (btree_write_cache_pages) uses\nto find and write dirty buffers. The transaction's dirty_pages io tree\nis a separate structure used by btrfs_write_and_wait_transaction() at\ncommit time to ensure all buffers allocated during the transaction are\npersisted. The dirty_pages range was originally registered in\nbtrfs_init_new_buffer() when the block was first allocated, but\nbackground writeback may have already written and cleared it.\n\nKeep BTRFS_HEADER_FLAG_WRITTEN set so that btrfs_free_tree_block()\ncorrectly pins the block if it is freed later.\n\nExclude cases where in-place overwrite is not safe:\n - EXTENT_BUFFER_WRITEBACK: buffer is mid-I/O\n - Zoned devices: require sequential writes\n - Log trees: log blocks are immediately referenced by a committed\n   superblock via btrfs_sync_log(), so overwriting could corrupt the\n   committed log\n - BTRFS_ROOT_FORCE_COW: snapshot in progress\n - BTRFS_HEADER_FLAG_RELOC: block being relocated\n\nSigned-off-by: Leo Martins <loemra.dev@gmail.com>\n---\n fs/btrfs/ctree.c | 53 +++++++++++++++++++++++++++++++++++++++++++++---\n 1 file changed, 50 insertions(+), 3 deletions(-)\n\ndiff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c\nindex 7267b2502665..a345e1be24d8 100644\n--- a/fs/btrfs/ctree.c\n+++ b/fs/btrfs/ctree.c\n@@ -599,9 +599,9 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,\n \treturn ret;\n }\n \n-static inline bool should_cow_block(const struct btrfs_trans_handle *trans,\n+static inline bool should_cow_block(struct btrfs_trans_handle *trans,\n \t\t\t\t    const struct btrfs_root *root,\n-\t\t\t\t    const struct extent_buffer *buf)\n+\t\t\t\t    struct extent_buffer *buf)\n {\n \tif (btrfs_is_testing(root->fs_info))\n \t\treturn false;\n@@ -621,8 +621,55 @@ static inline bool should_cow_block(const struct btrfs_trans_handle *trans,\n \tif (btrfs_header_generation(buf) != trans->transid)\n \t\treturn true;\n \n-\tif (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN))\n+\tif (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {\n+\t\t/*\n+\t\t * The buffer was allocated in this transaction and has been\n+\t\t * written back to disk (WRITTEN is set). Normally we'd COW\n+\t\t * it again, but since the committed superblock doesn't\n+\t\t * reference this buffer (it was allocated this transaction),\n+\t\t * we can safely overwrite it in place.\n+\t\t *\n+\t\t * We keep BTRFS_HEADER_FLAG_WRITTEN set. The block has been\n+\t\t * persisted at this bytenr and will be again after the\n+\t\t * in-place update. This is important so that\n+\t\t * btrfs_free_tree_block() correctly pins the block if it is\n+\t\t * freed later (e.g., during tree rebalancing or FORCE_COW).\n+\t\t *\n+\t\t * We re-dirty the buffer to ensure the in-place modifications\n+\t\t * will be written back to disk.\n+\t\t *\n+\t\t * Exclusions:\n+\t\t * - Log trees: log blocks are written and immediately\n+\t\t *   referenced by a committed superblock via\n+\t\t *   btrfs_sync_log(), bypassing the normal transaction\n+\t\t *   commit. Overwriting in place could corrupt the\n+\t\t *   committed log.\n+\t\t * - Zoned devices: require sequential writes\n+\t\t * - FORCE_COW: snapshot in progress\n+\t\t * - RELOC flag: block being relocated\n+\t\t */\n+\t\tif (!test_bit(EXTENT_BUFFER_WRITEBACK, &buf->bflags) &&\n+\t\t    !btrfs_is_zoned(root->fs_info) &&\n+\t\t    btrfs_root_id(root) != BTRFS_TREE_LOG_OBJECTID &&\n+\t\t    !test_bit(BTRFS_ROOT_FORCE_COW, &root->state) &&\n+\t\t    !btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC)) {\n+\t\t\t/*\n+\t\t\t * Re-register this block's range in the current\n+\t\t\t * transaction's dirty_pages so that\n+\t\t\t * btrfs_write_and_wait_transaction() writes it.\n+\t\t\t * The range was originally registered when the block\n+\t\t\t * was allocated, but that transaction's dirty_pages\n+\t\t\t * may have already been released.\n+\t\t\t */\n+\t\t\tbtrfs_set_extent_bit(&trans->transaction->dirty_pages,\n+\t\t\t\t\t     buf->start,\n+\t\t\t\t\t     buf->start + buf->len - 1,\n+\t\t\t\t\t     EXTENT_DIRTY, NULL);\n+\t\t\tset_extent_buffer_dirty(buf);\n+\t\t\treturn false;\n+\t\t}\n \t\treturn true;\n+\t}\n \n \t/* Ensure we can see the FORCE_COW bit. */\n \tsmp_mb__before_atomic();\n-- \n2.47.3\n\n\n\n---\n\nInhibit writeback on COW'd extent buffers for the lifetime of the\ntransaction handle, preventing background writeback from setting\nBTRFS_HEADER_FLAG_WRITTEN and causing unnecessary re-COW.\n\nCOW amplification occurs when background writeback flushes an extent\nbuffer that a transaction handle is still actively modifying. When\nlock_extent_buffer_for_io() transitions a buffer from dirty to\nwriteback, it sets BTRFS_HEADER_FLAG_WRITTEN, marking the block as\nhaving been persisted to disk at its current bytenr. Once WRITTEN is\nset, should_cow_block() must either COW the block again or overwrite\nit in place, both of which are unnecessary overhead when the buffer\nis still being modified by the same handle that allocated it. By\ninhibiting background writeback on actively-used buffers, WRITTEN is\nnever set while a transaction handle holds a reference to the buffer,\navoiding this overhead entirely.\n\nAdd an atomic_t writeback_inhibitors counter to struct extent_buffer,\nwhich fits in an existing 6-byte hole without increasing struct size.\nWhen a buffer is COW'd in btrfs_force_cow_block(), call\nbtrfs_inhibit_eb_writeback() to store the eb in the transaction\nhandle's writeback_inhibited_ebs xarray (keyed by eb->start), take a\nreference, and increment writeback_inhibitors. The function handles\ndedup (same eb inhibited twice by the same handle) and replacement\n(different eb at the same logical address). Allocation failure is\ngraceful: the buffer simply falls back to the pre-existing behavior\nwhere it may be written back and re-COW'd.\n\nIn lock_extent_buffer_for_io(), when writeback_inhibitors is non-zero\nand the writeback mode is WB_SYNC_NONE, skip the buffer. WB_SYNC_NONE\nis used by the VM flusher threads for background and periodic\nwriteback, which are the only paths that cause COW amplification by\nopportunistically writing out dirty extent buffers mid-transaction.\nSkipping these is safe because the buffers remain dirty in the page\ncache and will be written out at transaction commit time.\n\nWB_SYNC_ALL must always proceed regardless of writeback_inhibitors.\nThis is required for correctness in the fsync path: btrfs_sync_log()\nwrites log tree blocks via filemap_fdatawrite_range() (WB_SYNC_ALL)\nwhile the transaction handle that inhibited those same blocks is still\nactive. Without the WB_SYNC_ALL bypass, those inhibited log tree\nblocks would be silently skipped, resulting in an incomplete log on\ndisk and corruption on replay. btrfs_write_and_wait_transaction()\nalso uses WB_SYNC_ALL via filemap_fdatawrite_range(); for that path,\ninhibitors are already cleared beforehand, but the bypass ensures\ncorrectness regardless.\n\nUninhibit in __btrfs_end_transaction() before atomic_dec(num_writers)\nto prevent a race where the committer proceeds while buffers are still\ninhibited. Also uninhibit in btrfs_commit_transaction() before writing\nand in cleanup_transaction() for the error path.\n\nSigned-off-by: Leo Martins <loemra.dev@gmail.com>\n---\n fs/btrfs/ctree.c       |  4 +++\n fs/btrfs/extent_io.c   | 62 +++++++++++++++++++++++++++++++++++++++++-\n fs/btrfs/extent_io.h   |  5 ++++\n fs/btrfs/transaction.c | 19 +++++++++++++\n fs/btrfs/transaction.h |  2 ++\n 5 files changed, 91 insertions(+), 1 deletion(-)\n\ndiff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c\nindex a345e1be24d8..55187ba59cc0 100644\n--- a/fs/btrfs/ctree.c\n+++ b/fs/btrfs/ctree.c\n@@ -590,6 +590,10 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,\n \t\tbtrfs_tree_unlock(buf);\n \tfree_extent_buffer_stale(buf);\n \tbtrfs_mark_buffer_dirty(trans, cow);\n+\n+\t/* Inhibit writeback on the COW'd buffer for this transaction handle */\n+\tbtrfs_inhibit_eb_writeback(trans, cow);\n+\n \t*cow_ret = cow;\n \treturn 0;\n \ndiff --git a/fs/btrfs/extent_io.c b/fs/btrfs/extent_io.c\nindex dfc17c292217..0c9276cff299 100644\n--- a/fs/btrfs/extent_io.c\n+++ b/fs/btrfs/extent_io.c\n@@ -1940,7 +1940,9 @@ static noinline_for_stack bool lock_extent_buffer_for_io(struct extent_buffer *e\n \t * of time.\n \t */\n \tspin_lock(&eb->refs_lock);\n-\tif (test_and_clear_bit(EXTENT_BUFFER_DIRTY, &eb->bflags)) {\n+\tif ((wbc->sync_mode == WB_SYNC_ALL ||\n+\t     atomic_read(&eb->writeback_inhibitors) == 0) &&\n+\t    test_and_clear_bit(EXTENT_BUFFER_DIRTY, &eb->bflags)) {\n \t\tXA_STATE(xas, &fs_info->buffer_tree, eb->start >> fs_info->nodesize_bits);\n \t\tunsigned long flags;\n \n@@ -2999,6 +3001,63 @@ static inline void btrfs_release_extent_buffer(struct extent_buffer *eb)\n \tkmem_cache_free(extent_buffer_cache, eb);\n }\n \n+/*\n+ * btrfs_inhibit_eb_writeback - Inhibit writeback on buffer during transaction\n+ * @trans: transaction handle that will own the inhibitor\n+ * @eb: extent buffer to inhibit writeback on\n+ *\n+ * Attempts to track this extent buffer in the transaction's inhibited set.\n+ * If memory allocation fails, the buffer is simply not tracked. It may\n+ * be written back and need re-COW, which is the original behavior.\n+ * This is acceptable since inhibiting writeback is an optimization.\n+ */\n+void btrfs_inhibit_eb_writeback(struct btrfs_trans_handle *trans,\n+\t\t\t\tstruct extent_buffer *eb)\n+{\n+\tunsigned long index = eb->start >> trans->fs_info->nodesize_bits;\n+\tvoid *old;\n+\n+\t/* Check if already inhibited by this handle */\n+\told = xa_load(&trans->writeback_inhibited_ebs, index);\n+\tif (old == eb)\n+\t\treturn;\n+\n+\trefcount_inc(&eb->refs);\t/* Take reference */\n+\n+\told = xa_store(&trans->writeback_inhibited_ebs, index, eb, GFP_NOFS);\n+\tif (xa_is_err(old)) {\n+\t\t/* Allocation failed, just skip inhibiting this buffer */\n+\t\tfree_extent_buffer(eb);\n+\t\treturn;\n+\t}\n+\n+\t/* Handle replacement of different eb at same index */\n+\tif (old && old != eb) {\n+\t\tstruct extent_buffer *old_eb = old;\n+\n+\t\tatomic_dec(&old_eb->writeback_inhibitors);\n+\t\tfree_extent_buffer(old_eb);\n+\t}\n+\n+\tatomic_inc(&eb->writeback_inhibitors);\n+}\n+\n+/*\n+ * btrfs_uninhibit_all_eb_writeback - Uninhibit writeback on all buffers\n+ * @trans: transaction handle to clean up\n+ */\n+void btrfs_uninhibit_all_eb_writeback(struct btrfs_trans_handle *trans)\n+{\n+\tstruct extent_buffer *eb;\n+\tunsigned long index;\n+\n+\txa_for_each(&trans->writeback_inhibited_ebs, index, eb) {\n+\t\tatomic_dec(&eb->writeback_inhibitors);\n+\t\tfree_extent_buffer(eb);\n+\t}\n+\txa_destroy(&trans->writeback_inhibited_ebs);\n+}\n+\n static struct extent_buffer *__alloc_extent_buffer(struct btrfs_fs_info *fs_info,\n \t\t\t\t\t\t   u64 start)\n {\n@@ -3009,6 +3068,7 @@ static struct extent_buffer *__alloc_extent_buffer(struct btrfs_fs_info *fs_info\n \teb->len = fs_info->nodesize;\n \teb->fs_info = fs_info;\n \tinit_rwsem(&eb->lock);\n+\tatomic_set(&eb->writeback_inhibitors, 0);\n \n \tbtrfs_leak_debug_add_eb(eb);\n \ndiff --git a/fs/btrfs/extent_io.h b/fs/btrfs/extent_io.h\nindex 73571d5d3d5a..4b15a5d8bc0f 100644\n--- a/fs/btrfs/extent_io.h\n+++ b/fs/btrfs/extent_io.h\n@@ -102,6 +102,7 @@ struct extent_buffer {\n \t/* >= 0 if eb belongs to a log tree, -1 otherwise */\n \ts8 log_index;\n \tu8 folio_shift;\n+\tatomic_t writeback_inhibitors;\t/* inhibits writeback when > 0 */\n \tstruct rcu_head rcu_head;\n \n \tstruct rw_semaphore lock;\n@@ -381,4 +382,8 @@ void btrfs_extent_buffer_leak_debug_check(struct btrfs_fs_info *fs_info);\n #define btrfs_extent_buffer_leak_debug_check(fs_info)\tdo {} while (0)\n #endif\n \n+void btrfs_inhibit_eb_writeback(struct btrfs_trans_handle *trans,\n+\t\t\t       struct extent_buffer *eb);\n+void btrfs_uninhibit_all_eb_writeback(struct btrfs_trans_handle *trans);\n+\n #endif\ndiff --git a/fs/btrfs/transaction.c b/fs/btrfs/transaction.c\nindex f4cc9e1a1b93..a9a22629b49d 100644\n--- a/fs/btrfs/transaction.c\n+++ b/fs/btrfs/transaction.c\n@@ -15,6 +15,7 @@\n #include \"misc.h\"\n #include \"ctree.h\"\n #include \"disk-io.h\"\n+#include \"extent_io.h\"\n #include \"transaction.h\"\n #include \"locking.h\"\n #include \"tree-log.h\"\n@@ -688,6 +689,8 @@ start_transaction(struct btrfs_root *root, unsigned int num_items,\n \t\tgoto alloc_fail;\n \t}\n \n+\txa_init(&h->writeback_inhibited_ebs);\n+\n \t/*\n \t * If we are JOIN_NOLOCK we're already committing a transaction and\n \t * waiting on this guy, so we don't need to do the sb_start_intwrite\n@@ -1083,6 +1086,13 @@ static int __btrfs_end_transaction(struct btrfs_trans_handle *trans,\n \tif (trans->type & __TRANS_FREEZABLE)\n \t\tsb_end_intwrite(info->sb);\n \n+\t/*\n+\t * Uninhibit extent buffer writeback before decrementing num_writers,\n+\t * since the decrement wakes the committing thread which needs all\n+\t * buffers uninhibited to write them to disk.\n+\t */\n+\tbtrfs_uninhibit_all_eb_writeback(trans);\n+\n \tWARN_ON(cur_trans != info->running_transaction);\n \tWARN_ON(atomic_read(&cur_trans->num_writers) < 1);\n \tatomic_dec(&cur_trans->num_writers);\n@@ -2110,6 +2120,7 @@ static void cleanup_transaction(struct btrfs_trans_handle *trans, int err)\n \tif (!test_bit(BTRFS_FS_RELOC_RUNNING, &fs_info->flags))\n \t\tbtrfs_scrub_cancel(fs_info);\n \n+\tbtrfs_uninhibit_all_eb_writeback(trans);\n \tkmem_cache_free(btrfs_trans_handle_cachep, trans);\n }\n \n@@ -2556,6 +2567,14 @@ int btrfs_commit_transaction(struct btrfs_trans_handle *trans)\n \t    fs_info->cleaner_kthread)\n \t\twake_up_process(fs_info->cleaner_kthread);\n \n+\t/*\n+\t * Uninhibit writeback on all extent buffers inhibited during this\n+\t * transaction before writing them to disk. Inhibiting prevented\n+\t * writeback while the transaction was building, but now we need\n+\t * them written.\n+\t */\n+\tbtrfs_uninhibit_all_eb_writeback(trans);\n+\n \tret = btrfs_write_and_wait_transaction(trans);\n \tif (unlikely(ret)) {\n \t\tbtrfs_err(fs_info, \"error while writing out transaction: %d\", ret);\ndiff --git a/fs/btrfs/transaction.h b/fs/btrfs/transaction.h\nindex 18ef069197e5..f0d12c16d796 100644\n--- a/fs/btrfs/transaction.h\n+++ b/fs/btrfs/transaction.h\n@@ -12,6 +12,7 @@\n #include <linux/time64.h>\n #include <linux/mutex.h>\n #include <linux/wait.h>\n+#include <linux/xarray.h>\n #include \"btrfs_inode.h\"\n #include \"delayed-ref.h\"\n \n@@ -162,6 +163,7 @@ struct btrfs_trans_handle {\n \tstruct btrfs_fs_info *fs_info;\n \tstruct list_head new_bgs;\n \tstruct btrfs_block_rsv delayed_rsv;\n+\tstruct xarray writeback_inhibited_ebs;\t/* ebs with writeback inhibited */\n };\n \n /*\n-- \n2.47.3\n\n\n\n---\n\nAdd a btrfs_search_slot_stats tracepoint to btrfs_search_slot() for\nmeasuring COW amplification.\n\nThe tracepoint fires when a search with at least one COW completes,\nrecording the root, total cow_count, restart_count, and return value.\ncow_count and restart_count per search_slot call are useful metrics\nfor tracking COW amplification.\n\nSigned-off-by: Leo Martins <loemra.dev@gmail.com>\n---\n fs/btrfs/ctree.c             | 15 +++++++++++++--\n include/trace/events/btrfs.h | 26 ++++++++++++++++++++++++++\n 2 files changed, 39 insertions(+), 2 deletions(-)\n\ndiff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c\nindex 55187ba59cc0..1971d7bb5f60 100644\n--- a/fs/btrfs/ctree.c\n+++ b/fs/btrfs/ctree.c\n@@ -2069,6 +2069,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n \tu8 lowest_level = 0;\n \tint min_write_lock_level;\n \tint prev_cmp;\n+\tint cow_count = 0;\n+\tint restart_count = 0;\n \n \tif (!root)\n \t\treturn -EINVAL;\n@@ -2157,6 +2159,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n \t\t\t    p->nodes[level + 1])) {\n \t\t\t\twrite_lock_level = level + 1;\n \t\t\t\tbtrfs_release_path(p);\n+\t\t\t\trestart_count++;\n \t\t\t\tgoto again;\n \t\t\t}\n \n@@ -2172,6 +2175,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n \t\t\t\tret = ret2;\n \t\t\t\tgoto done;\n \t\t\t}\n+\t\t\tcow_count++;\n \t\t}\n cow_done:\n \t\tp->nodes[level] = b;\n@@ -2219,8 +2223,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n \t\tp->slots[level] = slot;\n \t\tret2 = setup_nodes_for_search(trans, root, p, b, level, ins_len,\n \t\t\t\t\t      &write_lock_level);\n-\t\tif (ret2 == -EAGAIN)\n+\t\tif (ret2 == -EAGAIN) {\n+\t\t\trestart_count++;\n \t\t\tgoto again;\n+\t\t}\n \t\tif (ret2) {\n \t\t\tret = ret2;\n \t\t\tgoto done;\n@@ -2236,6 +2242,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n \t\tif (slot == 0 && ins_len && write_lock_level < level + 1) {\n \t\t\twrite_lock_level = level + 1;\n \t\t\tbtrfs_release_path(p);\n+\t\t\trestart_count++;\n \t\t\tgoto again;\n \t\t}\n \n@@ -2249,8 +2256,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n \t\t}\n \n \t\tret2 = read_block_for_search(root, p, &b, slot, key);\n-\t\tif (ret2 == -EAGAIN && !p->nowait)\n+\t\tif (ret2 == -EAGAIN && !p->nowait) {\n+\t\t\trestart_count++;\n \t\t\tgoto again;\n+\t\t}\n \t\tif (ret2) {\n \t\t\tret = ret2;\n \t\t\tgoto done;\n@@ -2281,6 +2290,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n \t}\n \tret = 1;\n done:\n+\tif (cow_count > 0)\n+\t\ttrace_btrfs_search_slot_stats(root, cow_count, restart_count, ret);\n \tif (ret < 0 && !p->skip_release_on_error)\n \t\tbtrfs_release_path(p);\n \ndiff --git a/include/trace/events/btrfs.h b/include/trace/events/btrfs.h\nindex 125bdc166bfe..b8934938a087 100644\n--- a/include/trace/events/btrfs.h\n+++ b/include/trace/events/btrfs.h\n@@ -1110,6 +1110,32 @@ TRACE_EVENT(btrfs_cow_block,\n \t\t  __entry->cow_level)\n );\n \n+TRACE_EVENT(btrfs_search_slot_stats,\n+\n+\tTP_PROTO(const struct btrfs_root *root,\n+\t\t int cow_count, int restart_count, int ret),\n+\n+\tTP_ARGS(root, cow_count, restart_count, ret),\n+\n+\tTP_STRUCT__entry_btrfs(\n+\t\t__field(\tu64,\troot_objectid\t\t)\n+\t\t__field(\tint,\tcow_count\t\t)\n+\t\t__field(\tint,\trestart_count\t\t)\n+\t\t__field(\tint,\tret\t\t\t)\n+\t),\n+\n+\tTP_fast_assign_btrfs(root->fs_info,\n+\t\t__entry->root_objectid\t= btrfs_root_id(root);\n+\t\t__entry->cow_count\t= cow_count;\n+\t\t__entry->restart_count\t= restart_count;\n+\t\t__entry->ret\t\t= ret;\n+\t),\n+\n+\tTP_printk_btrfs(\"root=%llu(%s) cow_count=%d restarts=%d ret=%d\",\n+\t\t  show_root_type(__entry->root_objectid),\n+\t\t  __entry->cow_count, __entry->restart_count, __entry->ret)\n+);\n+\n TRACE_EVENT(btrfs_space_reservation,\n \n \tTP_PROTO(const struct btrfs_fs_info *fs_info, const char *type, u64 val,\n-- \n2.47.3\n\n\n\n---\n\nOn Mon, 16 Feb 2026 12:18:48 +0000 Filipe Manana <fdmanana@kernel.org> wrote:\n\n> On Fri, Feb 13, 2026 at 8:38\\u202fPM Leo Martins <loemra.dev@gmail.com> wrote:\n> >\n> > When memory pressure causes writeback of a recently COW'd buffer,\n> > btrfs sets BTRFS_HEADER_FLAG_WRITTEN on it. Subsequent\n> > btrfs_search_slot() restarts then see the WRITTEN flag and re-COW\n> > the buffer unnecessarily, causing COW amplification that can exhaust\n> > block reservations and degrade throughput.\n> >\n> > Overwriting in place is crash-safe because the committed superblock\n> > does not reference buffers allocated in the current (uncommitted)\n> > transaction, so no on-disk tree points to this block yet.\n> >\n> > When should_cow_block() encounters a WRITTEN buffer whose generation\n> > matches the current transaction, instead of requesting a COW, re-dirty\n> > the buffer and re-register its range in the transaction's dirty_pages.\n> >\n> > Both are necessary because btrfs tracks dirty metadata through two\n> > independent mechanisms. set_extent_buffer_dirty() sets the\n> > EXTENT_BUFFER_DIRTY flag and the buffer_tree xarray PAGECACHE_TAG_DIRTY\n> > mark, which is what background writeback (btree_write_cache_pages) uses\n> > to find and write dirty buffers. The transaction's dirty_pages io tree\n> > is a separate structure used by btrfs_write_and_wait_transaction() at\n> > commit time to ensure all buffers allocated during the transaction are\n> > persisted. The dirty_pages range was originally registered in\n> > btrfs_init_new_buffer() when the block was first allocated, but\n> > background writeback may have already written and cleared it.\n> \n> This is not quite correct, the dirty_pages range is never cleared on\n> background writeback.\n> We only clear it during a transaction commit, in\n> btrfs_write_and_wait_transaction().\n> \n> Normally we shouldn't care about setting the range again in\n> dirty_pages, because after\n> we call  btrfs_write_and_wait_transaction(), no more COW should be\n> possible using this\n> transaction (which is in the unblocked state, so any new COW attempt\n> will be in another transaction).\n> \n> The exception is if we have snapshots to create and qgroups are\n> enabled, since in qgroup_account_snapshot() we\n> call btrfs_write_and_wait_transaction() and after that we can get more\n> COW, due to all the stuff we need to do to\n> create a snapshot, before we get to the final call to\n> btrfs_write_and_wait_transaction() right before we write the\n> super blocks in btrfs_commit_transaction().\n\nGot it, thanks for the correction. Updated in v3.\n\nThanks,\nLeo\n\n> \n> >\n> > Keep BTRFS_HEADER_FLAG_WRITTEN set so that btrfs_free_tree_block()\n> > correctly pins the block if it is freed later.\n> >\n> > Exclude cases where in-place overwrite is not safe:\n> >  - EXTENT_BUFFER_WRITEBACK: buffer is mid-I/O\n> >  - Zoned devices: require sequential writes\n> >  - Log trees: log blocks are immediately referenced by a committed\n> >    superblock via btrfs_sync_log(), so overwriting could corrupt the\n> >    committed log\n> >  - BTRFS_ROOT_FORCE_COW: snapshot in progress\n> >  - BTRFS_HEADER_FLAG_RELOC: block being relocated\n> >\n> > Signed-off-by: Leo Martins <loemra.dev@gmail.com>\n> > ---\n> >  fs/btrfs/ctree.c | 53 +++++++++++++++++++++++++++++++++++++++++++++---\n> >  1 file changed, 50 insertions(+), 3 deletions(-)\n> >\n> > diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c\n> > index 7267b2502665..a345e1be24d8 100644\n> > --- a/fs/btrfs/ctree.c\n> > +++ b/fs/btrfs/ctree.c\n> > @@ -599,9 +599,9 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,\n> >         return ret;\n> >  }\n> >\n> > -static inline bool should_cow_block(const struct btrfs_trans_handle *trans,\n> > +static inline bool should_cow_block(struct btrfs_trans_handle *trans,\n> >                                     const struct btrfs_root *root,\n> > -                                   const struct extent_buffer *buf)\n> > +                                   struct extent_buffer *buf)\n> >  {\n> >         if (btrfs_is_testing(root->fs_info))\n> >                 return false;\n> > @@ -621,8 +621,55 @@ static inline bool should_cow_block(const struct btrfs_trans_handle *trans,\n> >         if (btrfs_header_generation(buf) != trans->transid)\n> >                 return true;\n> >\n> > -       if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN))\n> > +       if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {\n> > +               /*\n> > +                * The buffer was allocated in this transaction and has been\n> > +                * written back to disk (WRITTEN is set). Normally we'd COW\n> > +                * it again, but since the committed superblock doesn't\n> > +                * reference this buffer (it was allocated this transaction),\n> \n> Missing an \"in\" before \"this transaction\".\n> \n> > +                * we can safely overwrite it in place.\n> > +                *\n> > +                * We keep BTRFS_HEADER_FLAG_WRITTEN set. The block has been\n> > +                * persisted at this bytenr and will be again after the\n> > +                * in-place update. This is important so that\n> > +                * btrfs_free_tree_block() correctly pins the block if it is\n> > +                * freed later (e.g., during tree rebalancing or FORCE_COW).\n> > +                *\n> > +                * We re-dirty the buffer to ensure the in-place modifications\n> > +                * will be written back to disk.\n> > +                *\n> > +                * Exclusions:\n> > +                * - Log trees: log blocks are written and immediately\n> > +                *   referenced by a committed superblock via\n> > +                *   btrfs_sync_log(), bypassing the normal transaction\n> > +                *   commit. Overwriting in place could corrupt the\n> > +                *   committed log.\n> > +                * - Zoned devices: require sequential writes\n> > +                * - FORCE_COW: snapshot in progress\n> > +                * - RELOC flag: block being relocated\n> > +                */\n> > +               if (!test_bit(EXTENT_BUFFER_WRITEBACK, &buf->bflags) &&\n> > +                   !btrfs_is_zoned(root->fs_info) &&\n> > +                   btrfs_root_id(root) != BTRFS_TREE_LOG_OBJECTID &&\n> > +                   !test_bit(BTRFS_ROOT_FORCE_COW, &root->state) &&\n> \n> We need a  smp_mb__before_atomic() before checking FORCE_COW, see the\n> existing code below.\n> \n> > +                   !btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC)) {\n> > +                       /*\n> > +                        * Re-register this block's range in the current\n> > +                        * transaction's dirty_pages so that\n> > +                        * btrfs_write_and_wait_transaction() writes it.\n> > +                        * The range was originally registered when the block\n> > +                        * was allocated, but that transaction's dirty_pages\n> > +                        * may have already been released.\n> \n> I think it's worth adding something like: \"... already been released\n> if we are in a transaction that creates snapshots and we have qgroups\n> enabled.\"\n> \n> Otherwise it looks good, thanks!\n> \n> > +                        */\n> > +                       btrfs_set_extent_bit(&trans->transaction->dirty_pages,\n> > +                                            buf->start,\n> > +                                            buf->start + buf->len - 1,\n> > +                                            EXTENT_DIRTY, NULL);\n> > +                       set_extent_buffer_dirty(buf);\n> > +                       return false;\n> > +               }\n> >                 return true;\n> > +       }\n> >\n> >         /* Ensure we can see the FORCE_COW bit. */\n> >         smp_mb__before_atomic();\n> > --\n> > 2.47.3\n> >\n> >\n\n\n---\n\nOn Sat, 14 Feb 2026 09:25:03 +0800 Sun YangKai <sunk67188@gmail.com> wrote:\n\n> Thanks for your working on this and I've expecting this for a long time :)\n\nThanks for the review!\n\n> \n> On 2026/2/14 04:30, Leo Martins wrote:\n> > When memory pressure causes writeback of a recently COW'd buffer,\n> > btrfs sets BTRFS_HEADER_FLAG_WRITTEN on it. Subsequent\n> > btrfs_search_slot() restarts then see the WRITTEN flag and re-COW\n> > the buffer unnecessarily, causing COW amplification that can exhaust\n> > block reservations and degrade throughput.\n> > \n> > Overwriting in place is crash-safe because the committed superblock\n> > does not reference buffers allocated in the current (uncommitted)\n> > transaction, so no on-disk tree points to this block yet.\n> > \n> > When should_cow_block() encounters a WRITTEN buffer whose generation\n> > matches the current transaction, instead of requesting a COW, re-dirty\n> > the buffer and re-register its range in the transaction's dirty_pages.\n> > \n> > Both are necessary because btrfs tracks dirty metadata through two\n> > independent mechanisms. set_extent_buffer_dirty() sets the\n> > EXTENT_BUFFER_DIRTY flag and the buffer_tree xarray PAGECACHE_TAG_DIRTY\n> > mark, which is what background writeback (btree_write_cache_pages) uses\n> > to find and write dirty buffers. The transaction's dirty_pages io tree\n> > is a separate structure used by btrfs_write_and_wait_transaction() at\n> > commit time to ensure all buffers allocated during the transaction are\n> > persisted. The dirty_pages range was originally registered in\n> > btrfs_init_new_buffer() when the block was first allocated, but\n> > background writeback may have already written and cleared it.\n> > \n> > Keep BTRFS_HEADER_FLAG_WRITTEN set so that btrfs_free_tree_block()\n> > correctly pins the block if it is freed later.\n> > \n> > Exclude cases where in-place overwrite is not safe:\n> >   - EXTENT_BUFFER_WRITEBACK: buffer is mid-I/O\n> >   - Zoned devices: require sequential writes\n> >   - Log trees: log blocks are immediately referenced by a committed\n> >     superblock via btrfs_sync_log(), so overwriting could corrupt the\n> >     committed log\n> >   - BTRFS_ROOT_FORCE_COW: snapshot in progress\n> >   - BTRFS_HEADER_FLAG_RELOC: block being relocated\n> > \n> > Signed-off-by: Leo Martins <loemra.dev@gmail.com>\n> > ---\n> >   fs/btrfs/ctree.c | 53 +++++++++++++++++++++++++++++++++++++++++++++---\n> >   1 file changed, 50 insertions(+), 3 deletions(-)\n> > \n> > diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c\n> > index 7267b2502665..a345e1be24d8 100644\n> > --- a/fs/btrfs/ctree.c\n> > +++ b/fs/btrfs/ctree.c\n> > @@ -599,9 +599,9 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,\n> >   \treturn ret;\n> >   }\n> >   \n> > -static inline bool should_cow_block(const struct btrfs_trans_handle *trans,\n> > +static inline bool should_cow_block(struct btrfs_trans_handle *trans,\n> >   \t\t\t\t    const struct btrfs_root *root,\n> > -\t\t\t\t    const struct extent_buffer *buf)\n> > +\t\t\t\t    struct extent_buffer *buf)\n> >   {\n> >   \tif (btrfs_is_testing(root->fs_info))\n> >   \t\treturn false;\n> > @@ -621,8 +621,55 @@ static inline bool should_cow_block(const struct btrfs_trans_handle *trans,\n> >   \tif (btrfs_header_generation(buf) != trans->transid)\n> >   \t\treturn true;\n> >   \n> > -\tif (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN))\n> > +\tif (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {\n> > +\t\t/*\n> > +\t\t * The buffer was allocated in this transaction and has been\n> > +\t\t * written back to disk (WRITTEN is set). Normally we'd COW\n> > +\t\t * it again, but since the committed superblock doesn't\n> > +\t\t * reference this buffer (it was allocated this transaction),\n> > +\t\t * we can safely overwrite it in place.\n> > +\t\t *\n> > +\t\t * We keep BTRFS_HEADER_FLAG_WRITTEN set. The block has been\n> > +\t\t * persisted at this bytenr and will be again after the\n> > +\t\t * in-place update. This is important so that\n> > +\t\t * btrfs_free_tree_block() correctly pins the block if it is\n> > +\t\t * freed later (e.g., during tree rebalancing or FORCE_COW).\n> > +\t\t *\n> > +\t\t * We re-dirty the buffer to ensure the in-place modifications\n> > +\t\t * will be written back to disk.\n> > +\t\t *\n> > +\t\t * Exclusions:\n> > +\t\t * - Log trees: log blocks are written and immediately\n> > +\t\t *   referenced by a committed superblock via\n> > +\t\t *   btrfs_sync_log(), bypassing the normal transaction\n> > +\t\t *   commit. Overwriting in place could corrupt the\n> > +\t\t *   committed log.\n> > +\t\t * - Zoned devices: require sequential writes\n> > +\t\t * - FORCE_COW: snapshot in progress\n> > +\t\t * - RELOC flag: block being relocated\n> > +\t\t */\n> > +\t\tif (!test_bit(EXTENT_BUFFER_WRITEBACK, &buf->bflags) &&\n> > +\t\t    !btrfs_is_zoned(root->fs_info) &&\n> > +\t\t    btrfs_root_id(root) != BTRFS_TREE_LOG_OBJECTID &&\n> > +\t\t    !test_bit(BTRFS_ROOT_FORCE_COW, &root->state) &&\n> it seems we need smp_mb__before_atomic() to see the FORCE_COW bit?\n\nGood call, fixed in v3.\n\n> > +\t\t    !btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC)) {\n> > +\t\t\t/*\n> > +\t\t\t * Re-register this block's range in the current\n> > +\t\t\t * transaction's dirty_pages so that\n> > +\t\t\t * btrfs_write_and_wait_transaction() writes it.\n> > +\t\t\t * The range was originally registered when the block\n> > +\t\t\t * was allocated, but that transaction's dirty_pages\n> > +\t\t\t * may have already been released.\n> > +\t\t\t */\n> > +\t\t\tbtrfs_set_extent_bit(&trans->transaction->dirty_pages,\n> > +\t\t\t\t\t     buf->start,\n> > +\t\t\t\t\t     buf->start + buf->len - 1,\n> > +\t\t\t\t\t     EXTENT_DIRTY, NULL);\n> > +\t\t\tset_extent_buffer_dirty(buf);\n> why use set_extent_buffer_dirty() instead of btrfs_mark_buffer_dirty()? \n> I don't see any other callers doing this.\n\nbtrfs_mark_buffer_dirty() calls btrfs_assert_tree_write_locked(buf),\nbut should_cow_block() may be called from btrfs_search_slot() when the\nbuffer only holds a read lock (root node acquired with BTRFS_READ_LOCK\nin btrfs_search_slot_get_root()). Added a comment explaining this in \nv3.\n\n> > +\t\t\treturn false;\n> > +\t\t}\n> >   \t\treturn true;\n> > +\t}\n> >   \n> >   \t/* Ensure we can see the FORCE_COW bit. */\n> >   \tsmp_mb__before_atomic();\n> \n> And I wonder if we could have something more readable like this:\n> \n> \tif (btrfs_header_generation(buf) != trans->transid)\n> \t\treturn true;\n> \n> \tif (test_bit(EXTENT_BUFFER_WRITEBACK, &buf->bflags))\n> \t\treturn true;\n> \n> \tif (btrfs_root_id(root) != BTRFS_TREE_RELOC_OBJECTID &&\n> \t    btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC))\n> \t\treturn true;\n> \n> \t/* Ensure we can see the FORCE_COW bit. */\n> \tsmp_mb__before_atomic();\n> \tif (test_bit(BTRFS_ROOT_FORCE_COW, &root->state))\n> \t\treturn true;\n> \n> \tif (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {\n> \t\tif (btrfs_root_id(root) == BTRFS_TREE_LOG_OBJECTID ||\n> \t\t    btrfs_is_zoned(root->fs_info))\n> \t\t\t\treturn true;\n> \t\tbtrfs_set_extent_bit(&trans->transaction->dirty_pages,\n> \t\t\t\t     buf->start,\n> \t\t\t\t     buf->start + buf->len - 1,\n> \t\t\t\t     EXTENT_DIRTY, NULL);\n> \t\tbtrfs_mark_buffer_dirty(trans, buf);\n> \t}\n> \n> \treturn false;\n\nUpdated in v3!\n\n> \n> Thanks,\n> Sun YangKai\n\n\n---\n\nOn Mon, 16 Feb 2026 12:40:04 +0000 Filipe Manana <fdmanana@kernel.org> wrote:\n\n> On Fri, Feb 13, 2026 at 8:37\\u202fPM Leo Martins <loemra.dev@gmail.com> wrote:\n> >\n> > Add a btrfs_search_slot_stats tracepoint to btrfs_search_slot() for\n> > measuring COW amplification.\n> >\n> > The tracepoint fires when a search with at least one COW completes,\n> > recording the root, total cow_count, restart_count, and return value.\n> > cow_count and restart_count per search_slot call are useful metrics\n> > for tracking COW amplification.\n> >\n> > Signed-off-by: Leo Martins <loemra.dev@gmail.com>\n> > ---\n> >  fs/btrfs/ctree.c             | 15 +++++++++++++--\n> >  include/trace/events/btrfs.h | 26 ++++++++++++++++++++++++++\n> >  2 files changed, 39 insertions(+), 2 deletions(-)\n> >\n> > diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c\n> > index 55187ba59cc0..1971d7bb5f60 100644\n> > --- a/fs/btrfs/ctree.c\n> > +++ b/fs/btrfs/ctree.c\n> > @@ -2069,6 +2069,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n> >         u8 lowest_level = 0;\n> >         int min_write_lock_level;\n> >         int prev_cmp;\n> > +       int cow_count = 0;\n> > +       int restart_count = 0;\n> >\n> >         if (!root)\n> >                 return -EINVAL;\n> > @@ -2157,6 +2159,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n> >                             p->nodes[level + 1])) {\n> >                                 write_lock_level = level + 1;\n> >                                 btrfs_release_path(p);\n> > +                               restart_count++;\n> >                                 goto again;\n> >                         }\n> >\n> > @@ -2172,6 +2175,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n> >                                 ret = ret2;\n> >                                 goto done;\n> >                         }\n> > +                       cow_count++;\n> >                 }\n> >  cow_done:\n> >                 p->nodes[level] = b;\n> > @@ -2219,8 +2223,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n> >                 p->slots[level] = slot;\n> >                 ret2 = setup_nodes_for_search(trans, root, p, b, level, ins_len,\n> >                                               &write_lock_level);\n> > -               if (ret2 == -EAGAIN)\n> > +               if (ret2 == -EAGAIN) {\n> > +                       restart_count++;\n> >                         goto again;\n> > +               }\n> >                 if (ret2) {\n> >                         ret = ret2;\n> >                         goto done;\n> > @@ -2236,6 +2242,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n> >                 if (slot == 0 && ins_len && write_lock_level < level + 1) {\n> >                         write_lock_level = level + 1;\n> >                         btrfs_release_path(p);\n> > +                       restart_count++;\n> >                         goto again;\n> >                 }\n> >\n> > @@ -2249,8 +2256,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n> >                 }\n> >\n> >                 ret2 = read_block_for_search(root, p, &b, slot, key);\n> > -               if (ret2 == -EAGAIN && !p->nowait)\n> > +               if (ret2 == -EAGAIN && !p->nowait) {\n> > +                       restart_count++;\n> >                         goto again;\n> > +               }\n> >                 if (ret2) {\n> >                         ret = ret2;\n> >                         goto done;\n> > @@ -2281,6 +2290,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n> >         }\n> >         ret = 1;\n> >  done:\n> > +       if (cow_count > 0)\n> > +               trace_btrfs_search_slot_stats(root, cow_count, restart_count, ret);\n> \n> So I find this way too specific, plus even if trace points are\n> disabled we have the overhead of the counters (and inside critical\n> sections).\n> \n> We already have a tracepoint for COW, trace_btrfs_cow_block(), and we\n> could have one just for the retry thing, maybe naming it like\n> trace_btrfs_search_slot_restart() or something.\n> So we could use those two tracepoints to measure things (bpftrace\n> scripts could easily report a count of each trace point and such),\n> instead of this highly specialized tracepoint that adds some overhead\n> when tracepoints are disabled.\n\nGood point, added a per-restart-site trace_btrfs_search_slot_restart()\ntracepoint in v3.\n\nThanks,\nLeo\n\n> \n> Thanks.\n> \n> \n> >         if (ret < 0 && !p->skip_release_on_error)\n> >                 btrfs_release_path(p);\n> >\n> > diff --git a/include/trace/events/btrfs.h b/include/trace/events/btrfs.h\n> > index 125bdc166bfe..b8934938a087 100644\n> > --- a/include/trace/events/btrfs.h\n> > +++ b/include/trace/events/btrfs.h\n> > @@ -1110,6 +1110,32 @@ TRACE_EVENT(btrfs_cow_block,\n> >                   __entry->cow_level)\n> >  );\n> >\n> > +TRACE_EVENT(btrfs_search_slot_stats,\n> > +\n> > +       TP_PROTO(const struct btrfs_root *root,\n> > +                int cow_count, int restart_count, int ret),\n> > +\n> > +       TP_ARGS(root, cow_count, restart_count, ret),\n> > +\n> > +       TP_STRUCT__entry_btrfs(\n> > +               __field(        u64,    root_objectid           )\n> > +               __field(        int,    cow_count               )\n> > +               __field(        int,    restart_count           )\n> > +               __field(        int,    ret                     )\n> > +       ),\n> > +\n> > +       TP_fast_assign_btrfs(root->fs_info,\n> > +               __entry->root_objectid  = btrfs_root_id(root);\n> > +               __entry->cow_count      = cow_count;\n> > +               __entry->restart_count  = restart_count;\n> > +               __entry->ret            = ret;\n> > +       ),\n> > +\n> > +       TP_printk_btrfs(\"root=%llu(%s) cow_count=%d restarts=%d ret=%d\",\n> > +                 show_root_type(__entry->root_objectid),\n> > +                 __entry->cow_count, __entry->restart_count, __entry->ret)\n> > +);\n> > +\n> >  TRACE_EVENT(btrfs_space_reservation,\n> >\n> >         TP_PROTO(const struct btrfs_fs_info *fs_info, const char *type, u64 val,\n> > --\n> > 2.47.3\n> >\n> >\n\n"
        },
        {
          "author": "Sun YangKai",
          "summary": "The reviewer raised two technical concerns: the need for smp_mb__before_atomic() to ensure visibility of the FORCE_COW bit and a suggestion to use btrfs_mark_buffer_dirty() instead of set_extent_buffer_dirty(). They also provided an alternative code snippet for readability.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "suggested improvements"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Thanks for your working on this and I've expecting this for a long time :)\n\nOn 2026/2/14 04:30, Leo Martins wrote:\n> When memory pressure causes writeback of a recently COW'd buffer,\n> btrfs sets BTRFS_HEADER_FLAG_WRITTEN on it. Subsequent\n> btrfs_search_slot() restarts then see the WRITTEN flag and re-COW\n> the buffer unnecessarily, causing COW amplification that can exhaust\n> block reservations and degrade throughput.\n> \n> Overwriting in place is crash-safe because the committed superblock\n> does not reference buffers allocated in the current (uncommitted)\n> transaction, so no on-disk tree points to this block yet.\n> \n> When should_cow_block() encounters a WRITTEN buffer whose generation\n> matches the current transaction, instead of requesting a COW, re-dirty\n> the buffer and re-register its range in the transaction's dirty_pages.\n> \n> Both are necessary because btrfs tracks dirty metadata through two\n> independent mechanisms. set_extent_buffer_dirty() sets the\n> EXTENT_BUFFER_DIRTY flag and the buffer_tree xarray PAGECACHE_TAG_DIRTY\n> mark, which is what background writeback (btree_write_cache_pages) uses\n> to find and write dirty buffers. The transaction's dirty_pages io tree\n> is a separate structure used by btrfs_write_and_wait_transaction() at\n> commit time to ensure all buffers allocated during the transaction are\n> persisted. The dirty_pages range was originally registered in\n> btrfs_init_new_buffer() when the block was first allocated, but\n> background writeback may have already written and cleared it.\n> \n> Keep BTRFS_HEADER_FLAG_WRITTEN set so that btrfs_free_tree_block()\n> correctly pins the block if it is freed later.\n> \n> Exclude cases where in-place overwrite is not safe:\n>   - EXTENT_BUFFER_WRITEBACK: buffer is mid-I/O\n>   - Zoned devices: require sequential writes\n>   - Log trees: log blocks are immediately referenced by a committed\n>     superblock via btrfs_sync_log(), so overwriting could corrupt the\n>     committed log\n>   - BTRFS_ROOT_FORCE_COW: snapshot in progress\n>   - BTRFS_HEADER_FLAG_RELOC: block being relocated\n> \n> Signed-off-by: Leo Martins <loemra.dev@gmail.com>\n> ---\n>   fs/btrfs/ctree.c | 53 +++++++++++++++++++++++++++++++++++++++++++++---\n>   1 file changed, 50 insertions(+), 3 deletions(-)\n> \n> diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c\n> index 7267b2502665..a345e1be24d8 100644\n> --- a/fs/btrfs/ctree.c\n> +++ b/fs/btrfs/ctree.c\n> @@ -599,9 +599,9 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,\n>   \treturn ret;\n>   }\n>   \n> -static inline bool should_cow_block(const struct btrfs_trans_handle *trans,\n> +static inline bool should_cow_block(struct btrfs_trans_handle *trans,\n>   \t\t\t\t    const struct btrfs_root *root,\n> -\t\t\t\t    const struct extent_buffer *buf)\n> +\t\t\t\t    struct extent_buffer *buf)\n>   {\n>   \tif (btrfs_is_testing(root->fs_info))\n>   \t\treturn false;\n> @@ -621,8 +621,55 @@ static inline bool should_cow_block(const struct btrfs_trans_handle *trans,\n>   \tif (btrfs_header_generation(buf) != trans->transid)\n>   \t\treturn true;\n>   \n> -\tif (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN))\n> +\tif (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {\n> +\t\t/*\n> +\t\t * The buffer was allocated in this transaction and has been\n> +\t\t * written back to disk (WRITTEN is set). Normally we'd COW\n> +\t\t * it again, but since the committed superblock doesn't\n> +\t\t * reference this buffer (it was allocated this transaction),\n> +\t\t * we can safely overwrite it in place.\n> +\t\t *\n> +\t\t * We keep BTRFS_HEADER_FLAG_WRITTEN set. The block has been\n> +\t\t * persisted at this bytenr and will be again after the\n> +\t\t * in-place update. This is important so that\n> +\t\t * btrfs_free_tree_block() correctly pins the block if it is\n> +\t\t * freed later (e.g., during tree rebalancing or FORCE_COW).\n> +\t\t *\n> +\t\t * We re-dirty the buffer to ensure the in-place modifications\n> +\t\t * will be written back to disk.\n> +\t\t *\n> +\t\t * Exclusions:\n> +\t\t * - Log trees: log blocks are written and immediately\n> +\t\t *   referenced by a committed superblock via\n> +\t\t *   btrfs_sync_log(), bypassing the normal transaction\n> +\t\t *   commit. Overwriting in place could corrupt the\n> +\t\t *   committed log.\n> +\t\t * - Zoned devices: require sequential writes\n> +\t\t * - FORCE_COW: snapshot in progress\n> +\t\t * - RELOC flag: block being relocated\n> +\t\t */\n> +\t\tif (!test_bit(EXTENT_BUFFER_WRITEBACK, &buf->bflags) &&\n> +\t\t    !btrfs_is_zoned(root->fs_info) &&\n> +\t\t    btrfs_root_id(root) != BTRFS_TREE_LOG_OBJECTID &&\n> +\t\t    !test_bit(BTRFS_ROOT_FORCE_COW, &root->state) &&\nit seems we need smp_mb__before_atomic() to see the FORCE_COW bit?\n> +\t\t    !btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC)) {\n> +\t\t\t/*\n> +\t\t\t * Re-register this block's range in the current\n> +\t\t\t * transaction's dirty_pages so that\n> +\t\t\t * btrfs_write_and_wait_transaction() writes it.\n> +\t\t\t * The range was originally registered when the block\n> +\t\t\t * was allocated, but that transaction's dirty_pages\n> +\t\t\t * may have already been released.\n> +\t\t\t */\n> +\t\t\tbtrfs_set_extent_bit(&trans->transaction->dirty_pages,\n> +\t\t\t\t\t     buf->start,\n> +\t\t\t\t\t     buf->start + buf->len - 1,\n> +\t\t\t\t\t     EXTENT_DIRTY, NULL);\n> +\t\t\tset_extent_buffer_dirty(buf);\nwhy use set_extent_buffer_dirty() instead of btrfs_mark_buffer_dirty()? \nI don't see any other callers doing this.\n> +\t\t\treturn false;\n> +\t\t}\n>   \t\treturn true;\n> +\t}\n>   \n>   \t/* Ensure we can see the FORCE_COW bit. */\n>   \tsmp_mb__before_atomic();\n\nAnd I wonder if we could have something more readable like this:\n\n\tif (btrfs_header_generation(buf) != trans->transid)\n\t\treturn true;\n\n\tif (test_bit(EXTENT_BUFFER_WRITEBACK, &buf->bflags))\n\t\treturn true;\n\n\tif (btrfs_root_id(root) != BTRFS_TREE_RELOC_OBJECTID &&\n\t    btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC))\n\t\treturn true;\n\n\t/* Ensure we can see the FORCE_COW bit. */\n\tsmp_mb__before_atomic();\n\tif (test_bit(BTRFS_ROOT_FORCE_COW, &root->state))\n\t\treturn true;\n\n\tif (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {\n\t\tif (btrfs_root_id(root) == BTRFS_TREE_LOG_OBJECTID ||\n\t\t    btrfs_is_zoned(root->fs_info))\n\t\t\t\treturn true;\n\t\tbtrfs_set_extent_bit(&trans->transaction->dirty_pages,\n\t\t\t\t     buf->start,\n\t\t\t\t     buf->start + buf->len - 1,\n\t\t\t\t     EXTENT_DIRTY, NULL);\n\t\tbtrfs_mark_buffer_dirty(trans, buf);\n\t}\n\n\treturn false;\n\nThanks,\nSun YangKai\n\n\n"
        },
        {
          "author": "Filipe Manana",
          "summary": "Filipe Manana reviewed the patch for skipping COW on written extent buffers in current transactions, pointing out several issues including a missing 'in' before 'this transaction', the need for an smp_mb__before_atomic() before checking FORCE_COW, and minor style inconsistencies. He also suggested using existing tracepoints instead of adding a new one.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "minor issues",
            "requested changes"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Fri, Feb 13, 2026 at 8:38PM Leo Martins <loemra.dev@gmail.com> wrote:\n>\n> When memory pressure causes writeback of a recently COW'd buffer,\n> btrfs sets BTRFS_HEADER_FLAG_WRITTEN on it. Subsequent\n> btrfs_search_slot() restarts then see the WRITTEN flag and re-COW\n> the buffer unnecessarily, causing COW amplification that can exhaust\n> block reservations and degrade throughput.\n>\n> Overwriting in place is crash-safe because the committed superblock\n> does not reference buffers allocated in the current (uncommitted)\n> transaction, so no on-disk tree points to this block yet.\n>\n> When should_cow_block() encounters a WRITTEN buffer whose generation\n> matches the current transaction, instead of requesting a COW, re-dirty\n> the buffer and re-register its range in the transaction's dirty_pages.\n>\n> Both are necessary because btrfs tracks dirty metadata through two\n> independent mechanisms. set_extent_buffer_dirty() sets the\n> EXTENT_BUFFER_DIRTY flag and the buffer_tree xarray PAGECACHE_TAG_DIRTY\n> mark, which is what background writeback (btree_write_cache_pages) uses\n> to find and write dirty buffers. The transaction's dirty_pages io tree\n> is a separate structure used by btrfs_write_and_wait_transaction() at\n> commit time to ensure all buffers allocated during the transaction are\n> persisted. The dirty_pages range was originally registered in\n> btrfs_init_new_buffer() when the block was first allocated, but\n> background writeback may have already written and cleared it.\n\nThis is not quite correct, the dirty_pages range is never cleared on\nbackground writeback.\nWe only clear it during a transaction commit, in\nbtrfs_write_and_wait_transaction().\n\nNormally we shouldn't care about setting the range again in\ndirty_pages, because after\nwe call  btrfs_write_and_wait_transaction(), no more COW should be\npossible using this\ntransaction (which is in the unblocked state, so any new COW attempt\nwill be in another transaction).\n\nThe exception is if we have snapshots to create and qgroups are\nenabled, since in qgroup_account_snapshot() we\ncall btrfs_write_and_wait_transaction() and after that we can get more\nCOW, due to all the stuff we need to do to\ncreate a snapshot, before we get to the final call to\nbtrfs_write_and_wait_transaction() right before we write the\nsuper blocks in btrfs_commit_transaction().\n\n>\n> Keep BTRFS_HEADER_FLAG_WRITTEN set so that btrfs_free_tree_block()\n> correctly pins the block if it is freed later.\n>\n> Exclude cases where in-place overwrite is not safe:\n>  - EXTENT_BUFFER_WRITEBACK: buffer is mid-I/O\n>  - Zoned devices: require sequential writes\n>  - Log trees: log blocks are immediately referenced by a committed\n>    superblock via btrfs_sync_log(), so overwriting could corrupt the\n>    committed log\n>  - BTRFS_ROOT_FORCE_COW: snapshot in progress\n>  - BTRFS_HEADER_FLAG_RELOC: block being relocated\n>\n> Signed-off-by: Leo Martins <loemra.dev@gmail.com>\n> ---\n>  fs/btrfs/ctree.c | 53 +++++++++++++++++++++++++++++++++++++++++++++---\n>  1 file changed, 50 insertions(+), 3 deletions(-)\n>\n> diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c\n> index 7267b2502665..a345e1be24d8 100644\n> --- a/fs/btrfs/ctree.c\n> +++ b/fs/btrfs/ctree.c\n> @@ -599,9 +599,9 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,\n>         return ret;\n>  }\n>\n> -static inline bool should_cow_block(const struct btrfs_trans_handle *trans,\n> +static inline bool should_cow_block(struct btrfs_trans_handle *trans,\n>                                     const struct btrfs_root *root,\n> -                                   const struct extent_buffer *buf)\n> +                                   struct extent_buffer *buf)\n>  {\n>         if (btrfs_is_testing(root->fs_info))\n>                 return false;\n> @@ -621,8 +621,55 @@ static inline bool should_cow_block(const struct btrfs_trans_handle *trans,\n>         if (btrfs_header_generation(buf) != trans->transid)\n>                 return true;\n>\n> -       if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN))\n> +       if (btrfs_header_flag(buf, BTRFS_HEADER_FLAG_WRITTEN)) {\n> +               /*\n> +                * The buffer was allocated in this transaction and has been\n> +                * written back to disk (WRITTEN is set). Normally we'd COW\n> +                * it again, but since the committed superblock doesn't\n> +                * reference this buffer (it was allocated this transaction),\n\nMissing an \"in\" before \"this transaction\".\n\n> +                * we can safely overwrite it in place.\n> +                *\n> +                * We keep BTRFS_HEADER_FLAG_WRITTEN set. The block has been\n> +                * persisted at this bytenr and will be again after the\n> +                * in-place update. This is important so that\n> +                * btrfs_free_tree_block() correctly pins the block if it is\n> +                * freed later (e.g., during tree rebalancing or FORCE_COW).\n> +                *\n> +                * We re-dirty the buffer to ensure the in-place modifications\n> +                * will be written back to disk.\n> +                *\n> +                * Exclusions:\n> +                * - Log trees: log blocks are written and immediately\n> +                *   referenced by a committed superblock via\n> +                *   btrfs_sync_log(), bypassing the normal transaction\n> +                *   commit. Overwriting in place could corrupt the\n> +                *   committed log.\n> +                * - Zoned devices: require sequential writes\n> +                * - FORCE_COW: snapshot in progress\n> +                * - RELOC flag: block being relocated\n> +                */\n> +               if (!test_bit(EXTENT_BUFFER_WRITEBACK, &buf->bflags) &&\n> +                   !btrfs_is_zoned(root->fs_info) &&\n> +                   btrfs_root_id(root) != BTRFS_TREE_LOG_OBJECTID &&\n> +                   !test_bit(BTRFS_ROOT_FORCE_COW, &root->state) &&\n\nWe need a  smp_mb__before_atomic() before checking FORCE_COW, see the\nexisting code below.\n\n> +                   !btrfs_header_flag(buf, BTRFS_HEADER_FLAG_RELOC)) {\n> +                       /*\n> +                        * Re-register this block's range in the current\n> +                        * transaction's dirty_pages so that\n> +                        * btrfs_write_and_wait_transaction() writes it.\n> +                        * The range was originally registered when the block\n> +                        * was allocated, but that transaction's dirty_pages\n> +                        * may have already been released.\n\nI think it's worth adding something like: \"... already been released\nif we are in a transaction that creates snapshots and we have qgroups\nenabled.\"\n\nOtherwise it looks good, thanks!\n\n> +                        */\n> +                       btrfs_set_extent_bit(&trans->transaction->dirty_pages,\n> +                                            buf->start,\n> +                                            buf->start + buf->len - 1,\n> +                                            EXTENT_DIRTY, NULL);\n> +                       set_extent_buffer_dirty(buf);\n> +                       return false;\n> +               }\n>                 return true;\n> +       }\n>\n>         /* Ensure we can see the FORCE_COW bit. */\n>         smp_mb__before_atomic();\n> --\n> 2.47.3\n>\n>\n\n\n---\n\nOn Fri, Feb 13, 2026 at 8:38PM Leo Martins <loemra.dev@gmail.com> wrote:\n>\n> Inhibit writeback on COW'd extent buffers for the lifetime of the\n> transaction handle, preventing background writeback from setting\n> BTRFS_HEADER_FLAG_WRITTEN and causing unnecessary re-COW.\n>\n> COW amplification occurs when background writeback flushes an extent\n> buffer that a transaction handle is still actively modifying. When\n> lock_extent_buffer_for_io() transitions a buffer from dirty to\n> writeback, it sets BTRFS_HEADER_FLAG_WRITTEN, marking the block as\n> having been persisted to disk at its current bytenr. Once WRITTEN is\n> set, should_cow_block() must either COW the block again or overwrite\n> it in place, both of which are unnecessary overhead when the buffer\n> is still being modified by the same handle that allocated it. By\n> inhibiting background writeback on actively-used buffers, WRITTEN is\n> never set while a transaction handle holds a reference to the buffer,\n> avoiding this overhead entirely.\n>\n> Add an atomic_t writeback_inhibitors counter to struct extent_buffer,\n> which fits in an existing 6-byte hole without increasing struct size.\n> When a buffer is COW'd in btrfs_force_cow_block(), call\n> btrfs_inhibit_eb_writeback() to store the eb in the transaction\n> handle's writeback_inhibited_ebs xarray (keyed by eb->start), take a\n> reference, and increment writeback_inhibitors. The function handles\n> dedup (same eb inhibited twice by the same handle) and replacement\n> (different eb at the same logical address). Allocation failure is\n> graceful: the buffer simply falls back to the pre-existing behavior\n> where it may be written back and re-COW'd.\n>\n> In lock_extent_buffer_for_io(), when writeback_inhibitors is non-zero\n> and the writeback mode is WB_SYNC_NONE, skip the buffer. WB_SYNC_NONE\n> is used by the VM flusher threads for background and periodic\n> writeback, which are the only paths that cause COW amplification by\n> opportunistically writing out dirty extent buffers mid-transaction.\n> Skipping these is safe because the buffers remain dirty in the page\n> cache and will be written out at transaction commit time.\n>\n> WB_SYNC_ALL must always proceed regardless of writeback_inhibitors.\n> This is required for correctness in the fsync path: btrfs_sync_log()\n> writes log tree blocks via filemap_fdatawrite_range() (WB_SYNC_ALL)\n> while the transaction handle that inhibited those same blocks is still\n> active. Without the WB_SYNC_ALL bypass, those inhibited log tree\n> blocks would be silently skipped, resulting in an incomplete log on\n> disk and corruption on replay. btrfs_write_and_wait_transaction()\n> also uses WB_SYNC_ALL via filemap_fdatawrite_range(); for that path,\n> inhibitors are already cleared beforehand, but the bypass ensures\n> correctness regardless.\n>\n> Uninhibit in __btrfs_end_transaction() before atomic_dec(num_writers)\n> to prevent a race where the committer proceeds while buffers are still\n> inhibited. Also uninhibit in btrfs_commit_transaction() before writing\n> and in cleanup_transaction() for the error path.\n>\n> Signed-off-by: Leo Martins <loemra.dev@gmail.com>\n> ---\n>  fs/btrfs/ctree.c       |  4 +++\n>  fs/btrfs/extent_io.c   | 62 +++++++++++++++++++++++++++++++++++++++++-\n>  fs/btrfs/extent_io.h   |  5 ++++\n>  fs/btrfs/transaction.c | 19 +++++++++++++\n>  fs/btrfs/transaction.h |  2 ++\n>  5 files changed, 91 insertions(+), 1 deletion(-)\n>\n> diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c\n> index a345e1be24d8..55187ba59cc0 100644\n> --- a/fs/btrfs/ctree.c\n> +++ b/fs/btrfs/ctree.c\n> @@ -590,6 +590,10 @@ int btrfs_force_cow_block(struct btrfs_trans_handle *trans,\n>                 btrfs_tree_unlock(buf);\n>         free_extent_buffer_stale(buf);\n>         btrfs_mark_buffer_dirty(trans, cow);\n> +\n> +       /* Inhibit writeback on the COW'd buffer for this transaction handle */\n\nPlease always end sentences in a comment with punctuation.\n\n\n> +       btrfs_inhibit_eb_writeback(trans, cow);\n> +\n>         *cow_ret = cow;\n>         return 0;\n>\n> diff --git a/fs/btrfs/extent_io.c b/fs/btrfs/extent_io.c\n> index dfc17c292217..0c9276cff299 100644\n> --- a/fs/btrfs/extent_io.c\n> +++ b/fs/btrfs/extent_io.c\n> @@ -1940,7 +1940,9 @@ static noinline_for_stack bool lock_extent_buffer_for_io(struct extent_buffer *e\n>          * of time.\n>          */\n>         spin_lock(&eb->refs_lock);\n> -       if (test_and_clear_bit(EXTENT_BUFFER_DIRTY, &eb->bflags)) {\n> +       if ((wbc->sync_mode == WB_SYNC_ALL ||\n> +            atomic_read(&eb->writeback_inhibitors) == 0) &&\n> +           test_and_clear_bit(EXTENT_BUFFER_DIRTY, &eb->bflags)) {\n>                 XA_STATE(xas, &fs_info->buffer_tree, eb->start >> fs_info->nodesize_bits);\n>                 unsigned long flags;\n>\n> @@ -2999,6 +3001,63 @@ static inline void btrfs_release_extent_buffer(struct extent_buffer *eb)\n>         kmem_cache_free(extent_buffer_cache, eb);\n>  }\n>\n> +/*\n> + * btrfs_inhibit_eb_writeback - Inhibit writeback on buffer during transaction\n> + * @trans: transaction handle that will own the inhibitor\n> + * @eb: extent buffer to inhibit writeback on\n> + *\n> + * Attempts to track this extent buffer in the transaction's inhibited set.\n> + * If memory allocation fails, the buffer is simply not tracked. It may\n> + * be written back and need re-COW, which is the original behavior.\n> + * This is acceptable since inhibiting writeback is an optimization.\n> + */\n> +void btrfs_inhibit_eb_writeback(struct btrfs_trans_handle *trans,\n> +                               struct extent_buffer *eb)\n> +{\n> +       unsigned long index = eb->start >> trans->fs_info->nodesize_bits;\n> +       void *old;\n> +\n> +       /* Check if already inhibited by this handle */\n\nSame here.\n\n> +       old = xa_load(&trans->writeback_inhibited_ebs, index);\n> +       if (old == eb)\n> +               return;\n> +\n> +       refcount_inc(&eb->refs);        /* Take reference */\n\nAlways place comments above the code line, not in the same line.\n\n> +\n> +       old = xa_store(&trans->writeback_inhibited_ebs, index, eb, GFP_NOFS);\n> +       if (xa_is_err(old)) {\n> +               /* Allocation failed, just skip inhibiting this buffer */\n\nPunctuation missing.\n\n> +               free_extent_buffer(eb);\n> +               return;\n> +       }\n> +\n> +       /* Handle replacement of different eb at same index */\n\nPunctuation missing.\n\n> +       if (old && old != eb) {\n> +               struct extent_buffer *old_eb = old;\n> +\n> +               atomic_dec(&old_eb->writeback_inhibitors);\n> +               free_extent_buffer(old_eb);\n> +       }\n> +\n> +       atomic_inc(&eb->writeback_inhibitors);\n> +}\n> +\n> +/*\n> + * btrfs_uninhibit_all_eb_writeback - Uninhibit writeback on all buffers\n> + * @trans: transaction handle to clean up\n> + */\n> +void btrfs_uninhibit_all_eb_writeback(struct btrfs_trans_handle *trans)\n> +{\n> +       struct extent_buffer *eb;\n> +       unsigned long index;\n> +\n> +       xa_for_each(&trans->writeback_inhibited_ebs, index, eb) {\n> +               atomic_dec(&eb->writeback_inhibitors);\n> +               free_extent_buffer(eb);\n> +       }\n> +       xa_destroy(&trans->writeback_inhibited_ebs);\n> +}\n> +\n>  static struct extent_buffer *__alloc_extent_buffer(struct btrfs_fs_info *fs_info,\n>                                                    u64 start)\n>  {\n> @@ -3009,6 +3068,7 @@ static struct extent_buffer *__alloc_extent_buffer(struct btrfs_fs_info *fs_info\n>         eb->len = fs_info->nodesize;\n>         eb->fs_info = fs_info;\n>         init_rwsem(&eb->lock);\n> +       atomic_set(&eb->writeback_inhibitors, 0);\n>\n>         btrfs_leak_debug_add_eb(eb);\n>\n> diff --git a/fs/btrfs/extent_io.h b/fs/btrfs/extent_io.h\n> index 73571d5d3d5a..4b15a5d8bc0f 100644\n> --- a/fs/btrfs/extent_io.h\n> +++ b/fs/btrfs/extent_io.h\n> @@ -102,6 +102,7 @@ struct extent_buffer {\n>         /* >= 0 if eb belongs to a log tree, -1 otherwise */\n>         s8 log_index;\n>         u8 folio_shift;\n> +       atomic_t writeback_inhibitors;  /* inhibits writeback when > 0 */\n\nAlways place the comment above the structure's member (just like for\ncode), and add punctuation and capitalize the first word.\n\nWe have old code that does follow this, from the old days where\n\"anything goes\", but we try to be consistent nowadays, see:\n\nhttps://btrfs.readthedocs.io/en/latest/dev/Development-notes.html#comments\n\nOtherwise it looks good, with those minor changes:\n\nReviewed-by: Filipe Manana <fdmanana@suse.com>\n\nThanks.\n\n\n\n>         struct rcu_head rcu_head;\n>\n>         struct rw_semaphore lock;\n> @@ -381,4 +382,8 @@ void btrfs_extent_buffer_leak_debug_check(struct btrfs_fs_info *fs_info);\n>  #define btrfs_extent_buffer_leak_debug_check(fs_info)  do {} while (0)\n>  #endif\n>\n> +void btrfs_inhibit_eb_writeback(struct btrfs_trans_handle *trans,\n> +                              struct extent_buffer *eb);\n> +void btrfs_uninhibit_all_eb_writeback(struct btrfs_trans_handle *trans);\n> +\n>  #endif\n> diff --git a/fs/btrfs/transaction.c b/fs/btrfs/transaction.c\n> index f4cc9e1a1b93..a9a22629b49d 100644\n> --- a/fs/btrfs/transaction.c\n> +++ b/fs/btrfs/transaction.c\n> @@ -15,6 +15,7 @@\n>  #include \"misc.h\"\n>  #include \"ctree.h\"\n>  #include \"disk-io.h\"\n> +#include \"extent_io.h\"\n>  #include \"transaction.h\"\n>  #include \"locking.h\"\n>  #include \"tree-log.h\"\n> @@ -688,6 +689,8 @@ start_transaction(struct btrfs_root *root, unsigned int num_items,\n>                 goto alloc_fail;\n>         }\n>\n> +       xa_init(&h->writeback_inhibited_ebs);\n> +\n>         /*\n>          * If we are JOIN_NOLOCK we're already committing a transaction and\n>          * waiting on this guy, so we don't need to do the sb_start_intwrite\n> @@ -1083,6 +1086,13 @@ static int __btrfs_end_transaction(struct btrfs_trans_handle *trans,\n>         if (trans->type & __TRANS_FREEZABLE)\n>                 sb_end_intwrite(info->sb);\n>\n> +       /*\n> +        * Uninhibit extent buffer writeback before decrementing num_writers,\n> +        * since the decrement wakes the committing thread which needs all\n> +        * buffers uninhibited to write them to disk.\n> +        */\n> +       btrfs_uninhibit_all_eb_writeback(trans);\n> +\n>         WARN_ON(cur_trans != info->running_transaction);\n>         WARN_ON(atomic_read(&cur_trans->num_writers) < 1);\n>         atomic_dec(&cur_trans->num_writers);\n> @@ -2110,6 +2120,7 @@ static void cleanup_transaction(struct btrfs_trans_handle *trans, int err)\n>         if (!test_bit(BTRFS_FS_RELOC_RUNNING, &fs_info->flags))\n>                 btrfs_scrub_cancel(fs_info);\n>\n> +       btrfs_uninhibit_all_eb_writeback(trans);\n>         kmem_cache_free(btrfs_trans_handle_cachep, trans);\n>  }\n>\n> @@ -2556,6 +2567,14 @@ int btrfs_commit_transaction(struct btrfs_trans_handle *trans)\n>             fs_info->cleaner_kthread)\n>                 wake_up_process(fs_info->cleaner_kthread);\n>\n> +       /*\n> +        * Uninhibit writeback on all extent buffers inhibited during this\n> +        * transaction before writing them to disk. Inhibiting prevented\n> +        * writeback while the transaction was building, but now we need\n> +        * them written.\n> +        */\n> +       btrfs_uninhibit_all_eb_writeback(trans);\n> +\n>         ret = btrfs_write_and_wait_transaction(trans);\n>         if (unlikely(ret)) {\n>                 btrfs_err(fs_info, \"error while writing out transaction: %d\", ret);\n> diff --git a/fs/btrfs/transaction.h b/fs/btrfs/transaction.h\n> index 18ef069197e5..f0d12c16d796 100644\n> --- a/fs/btrfs/transaction.h\n> +++ b/fs/btrfs/transaction.h\n> @@ -12,6 +12,7 @@\n>  #include <linux/time64.h>\n>  #include <linux/mutex.h>\n>  #include <linux/wait.h>\n> +#include <linux/xarray.h>\n>  #include \"btrfs_inode.h\"\n>  #include \"delayed-ref.h\"\n>\n> @@ -162,6 +163,7 @@ struct btrfs_trans_handle {\n>         struct btrfs_fs_info *fs_info;\n>         struct list_head new_bgs;\n>         struct btrfs_block_rsv delayed_rsv;\n> +       struct xarray writeback_inhibited_ebs;  /* ebs with writeback inhibited */\n>  };\n>\n>  /*\n> --\n> 2.47.3\n>\n>\n\n\n---\n\nOn Fri, Feb 13, 2026 at 8:37PM Leo Martins <loemra.dev@gmail.com> wrote:\n>\n> Add a btrfs_search_slot_stats tracepoint to btrfs_search_slot() for\n> measuring COW amplification.\n>\n> The tracepoint fires when a search with at least one COW completes,\n> recording the root, total cow_count, restart_count, and return value.\n> cow_count and restart_count per search_slot call are useful metrics\n> for tracking COW amplification.\n>\n> Signed-off-by: Leo Martins <loemra.dev@gmail.com>\n> ---\n>  fs/btrfs/ctree.c             | 15 +++++++++++++--\n>  include/trace/events/btrfs.h | 26 ++++++++++++++++++++++++++\n>  2 files changed, 39 insertions(+), 2 deletions(-)\n>\n> diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c\n> index 55187ba59cc0..1971d7bb5f60 100644\n> --- a/fs/btrfs/ctree.c\n> +++ b/fs/btrfs/ctree.c\n> @@ -2069,6 +2069,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n>         u8 lowest_level = 0;\n>         int min_write_lock_level;\n>         int prev_cmp;\n> +       int cow_count = 0;\n> +       int restart_count = 0;\n>\n>         if (!root)\n>                 return -EINVAL;\n> @@ -2157,6 +2159,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n>                             p->nodes[level + 1])) {\n>                                 write_lock_level = level + 1;\n>                                 btrfs_release_path(p);\n> +                               restart_count++;\n>                                 goto again;\n>                         }\n>\n> @@ -2172,6 +2175,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n>                                 ret = ret2;\n>                                 goto done;\n>                         }\n> +                       cow_count++;\n>                 }\n>  cow_done:\n>                 p->nodes[level] = b;\n> @@ -2219,8 +2223,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n>                 p->slots[level] = slot;\n>                 ret2 = setup_nodes_for_search(trans, root, p, b, level, ins_len,\n>                                               &write_lock_level);\n> -               if (ret2 == -EAGAIN)\n> +               if (ret2 == -EAGAIN) {\n> +                       restart_count++;\n>                         goto again;\n> +               }\n>                 if (ret2) {\n>                         ret = ret2;\n>                         goto done;\n> @@ -2236,6 +2242,7 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n>                 if (slot == 0 && ins_len && write_lock_level < level + 1) {\n>                         write_lock_level = level + 1;\n>                         btrfs_release_path(p);\n> +                       restart_count++;\n>                         goto again;\n>                 }\n>\n> @@ -2249,8 +2256,10 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n>                 }\n>\n>                 ret2 = read_block_for_search(root, p, &b, slot, key);\n> -               if (ret2 == -EAGAIN && !p->nowait)\n> +               if (ret2 == -EAGAIN && !p->nowait) {\n> +                       restart_count++;\n>                         goto again;\n> +               }\n>                 if (ret2) {\n>                         ret = ret2;\n>                         goto done;\n> @@ -2281,6 +2290,8 @@ int btrfs_search_slot(struct btrfs_trans_handle *trans, struct btrfs_root *root,\n>         }\n>         ret = 1;\n>  done:\n> +       if (cow_count > 0)\n> +               trace_btrfs_search_slot_stats(root, cow_count, restart_count, ret);\n\nSo I find this way too specific, plus even if trace points are\ndisabled we have the overhead of the counters (and inside critical\nsections).\n\nWe already have a tracepoint for COW, trace_btrfs_cow_block(), and we\ncould have one just for the retry thing, maybe naming it like\ntrace_btrfs_search_slot_restart() or something.\nSo we could use those two tracepoints to measure things (bpftrace\nscripts could easily report a count of each trace point and such),\ninstead of this highly specialized tracepoint that adds some overhead\nwhen tracepoints are disabled.\n\nThanks.\n\n\n>         if (ret < 0 && !p->skip_release_on_error)\n>                 btrfs_release_path(p);\n>\n> diff --git a/include/trace/events/btrfs.h b/include/trace/events/btrfs.h\n> index 125bdc166bfe..b8934938a087 100644\n> --- a/include/trace/events/btrfs.h\n> +++ b/include/trace/events/btrfs.h\n> @@ -1110,6 +1110,32 @@ TRACE_EVENT(btrfs_cow_block,\n>                   __entry->cow_level)\n>  );\n>\n> +TRACE_EVENT(btrfs_search_slot_stats,\n> +\n> +       TP_PROTO(const struct btrfs_root *root,\n> +                int cow_count, int restart_count, int ret),\n> +\n> +       TP_ARGS(root, cow_count, restart_count, ret),\n> +\n> +       TP_STRUCT__entry_btrfs(\n> +               __field(        u64,    root_objectid           )\n> +               __field(        int,    cow_count               )\n> +               __field(        int,    restart_count           )\n> +               __field(        int,    ret                     )\n> +       ),\n> +\n> +       TP_fast_assign_btrfs(root->fs_info,\n> +               __entry->root_objectid  = btrfs_root_id(root);\n> +               __entry->cow_count      = cow_count;\n> +               __entry->restart_count  = restart_count;\n> +               __entry->ret            = ret;\n> +       ),\n> +\n> +       TP_printk_btrfs(\"root=%llu(%s) cow_count=%d restarts=%d ret=%d\",\n> +                 show_root_type(__entry->root_objectid),\n> +                 __entry->cow_count, __entry->restart_count, __entry->ret)\n> +);\n> +\n>  TRACE_EVENT(btrfs_space_reservation,\n>\n>         TP_PROTO(const struct btrfs_fs_info *fs_info, const char *type, u64 val,\n> --\n> 2.47.3\n>\n>\n"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    }
  }
}