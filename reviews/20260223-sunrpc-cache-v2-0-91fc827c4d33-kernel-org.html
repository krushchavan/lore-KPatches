<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Review Comments: [PATCH v2 0/4] sunrpc: cache infrastructure scalability improvements</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto,
                         "Helvetica Neue", Arial, sans-serif;
            background: #f5f5f5;
            color: #333;
            line-height: 1.6;
            padding: 20px;
            max-width: 900px;
            margin: 0 auto;
        }
        .home-link { margin-bottom: 12px; display: block; }
        .home-link a { color: #0366d6; text-decoration: none; font-size: 0.9em; }
        .home-link a:hover { text-decoration: underline; }

        h1 { font-size: 1.3em; margin-bottom: 2px; color: #1a1a1a; line-height: 1.3; }

        .lore-link { font-size: 0.85em; margin: 4px 0 6px; display: block; }
        .lore-link a { color: #0366d6; text-decoration: none; }
        .lore-link a:hover { text-decoration: underline; }

        .date-range {
            font-size: 0.8em;
            color: #888;
            margin-bottom: 16px;
        }
        .date-range a { color: #0366d6; text-decoration: none; }
        .date-range a:hover { text-decoration: underline; }

        /* thread-node scroll margin so the card isn't clipped at the top */
        .thread-node { scroll-margin-top: 8px; }

        /* ── Patch summary ──────────────────────────────────────────── */
        .patch-summary-block {
            background: #fff;
            border-radius: 8px;
            padding: 12px 16px;
            margin-bottom: 20px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            border-left: 3px solid #4a90d9;
        }
        .patch-summary-label {
            font-size: 0.72em;
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 0.06em;
            color: #4a90d9;
            margin-bottom: 4px;
        }
        .patch-summary-text {
            font-size: 0.88em;
            color: #444;
            line-height: 1.55;
        }

        /* ── Thread tree ────────────────────────────────────────────── */
        .thread-tree {
            display: flex;
            flex-direction: column;
            gap: 6px;
        }

        /* Depth indentation via left border */
        .thread-node { position: relative; }
        .thread-children {
            margin-left: 20px;
            padding-left: 12px;
            border-left: 2px solid #e0e0e0;
            margin-top: 6px;
            display: flex;
            flex-direction: column;
            gap: 6px;
        }

        /* ── Review comment card ────────────────────────────────────── */
        .review-comment {
            background: #fff;
            border-radius: 6px;
            padding: 10px 14px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            font-size: 0.88em;
        }
        .review-comment-header {
            display: flex;
            flex-wrap: wrap;
            align-items: center;
            gap: 6px;
            margin-bottom: 5px;
        }
        .review-author {
            font-weight: 700;
            color: #1a1a1a;
            font-size: 0.95em;
        }

        /* Date chip — links back to the daily report */
        .date-chip {
            font-size: 0.75em;
            color: #777;
            background: #f0f0f0;
            border-radius: 10px;
            padding: 1px 7px;
            text-decoration: none;
            white-space: nowrap;
        }
        a.date-chip:hover { background: #e0e8f5; color: #0366d6; }

        .badge {
            display: inline-block;
            padding: 1px 8px;
            border-radius: 10px;
            font-size: 0.75em;
            font-weight: 600;
        }
        .inline-review-badge {
            display: inline-block;
            padding: 0 6px;
            border-radius: 8px;
            font-size: 0.78em;
            font-weight: 500;
            background: #e3f2fd;
            color: #1565c0;
        }
        .review-tag-badge {
            display: inline-block;
            padding: 0 6px;
            border-radius: 8px;
            font-size: 0.78em;
            font-weight: 500;
            background: #e8f5e9;
            color: #2e7d32;
        }
        .analysis-source-badge {
            display: inline-block;
            padding: 1px 7px;
            border-radius: 10px;
            font-size: 0.72em;
            font-weight: 600;
            border: 1px solid rgba(0,0,0,0.1);
        }

        .review-comment-text {
            color: #444;
            line-height: 1.55;
            margin-bottom: 4px;
        }
        .review-comment-signals {
            margin-top: 3px;
            font-size: 0.85em;
            color: #aaa;
            font-style: italic;
        }

        /* ── Collapsible raw body ───────────────────────────────────── */
        .raw-body-toggle {
            margin-top: 5px;
            font-size: 0.85em;
        }
        .raw-body-toggle summary {
            cursor: pointer;
            color: #888;
            padding: 2px 0;
            font-weight: 500;
            font-size: 0.9em;
            list-style: none;
        }
        .raw-body-toggle summary::-webkit-details-marker { display: none; }
        .raw-body-toggle summary::before { content: "▶ "; font-size: 0.7em; }
        .raw-body-toggle[open] summary::before { content: "▼ "; }
        .raw-body-toggle summary:hover { color: #555; }
        .raw-body-text {
            white-space: pre-wrap;
            font-size: 0.95em;
            background: #f8f8f8;
            padding: 8px 10px;
            border-radius: 4px;
            max-height: 360px;
            overflow-y: auto;
            margin-top: 4px;
            line-height: 1.5;
            color: #444;
            border: 1px solid #e8e8e8;
        }

        .no-reviews {
            color: #aaa;
            font-size: 0.85em;
            font-style: italic;
            padding: 8px 0;
        }

        footer {
            text-align: center;
            color: #bbb;
            font-size: 0.78em;
            margin-top: 36px;
            padding: 16px;
        }
    </style>
</head>
<body>
    <div class="home-link"><a href="../">&larr; Back to reports</a></div>
    <h1>[PATCH v2 0/4] sunrpc: cache infrastructure scalability improvements</h1>
    <div class="lore-link"><a href="https://lore.kernel.org/all/20260223-sunrpc-cache-v2-0-91fc827c4d33@kernel.org/" target="_blank">View on lore.kernel.org &rarr;</a></div>
    <div class="date-range">Active on: <a href="#2026-02-23">2026-02-23</a></div>
    <div class="patch-summary-block"><div class="patch-summary-label">Patch summary</div><div class="patch-summary-text">This patch series improves the scalability of the sunrpc cache infrastructure by converting a global spinlock and waitqueue to per-cache-detail locks, simplifying the code and reducing contention. It also fixes a pre-existing bug that could cause cache requests to leak, and introduces a new sequence number to help readers track their position in the queue.</div></div>
    <div class="thread-tree">
<div class="thread-node depth-0" id="2026-02-23">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Jeff Layton (author)</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-23">2026-02-23</a>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed a concern about cache_request leak in cache_release() by adding cleanup logic to decrement readers and check if it reached 0 with CACHE_PENDING clear, then dequeue and free the cache_request. A fix is planned for v2.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">When a reader&#x27;s file descriptor is closed while in the middle of reading
a cache_request (rp-&gt;offset != 0), cache_release() decrements the
request&#x27;s readers count but never checks whether it should free the
request.

In cache_read(), when readers drops to 0 and CACHE_PENDING is clear, the
cache_request is removed from the queue and freed along with its buffer
and cache_head reference. cache_release() lacks this cleanup.

The only other path that frees requests with readers == 0 is
cache_dequeue(), but it runs only when CACHE_PENDING transitions from
set to clear. If that transition already happened while readers was
still non-zero, cache_dequeue() will have skipped the request, and no
subsequent call will clean it up.

Add the same cleanup logic from cache_read() to cache_release(): after
decrementing readers, check if it reached 0 with CACHE_PENDING clear,
and if so, dequeue and free the cache_request.

Reported-by: NeilBrown &lt;neilb@ownmail.net&gt;
Signed-off-by: Jeff Layton &lt;jlayton@kernel.org&gt;
---
 net/sunrpc/cache.c | 26 +++++++++++++++++++++-----
 1 file changed, 21 insertions(+), 5 deletions(-)

diff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c
index b82f7cde0c9be6071ee4040150672872e548161d..86b3fd5a429d77f7f917f398a02cb7a5ff8dd1e0 100644
--- a/net/sunrpc/cache.c
+++ b/net/sunrpc/cache.c
@@ -1062,14 +1062,25 @@ static int cache_release(struct inode *inode, struct file *filp,
 	struct cache_reader *rp = filp-&gt;private_data;
 
 	if (rp) {
+		struct cache_request *rq = NULL;
+
 		spin_lock(&amp;queue_lock);
 		if (rp-&gt;offset) {
 			struct cache_queue *cq;
-			for (cq= &amp;rp-&gt;q; &amp;cq-&gt;list != &amp;cd-&gt;queue;
-			     cq = list_entry(cq-&gt;list.next, struct cache_queue, list))
+			for (cq = &amp;rp-&gt;q; &amp;cq-&gt;list != &amp;cd-&gt;queue;
+			     cq = list_entry(cq-&gt;list.next,
+					     struct cache_queue, list))
 				if (!cq-&gt;reader) {
-					container_of(cq, struct cache_request, q)
-						-&gt;readers--;
+					struct cache_request *cr =
+						container_of(cq,
+						struct cache_request, q);
+					cr-&gt;readers--;
+					if (cr-&gt;readers == 0 &amp;&amp;
+					    !test_bit(CACHE_PENDING,
+						      &amp;cr-&gt;item-&gt;flags)) {
+						list_del(&amp;cr-&gt;q.list);
+						rq = cr;
+					}
 					break;
 				}
 			rp-&gt;offset = 0;
@@ -1077,9 +1088,14 @@ static int cache_release(struct inode *inode, struct file *filp,
 		list_del(&amp;rp-&gt;q.list);
 		spin_unlock(&amp;queue_lock);
 
+		if (rq) {
+			cache_put(rq-&gt;item, cd);
+			kfree(rq-&gt;buf);
+			kfree(rq);
+		}
+
 		filp-&gt;private_data = NULL;
 		kfree(rp);
-
 	}
 	if (filp-&gt;f_mode &amp; FMODE_WRITE) {
 		atomic_dec(&amp;cd-&gt;writers);

-- 
2.53.0</pre>
</details>
<div class="review-comment-signals">Signals: fix_planned</div>
</div>
<div class="thread-children">
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Chuck Lever</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-23">2026-02-23</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">reviewer noted that the patch does not address the issue of cache_detail-&gt;queue being accessed concurrently by multiple threads, and requested a lock be added to protect this access

reviewer noted that the per-cache-detail lock ordering is still a concern, specifically when vswap_free() acquires the spinlock while holding the folio lock, and requested further investigation into this issue</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">From: Chuck Lever &lt;chuck.lever@oracle.com&gt;

On Mon, 23 Feb 2026 12:09:57 -0500, Jeff Layton wrote:

---

Applied to nfsd-testing, replacing v1. Thanks!

[1/4] sunrpc: fix cache_request leak in cache_release
      commit: dad5f78046759eb5c95970198eb9865550eb6227
[2/4] sunrpc: convert queue_lock from global spinlock to per-cache-detail lock
      commit: c94ad34b7ecd5928cf3fdb6ea4fcf6ef55765e97
[3/4] sunrpc: convert queue_wait from global to per-cache-detail waitqueue
      commit: 951696964e9c370a5f91d5e3e136d39aa08d912c
[4/4] sunrpc: split cache_detail queue into request and reader lists
      commit: 3557b9c71039b2435b383fc57283a0b847b40144

--
Chuck Lever</pre>
</details>
<div class="review-comment-signals">Signals: concurrency, thread safety, requested changes</div>
</div>
</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Jeff Layton (author)</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-23">2026-02-23</a>
<span class="badge" style="color:#155724;background:#d4edda">Positive</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed a concern about the global queue_lock serializing upcall queue operations across all cache_detail instances. They agreed to convert it to a per-cache-detail spinlock so that different caches no longer contend with each other on queue operations.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">The global queue_lock serializes all upcall queue operations across
every cache_detail instance. Convert it to a per-cache-detail spinlock
so that different caches (e.g. auth.unix.ip vs nfsd.fh) no longer
contend with each other on queue operations.

Signed-off-by: Jeff Layton &lt;jlayton@kernel.org&gt;
---
 include/linux/sunrpc/cache.h |  1 +
 net/sunrpc/cache.c           | 47 ++++++++++++++++++++++----------------------
 2 files changed, 24 insertions(+), 24 deletions(-)

diff --git a/include/linux/sunrpc/cache.h b/include/linux/sunrpc/cache.h
index e783132e481ff2593fdc5d323f7b3a08f85d4cd8..3d32dd1f7b05d35562d2064fed69877b3950fb51 100644
--- a/include/linux/sunrpc/cache.h
+++ b/include/linux/sunrpc/cache.h
@@ -113,6 +113,7 @@ struct cache_detail {
 
 	/* fields for communication over channel */
 	struct list_head	queue;
+	spinlock_t		queue_lock;
 
 	atomic_t		writers;		/* how many time is /channel open */
 	time64_t		last_close;		/* if no writers, when did last close */
diff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c
index 86b3fd5a429d77f7f917f398a02cb7a5ff8dd1e0..1cfaae488c6c67a9797511804e4bbba16bcc70ae 100644
--- a/net/sunrpc/cache.c
+++ b/net/sunrpc/cache.c
@@ -400,6 +400,7 @@ void sunrpc_init_cache_detail(struct cache_detail *cd)
 {
 	spin_lock_init(&amp;cd-&gt;hash_lock);
 	INIT_LIST_HEAD(&amp;cd-&gt;queue);
+	spin_lock_init(&amp;cd-&gt;queue_lock);
 	spin_lock(&amp;cache_list_lock);
 	cd-&gt;nextcheck = 0;
 	cd-&gt;entries = 0;
@@ -803,8 +804,6 @@ void cache_clean_deferred(void *owner)
  *
  */
 
-static DEFINE_SPINLOCK(queue_lock);
-
 struct cache_queue {
 	struct list_head	list;
 	int			reader;	/* if 0, then request */
@@ -847,7 +846,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,
 	inode_lock(inode); /* protect against multiple concurrent
 			      * readers on this file */
  again:
-	spin_lock(&amp;queue_lock);
+	spin_lock(&amp;cd-&gt;queue_lock);
 	/* need to find next request */
 	while (rp-&gt;q.list.next != &amp;cd-&gt;queue &amp;&amp;
 	       list_entry(rp-&gt;q.list.next, struct cache_queue, list)
@@ -856,7 +855,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,
 		list_move(&amp;rp-&gt;q.list, next);
 	}
 	if (rp-&gt;q.list.next == &amp;cd-&gt;queue) {
-		spin_unlock(&amp;queue_lock);
+		spin_unlock(&amp;cd-&gt;queue_lock);
 		inode_unlock(inode);
 		WARN_ON_ONCE(rp-&gt;offset);
 		return 0;
@@ -865,7 +864,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,
 	WARN_ON_ONCE(rq-&gt;q.reader);
 	if (rp-&gt;offset == 0)
 		rq-&gt;readers++;
-	spin_unlock(&amp;queue_lock);
+	spin_unlock(&amp;cd-&gt;queue_lock);
 
 	if (rq-&gt;len == 0) {
 		err = cache_request(cd, rq);
@@ -876,9 +875,9 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,
 
 	if (rp-&gt;offset == 0 &amp;&amp; !test_bit(CACHE_PENDING, &amp;rq-&gt;item-&gt;flags)) {
 		err = -EAGAIN;
-		spin_lock(&amp;queue_lock);
+		spin_lock(&amp;cd-&gt;queue_lock);
 		list_move(&amp;rp-&gt;q.list, &amp;rq-&gt;q.list);
-		spin_unlock(&amp;queue_lock);
+		spin_unlock(&amp;cd-&gt;queue_lock);
 	} else {
 		if (rp-&gt;offset + count &gt; rq-&gt;len)
 			count = rq-&gt;len - rp-&gt;offset;
@@ -888,26 +887,26 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,
 		rp-&gt;offset += count;
 		if (rp-&gt;offset &gt;= rq-&gt;len) {
 			rp-&gt;offset = 0;
-			spin_lock(&amp;queue_lock);
+			spin_lock(&amp;cd-&gt;queue_lock);
 			list_move(&amp;rp-&gt;q.list, &amp;rq-&gt;q.list);
-			spin_unlock(&amp;queue_lock);
+			spin_unlock(&amp;cd-&gt;queue_lock);
 		}
 		err = 0;
 	}
  out:
 	if (rp-&gt;offset == 0) {
 		/* need to release rq */
-		spin_lock(&amp;queue_lock);
+		spin_lock(&amp;cd-&gt;queue_lock);
 		rq-&gt;readers--;
 		if (rq-&gt;readers == 0 &amp;&amp;
 		    !test_bit(CACHE_PENDING, &amp;rq-&gt;item-&gt;flags)) {
 			list_del(&amp;rq-&gt;q.list);
-			spin_unlock(&amp;queue_lock);
+			spin_unlock(&amp;cd-&gt;queue_lock);
 			cache_put(rq-&gt;item, cd);
 			kfree(rq-&gt;buf);
 			kfree(rq);
 		} else
-			spin_unlock(&amp;queue_lock);
+			spin_unlock(&amp;cd-&gt;queue_lock);
 	}
 	if (err == -EAGAIN)
 		goto again;
@@ -988,7 +987,7 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,
 	if (!rp)
 		return mask;
 
-	spin_lock(&amp;queue_lock);
+	spin_lock(&amp;cd-&gt;queue_lock);
 
 	for (cq= &amp;rp-&gt;q; &amp;cq-&gt;list != &amp;cd-&gt;queue;
 	     cq = list_entry(cq-&gt;list.next, struct cache_queue, list))
@@ -996,7 +995,7 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,
 			mask |= EPOLLIN | EPOLLRDNORM;
 			break;
 		}
-	spin_unlock(&amp;queue_lock);
+	spin_unlock(&amp;cd-&gt;queue_lock);
 	return mask;
 }
 
@@ -1011,7 +1010,7 @@ static int cache_ioctl(struct inode *ino, struct file *filp,
 	if (cmd != FIONREAD || !rp)
 		return -EINVAL;
 
-	spin_lock(&amp;queue_lock);
+	spin_lock(&amp;cd-&gt;queue_lock);
 
 	/* only find the length remaining in current request,
 	 * or the length of the next request
@@ -1024,7 +1023,7 @@ static int cache_ioctl(struct inode *ino, struct file *filp,
 			len = cr-&gt;len - rp-&gt;offset;
 			break;
 		}
-	spin_unlock(&amp;queue_lock);
+	spin_unlock(&amp;cd-&gt;queue_lock);
 
 	return put_user(len, (int __user *)arg);
 }
@@ -1046,9 +1045,9 @@ static int cache_open(struct inode *inode, struct file *filp,
 		rp-&gt;offset = 0;
 		rp-&gt;q.reader = 1;
 
-		spin_lock(&amp;queue_lock);
+		spin_lock(&amp;cd-&gt;queue_lock);
 		list_add(&amp;rp-&gt;q.list, &amp;cd-&gt;queue);
-		spin_unlock(&amp;queue_lock);
+		spin_unlock(&amp;cd-&gt;queue_lock);
 	}
 	if (filp-&gt;f_mode &amp; FMODE_WRITE)
 		atomic_inc(&amp;cd-&gt;writers);
@@ -1064,7 +1063,7 @@ static int cache_release(struct inode *inode, struct file *filp,
 	if (rp) {
 		struct cache_request *rq = NULL;
 
-		spin_lock(&amp;queue_lock);
+		spin_lock(&amp;cd-&gt;queue_lock);
 		if (rp-&gt;offset) {
 			struct cache_queue *cq;
 			for (cq = &amp;rp-&gt;q; &amp;cq-&gt;list != &amp;cd-&gt;queue;
@@ -1086,7 +1085,7 @@ static int cache_release(struct inode *inode, struct file *filp,
 			rp-&gt;offset = 0;
 		}
 		list_del(&amp;rp-&gt;q.list);
-		spin_unlock(&amp;queue_lock);
+		spin_unlock(&amp;cd-&gt;queue_lock);
 
 		if (rq) {
 			cache_put(rq-&gt;item, cd);
@@ -1113,7 +1112,7 @@ static void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)
 	struct cache_request *cr;
 	LIST_HEAD(dequeued);
 
-	spin_lock(&amp;queue_lock);
+	spin_lock(&amp;detail-&gt;queue_lock);
 	list_for_each_entry_safe(cq, tmp, &amp;detail-&gt;queue, list)
 		if (!cq-&gt;reader) {
 			cr = container_of(cq, struct cache_request, q);
@@ -1126,7 +1125,7 @@ static void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)
 				continue;
 			list_move(&amp;cr-&gt;q.list, &amp;dequeued);
 		}
-	spin_unlock(&amp;queue_lock);
+	spin_unlock(&amp;detail-&gt;queue_lock);
 	while (!list_empty(&amp;dequeued)) {
 		cr = list_entry(dequeued.next, struct cache_request, q.list);
 		list_del(&amp;cr-&gt;q.list);
@@ -1251,7 +1250,7 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)
 	crq-&gt;buf = buf;
 	crq-&gt;len = 0;
 	crq-&gt;readers = 0;
-	spin_lock(&amp;queue_lock);
+	spin_lock(&amp;detail-&gt;queue_lock);
 	if (test_bit(CACHE_PENDING, &amp;h-&gt;flags)) {
 		crq-&gt;item = cache_get(h);
 		list_add_tail(&amp;crq-&gt;q.list, &amp;detail-&gt;queue);
@@ -1259,7 +1258,7 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)
 	} else
 		/* Lost a race, no longer PENDING, so don&#x27;t enqueue */
 		ret = -EAGAIN;
-	spin_unlock(&amp;queue_lock);
+	spin_unlock(&amp;detail-&gt;queue_lock);
 	wake_up(&amp;queue_wait);
 	if (ret == -EAGAIN) {
 		kfree(buf);

-- 
2.53.0</pre>
</details>
<div class="review-comment-signals">Signals: acknowledged fix needed, agreed to restructure</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Jeff Layton (author)</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-23">2026-02-23</a>
<span class="badge" style="color:#155724;background:#d4edda">Positive</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author is addressing a concern about the queue_wait waitqueue being global, which wakes pollers on all caches when one cache_detail is woken. The author agrees that this is an issue and has converted it to a per-cache-detail field so only relevant pollers are woken.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">The queue_wait waitqueue is currently a file-scoped global, so a
wake_up for one cache_detail wakes pollers on all caches. Convert it
to a per-cache-detail field so that only pollers on the relevant cache
are woken.

Signed-off-by: Jeff Layton &lt;jlayton@kernel.org&gt;
---
 include/linux/sunrpc/cache.h | 2 ++
 net/sunrpc/cache.c           | 7 +++----
 2 files changed, 5 insertions(+), 4 deletions(-)

diff --git a/include/linux/sunrpc/cache.h b/include/linux/sunrpc/cache.h
index 3d32dd1f7b05d35562d2064fed69877b3950fb51..031379efba24d40f64ce346cf1032261d4b98d05 100644
--- a/include/linux/sunrpc/cache.h
+++ b/include/linux/sunrpc/cache.h
@@ -16,6 +16,7 @@
 #include &lt;linux/atomic.h&gt;
 #include &lt;linux/kstrtox.h&gt;
 #include &lt;linux/proc_fs.h&gt;
+#include &lt;linux/wait.h&gt;
 
 /*
  * Each cache requires:
@@ -114,6 +115,7 @@ struct cache_detail {
 	/* fields for communication over channel */
 	struct list_head	queue;
 	spinlock_t		queue_lock;
+	wait_queue_head_t	queue_wait;
 
 	atomic_t		writers;		/* how many time is /channel open */
 	time64_t		last_close;		/* if no writers, when did last close */
diff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c
index 1cfaae488c6c67a9797511804e4bbba16bcc70ae..fd02dca1f07afec2f09c591037bac3ea3e8d7e17 100644
--- a/net/sunrpc/cache.c
+++ b/net/sunrpc/cache.c
@@ -401,6 +401,7 @@ void sunrpc_init_cache_detail(struct cache_detail *cd)
 	spin_lock_init(&amp;cd-&gt;hash_lock);
 	INIT_LIST_HEAD(&amp;cd-&gt;queue);
 	spin_lock_init(&amp;cd-&gt;queue_lock);
+	init_waitqueue_head(&amp;cd-&gt;queue_wait);
 	spin_lock(&amp;cache_list_lock);
 	cd-&gt;nextcheck = 0;
 	cd-&gt;entries = 0;
@@ -970,8 +971,6 @@ static ssize_t cache_write(struct file *filp, const char __user *buf,
 	return ret;
 }
 
-static DECLARE_WAIT_QUEUE_HEAD(queue_wait);
-
 static __poll_t cache_poll(struct file *filp, poll_table *wait,
 			       struct cache_detail *cd)
 {
@@ -979,7 +978,7 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,
 	struct cache_reader *rp = filp-&gt;private_data;
 	struct cache_queue *cq;
 
-	poll_wait(filp, &amp;queue_wait, wait);
+	poll_wait(filp, &amp;cd-&gt;queue_wait, wait);
 
 	/* alway allow write */
 	mask = EPOLLOUT | EPOLLWRNORM;
@@ -1259,7 +1258,7 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)
 		/* Lost a race, no longer PENDING, so don&#x27;t enqueue */
 		ret = -EAGAIN;
 	spin_unlock(&amp;detail-&gt;queue_lock);
-	wake_up(&amp;queue_wait);
+	wake_up(&amp;detail-&gt;queue_wait);
 	if (ret == -EAGAIN) {
 		kfree(buf);
 		kfree(crq);

-- 
2.53.0</pre>
</details>
<div class="review-comment-signals">Signals: acknowledged the problem, provided a fix</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Jeff Layton (author)</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-23">2026-02-23</a>
<span class="badge" style="color:#155724;background:#d4edda">Positive</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed a concern about the complexity of the reader-skipping loops in cache_read, cache_poll, and cache_ioctl by replacing the single interleaved queue with two dedicated lists for upcall requests and open file handles. The readers now track their position via a monotonically increasing sequence number (next_seqno) rather than by their position in the shared list. A new helper function cache_next_request() finds the next request at or after a given seqno, eliminating the need for the cache_queue wrapper struct.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Replace the single interleaved queue (which mixed cache_request and
cache_reader entries distinguished by a -&gt;reader flag) with two
dedicated lists: cd-&gt;requests for upcall requests and cd-&gt;readers
for open file handles.

Readers now track their position via a monotonically increasing
sequence number (next_seqno) rather than by their position in the
shared list. Each cache_request is assigned a seqno when enqueued,
and a new cache_next_request() helper finds the next request at or
after a given seqno.

This eliminates the cache_queue wrapper struct entirely, simplifies
the reader-skipping loops in cache_read/cache_poll/cache_ioctl/
cache_release, and makes the data flow easier to reason about.

Signed-off-by: Jeff Layton &lt;jlayton@kernel.org&gt;
---
 include/linux/sunrpc/cache.h |   4 +-
 net/sunrpc/cache.c           | 143 ++++++++++++++++++-------------------------
 2 files changed, 62 insertions(+), 85 deletions(-)

diff --git a/include/linux/sunrpc/cache.h b/include/linux/sunrpc/cache.h
index 031379efba24d40f64ce346cf1032261d4b98d05..b1e595c2615bd4be4d9ad19f71a8f4d08bd74a9b 100644
--- a/include/linux/sunrpc/cache.h
+++ b/include/linux/sunrpc/cache.h
@@ -113,9 +113,11 @@ struct cache_detail {
 	int			entries;
 
 	/* fields for communication over channel */
-	struct list_head	queue;
+	struct list_head	requests;
+	struct list_head	readers;
 	spinlock_t		queue_lock;
 	wait_queue_head_t	queue_wait;
+	u64			next_seqno;
 
 	atomic_t		writers;		/* how many time is /channel open */
 	time64_t		last_close;		/* if no writers, when did last close */
diff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c
index fd02dca1f07afec2f09c591037bac3ea3e8d7e17..7081c1214e6c3226f8ac82c8bc7ff6c36f598744 100644
--- a/net/sunrpc/cache.c
+++ b/net/sunrpc/cache.c
@@ -399,9 +399,11 @@ static struct delayed_work cache_cleaner;
 void sunrpc_init_cache_detail(struct cache_detail *cd)
 {
 	spin_lock_init(&amp;cd-&gt;hash_lock);
-	INIT_LIST_HEAD(&amp;cd-&gt;queue);
+	INIT_LIST_HEAD(&amp;cd-&gt;requests);
+	INIT_LIST_HEAD(&amp;cd-&gt;readers);
 	spin_lock_init(&amp;cd-&gt;queue_lock);
 	init_waitqueue_head(&amp;cd-&gt;queue_wait);
+	cd-&gt;next_seqno = 0;
 	spin_lock(&amp;cache_list_lock);
 	cd-&gt;nextcheck = 0;
 	cd-&gt;entries = 0;
@@ -796,29 +798,20 @@ void cache_clean_deferred(void *owner)
  * On read, you get a full request, or block.
  * On write, an update request is processed.
  * Poll works if anything to read, and always allows write.
- *
- * Implemented by linked list of requests.  Each open file has
- * a -&gt;private that also exists in this list.  New requests are added
- * to the end and may wakeup and preceding readers.
- * New readers are added to the head.  If, on read, an item is found with
- * CACHE_UPCALLING clear, we free it from the list.
- *
  */
 
-struct cache_queue {
-	struct list_head	list;
-	int			reader;	/* if 0, then request */
-};
 struct cache_request {
-	struct cache_queue	q;
+	struct list_head	list;
 	struct cache_head	*item;
-	char			* buf;
+	char			*buf;
 	int			len;
 	int			readers;
+	u64			seqno;
 };
 struct cache_reader {
-	struct cache_queue	q;
+	struct list_head	list;
 	int			offset;	/* if non-0, we have a refcnt on next request */
+	u64			next_seqno;
 };
 
 static int cache_request(struct cache_detail *detail,
@@ -833,6 +826,17 @@ static int cache_request(struct cache_detail *detail,
 	return PAGE_SIZE - len;
 }
 
+static struct cache_request *
+cache_next_request(struct cache_detail *cd, u64 seqno)
+{
+	struct cache_request *rq;
+
+	list_for_each_entry(rq, &amp;cd-&gt;requests, list)
+		if (rq-&gt;seqno &gt;= seqno)
+			return rq;
+	return NULL;
+}
+
 static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,
 			  loff_t *ppos, struct cache_detail *cd)
 {
@@ -849,20 +853,13 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,
  again:
 	spin_lock(&amp;cd-&gt;queue_lock);
 	/* need to find next request */
-	while (rp-&gt;q.list.next != &amp;cd-&gt;queue &amp;&amp;
-	       list_entry(rp-&gt;q.list.next, struct cache_queue, list)
-	       -&gt;reader) {
-		struct list_head *next = rp-&gt;q.list.next;
-		list_move(&amp;rp-&gt;q.list, next);
-	}
-	if (rp-&gt;q.list.next == &amp;cd-&gt;queue) {
+	rq = cache_next_request(cd, rp-&gt;next_seqno);
+	if (!rq) {
 		spin_unlock(&amp;cd-&gt;queue_lock);
 		inode_unlock(inode);
 		WARN_ON_ONCE(rp-&gt;offset);
 		return 0;
 	}
-	rq = container_of(rp-&gt;q.list.next, struct cache_request, q.list);
-	WARN_ON_ONCE(rq-&gt;q.reader);
 	if (rp-&gt;offset == 0)
 		rq-&gt;readers++;
 	spin_unlock(&amp;cd-&gt;queue_lock);
@@ -876,9 +873,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,
 
 	if (rp-&gt;offset == 0 &amp;&amp; !test_bit(CACHE_PENDING, &amp;rq-&gt;item-&gt;flags)) {
 		err = -EAGAIN;
-		spin_lock(&amp;cd-&gt;queue_lock);
-		list_move(&amp;rp-&gt;q.list, &amp;rq-&gt;q.list);
-		spin_unlock(&amp;cd-&gt;queue_lock);
+		rp-&gt;next_seqno = rq-&gt;seqno + 1;
 	} else {
 		if (rp-&gt;offset + count &gt; rq-&gt;len)
 			count = rq-&gt;len - rp-&gt;offset;
@@ -888,9 +883,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,
 		rp-&gt;offset += count;
 		if (rp-&gt;offset &gt;= rq-&gt;len) {
 			rp-&gt;offset = 0;
-			spin_lock(&amp;cd-&gt;queue_lock);
-			list_move(&amp;rp-&gt;q.list, &amp;rq-&gt;q.list);
-			spin_unlock(&amp;cd-&gt;queue_lock);
+			rp-&gt;next_seqno = rq-&gt;seqno + 1;
 		}
 		err = 0;
 	}
@@ -901,7 +894,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,
 		rq-&gt;readers--;
 		if (rq-&gt;readers == 0 &amp;&amp;
 		    !test_bit(CACHE_PENDING, &amp;rq-&gt;item-&gt;flags)) {
-			list_del(&amp;rq-&gt;q.list);
+			list_del(&amp;rq-&gt;list);
 			spin_unlock(&amp;cd-&gt;queue_lock);
 			cache_put(rq-&gt;item, cd);
 			kfree(rq-&gt;buf);
@@ -976,7 +969,6 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,
 {
 	__poll_t mask;
 	struct cache_reader *rp = filp-&gt;private_data;
-	struct cache_queue *cq;
 
 	poll_wait(filp, &amp;cd-&gt;queue_wait, wait);
 
@@ -988,12 +980,8 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,
 
 	spin_lock(&amp;cd-&gt;queue_lock);
 
-	for (cq= &amp;rp-&gt;q; &amp;cq-&gt;list != &amp;cd-&gt;queue;
-	     cq = list_entry(cq-&gt;list.next, struct cache_queue, list))
-		if (!cq-&gt;reader) {
-			mask |= EPOLLIN | EPOLLRDNORM;
-			break;
-		}
+	if (cache_next_request(cd, rp-&gt;next_seqno))
+		mask |= EPOLLIN | EPOLLRDNORM;
 	spin_unlock(&amp;cd-&gt;queue_lock);
 	return mask;
 }
@@ -1004,7 +992,7 @@ static int cache_ioctl(struct inode *ino, struct file *filp,
 {
 	int len = 0;
 	struct cache_reader *rp = filp-&gt;private_data;
-	struct cache_queue *cq;
+	struct cache_request *rq;
 
 	if (cmd != FIONREAD || !rp)
 		return -EINVAL;
@@ -1014,14 +1002,9 @@ static int cache_ioctl(struct inode *ino, struct file *filp,
 	/* only find the length remaining in current request,
 	 * or the length of the next request
 	 */
-	for (cq= &amp;rp-&gt;q; &amp;cq-&gt;list != &amp;cd-&gt;queue;
-	     cq = list_entry(cq-&gt;list.next, struct cache_queue, list))
-		if (!cq-&gt;reader) {
-			struct cache_request *cr =
-				container_of(cq, struct cache_request, q);
-			len = cr-&gt;len - rp-&gt;offset;
-			break;
-		}
+	rq = cache_next_request(cd, rp-&gt;next_seqno);
+	if (rq)
+		len = rq-&gt;len - rp-&gt;offset;
 	spin_unlock(&amp;cd-&gt;queue_lock);
 
 	return put_user(len, (int __user *)arg);
@@ -1042,10 +1025,10 @@ static int cache_open(struct inode *inode, struct file *filp,
 			return -ENOMEM;
 		}
 		rp-&gt;offset = 0;
-		rp-&gt;q.reader = 1;
+		rp-&gt;next_seqno = 0;
 
 		spin_lock(&amp;cd-&gt;queue_lock);
-		list_add(&amp;rp-&gt;q.list, &amp;cd-&gt;queue);
+		list_add(&amp;rp-&gt;list, &amp;cd-&gt;readers);
 		spin_unlock(&amp;cd-&gt;queue_lock);
 	}
 	if (filp-&gt;f_mode &amp; FMODE_WRITE)
@@ -1064,26 +1047,21 @@ static int cache_release(struct inode *inode, struct file *filp,
 
 		spin_lock(&amp;cd-&gt;queue_lock);
 		if (rp-&gt;offset) {
-			struct cache_queue *cq;
-			for (cq = &amp;rp-&gt;q; &amp;cq-&gt;list != &amp;cd-&gt;queue;
-			     cq = list_entry(cq-&gt;list.next,
-					     struct cache_queue, list))
-				if (!cq-&gt;reader) {
-					struct cache_request *cr =
-						container_of(cq,
-						struct cache_request, q);
-					cr-&gt;readers--;
-					if (cr-&gt;readers == 0 &amp;&amp;
-					    !test_bit(CACHE_PENDING,
-						      &amp;cr-&gt;item-&gt;flags)) {
-						list_del(&amp;cr-&gt;q.list);
-						rq = cr;
-					}
-					break;
+			struct cache_request *cr;
+
+			cr = cache_next_request(cd, rp-&gt;next_seqno);
+			if (cr) {
+				cr-&gt;readers--;
+				if (cr-&gt;readers == 0 &amp;&amp;
+				    !test_bit(CACHE_PENDING,
+					      &amp;cr-&gt;item-&gt;flags)) {
+					list_del(&amp;cr-&gt;list);
+					rq = cr;
 				}
+			}
 			rp-&gt;offset = 0;
 		}
-		list_del(&amp;rp-&gt;q.list);
+		list_del(&amp;rp-&gt;list);
 		spin_unlock(&amp;cd-&gt;queue_lock);
 
 		if (rq) {
@@ -1107,27 +1085,24 @@ static int cache_release(struct inode *inode, struct file *filp,
 
 static void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)
 {
-	struct cache_queue *cq, *tmp;
-	struct cache_request *cr;
+	struct cache_request *cr, *tmp;
 	LIST_HEAD(dequeued);
 
 	spin_lock(&amp;detail-&gt;queue_lock);
-	list_for_each_entry_safe(cq, tmp, &amp;detail-&gt;queue, list)
-		if (!cq-&gt;reader) {
-			cr = container_of(cq, struct cache_request, q);
-			if (cr-&gt;item != ch)
-				continue;
-			if (test_bit(CACHE_PENDING, &amp;ch-&gt;flags))
-				/* Lost a race and it is pending again */
-				break;
-			if (cr-&gt;readers != 0)
-				continue;
-			list_move(&amp;cr-&gt;q.list, &amp;dequeued);
-		}
+	list_for_each_entry_safe(cr, tmp, &amp;detail-&gt;requests, list) {
+		if (cr-&gt;item != ch)
+			continue;
+		if (test_bit(CACHE_PENDING, &amp;ch-&gt;flags))
+			/* Lost a race and it is pending again */
+			break;
+		if (cr-&gt;readers != 0)
+			continue;
+		list_move(&amp;cr-&gt;list, &amp;dequeued);
+	}
 	spin_unlock(&amp;detail-&gt;queue_lock);
 	while (!list_empty(&amp;dequeued)) {
-		cr = list_entry(dequeued.next, struct cache_request, q.list);
-		list_del(&amp;cr-&gt;q.list);
+		cr = list_entry(dequeued.next, struct cache_request, list);
+		list_del(&amp;cr-&gt;list);
 		cache_put(cr-&gt;item, detail);
 		kfree(cr-&gt;buf);
 		kfree(cr);
@@ -1245,14 +1220,14 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)
 		return -EAGAIN;
 	}
 
-	crq-&gt;q.reader = 0;
 	crq-&gt;buf = buf;
 	crq-&gt;len = 0;
 	crq-&gt;readers = 0;
 	spin_lock(&amp;detail-&gt;queue_lock);
 	if (test_bit(CACHE_PENDING, &amp;h-&gt;flags)) {
 		crq-&gt;item = cache_get(h);
-		list_add_tail(&amp;crq-&gt;q.list, &amp;detail-&gt;queue);
+		crq-&gt;seqno = detail-&gt;next_seqno++;
+		list_add_tail(&amp;crq-&gt;list, &amp;detail-&gt;requests);
 		trace_cache_entry_upcall(detail, h);
 	} else
 		/* Lost a race, no longer PENDING, so don&#x27;t enqueue */

-- 
2.53.0</pre>
</details>
<div class="review-comment-signals">Signals: acknowledged, agreed</div>
</div>
</div>
</div>

    <footer>LKML Daily Activity Tracker</footer>
    <script>
    // When arriving via a date anchor (e.g. #2026-02-15 from a daily report),
    // scroll the anchor into view after a brief delay so layout is complete.
    (function () {
        var hash = window.location.hash;
        if (!hash) return;
        var target = document.getElementById(hash.slice(1));
        if (!target) return;
        setTimeout(function () {
            target.scrollIntoView({behavior: 'smooth', block: 'start'});
        }, 80);
    })();
    </script>
</body>
</html>