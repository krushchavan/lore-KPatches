{
  "thread_id": "20260223-sunrpc-cache-v2-0-91fc827c4d33@kernel.org",
  "subject": "[PATCH v2 0/4] sunrpc: cache infrastructure scalability improvements",
  "url": "https://lore.kernel.org/all/20260223-sunrpc-cache-v2-0-91fc827c4d33@kernel.org/",
  "dates": {
    "2026-02-23": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Jeff Layton",
      "reviews": [
        {
          "author": "Jeff Layton (author)",
          "summary": "The author addressed a concern about cache_request leak in cache_release() by adding cleanup logic to decrement readers and check if it reached 0 with CACHE_PENDING clear, then dequeue and free the cache_request. A fix is planned for v2.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "fix_planned"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "When a reader's file descriptor is closed while in the middle of reading\na cache_request (rp->offset != 0), cache_release() decrements the\nrequest's readers count but never checks whether it should free the\nrequest.\n\nIn cache_read(), when readers drops to 0 and CACHE_PENDING is clear, the\ncache_request is removed from the queue and freed along with its buffer\nand cache_head reference. cache_release() lacks this cleanup.\n\nThe only other path that frees requests with readers == 0 is\ncache_dequeue(), but it runs only when CACHE_PENDING transitions from\nset to clear. If that transition already happened while readers was\nstill non-zero, cache_dequeue() will have skipped the request, and no\nsubsequent call will clean it up.\n\nAdd the same cleanup logic from cache_read() to cache_release(): after\ndecrementing readers, check if it reached 0 with CACHE_PENDING clear,\nand if so, dequeue and free the cache_request.\n\nReported-by: NeilBrown <neilb@ownmail.net>\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n net/sunrpc/cache.c | 26 +++++++++++++++++++++-----\n 1 file changed, 21 insertions(+), 5 deletions(-)\n\ndiff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c\nindex b82f7cde0c9be6071ee4040150672872e548161d..86b3fd5a429d77f7f917f398a02cb7a5ff8dd1e0 100644\n--- a/net/sunrpc/cache.c\n+++ b/net/sunrpc/cache.c\n@@ -1062,14 +1062,25 @@ static int cache_release(struct inode *inode, struct file *filp,\n \tstruct cache_reader *rp = filp->private_data;\n \n \tif (rp) {\n+\t\tstruct cache_request *rq = NULL;\n+\n \t\tspin_lock(&queue_lock);\n \t\tif (rp->offset) {\n \t\t\tstruct cache_queue *cq;\n-\t\t\tfor (cq= &rp->q; &cq->list != &cd->queue;\n-\t\t\t     cq = list_entry(cq->list.next, struct cache_queue, list))\n+\t\t\tfor (cq = &rp->q; &cq->list != &cd->queue;\n+\t\t\t     cq = list_entry(cq->list.next,\n+\t\t\t\t\t     struct cache_queue, list))\n \t\t\t\tif (!cq->reader) {\n-\t\t\t\t\tcontainer_of(cq, struct cache_request, q)\n-\t\t\t\t\t\t->readers--;\n+\t\t\t\t\tstruct cache_request *cr =\n+\t\t\t\t\t\tcontainer_of(cq,\n+\t\t\t\t\t\tstruct cache_request, q);\n+\t\t\t\t\tcr->readers--;\n+\t\t\t\t\tif (cr->readers == 0 &&\n+\t\t\t\t\t    !test_bit(CACHE_PENDING,\n+\t\t\t\t\t\t      &cr->item->flags)) {\n+\t\t\t\t\t\tlist_del(&cr->q.list);\n+\t\t\t\t\t\trq = cr;\n+\t\t\t\t\t}\n \t\t\t\t\tbreak;\n \t\t\t\t}\n \t\t\trp->offset = 0;\n@@ -1077,9 +1088,14 @@ static int cache_release(struct inode *inode, struct file *filp,\n \t\tlist_del(&rp->q.list);\n \t\tspin_unlock(&queue_lock);\n \n+\t\tif (rq) {\n+\t\t\tcache_put(rq->item, cd);\n+\t\t\tkfree(rq->buf);\n+\t\t\tkfree(rq);\n+\t\t}\n+\n \t\tfilp->private_data = NULL;\n \t\tkfree(rp);\n-\n \t}\n \tif (filp->f_mode & FMODE_WRITE) {\n \t\tatomic_dec(&cd->writers);\n\n-- \n2.53.0",
          "reply_to": "",
          "message_date": "2026-02-23"
        },
        {
          "author": "Jeff Layton (author)",
          "summary": "The author addressed a concern about the global queue_lock serializing upcall queue operations across all cache_detail instances. They agreed to convert it to a per-cache-detail spinlock so that different caches no longer contend with each other on queue operations.",
          "sentiment": "positive",
          "sentiment_signals": [
            "acknowledged fix needed",
            "agreed to restructure"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "The global queue_lock serializes all upcall queue operations across\nevery cache_detail instance. Convert it to a per-cache-detail spinlock\nso that different caches (e.g. auth.unix.ip vs nfsd.fh) no longer\ncontend with each other on queue operations.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n include/linux/sunrpc/cache.h |  1 +\n net/sunrpc/cache.c           | 47 ++++++++++++++++++++++----------------------\n 2 files changed, 24 insertions(+), 24 deletions(-)\n\ndiff --git a/include/linux/sunrpc/cache.h b/include/linux/sunrpc/cache.h\nindex e783132e481ff2593fdc5d323f7b3a08f85d4cd8..3d32dd1f7b05d35562d2064fed69877b3950fb51 100644\n--- a/include/linux/sunrpc/cache.h\n+++ b/include/linux/sunrpc/cache.h\n@@ -113,6 +113,7 @@ struct cache_detail {\n \n \t/* fields for communication over channel */\n \tstruct list_head\tqueue;\n+\tspinlock_t\t\tqueue_lock;\n \n \tatomic_t\t\twriters;\t\t/* how many time is /channel open */\n \ttime64_t\t\tlast_close;\t\t/* if no writers, when did last close */\ndiff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c\nindex 86b3fd5a429d77f7f917f398a02cb7a5ff8dd1e0..1cfaae488c6c67a9797511804e4bbba16bcc70ae 100644\n--- a/net/sunrpc/cache.c\n+++ b/net/sunrpc/cache.c\n@@ -400,6 +400,7 @@ void sunrpc_init_cache_detail(struct cache_detail *cd)\n {\n \tspin_lock_init(&cd->hash_lock);\n \tINIT_LIST_HEAD(&cd->queue);\n+\tspin_lock_init(&cd->queue_lock);\n \tspin_lock(&cache_list_lock);\n \tcd->nextcheck = 0;\n \tcd->entries = 0;\n@@ -803,8 +804,6 @@ void cache_clean_deferred(void *owner)\n  *\n  */\n \n-static DEFINE_SPINLOCK(queue_lock);\n-\n struct cache_queue {\n \tstruct list_head\tlist;\n \tint\t\t\treader;\t/* if 0, then request */\n@@ -847,7 +846,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \tinode_lock(inode); /* protect against multiple concurrent\n \t\t\t      * readers on this file */\n  again:\n-\tspin_lock(&queue_lock);\n+\tspin_lock(&cd->queue_lock);\n \t/* need to find next request */\n \twhile (rp->q.list.next != &cd->queue &&\n \t       list_entry(rp->q.list.next, struct cache_queue, list)\n@@ -856,7 +855,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\tlist_move(&rp->q.list, next);\n \t}\n \tif (rp->q.list.next == &cd->queue) {\n-\t\tspin_unlock(&queue_lock);\n+\t\tspin_unlock(&cd->queue_lock);\n \t\tinode_unlock(inode);\n \t\tWARN_ON_ONCE(rp->offset);\n \t\treturn 0;\n@@ -865,7 +864,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \tWARN_ON_ONCE(rq->q.reader);\n \tif (rp->offset == 0)\n \t\trq->readers++;\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&cd->queue_lock);\n \n \tif (rq->len == 0) {\n \t\terr = cache_request(cd, rq);\n@@ -876,9 +875,9 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \n \tif (rp->offset == 0 && !test_bit(CACHE_PENDING, &rq->item->flags)) {\n \t\terr = -EAGAIN;\n-\t\tspin_lock(&queue_lock);\n+\t\tspin_lock(&cd->queue_lock);\n \t\tlist_move(&rp->q.list, &rq->q.list);\n-\t\tspin_unlock(&queue_lock);\n+\t\tspin_unlock(&cd->queue_lock);\n \t} else {\n \t\tif (rp->offset + count > rq->len)\n \t\t\tcount = rq->len - rp->offset;\n@@ -888,26 +887,26 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\trp->offset += count;\n \t\tif (rp->offset >= rq->len) {\n \t\t\trp->offset = 0;\n-\t\t\tspin_lock(&queue_lock);\n+\t\t\tspin_lock(&cd->queue_lock);\n \t\t\tlist_move(&rp->q.list, &rq->q.list);\n-\t\t\tspin_unlock(&queue_lock);\n+\t\t\tspin_unlock(&cd->queue_lock);\n \t\t}\n \t\terr = 0;\n \t}\n  out:\n \tif (rp->offset == 0) {\n \t\t/* need to release rq */\n-\t\tspin_lock(&queue_lock);\n+\t\tspin_lock(&cd->queue_lock);\n \t\trq->readers--;\n \t\tif (rq->readers == 0 &&\n \t\t    !test_bit(CACHE_PENDING, &rq->item->flags)) {\n \t\t\tlist_del(&rq->q.list);\n-\t\t\tspin_unlock(&queue_lock);\n+\t\t\tspin_unlock(&cd->queue_lock);\n \t\t\tcache_put(rq->item, cd);\n \t\t\tkfree(rq->buf);\n \t\t\tkfree(rq);\n \t\t} else\n-\t\t\tspin_unlock(&queue_lock);\n+\t\t\tspin_unlock(&cd->queue_lock);\n \t}\n \tif (err == -EAGAIN)\n \t\tgoto again;\n@@ -988,7 +987,7 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \tif (!rp)\n \t\treturn mask;\n \n-\tspin_lock(&queue_lock);\n+\tspin_lock(&cd->queue_lock);\n \n \tfor (cq= &rp->q; &cq->list != &cd->queue;\n \t     cq = list_entry(cq->list.next, struct cache_queue, list))\n@@ -996,7 +995,7 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n \t\t\tbreak;\n \t\t}\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&cd->queue_lock);\n \treturn mask;\n }\n \n@@ -1011,7 +1010,7 @@ static int cache_ioctl(struct inode *ino, struct file *filp,\n \tif (cmd != FIONREAD || !rp)\n \t\treturn -EINVAL;\n \n-\tspin_lock(&queue_lock);\n+\tspin_lock(&cd->queue_lock);\n \n \t/* only find the length remaining in current request,\n \t * or the length of the next request\n@@ -1024,7 +1023,7 @@ static int cache_ioctl(struct inode *ino, struct file *filp,\n \t\t\tlen = cr->len - rp->offset;\n \t\t\tbreak;\n \t\t}\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&cd->queue_lock);\n \n \treturn put_user(len, (int __user *)arg);\n }\n@@ -1046,9 +1045,9 @@ static int cache_open(struct inode *inode, struct file *filp,\n \t\trp->offset = 0;\n \t\trp->q.reader = 1;\n \n-\t\tspin_lock(&queue_lock);\n+\t\tspin_lock(&cd->queue_lock);\n \t\tlist_add(&rp->q.list, &cd->queue);\n-\t\tspin_unlock(&queue_lock);\n+\t\tspin_unlock(&cd->queue_lock);\n \t}\n \tif (filp->f_mode & FMODE_WRITE)\n \t\tatomic_inc(&cd->writers);\n@@ -1064,7 +1063,7 @@ static int cache_release(struct inode *inode, struct file *filp,\n \tif (rp) {\n \t\tstruct cache_request *rq = NULL;\n \n-\t\tspin_lock(&queue_lock);\n+\t\tspin_lock(&cd->queue_lock);\n \t\tif (rp->offset) {\n \t\t\tstruct cache_queue *cq;\n \t\t\tfor (cq = &rp->q; &cq->list != &cd->queue;\n@@ -1086,7 +1085,7 @@ static int cache_release(struct inode *inode, struct file *filp,\n \t\t\trp->offset = 0;\n \t\t}\n \t\tlist_del(&rp->q.list);\n-\t\tspin_unlock(&queue_lock);\n+\t\tspin_unlock(&cd->queue_lock);\n \n \t\tif (rq) {\n \t\t\tcache_put(rq->item, cd);\n@@ -1113,7 +1112,7 @@ static void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)\n \tstruct cache_request *cr;\n \tLIST_HEAD(dequeued);\n \n-\tspin_lock(&queue_lock);\n+\tspin_lock(&detail->queue_lock);\n \tlist_for_each_entry_safe(cq, tmp, &detail->queue, list)\n \t\tif (!cq->reader) {\n \t\t\tcr = container_of(cq, struct cache_request, q);\n@@ -1126,7 +1125,7 @@ static void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)\n \t\t\t\tcontinue;\n \t\t\tlist_move(&cr->q.list, &dequeued);\n \t\t}\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&detail->queue_lock);\n \twhile (!list_empty(&dequeued)) {\n \t\tcr = list_entry(dequeued.next, struct cache_request, q.list);\n \t\tlist_del(&cr->q.list);\n@@ -1251,7 +1250,7 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n \tcrq->buf = buf;\n \tcrq->len = 0;\n \tcrq->readers = 0;\n-\tspin_lock(&queue_lock);\n+\tspin_lock(&detail->queue_lock);\n \tif (test_bit(CACHE_PENDING, &h->flags)) {\n \t\tcrq->item = cache_get(h);\n \t\tlist_add_tail(&crq->q.list, &detail->queue);\n@@ -1259,7 +1258,7 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n \t} else\n \t\t/* Lost a race, no longer PENDING, so don't enqueue */\n \t\tret = -EAGAIN;\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&detail->queue_lock);\n \twake_up(&queue_wait);\n \tif (ret == -EAGAIN) {\n \t\tkfree(buf);\n\n-- \n2.53.0",
          "reply_to": "",
          "message_date": "2026-02-23"
        },
        {
          "author": "Jeff Layton (author)",
          "summary": "The author is addressing a concern about the queue_wait waitqueue being global, which wakes pollers on all caches when one cache_detail is woken. The author agrees that this is an issue and has converted it to a per-cache-detail field so only relevant pollers are woken.",
          "sentiment": "positive",
          "sentiment_signals": [
            "acknowledged the problem",
            "provided a fix"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "The queue_wait waitqueue is currently a file-scoped global, so a\nwake_up for one cache_detail wakes pollers on all caches. Convert it\nto a per-cache-detail field so that only pollers on the relevant cache\nare woken.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n include/linux/sunrpc/cache.h | 2 ++\n net/sunrpc/cache.c           | 7 +++----\n 2 files changed, 5 insertions(+), 4 deletions(-)\n\ndiff --git a/include/linux/sunrpc/cache.h b/include/linux/sunrpc/cache.h\nindex 3d32dd1f7b05d35562d2064fed69877b3950fb51..031379efba24d40f64ce346cf1032261d4b98d05 100644\n--- a/include/linux/sunrpc/cache.h\n+++ b/include/linux/sunrpc/cache.h\n@@ -16,6 +16,7 @@\n #include <linux/atomic.h>\n #include <linux/kstrtox.h>\n #include <linux/proc_fs.h>\n+#include <linux/wait.h>\n \n /*\n  * Each cache requires:\n@@ -114,6 +115,7 @@ struct cache_detail {\n \t/* fields for communication over channel */\n \tstruct list_head\tqueue;\n \tspinlock_t\t\tqueue_lock;\n+\twait_queue_head_t\tqueue_wait;\n \n \tatomic_t\t\twriters;\t\t/* how many time is /channel open */\n \ttime64_t\t\tlast_close;\t\t/* if no writers, when did last close */\ndiff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c\nindex 1cfaae488c6c67a9797511804e4bbba16bcc70ae..fd02dca1f07afec2f09c591037bac3ea3e8d7e17 100644\n--- a/net/sunrpc/cache.c\n+++ b/net/sunrpc/cache.c\n@@ -401,6 +401,7 @@ void sunrpc_init_cache_detail(struct cache_detail *cd)\n \tspin_lock_init(&cd->hash_lock);\n \tINIT_LIST_HEAD(&cd->queue);\n \tspin_lock_init(&cd->queue_lock);\n+\tinit_waitqueue_head(&cd->queue_wait);\n \tspin_lock(&cache_list_lock);\n \tcd->nextcheck = 0;\n \tcd->entries = 0;\n@@ -970,8 +971,6 @@ static ssize_t cache_write(struct file *filp, const char __user *buf,\n \treturn ret;\n }\n \n-static DECLARE_WAIT_QUEUE_HEAD(queue_wait);\n-\n static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \t\t\t       struct cache_detail *cd)\n {\n@@ -979,7 +978,7 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \tstruct cache_reader *rp = filp->private_data;\n \tstruct cache_queue *cq;\n \n-\tpoll_wait(filp, &queue_wait, wait);\n+\tpoll_wait(filp, &cd->queue_wait, wait);\n \n \t/* alway allow write */\n \tmask = EPOLLOUT | EPOLLWRNORM;\n@@ -1259,7 +1258,7 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n \t\t/* Lost a race, no longer PENDING, so don't enqueue */\n \t\tret = -EAGAIN;\n \tspin_unlock(&detail->queue_lock);\n-\twake_up(&queue_wait);\n+\twake_up(&detail->queue_wait);\n \tif (ret == -EAGAIN) {\n \t\tkfree(buf);\n \t\tkfree(crq);\n\n-- \n2.53.0",
          "reply_to": "",
          "message_date": "2026-02-23"
        },
        {
          "author": "Jeff Layton (author)",
          "summary": "The author addressed a concern about the complexity of the reader-skipping loops in cache_read, cache_poll, and cache_ioctl by replacing the single interleaved queue with two dedicated lists for upcall requests and open file handles. The readers now track their position via a monotonically increasing sequence number (next_seqno) rather than by their position in the shared list. A new helper function cache_next_request() finds the next request at or after a given seqno, eliminating the need for the cache_queue wrapper struct.",
          "sentiment": "positive",
          "sentiment_signals": [
            "acknowledged",
            "agreed"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Replace the single interleaved queue (which mixed cache_request and\ncache_reader entries distinguished by a ->reader flag) with two\ndedicated lists: cd->requests for upcall requests and cd->readers\nfor open file handles.\n\nReaders now track their position via a monotonically increasing\nsequence number (next_seqno) rather than by their position in the\nshared list. Each cache_request is assigned a seqno when enqueued,\nand a new cache_next_request() helper finds the next request at or\nafter a given seqno.\n\nThis eliminates the cache_queue wrapper struct entirely, simplifies\nthe reader-skipping loops in cache_read/cache_poll/cache_ioctl/\ncache_release, and makes the data flow easier to reason about.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n include/linux/sunrpc/cache.h |   4 +-\n net/sunrpc/cache.c           | 143 ++++++++++++++++++-------------------------\n 2 files changed, 62 insertions(+), 85 deletions(-)\n\ndiff --git a/include/linux/sunrpc/cache.h b/include/linux/sunrpc/cache.h\nindex 031379efba24d40f64ce346cf1032261d4b98d05..b1e595c2615bd4be4d9ad19f71a8f4d08bd74a9b 100644\n--- a/include/linux/sunrpc/cache.h\n+++ b/include/linux/sunrpc/cache.h\n@@ -113,9 +113,11 @@ struct cache_detail {\n \tint\t\t\tentries;\n \n \t/* fields for communication over channel */\n-\tstruct list_head\tqueue;\n+\tstruct list_head\trequests;\n+\tstruct list_head\treaders;\n \tspinlock_t\t\tqueue_lock;\n \twait_queue_head_t\tqueue_wait;\n+\tu64\t\t\tnext_seqno;\n \n \tatomic_t\t\twriters;\t\t/* how many time is /channel open */\n \ttime64_t\t\tlast_close;\t\t/* if no writers, when did last close */\ndiff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c\nindex fd02dca1f07afec2f09c591037bac3ea3e8d7e17..7081c1214e6c3226f8ac82c8bc7ff6c36f598744 100644\n--- a/net/sunrpc/cache.c\n+++ b/net/sunrpc/cache.c\n@@ -399,9 +399,11 @@ static struct delayed_work cache_cleaner;\n void sunrpc_init_cache_detail(struct cache_detail *cd)\n {\n \tspin_lock_init(&cd->hash_lock);\n-\tINIT_LIST_HEAD(&cd->queue);\n+\tINIT_LIST_HEAD(&cd->requests);\n+\tINIT_LIST_HEAD(&cd->readers);\n \tspin_lock_init(&cd->queue_lock);\n \tinit_waitqueue_head(&cd->queue_wait);\n+\tcd->next_seqno = 0;\n \tspin_lock(&cache_list_lock);\n \tcd->nextcheck = 0;\n \tcd->entries = 0;\n@@ -796,29 +798,20 @@ void cache_clean_deferred(void *owner)\n  * On read, you get a full request, or block.\n  * On write, an update request is processed.\n  * Poll works if anything to read, and always allows write.\n- *\n- * Implemented by linked list of requests.  Each open file has\n- * a ->private that also exists in this list.  New requests are added\n- * to the end and may wakeup and preceding readers.\n- * New readers are added to the head.  If, on read, an item is found with\n- * CACHE_UPCALLING clear, we free it from the list.\n- *\n  */\n \n-struct cache_queue {\n-\tstruct list_head\tlist;\n-\tint\t\t\treader;\t/* if 0, then request */\n-};\n struct cache_request {\n-\tstruct cache_queue\tq;\n+\tstruct list_head\tlist;\n \tstruct cache_head\t*item;\n-\tchar\t\t\t* buf;\n+\tchar\t\t\t*buf;\n \tint\t\t\tlen;\n \tint\t\t\treaders;\n+\tu64\t\t\tseqno;\n };\n struct cache_reader {\n-\tstruct cache_queue\tq;\n+\tstruct list_head\tlist;\n \tint\t\t\toffset;\t/* if non-0, we have a refcnt on next request */\n+\tu64\t\t\tnext_seqno;\n };\n \n static int cache_request(struct cache_detail *detail,\n@@ -833,6 +826,17 @@ static int cache_request(struct cache_detail *detail,\n \treturn PAGE_SIZE - len;\n }\n \n+static struct cache_request *\n+cache_next_request(struct cache_detail *cd, u64 seqno)\n+{\n+\tstruct cache_request *rq;\n+\n+\tlist_for_each_entry(rq, &cd->requests, list)\n+\t\tif (rq->seqno >= seqno)\n+\t\t\treturn rq;\n+\treturn NULL;\n+}\n+\n static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\t\t  loff_t *ppos, struct cache_detail *cd)\n {\n@@ -849,20 +853,13 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n  again:\n \tspin_lock(&cd->queue_lock);\n \t/* need to find next request */\n-\twhile (rp->q.list.next != &cd->queue &&\n-\t       list_entry(rp->q.list.next, struct cache_queue, list)\n-\t       ->reader) {\n-\t\tstruct list_head *next = rp->q.list.next;\n-\t\tlist_move(&rp->q.list, next);\n-\t}\n-\tif (rp->q.list.next == &cd->queue) {\n+\trq = cache_next_request(cd, rp->next_seqno);\n+\tif (!rq) {\n \t\tspin_unlock(&cd->queue_lock);\n \t\tinode_unlock(inode);\n \t\tWARN_ON_ONCE(rp->offset);\n \t\treturn 0;\n \t}\n-\trq = container_of(rp->q.list.next, struct cache_request, q.list);\n-\tWARN_ON_ONCE(rq->q.reader);\n \tif (rp->offset == 0)\n \t\trq->readers++;\n \tspin_unlock(&cd->queue_lock);\n@@ -876,9 +873,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \n \tif (rp->offset == 0 && !test_bit(CACHE_PENDING, &rq->item->flags)) {\n \t\terr = -EAGAIN;\n-\t\tspin_lock(&cd->queue_lock);\n-\t\tlist_move(&rp->q.list, &rq->q.list);\n-\t\tspin_unlock(&cd->queue_lock);\n+\t\trp->next_seqno = rq->seqno + 1;\n \t} else {\n \t\tif (rp->offset + count > rq->len)\n \t\t\tcount = rq->len - rp->offset;\n@@ -888,9 +883,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\trp->offset += count;\n \t\tif (rp->offset >= rq->len) {\n \t\t\trp->offset = 0;\n-\t\t\tspin_lock(&cd->queue_lock);\n-\t\t\tlist_move(&rp->q.list, &rq->q.list);\n-\t\t\tspin_unlock(&cd->queue_lock);\n+\t\t\trp->next_seqno = rq->seqno + 1;\n \t\t}\n \t\terr = 0;\n \t}\n@@ -901,7 +894,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\trq->readers--;\n \t\tif (rq->readers == 0 &&\n \t\t    !test_bit(CACHE_PENDING, &rq->item->flags)) {\n-\t\t\tlist_del(&rq->q.list);\n+\t\t\tlist_del(&rq->list);\n \t\t\tspin_unlock(&cd->queue_lock);\n \t\t\tcache_put(rq->item, cd);\n \t\t\tkfree(rq->buf);\n@@ -976,7 +969,6 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n {\n \t__poll_t mask;\n \tstruct cache_reader *rp = filp->private_data;\n-\tstruct cache_queue *cq;\n \n \tpoll_wait(filp, &cd->queue_wait, wait);\n \n@@ -988,12 +980,8 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \n \tspin_lock(&cd->queue_lock);\n \n-\tfor (cq= &rp->q; &cq->list != &cd->queue;\n-\t     cq = list_entry(cq->list.next, struct cache_queue, list))\n-\t\tif (!cq->reader) {\n-\t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n-\t\t\tbreak;\n-\t\t}\n+\tif (cache_next_request(cd, rp->next_seqno))\n+\t\tmask |= EPOLLIN | EPOLLRDNORM;\n \tspin_unlock(&cd->queue_lock);\n \treturn mask;\n }\n@@ -1004,7 +992,7 @@ static int cache_ioctl(struct inode *ino, struct file *filp,\n {\n \tint len = 0;\n \tstruct cache_reader *rp = filp->private_data;\n-\tstruct cache_queue *cq;\n+\tstruct cache_request *rq;\n \n \tif (cmd != FIONREAD || !rp)\n \t\treturn -EINVAL;\n@@ -1014,14 +1002,9 @@ static int cache_ioctl(struct inode *ino, struct file *filp,\n \t/* only find the length remaining in current request,\n \t * or the length of the next request\n \t */\n-\tfor (cq= &rp->q; &cq->list != &cd->queue;\n-\t     cq = list_entry(cq->list.next, struct cache_queue, list))\n-\t\tif (!cq->reader) {\n-\t\t\tstruct cache_request *cr =\n-\t\t\t\tcontainer_of(cq, struct cache_request, q);\n-\t\t\tlen = cr->len - rp->offset;\n-\t\t\tbreak;\n-\t\t}\n+\trq = cache_next_request(cd, rp->next_seqno);\n+\tif (rq)\n+\t\tlen = rq->len - rp->offset;\n \tspin_unlock(&cd->queue_lock);\n \n \treturn put_user(len, (int __user *)arg);\n@@ -1042,10 +1025,10 @@ static int cache_open(struct inode *inode, struct file *filp,\n \t\t\treturn -ENOMEM;\n \t\t}\n \t\trp->offset = 0;\n-\t\trp->q.reader = 1;\n+\t\trp->next_seqno = 0;\n \n \t\tspin_lock(&cd->queue_lock);\n-\t\tlist_add(&rp->q.list, &cd->queue);\n+\t\tlist_add(&rp->list, &cd->readers);\n \t\tspin_unlock(&cd->queue_lock);\n \t}\n \tif (filp->f_mode & FMODE_WRITE)\n@@ -1064,26 +1047,21 @@ static int cache_release(struct inode *inode, struct file *filp,\n \n \t\tspin_lock(&cd->queue_lock);\n \t\tif (rp->offset) {\n-\t\t\tstruct cache_queue *cq;\n-\t\t\tfor (cq = &rp->q; &cq->list != &cd->queue;\n-\t\t\t     cq = list_entry(cq->list.next,\n-\t\t\t\t\t     struct cache_queue, list))\n-\t\t\t\tif (!cq->reader) {\n-\t\t\t\t\tstruct cache_request *cr =\n-\t\t\t\t\t\tcontainer_of(cq,\n-\t\t\t\t\t\tstruct cache_request, q);\n-\t\t\t\t\tcr->readers--;\n-\t\t\t\t\tif (cr->readers == 0 &&\n-\t\t\t\t\t    !test_bit(CACHE_PENDING,\n-\t\t\t\t\t\t      &cr->item->flags)) {\n-\t\t\t\t\t\tlist_del(&cr->q.list);\n-\t\t\t\t\t\trq = cr;\n-\t\t\t\t\t}\n-\t\t\t\t\tbreak;\n+\t\t\tstruct cache_request *cr;\n+\n+\t\t\tcr = cache_next_request(cd, rp->next_seqno);\n+\t\t\tif (cr) {\n+\t\t\t\tcr->readers--;\n+\t\t\t\tif (cr->readers == 0 &&\n+\t\t\t\t    !test_bit(CACHE_PENDING,\n+\t\t\t\t\t      &cr->item->flags)) {\n+\t\t\t\t\tlist_del(&cr->list);\n+\t\t\t\t\trq = cr;\n \t\t\t\t}\n+\t\t\t}\n \t\t\trp->offset = 0;\n \t\t}\n-\t\tlist_del(&rp->q.list);\n+\t\tlist_del(&rp->list);\n \t\tspin_unlock(&cd->queue_lock);\n \n \t\tif (rq) {\n@@ -1107,27 +1085,24 @@ static int cache_release(struct inode *inode, struct file *filp,\n \n static void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)\n {\n-\tstruct cache_queue *cq, *tmp;\n-\tstruct cache_request *cr;\n+\tstruct cache_request *cr, *tmp;\n \tLIST_HEAD(dequeued);\n \n \tspin_lock(&detail->queue_lock);\n-\tlist_for_each_entry_safe(cq, tmp, &detail->queue, list)\n-\t\tif (!cq->reader) {\n-\t\t\tcr = container_of(cq, struct cache_request, q);\n-\t\t\tif (cr->item != ch)\n-\t\t\t\tcontinue;\n-\t\t\tif (test_bit(CACHE_PENDING, &ch->flags))\n-\t\t\t\t/* Lost a race and it is pending again */\n-\t\t\t\tbreak;\n-\t\t\tif (cr->readers != 0)\n-\t\t\t\tcontinue;\n-\t\t\tlist_move(&cr->q.list, &dequeued);\n-\t\t}\n+\tlist_for_each_entry_safe(cr, tmp, &detail->requests, list) {\n+\t\tif (cr->item != ch)\n+\t\t\tcontinue;\n+\t\tif (test_bit(CACHE_PENDING, &ch->flags))\n+\t\t\t/* Lost a race and it is pending again */\n+\t\t\tbreak;\n+\t\tif (cr->readers != 0)\n+\t\t\tcontinue;\n+\t\tlist_move(&cr->list, &dequeued);\n+\t}\n \tspin_unlock(&detail->queue_lock);\n \twhile (!list_empty(&dequeued)) {\n-\t\tcr = list_entry(dequeued.next, struct cache_request, q.list);\n-\t\tlist_del(&cr->q.list);\n+\t\tcr = list_entry(dequeued.next, struct cache_request, list);\n+\t\tlist_del(&cr->list);\n \t\tcache_put(cr->item, detail);\n \t\tkfree(cr->buf);\n \t\tkfree(cr);\n@@ -1245,14 +1220,14 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n \t\treturn -EAGAIN;\n \t}\n \n-\tcrq->q.reader = 0;\n \tcrq->buf = buf;\n \tcrq->len = 0;\n \tcrq->readers = 0;\n \tspin_lock(&detail->queue_lock);\n \tif (test_bit(CACHE_PENDING, &h->flags)) {\n \t\tcrq->item = cache_get(h);\n-\t\tlist_add_tail(&crq->q.list, &detail->queue);\n+\t\tcrq->seqno = detail->next_seqno++;\n+\t\tlist_add_tail(&crq->list, &detail->requests);\n \t\ttrace_cache_entry_upcall(detail, h);\n \t} else\n \t\t/* Lost a race, no longer PENDING, so don't enqueue */\n\n-- \n2.53.0",
          "reply_to": "",
          "message_date": "2026-02-23"
        },
        {
          "author": "Chuck Lever",
          "summary": "reviewer noted that the patch does not address the issue of cache_detail->queue being accessed concurrently by multiple threads, and requested a lock be added to protect this access\n\nreviewer noted that the per-cache-detail lock ordering is still a concern, specifically when vswap_free() acquires the spinlock while holding the folio lock, and requested further investigation into this issue",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "concurrency",
            "thread safety",
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "From: Chuck Lever <chuck.lever@oracle.com>\n\nOn Mon, 23 Feb 2026 12:09:57 -0500, Jeff Layton wrote:\n\n---\n\nApplied to nfsd-testing, replacing v1. Thanks!\n\n[1/4] sunrpc: fix cache_request leak in cache_release\n      commit: dad5f78046759eb5c95970198eb9865550eb6227\n[2/4] sunrpc: convert queue_lock from global spinlock to per-cache-detail lock\n      commit: c94ad34b7ecd5928cf3fdb6ea4fcf6ef55765e97\n[3/4] sunrpc: convert queue_wait from global to per-cache-detail waitqueue\n      commit: 951696964e9c370a5f91d5e3e136d39aa08d912c\n[4/4] sunrpc: split cache_detail queue into request and reader lists\n      commit: 3557b9c71039b2435b383fc57283a0b847b40144\n\n--\nChuck Lever",
          "reply_to": "Jeff Layton",
          "message_date": "2026-02-23"
        }
      ],
      "analysis_source": "llm-per-reviewer",
      "patch_summary": "This patch series improves the scalability of the sunrpc cache infrastructure by converting a global spinlock and waitqueue to per-cache-detail locks, simplifying the code and reducing contention. It also fixes a pre-existing bug that could cause cache requests to leak, and introduces a new sequence number to help readers track their position in the queue."
    }
  }
}