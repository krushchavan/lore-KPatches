<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Review Comments: [PATCH 1/2] mm: vmalloc: streamline vmalloc memory accounting</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto,
                         "Helvetica Neue", Arial, sans-serif;
            background: #f5f5f5;
            color: #333;
            line-height: 1.6;
            padding: 20px;
            max-width: 900px;
            margin: 0 auto;
        }
        .home-link { margin-bottom: 12px; display: block; }
        .home-link a { color: #0366d6; text-decoration: none; font-size: 0.9em; }
        .home-link a:hover { text-decoration: underline; }

        h1 { font-size: 1.3em; margin-bottom: 2px; color: #1a1a1a; line-height: 1.3; }

        .lore-link { font-size: 0.85em; margin: 4px 0 6px; display: block; }
        .lore-link a { color: #0366d6; text-decoration: none; }
        .lore-link a:hover { text-decoration: underline; }

        .date-range {
            font-size: 0.8em;
            color: #888;
            margin-bottom: 16px;
        }
        .date-range a { color: #0366d6; text-decoration: none; }
        .date-range a:hover { text-decoration: underline; }

        /* thread-node scroll margin so the card isn't clipped at the top */
        .thread-node { scroll-margin-top: 8px; }

        /* ── Patch summary ──────────────────────────────────────────── */
        .patch-summary-block {
            background: #fff;
            border-radius: 8px;
            padding: 12px 16px;
            margin-bottom: 20px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            border-left: 3px solid #4a90d9;
        }
        .patch-summary-label {
            font-size: 0.72em;
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 0.06em;
            color: #4a90d9;
            margin-bottom: 4px;
        }
        .patch-summary-text {
            font-size: 0.88em;
            color: #444;
            line-height: 1.55;
        }

        /* ── Thread tree ────────────────────────────────────────────── */
        .thread-tree {
            display: flex;
            flex-direction: column;
            gap: 6px;
        }

        /* Depth indentation via left border */
        .thread-node { position: relative; }
        .thread-children {
            margin-left: 20px;
            padding-left: 12px;
            border-left: 2px solid #e0e0e0;
            margin-top: 6px;
            display: flex;
            flex-direction: column;
            gap: 6px;
        }

        /* ── Review comment card ────────────────────────────────────── */
        .review-comment {
            background: #fff;
            border-radius: 6px;
            padding: 10px 14px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            font-size: 0.88em;
        }
        .review-comment-header {
            display: flex;
            flex-wrap: wrap;
            align-items: center;
            gap: 6px;
            margin-bottom: 5px;
        }
        .review-author {
            font-weight: 700;
            color: #1a1a1a;
            font-size: 0.95em;
        }

        /* Date chip — links back to the daily report */
        .date-chip {
            font-size: 0.75em;
            color: #777;
            background: #f0f0f0;
            border-radius: 10px;
            padding: 1px 7px;
            text-decoration: none;
            white-space: nowrap;
        }
        a.date-chip:hover { background: #e0e8f5; color: #0366d6; }

        .badge {
            display: inline-block;
            padding: 1px 8px;
            border-radius: 10px;
            font-size: 0.75em;
            font-weight: 600;
        }
        .inline-review-badge {
            display: inline-block;
            padding: 0 6px;
            border-radius: 8px;
            font-size: 0.78em;
            font-weight: 500;
            background: #e3f2fd;
            color: #1565c0;
        }
        .review-tag-badge {
            display: inline-block;
            padding: 0 6px;
            border-radius: 8px;
            font-size: 0.78em;
            font-weight: 500;
            background: #e8f5e9;
            color: #2e7d32;
        }
        .analysis-source-badge {
            display: inline-block;
            padding: 1px 7px;
            border-radius: 10px;
            font-size: 0.72em;
            font-weight: 600;
            border: 1px solid rgba(0,0,0,0.1);
        }

        .review-comment-text {
            color: #444;
            line-height: 1.55;
            margin-bottom: 4px;
        }
        .review-comment-signals {
            margin-top: 3px;
            font-size: 0.85em;
            color: #aaa;
            font-style: italic;
        }

        /* ── Collapsible raw body ───────────────────────────────────── */
        .raw-body-toggle {
            margin-top: 5px;
            font-size: 0.85em;
        }
        .raw-body-toggle summary {
            cursor: pointer;
            color: #888;
            padding: 2px 0;
            font-weight: 500;
            font-size: 0.9em;
            list-style: none;
        }
        .raw-body-toggle summary::-webkit-details-marker { display: none; }
        .raw-body-toggle summary::before { content: "▶ "; font-size: 0.7em; }
        .raw-body-toggle[open] summary::before { content: "▼ "; }
        .raw-body-toggle summary:hover { color: #555; }
        .raw-body-text {
            white-space: pre-wrap;
            font-size: 0.95em;
            background: #f8f8f8;
            padding: 8px 10px;
            border-radius: 4px;
            max-height: 360px;
            overflow-y: auto;
            margin-top: 4px;
            line-height: 1.5;
            color: #444;
            border: 1px solid #e8e8e8;
        }
        .reply-to-label {
            font-size: 0.8em;
            color: #999;
            font-style: italic;
            margin-top: 3px;
        }
        .lore-link {
            display: inline-block;
            margin-top: 4px;
            font-size: 0.82em;
            color: #0366d6;
            text-decoration: none;
            font-weight: 500;
            white-space: nowrap;
        }
        .lore-link:hover { text-decoration: underline; color: #0056b3; }

        .no-reviews {
            color: #aaa;
            font-size: 0.85em;
            font-style: italic;
            padding: 8px 0;
        }

        footer {
            text-align: center;
            color: #bbb;
            font-size: 0.78em;
            margin-top: 36px;
            padding: 16px;
        }
    </style>
</head>
<body>
    <div class="home-link"><a href="../index.html">&larr; Back to reports</a></div>
    <h1>[PATCH 1/2] mm: vmalloc: streamline vmalloc memory accounting</h1>
    <div class="lore-link"><a href="https://lore.kernel.org/all/20260220191035.3703800-1-hannes@cmpxchg.org/" target="_blank">View on lore.kernel.org &rarr;</a></div>
    <div class="date-range">Active on: <a href="#2026-02-24">2026-02-24</a> &bull; <a href="#2026-02-23">2026-02-23</a> &bull; <a href="#2026-02-20">2026-02-20</a></div>
    <div class="patch-summary-block"><div class="patch-summary-label">Patch summary</div><div class="patch-summary-text">Use a vmstat counter instead of a custom, open-coded atomic. This has the added benefit of making the data available per-node, and prepares for cleaning up the memcg accounting as well.</div></div>
    <div class="thread-tree">
<div class="thread-node depth-0" id="2026-02-20">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Shakeel Butt</span>
<a class="date-chip" href="../2026-02-20.html" title="First appeared in report for 2026-02-20">2026-02-20</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Acked-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#6c4b00;background:#ffeeba" title="Analysis source: Heuristic">Heuristic</span>
</div>
<div class="review-comment-text">mod_node_page_state() takes &#x27;struct pglist_data *pgdat&#x27;, you need to use page_pgdat(page) as first param. Same here. With above fixes, you can add:</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Fri, Feb 20, 2026 at 02:10:34PM -0500, Johannes Weiner wrote:
[...]
&gt;  static struct vmap_area *__find_vmap_area(unsigned long addr, struct rb_root *root)
&gt;  {
&gt;  	struct rb_node *n = root-&gt;rb_node;
&gt; @@ -3463,11 +3457,11 @@ void vfree(const void *addr)
&gt;  		 * High-order allocs for huge vmallocs are split, so
&gt;  		 * can be freed as an array of order-0 allocations
&gt;  		 */
&gt; +		if (!(vm-&gt;flags &amp; VM_MAP_PUT_PAGES))
&gt; +			dec_node_page_state(page, NR_VMALLOC);
&gt;  		__free_page(page);
&gt;  		cond_resched();
&gt;  	}
&gt; -	if (!(vm-&gt;flags &amp; VM_MAP_PUT_PAGES))
&gt; -		atomic_long_sub(vm-&gt;nr_pages, &amp;nr_vmalloc_pages);
&gt;  	kvfree(vm-&gt;pages);
&gt;  	kfree(vm);
&gt;  }
&gt; @@ -3655,6 +3649,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,
&gt;  			continue;
&gt;  		}
&gt;  
&gt; +		mod_node_page_state(page, NR_VMALLOC, 1 &lt;&lt; large_order);

mod_node_page_state() takes &#x27;struct pglist_data *pgdat&#x27;, you need to use
page_pgdat(page) as first param.

&gt; +
&gt;  		split_page(page, large_order);
&gt;  		for (i = 0; i &lt; (1U &lt;&lt; large_order); i++)
&gt;  			pages[nr_allocated + i] = page + i;
&gt; @@ -3675,6 +3671,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,
&gt;  	if (!order) {
&gt;  		while (nr_allocated &lt; nr_pages) {
&gt;  			unsigned int nr, nr_pages_request;
&gt; +			int i;
&gt;  
&gt;  			/*
&gt;  			 * A maximum allowed request is hard-coded and is 100
&gt; @@ -3698,6 +3695,9 @@ vm_area_alloc_pages(gfp_t gfp, int nid,
&gt;  							nr_pages_request,
&gt;  							pages + nr_allocated);
&gt;  
&gt; +			for (i = nr_allocated; i &lt; nr_allocated + nr; i++)
&gt; +				inc_node_page_state(pages[i], NR_VMALLOC);
&gt; +
&gt;  			nr_allocated += nr;
&gt;  
&gt;  			/*
&gt; @@ -3722,6 +3722,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,
&gt;  		if (unlikely(!page))
&gt;  			break;
&gt;  
&gt; +		mod_node_page_state(page, NR_VMALLOC, 1 &lt;&lt; order);

Same here.

With above fixes, you can add:

Acked-by: Shakeel Butt &lt;shakeel.butt@linux.dev&gt;
</pre>
</details>
</div>
</div>
<div class="thread-node depth-0" id="2026-02-23">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Uladzislau Rezki</span>
<a class="date-chip" href="../2026-02-23.html" title="First appeared in report for 2026-02-23">2026-02-23</a>
<span class="inline-review-badge">Inline Review</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#6c4b00;background:#ffeeba" title="Analysis source: Heuristic">Heuristic</span>
</div>
<div class="review-comment-text">Can we move *_node_page_stat() to the end of the vm_area_alloc_pages()? Or mod_node_page_state in first place should be invoked on high-order page before split(to avoid of looping over small pages afterword)? I mean it would be good to place to the one solid place. If it is possible of course.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Fri, Feb 20, 2026 at 02:10:34PM -0500, Johannes Weiner wrote:
&gt; Use a vmstat counter instead of a custom, open-coded atomic. This has
&gt; the added benefit of making the data available per-node, and prepares
&gt; for cleaning up the memcg accounting as well.
&gt; 
&gt; Signed-off-by: Johannes Weiner &lt;hannes@cmpxchg.org&gt;
&gt; ---
&gt;  fs/proc/meminfo.c       |  3 ++-
&gt;  include/linux/mmzone.h  |  1 +
&gt;  include/linux/vmalloc.h |  3 ---
&gt;  mm/vmalloc.c            | 19 ++++++++++---------
&gt;  mm/vmstat.c             |  1 +
&gt;  5 files changed, 14 insertions(+), 13 deletions(-)
&gt; 
&gt; diff --git a/fs/proc/meminfo.c b/fs/proc/meminfo.c
&gt; index a458f1e112fd..549793f44726 100644
&gt; --- a/fs/proc/meminfo.c
&gt; +++ b/fs/proc/meminfo.c
&gt; @@ -126,7 +126,8 @@ static int meminfo_proc_show(struct seq_file *m, void *v)
&gt;  	show_val_kb(m, &quot;Committed_AS:   &quot;, committed);
&gt;  	seq_printf(m, &quot;VmallocTotal:   %8lu kB\n&quot;,
&gt;  		   (unsigned long)VMALLOC_TOTAL &gt;&gt; 10);
&gt; -	show_val_kb(m, &quot;VmallocUsed:    &quot;, vmalloc_nr_pages());
&gt; +	show_val_kb(m, &quot;VmallocUsed:    &quot;,
&gt; +		    global_node_page_state(NR_VMALLOC));
&gt;  	show_val_kb(m, &quot;VmallocChunk:   &quot;, 0ul);
&gt;  	show_val_kb(m, &quot;Percpu:         &quot;, pcpu_nr_pages());
&gt;  
&gt; diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
&gt; index fc5d6c88d2f0..64df797d45c6 100644
&gt; --- a/include/linux/mmzone.h
&gt; +++ b/include/linux/mmzone.h
&gt; @@ -220,6 +220,7 @@ enum node_stat_item {
&gt;  	NR_KERNEL_MISC_RECLAIMABLE,	/* reclaimable non-slab kernel pages */
&gt;  	NR_FOLL_PIN_ACQUIRED,	/* via: pin_user_page(), gup flag: FOLL_PIN */
&gt;  	NR_FOLL_PIN_RELEASED,	/* pages returned via unpin_user_page() */
&gt; +	NR_VMALLOC,
&gt;  	NR_KERNEL_STACK_KB,	/* measured in KiB */
&gt;  #if IS_ENABLED(CONFIG_SHADOW_CALL_STACK)
&gt;  	NR_KERNEL_SCS_KB,	/* measured in KiB */
&gt; diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
&gt; index e8e94f90d686..3b02c0c6b371 100644
&gt; --- a/include/linux/vmalloc.h
&gt; +++ b/include/linux/vmalloc.h
&gt; @@ -286,8 +286,6 @@ int unregister_vmap_purge_notifier(struct notifier_block *nb);
&gt;  #ifdef CONFIG_MMU
&gt;  #define VMALLOC_TOTAL (VMALLOC_END - VMALLOC_START)
&gt;  
&gt; -unsigned long vmalloc_nr_pages(void);
&gt; -
&gt;  int vm_area_map_pages(struct vm_struct *area, unsigned long start,
&gt;  		      unsigned long end, struct page **pages);
&gt;  void vm_area_unmap_pages(struct vm_struct *area, unsigned long start,
&gt; @@ -304,7 +302,6 @@ static inline void set_vm_flush_reset_perms(void *addr)
&gt;  #else  /* !CONFIG_MMU */
&gt;  #define VMALLOC_TOTAL 0UL
&gt;  
&gt; -static inline unsigned long vmalloc_nr_pages(void) { return 0; }
&gt;  static inline void set_vm_flush_reset_perms(void *addr) {}
&gt;  #endif /* CONFIG_MMU */
&gt;  
&gt; diff --git a/mm/vmalloc.c b/mm/vmalloc.c
&gt; index e286c2d2068c..a49a46de9c4f 100644
&gt; --- a/mm/vmalloc.c
&gt; +++ b/mm/vmalloc.c
&gt; @@ -1063,14 +1063,8 @@ static BLOCKING_NOTIFIER_HEAD(vmap_notify_list);
&gt;  static void drain_vmap_area_work(struct work_struct *work);
&gt;  static DECLARE_WORK(drain_vmap_work, drain_vmap_area_work);
&gt;  
&gt; -static __cacheline_aligned_in_smp atomic_long_t nr_vmalloc_pages;
&gt;  static __cacheline_aligned_in_smp atomic_long_t vmap_lazy_nr;
&gt;  
&gt; -unsigned long vmalloc_nr_pages(void)
&gt; -{
&gt; -	return atomic_long_read(&amp;nr_vmalloc_pages);
&gt; -}
&gt; -
&gt;  static struct vmap_area *__find_vmap_area(unsigned long addr, struct rb_root *root)
&gt;  {
&gt;  	struct rb_node *n = root-&gt;rb_node;
&gt; @@ -3463,11 +3457,11 @@ void vfree(const void *addr)
&gt;  		 * High-order allocs for huge vmallocs are split, so
&gt;  		 * can be freed as an array of order-0 allocations
&gt;  		 */
&gt; +		if (!(vm-&gt;flags &amp; VM_MAP_PUT_PAGES))
&gt; +			dec_node_page_state(page, NR_VMALLOC);
&gt;  		__free_page(page);
&gt;  		cond_resched();
&gt;  	}
&gt; -	if (!(vm-&gt;flags &amp; VM_MAP_PUT_PAGES))
&gt; -		atomic_long_sub(vm-&gt;nr_pages, &amp;nr_vmalloc_pages);
&gt;  	kvfree(vm-&gt;pages);
&gt;  	kfree(vm);
&gt;  }
&gt; @@ -3655,6 +3649,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,
&gt;  			continue;
&gt;  		}
&gt;  
&gt; +		mod_node_page_state(page, NR_VMALLOC, 1 &lt;&lt; large_order);
&gt; +
&gt;  		split_page(page, large_order);
&gt;  		for (i = 0; i &lt; (1U &lt;&lt; large_order); i++)
&gt;  			pages[nr_allocated + i] = page + i;
&gt; @@ -3675,6 +3671,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,
&gt;  	if (!order) {
&gt;  		while (nr_allocated &lt; nr_pages) {
&gt;  			unsigned int nr, nr_pages_request;
&gt; +			int i;
&gt;  
&gt;  			/*
&gt;  			 * A maximum allowed request is hard-coded and is 100
&gt; @@ -3698,6 +3695,9 @@ vm_area_alloc_pages(gfp_t gfp, int nid,
&gt;  							nr_pages_request,
&gt;  							pages + nr_allocated);
&gt;  
&gt; +			for (i = nr_allocated; i &lt; nr_allocated + nr; i++)
&gt; +				inc_node_page_state(pages[i], NR_VMALLOC);
&gt; +
&gt;  			nr_allocated += nr;
&gt;  
&gt;  			/*
&gt; @@ -3722,6 +3722,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,
&gt;  		if (unlikely(!page))
&gt;  			break;
&gt;  
&gt; +		mod_node_page_state(page, NR_VMALLOC, 1 &lt;&lt; order);
&gt; +
&gt;  		/*
Can we move *_node_page_stat() to the end of the vm_area_alloc_pages()?

Or mod_node_page_state in first place should be invoked on high-order
page before split(to avoid of looping over small pages afterword)?

I mean it would be good to place to the one solid place. If it is possible
of course.

--
Uladzislau Rezk
</pre>
</details>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Johannes Weiner (author)</span>
<a class="date-chip" href="../2026-02-23.html" title="First appeared in report for 2026-02-23">2026-02-23</a>
<span class="inline-review-badge">Inline Review</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#6c4b00;background:#ffeeba" title="Analysis source: Heuristic">Heuristic</span>
</div>
<div class="review-comment-text">Good catch, my apologies. Serves me right for not compiling incrementally.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Fri, Feb 20, 2026 at 02:09:28PM -0800, Shakeel Butt wrote:
&gt; On Fri, Feb 20, 2026 at 02:10:34PM -0500, Johannes Weiner wrote:
&gt; [...]
&gt; &gt;  static struct vmap_area *__find_vmap_area(unsigned long addr, struct rb_root *root)
&gt; &gt;  {
&gt; &gt;  	struct rb_node *n = root-&gt;rb_node;
&gt; &gt; @@ -3463,11 +3457,11 @@ void vfree(const void *addr)
&gt; &gt;  		 * High-order allocs for huge vmallocs are split, so
&gt; &gt;  		 * can be freed as an array of order-0 allocations
&gt; &gt;  		 */
&gt; &gt; +		if (!(vm-&gt;flags &amp; VM_MAP_PUT_PAGES))
&gt; &gt; +			dec_node_page_state(page, NR_VMALLOC);
&gt; &gt;  		__free_page(page);
&gt; &gt;  		cond_resched();
&gt; &gt;  	}
&gt; &gt; -	if (!(vm-&gt;flags &amp; VM_MAP_PUT_PAGES))
&gt; &gt; -		atomic_long_sub(vm-&gt;nr_pages, &amp;nr_vmalloc_pages);
&gt; &gt;  	kvfree(vm-&gt;pages);
&gt; &gt;  	kfree(vm);
&gt; &gt;  }
&gt; &gt; @@ -3655,6 +3649,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,
&gt; &gt;  			continue;
&gt; &gt;  		}
&gt; &gt;  
&gt; &gt; +		mod_node_page_state(page, NR_VMALLOC, 1 &lt;&lt; large_order);
&gt; 
&gt; mod_node_page_state() takes &#x27;struct pglist_data *pgdat&#x27;, you need to use
&gt; page_pgdat(page) as first param.

Good catch, my apologies. Serves me right for not compiling
incrementally.

&gt; With above fixes, you can add:
&gt; 
&gt; Acked-by: Shakeel Butt &lt;shakeel.butt@linux.dev&gt;

Thanks! I&#x27;ll send out v2.
</pre>
</details>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Johannes Weiner (author)</span>
<a class="date-chip" href="../2026-02-23.html" title="First appeared in report for 2026-02-23">2026-02-23</a>
<span class="inline-review-badge">Inline Review</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#6c4b00;background:#ffeeba" title="Analysis source: Heuristic">Heuristic</span>
</div>
<div class="review-comment-text">Note that the top one in the fast path IS called before the split. We&#x27;re accounting in the same step size as the page allocator can give us. In the fallback paths (bulk allocator, and one-by-one loop), the issue is that the individual pages could be coming from different nodes, so they need to bump different counters. One possible solution would be to remember the last node and accumulate until it differs, then flush: fallback_loop() { page = alloc_pages(); nid = page_to_nid(page); if (nid != last_nid) { if (node_count) { mod_node_page_state(...); node_count = 0; } last_nid = nid; } } if (node_count) mod_node_page_state(...); But it IS the slow path, and these are fairly cheap per-cpu counters. Especially compared to the cost of calling into the allocator.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Mon, Feb 23, 2026 at 04:30:32PM +0100, Uladzislau Rezki wrote:
&gt; On Fri, Feb 20, 2026 at 02:10:34PM -0500, Johannes Weiner wrote:
&gt; &gt; @@ -3655,6 +3649,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,
&gt; &gt;  			continue;
&gt; &gt;  		}
&gt; &gt;  
&gt; &gt; +		mod_node_page_state(page, NR_VMALLOC, 1 &lt;&lt; large_order);
&gt; &gt; +
&gt; &gt;  		split_page(page, large_order);
&gt; &gt;  		for (i = 0; i &lt; (1U &lt;&lt; large_order); i++)
&gt; &gt;  			pages[nr_allocated + i] = page + i;
&gt; &gt; @@ -3675,6 +3671,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,
&gt; &gt;  	if (!order) {
&gt; &gt;  		while (nr_allocated &lt; nr_pages) {
&gt; &gt;  			unsigned int nr, nr_pages_request;
&gt; &gt; +			int i;
&gt; &gt;  
&gt; &gt;  			/*
&gt; &gt;  			 * A maximum allowed request is hard-coded and is 100
&gt; &gt; @@ -3698,6 +3695,9 @@ vm_area_alloc_pages(gfp_t gfp, int nid,
&gt; &gt;  							nr_pages_request,
&gt; &gt;  							pages + nr_allocated);
&gt; &gt;  
&gt; &gt; +			for (i = nr_allocated; i &lt; nr_allocated + nr; i++)
&gt; &gt; +				inc_node_page_state(pages[i], NR_VMALLOC);
&gt; &gt; +
&gt; &gt;  			nr_allocated += nr;
&gt; &gt;  
&gt; &gt;  			/*
&gt; &gt; @@ -3722,6 +3722,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,
&gt; &gt;  		if (unlikely(!page))
&gt; &gt;  			break;
&gt; &gt;  
&gt; &gt; +		mod_node_page_state(page, NR_VMALLOC, 1 &lt;&lt; order);
&gt; &gt; +
&gt; &gt;  		/*
&gt; Can we move *_node_page_stat() to the end of the vm_area_alloc_pages()?
&gt; 
&gt; Or mod_node_page_state in first place should be invoked on high-order
&gt; page before split(to avoid of looping over small pages afterword)?
&gt; 
&gt; I mean it would be good to place to the one solid place. If it is possible
&gt; of course.

Note that the top one in the fast path IS called before the
split. We&#x27;re accounting in the same step size as the page allocator
can give us.

In the fallback paths (bulk allocator, and one-by-one loop), the issue
is that the individual pages could be coming from different nodes, so
they need to bump different counters. One possible solution would be
to remember the last node and accumulate until it differs, then flush:

fallback_loop() {
	page = alloc_pages();
	nid = page_to_nid(page);
	if (nid != last_nid) {
		if (node_count) {
			mod_node_page_state(...);
			node_count = 0;
		}
		last_nid = nid;
	}
}

if (node_count)
	mod_node_page_state(...);

But it IS the slow path, and these are fairly cheap per-cpu
counters. Especially compared to the cost of calling into the
allocator. So I&#x27;m not sure it&#x27;s worth it... What do you think?
</pre>
</details>
</div>
</div>
<div class="thread-node depth-0" id="2026-02-24">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Uladzislau Rezki</span>
<a class="date-chip" href="../2026-02-23_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-24">2026-02-24</a>
<span class="inline-review-badge">Inline Review</span>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#383d41;background:#e2e3e5">Neutral</span>
<span class="analysis-source-badge" style="color:#6c4b00;background:#ffeeba" title="Analysis source: Heuristic">Heuristic</span>
</div>
<div class="review-comment-text">I see. I agree it is easier to keep original solution. I see that Andrew took it, but just in case:</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">On Mon, Feb 23, 2026 at 03:19:20PM -0500, Johannes Weiner wrote:
&gt; On Mon, Feb 23, 2026 at 04:30:32PM +0100, Uladzislau Rezki wrote:
&gt; &gt; On Fri, Feb 20, 2026 at 02:10:34PM -0500, Johannes Weiner wrote:
&gt; &gt; &gt; @@ -3655,6 +3649,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,
&gt; &gt; &gt;  			continue;
&gt; &gt; &gt;  		}
&gt; &gt; &gt;  
&gt; &gt; &gt; +		mod_node_page_state(page, NR_VMALLOC, 1 &lt;&lt; large_order);
&gt; &gt; &gt; +
&gt; &gt; &gt;  		split_page(page, large_order);
&gt; &gt; &gt;  		for (i = 0; i &lt; (1U &lt;&lt; large_order); i++)
&gt; &gt; &gt;  			pages[nr_allocated + i] = page + i;
&gt; &gt; &gt; @@ -3675,6 +3671,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,
&gt; &gt; &gt;  	if (!order) {
&gt; &gt; &gt;  		while (nr_allocated &lt; nr_pages) {
&gt; &gt; &gt;  			unsigned int nr, nr_pages_request;
&gt; &gt; &gt; +			int i;
&gt; &gt; &gt;  
&gt; &gt; &gt;  			/*
&gt; &gt; &gt;  			 * A maximum allowed request is hard-coded and is 100
&gt; &gt; &gt; @@ -3698,6 +3695,9 @@ vm_area_alloc_pages(gfp_t gfp, int nid,
&gt; &gt; &gt;  							nr_pages_request,
&gt; &gt; &gt;  							pages + nr_allocated);
&gt; &gt; &gt;  
&gt; &gt; &gt; +			for (i = nr_allocated; i &lt; nr_allocated + nr; i++)
&gt; &gt; &gt; +				inc_node_page_state(pages[i], NR_VMALLOC);
&gt; &gt; &gt; +
&gt; &gt; &gt;  			nr_allocated += nr;
&gt; &gt; &gt;  
&gt; &gt; &gt;  			/*
&gt; &gt; &gt; @@ -3722,6 +3722,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,
&gt; &gt; &gt;  		if (unlikely(!page))
&gt; &gt; &gt;  			break;
&gt; &gt; &gt;  
&gt; &gt; &gt; +		mod_node_page_state(page, NR_VMALLOC, 1 &lt;&lt; order);
&gt; &gt; &gt; +
&gt; &gt; &gt;  		/*
&gt; &gt; Can we move *_node_page_stat() to the end of the vm_area_alloc_pages()?
&gt; &gt; 
&gt; &gt; Or mod_node_page_state in first place should be invoked on high-order
&gt; &gt; page before split(to avoid of looping over small pages afterword)?
&gt; &gt; 
&gt; &gt; I mean it would be good to place to the one solid place. If it is possible
&gt; &gt; of course.
&gt; 
&gt; Note that the top one in the fast path IS called before the
&gt; split. We&#x27;re accounting in the same step size as the page allocator
&gt; can give us.
&gt; 
&gt; In the fallback paths (bulk allocator, and one-by-one loop), the issue
&gt; is that the individual pages could be coming from different nodes, so
&gt; they need to bump different counters. One possible solution would be
&gt; to remember the last node and accumulate until it differs, then flush:
&gt; 
&gt; fallback_loop() {
&gt; 	page = alloc_pages();
&gt; 	nid = page_to_nid(page);
&gt; 	if (nid != last_nid) {
&gt; 		if (node_count) {
&gt; 			mod_node_page_state(...);
&gt; 			node_count = 0;
&gt; 		}
&gt; 		last_nid = nid;
&gt; 	}
&gt; }
&gt; 
&gt; if (node_count)
&gt; 	mod_node_page_state(...);
&gt; 
&gt; But it IS the slow path, and these are fairly cheap per-cpu
&gt; counters. Especially compared to the cost of calling into the
&gt; allocator. So I&#x27;m not sure it&#x27;s worth it... What do you think?
&gt;
I see. I agree it is easier to keep original solution. I see that
Andrew took it, but just in case:

Reviewed-by: Uladzislau Rezki (Sony) &lt;urezki@gmail.com&gt;

--
Uladzislau Rezki

</pre>
</details>
</div>
</div>
</div>

    <footer>LKML Daily Activity Tracker</footer>
    <script>
    // When arriving via a date anchor (e.g. #2026-02-15 from a daily report),
    // scroll the anchor into view after a brief delay so layout is complete.
    (function () {
        var hash = window.location.hash;
        if (!hash) return;
        var target = document.getElementById(hash.slice(1));
        if (!target) return;
        setTimeout(function () {
            target.scrollIntoView({behavior: 'smooth', block: 'start'});
        }, 80);
    })();
    </script>
</body>
</html>