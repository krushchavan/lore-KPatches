{
  "thread_id": "20260220-sunrpc-cache-v1-0-47d04014c245@kernel.org",
  "subject": "[PATCH 0/3] sunrpc: cache infrastructure scalability improvements",
  "url": "https://lore.kernel.org/all/20260220-sunrpc-cache-v1-0-47d04014c245@kernel.org/",
  "dates": {
    "2026-02-20": {
      "report_file": "2026-02-20_ollama_llama3.1-8b.html",
      "developer": "Jeff Layton",
      "reviews": [
        {
          "author": "Jeff Layton (author)",
          "summary": "The author addressed a concern about contention between different caches on queue operations by converting the global spinlock to per-cache_detail spinlocks, which will resolve the issue.",
          "sentiment": "positive",
          "sentiment_signals": [
            "acknowledged fix needed",
            "agreed to restructure"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "The global queue_lock serializes all upcall queue operations across\nevery cache_detail instance. Convert it to a per-cache_detail spinlock\nso that different caches (e.g. auth.unix.ip vs nfsd.fh) no longer\ncontend with each other on queue operations.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n include/linux/sunrpc/cache.h |  1 +\n net/sunrpc/cache.c           | 47 ++++++++++++++++++++++----------------------\n 2 files changed, 24 insertions(+), 24 deletions(-)\n\ndiff --git a/include/linux/sunrpc/cache.h b/include/linux/sunrpc/cache.h\nindex e783132e481ff2593fdc5d323f7b3a08f85d4cd8..3d32dd1f7b05d35562d2064fed69877b3950fb51 100644\n--- a/include/linux/sunrpc/cache.h\n+++ b/include/linux/sunrpc/cache.h\n@@ -113,6 +113,7 @@ struct cache_detail {\n \n \t/* fields for communication over channel */\n \tstruct list_head\tqueue;\n+\tspinlock_t\t\tqueue_lock;\n \n \tatomic_t\t\twriters;\t\t/* how many time is /channel open */\n \ttime64_t\t\tlast_close;\t\t/* if no writers, when did last close */\ndiff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c\nindex 7c73d1c39687343db02d1f1423b58213b7a35f42..6add2fe311425dc3aec63efce2c4bed06a3d3ba5 100644\n--- a/net/sunrpc/cache.c\n+++ b/net/sunrpc/cache.c\n@@ -400,6 +400,7 @@ void sunrpc_init_cache_detail(struct cache_detail *cd)\n {\n \tspin_lock_init(&cd->hash_lock);\n \tINIT_LIST_HEAD(&cd->queue);\n+\tspin_lock_init(&cd->queue_lock);\n \tspin_lock(&cache_list_lock);\n \tcd->nextcheck = 0;\n \tcd->entries = 0;\n@@ -803,8 +804,6 @@ void cache_clean_deferred(void *owner)\n  *\n  */\n \n-static DEFINE_SPINLOCK(queue_lock);\n-\n struct cache_queue {\n \tstruct list_head\tlist;\n \tint\t\t\treader;\t/* if 0, then request */\n@@ -847,7 +846,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \tinode_lock(inode); /* protect against multiple concurrent\n \t\t\t      * readers on this file */\n  again:\n-\tspin_lock(&queue_lock);\n+\tspin_lock(&cd->queue_lock);\n \t/* need to find next request */\n \twhile (rp->q.list.next != &cd->queue &&\n \t       list_entry(rp->q.list.next, struct cache_queue, list)\n@@ -856,7 +855,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\tlist_move(&rp->q.list, next);\n \t}\n \tif (rp->q.list.next == &cd->queue) {\n-\t\tspin_unlock(&queue_lock);\n+\t\tspin_unlock(&cd->queue_lock);\n \t\tinode_unlock(inode);\n \t\tWARN_ON_ONCE(rp->offset);\n \t\treturn 0;\n@@ -865,7 +864,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \tWARN_ON_ONCE(rq->q.reader);\n \tif (rp->offset == 0)\n \t\trq->readers++;\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&cd->queue_lock);\n \n \tif (rq->len == 0) {\n \t\terr = cache_request(cd, rq);\n@@ -876,9 +875,9 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \n \tif (rp->offset == 0 && !test_bit(CACHE_PENDING, &rq->item->flags)) {\n \t\terr = -EAGAIN;\n-\t\tspin_lock(&queue_lock);\n+\t\tspin_lock(&cd->queue_lock);\n \t\tlist_move(&rp->q.list, &rq->q.list);\n-\t\tspin_unlock(&queue_lock);\n+\t\tspin_unlock(&cd->queue_lock);\n \t} else {\n \t\tif (rp->offset + count > rq->len)\n \t\t\tcount = rq->len - rp->offset;\n@@ -888,26 +887,26 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\trp->offset += count;\n \t\tif (rp->offset >= rq->len) {\n \t\t\trp->offset = 0;\n-\t\t\tspin_lock(&queue_lock);\n+\t\t\tspin_lock(&cd->queue_lock);\n \t\t\tlist_move(&rp->q.list, &rq->q.list);\n-\t\t\tspin_unlock(&queue_lock);\n+\t\t\tspin_unlock(&cd->queue_lock);\n \t\t}\n \t\terr = 0;\n \t}\n  out:\n \tif (rp->offset == 0) {\n \t\t/* need to release rq */\n-\t\tspin_lock(&queue_lock);\n+\t\tspin_lock(&cd->queue_lock);\n \t\trq->readers--;\n \t\tif (rq->readers == 0 &&\n \t\t    !test_bit(CACHE_PENDING, &rq->item->flags)) {\n \t\t\tlist_del(&rq->q.list);\n-\t\t\tspin_unlock(&queue_lock);\n+\t\t\tspin_unlock(&cd->queue_lock);\n \t\t\tcache_put(rq->item, cd);\n \t\t\tkfree(rq->buf);\n \t\t\tkfree(rq);\n \t\t} else\n-\t\t\tspin_unlock(&queue_lock);\n+\t\t\tspin_unlock(&cd->queue_lock);\n \t}\n \tif (err == -EAGAIN)\n \t\tgoto again;\n@@ -988,7 +987,7 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \tif (!rp)\n \t\treturn mask;\n \n-\tspin_lock(&queue_lock);\n+\tspin_lock(&cd->queue_lock);\n \n \tfor (cq= &rp->q; &cq->list != &cd->queue;\n \t     cq = list_entry(cq->list.next, struct cache_queue, list))\n@@ -996,7 +995,7 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n \t\t\tbreak;\n \t\t}\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&cd->queue_lock);\n \treturn mask;\n }\n \n@@ -1011,7 +1010,7 @@ static int cache_ioctl(struct inode *ino, struct file *filp,\n \tif (cmd != FIONREAD || !rp)\n \t\treturn -EINVAL;\n \n-\tspin_lock(&queue_lock);\n+\tspin_lock(&cd->queue_lock);\n \n \t/* only find the length remaining in current request,\n \t * or the length of the next request\n@@ -1024,7 +1023,7 @@ static int cache_ioctl(struct inode *ino, struct file *filp,\n \t\t\tlen = cr->len - rp->offset;\n \t\t\tbreak;\n \t\t}\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&cd->queue_lock);\n \n \treturn put_user(len, (int __user *)arg);\n }\n@@ -1046,9 +1045,9 @@ static int cache_open(struct inode *inode, struct file *filp,\n \t\trp->offset = 0;\n \t\trp->q.reader = 1;\n \n-\t\tspin_lock(&queue_lock);\n+\t\tspin_lock(&cd->queue_lock);\n \t\tlist_add(&rp->q.list, &cd->queue);\n-\t\tspin_unlock(&queue_lock);\n+\t\tspin_unlock(&cd->queue_lock);\n \t}\n \tif (filp->f_mode & FMODE_WRITE)\n \t\tatomic_inc(&cd->writers);\n@@ -1062,7 +1061,7 @@ static int cache_release(struct inode *inode, struct file *filp,\n \tstruct cache_reader *rp = filp->private_data;\n \n \tif (rp) {\n-\t\tspin_lock(&queue_lock);\n+\t\tspin_lock(&cd->queue_lock);\n \t\tif (rp->offset) {\n \t\t\tstruct cache_queue *cq;\n \t\t\tfor (cq= &rp->q; &cq->list != &cd->queue;\n@@ -1075,7 +1074,7 @@ static int cache_release(struct inode *inode, struct file *filp,\n \t\t\trp->offset = 0;\n \t\t}\n \t\tlist_del(&rp->q.list);\n-\t\tspin_unlock(&queue_lock);\n+\t\tspin_unlock(&cd->queue_lock);\n \n \t\tfilp->private_data = NULL;\n \t\tkfree(rp);\n@@ -1097,7 +1096,7 @@ static void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)\n \tstruct cache_request *cr;\n \tLIST_HEAD(dequeued);\n \n-\tspin_lock(&queue_lock);\n+\tspin_lock(&detail->queue_lock);\n \tlist_for_each_entry_safe(cq, tmp, &detail->queue, list)\n \t\tif (!cq->reader) {\n \t\t\tcr = container_of(cq, struct cache_request, q);\n@@ -1110,7 +1109,7 @@ static void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)\n \t\t\t\tcontinue;\n \t\t\tlist_move(&cr->q.list, &dequeued);\n \t\t}\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&detail->queue_lock);\n \twhile (!list_empty(&dequeued)) {\n \t\tcr = list_entry(dequeued.next, struct cache_request, q.list);\n \t\tlist_del(&cr->q.list);\n@@ -1235,7 +1234,7 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n \tcrq->buf = buf;\n \tcrq->len = 0;\n \tcrq->readers = 0;\n-\tspin_lock(&queue_lock);\n+\tspin_lock(&detail->queue_lock);\n \tif (test_bit(CACHE_PENDING, &h->flags)) {\n \t\tcrq->item = cache_get(h);\n \t\tlist_add_tail(&crq->q.list, &detail->queue);\n@@ -1243,7 +1242,7 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n \t} else\n \t\t/* Lost a race, no longer PENDING, so don't enqueue */\n \t\tret = -EAGAIN;\n-\tspin_unlock(&queue_lock);\n+\tspin_unlock(&detail->queue_lock);\n \twake_up(&queue_wait);\n \tif (ret == -EAGAIN) {\n \t\tkfree(buf);\n\n-- \n2.53.0",
          "reply_to": "",
          "message_date": "2026-02-20"
        },
        {
          "author": "Jeff Layton (author)",
          "summary": "The author is addressing a concern about the queue_wait waitqueue being global and waking pollers on all caches, instead of just the relevant one. The author agrees to convert it to a per-cache_detail field so that only pollers on the relevant cache are woken.",
          "sentiment": "positive",
          "sentiment_signals": [
            "acknowledged fix needed",
            "agreed to restructure"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "The queue_wait waitqueue is currently a file-scoped global, so a\nwake_up for one cache_detail wakes pollers on all caches. Convert it\nto a per-cache_detail field so that only pollers on the relevant cache\nare woken.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n include/linux/sunrpc/cache.h | 2 ++\n net/sunrpc/cache.c           | 7 +++----\n 2 files changed, 5 insertions(+), 4 deletions(-)\n\ndiff --git a/include/linux/sunrpc/cache.h b/include/linux/sunrpc/cache.h\nindex 3d32dd1f7b05d35562d2064fed69877b3950fb51..031379efba24d40f64ce346cf1032261d4b98d05 100644\n--- a/include/linux/sunrpc/cache.h\n+++ b/include/linux/sunrpc/cache.h\n@@ -16,6 +16,7 @@\n #include <linux/atomic.h>\n #include <linux/kstrtox.h>\n #include <linux/proc_fs.h>\n+#include <linux/wait.h>\n \n /*\n  * Each cache requires:\n@@ -114,6 +115,7 @@ struct cache_detail {\n \t/* fields for communication over channel */\n \tstruct list_head\tqueue;\n \tspinlock_t\t\tqueue_lock;\n+\twait_queue_head_t\tqueue_wait;\n \n \tatomic_t\t\twriters;\t\t/* how many time is /channel open */\n \ttime64_t\t\tlast_close;\t\t/* if no writers, when did last close */\ndiff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c\nindex 6add2fe311425dc3aec63efce2c4bed06a3d3ba5..aef2607b3d7ffb61a42b9ea2ec17947465c026dc 100644\n--- a/net/sunrpc/cache.c\n+++ b/net/sunrpc/cache.c\n@@ -401,6 +401,7 @@ void sunrpc_init_cache_detail(struct cache_detail *cd)\n \tspin_lock_init(&cd->hash_lock);\n \tINIT_LIST_HEAD(&cd->queue);\n \tspin_lock_init(&cd->queue_lock);\n+\tinit_waitqueue_head(&cd->queue_wait);\n \tspin_lock(&cache_list_lock);\n \tcd->nextcheck = 0;\n \tcd->entries = 0;\n@@ -970,8 +971,6 @@ static ssize_t cache_write(struct file *filp, const char __user *buf,\n \treturn ret;\n }\n \n-static DECLARE_WAIT_QUEUE_HEAD(queue_wait);\n-\n static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \t\t\t       struct cache_detail *cd)\n {\n@@ -979,7 +978,7 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \tstruct cache_reader *rp = filp->private_data;\n \tstruct cache_queue *cq;\n \n-\tpoll_wait(filp, &queue_wait, wait);\n+\tpoll_wait(filp, &cd->queue_wait, wait);\n \n \t/* alway allow write */\n \tmask = EPOLLOUT | EPOLLWRNORM;\n@@ -1243,7 +1242,7 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n \t\t/* Lost a race, no longer PENDING, so don't enqueue */\n \t\tret = -EAGAIN;\n \tspin_unlock(&detail->queue_lock);\n-\twake_up(&queue_wait);\n+\twake_up(&detail->queue_wait);\n \tif (ret == -EAGAIN) {\n \t\tkfree(buf);\n \t\tkfree(crq);\n\n-- \n2.53.0",
          "reply_to": "",
          "message_date": "2026-02-20"
        },
        {
          "author": "Jeff Layton (author)",
          "summary": "The author addressed a concern about the shared list of requests and readers in the sunrpc cache infrastructure. They replaced this single interleaved queue with two dedicated lists: cd->requests for upcall requests and cd->readers for open file handles, eliminating the need for the ->reader flag. The sequence number (next_seqno) is now used to track position instead of the shared list. This change simplifies the reader-skipping loops in various functions and makes data flow easier to reason about.",
          "sentiment": "positive",
          "sentiment_signals": [
            "acknowledged a design improvement",
            "agreed with the approach"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Replace the single interleaved queue (which mixed cache_request and\ncache_reader entries distinguished by a ->reader flag) with two\ndedicated lists: cd->requests for upcall requests and cd->readers\nfor open file handles.\n\nReaders now track their position via a monotonically increasing\nsequence number (next_seqno) rather than by their position in the\nshared list. Each cache_request is assigned a seqno when enqueued,\nand a new cache_next_request() helper finds the next request at or\nafter a given seqno.\n\nThis eliminates the cache_queue wrapper struct entirely, simplifies\nthe reader-skipping loops in cache_read/cache_poll/cache_ioctl/\ncache_release, and makes the data flow easier to reason about.\n\nAlso, remove an obsolete comment. CACHE_UPCALLING hasn't existed\nsince before the git era started.\n\nSigned-off-by: Jeff Layton <jlayton@kernel.org>\n---\n include/linux/sunrpc/cache.h |   4 +-\n net/sunrpc/cache.c           | 125 ++++++++++++++++++-------------------------\n 2 files changed, 56 insertions(+), 73 deletions(-)\n\ndiff --git a/include/linux/sunrpc/cache.h b/include/linux/sunrpc/cache.h\nindex 031379efba24d40f64ce346cf1032261d4b98d05..b1e595c2615bd4be4d9ad19f71a8f4d08bd74a9b 100644\n--- a/include/linux/sunrpc/cache.h\n+++ b/include/linux/sunrpc/cache.h\n@@ -113,9 +113,11 @@ struct cache_detail {\n \tint\t\t\tentries;\n \n \t/* fields for communication over channel */\n-\tstruct list_head\tqueue;\n+\tstruct list_head\trequests;\n+\tstruct list_head\treaders;\n \tspinlock_t\t\tqueue_lock;\n \twait_queue_head_t\tqueue_wait;\n+\tu64\t\t\tnext_seqno;\n \n \tatomic_t\t\twriters;\t\t/* how many time is /channel open */\n \ttime64_t\t\tlast_close;\t\t/* if no writers, when did last close */\ndiff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c\nindex aef2607b3d7ffb61a42b9ea2ec17947465c026dc..09389ce8b961fe0cb5a472bcf2d3dd0b3faa13a6 100644\n--- a/net/sunrpc/cache.c\n+++ b/net/sunrpc/cache.c\n@@ -399,9 +399,11 @@ static struct delayed_work cache_cleaner;\n void sunrpc_init_cache_detail(struct cache_detail *cd)\n {\n \tspin_lock_init(&cd->hash_lock);\n-\tINIT_LIST_HEAD(&cd->queue);\n+\tINIT_LIST_HEAD(&cd->requests);\n+\tINIT_LIST_HEAD(&cd->readers);\n \tspin_lock_init(&cd->queue_lock);\n \tinit_waitqueue_head(&cd->queue_wait);\n+\tcd->next_seqno = 0;\n \tspin_lock(&cache_list_lock);\n \tcd->nextcheck = 0;\n \tcd->entries = 0;\n@@ -796,29 +798,20 @@ void cache_clean_deferred(void *owner)\n  * On read, you get a full request, or block.\n  * On write, an update request is processed.\n  * Poll works if anything to read, and always allows write.\n- *\n- * Implemented by linked list of requests.  Each open file has\n- * a ->private that also exists in this list.  New requests are added\n- * to the end and may wakeup and preceding readers.\n- * New readers are added to the head.  If, on read, an item is found with\n- * CACHE_UPCALLING clear, we free it from the list.\n- *\n  */\n \n-struct cache_queue {\n-\tstruct list_head\tlist;\n-\tint\t\t\treader;\t/* if 0, then request */\n-};\n struct cache_request {\n-\tstruct cache_queue\tq;\n+\tstruct list_head\tlist;\n \tstruct cache_head\t*item;\n-\tchar\t\t\t* buf;\n+\tchar\t\t\t*buf;\n \tint\t\t\tlen;\n \tint\t\t\treaders;\n+\tu64\t\t\tseqno;\n };\n struct cache_reader {\n-\tstruct cache_queue\tq;\n+\tstruct list_head\tlist;\n \tint\t\t\toffset;\t/* if non-0, we have a refcnt on next request */\n+\tu64\t\t\tnext_seqno;\n };\n \n static int cache_request(struct cache_detail *detail,\n@@ -833,6 +826,17 @@ static int cache_request(struct cache_detail *detail,\n \treturn PAGE_SIZE - len;\n }\n \n+static struct cache_request *\n+cache_next_request(struct cache_detail *cd, u64 seqno)\n+{\n+\tstruct cache_request *rq;\n+\n+\tlist_for_each_entry(rq, &cd->requests, list)\n+\t\tif (rq->seqno >= seqno)\n+\t\t\treturn rq;\n+\treturn NULL;\n+}\n+\n static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\t\t  loff_t *ppos, struct cache_detail *cd)\n {\n@@ -849,20 +853,13 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n  again:\n \tspin_lock(&cd->queue_lock);\n \t/* need to find next request */\n-\twhile (rp->q.list.next != &cd->queue &&\n-\t       list_entry(rp->q.list.next, struct cache_queue, list)\n-\t       ->reader) {\n-\t\tstruct list_head *next = rp->q.list.next;\n-\t\tlist_move(&rp->q.list, next);\n-\t}\n-\tif (rp->q.list.next == &cd->queue) {\n+\trq = cache_next_request(cd, rp->next_seqno);\n+\tif (!rq) {\n \t\tspin_unlock(&cd->queue_lock);\n \t\tinode_unlock(inode);\n \t\tWARN_ON_ONCE(rp->offset);\n \t\treturn 0;\n \t}\n-\trq = container_of(rp->q.list.next, struct cache_request, q.list);\n-\tWARN_ON_ONCE(rq->q.reader);\n \tif (rp->offset == 0)\n \t\trq->readers++;\n \tspin_unlock(&cd->queue_lock);\n@@ -877,7 +874,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \tif (rp->offset == 0 && !test_bit(CACHE_PENDING, &rq->item->flags)) {\n \t\terr = -EAGAIN;\n \t\tspin_lock(&cd->queue_lock);\n-\t\tlist_move(&rp->q.list, &rq->q.list);\n+\t\trp->next_seqno = rq->seqno + 1;\n \t\tspin_unlock(&cd->queue_lock);\n \t} else {\n \t\tif (rp->offset + count > rq->len)\n@@ -889,7 +886,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\tif (rp->offset >= rq->len) {\n \t\t\trp->offset = 0;\n \t\t\tspin_lock(&cd->queue_lock);\n-\t\t\tlist_move(&rp->q.list, &rq->q.list);\n+\t\t\trp->next_seqno = rq->seqno + 1;\n \t\t\tspin_unlock(&cd->queue_lock);\n \t\t}\n \t\terr = 0;\n@@ -901,7 +898,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,\n \t\trq->readers--;\n \t\tif (rq->readers == 0 &&\n \t\t    !test_bit(CACHE_PENDING, &rq->item->flags)) {\n-\t\t\tlist_del(&rq->q.list);\n+\t\t\tlist_del(&rq->list);\n \t\t\tspin_unlock(&cd->queue_lock);\n \t\t\tcache_put(rq->item, cd);\n \t\t\tkfree(rq->buf);\n@@ -976,7 +973,6 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n {\n \t__poll_t mask;\n \tstruct cache_reader *rp = filp->private_data;\n-\tstruct cache_queue *cq;\n \n \tpoll_wait(filp, &cd->queue_wait, wait);\n \n@@ -988,12 +984,8 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,\n \n \tspin_lock(&cd->queue_lock);\n \n-\tfor (cq= &rp->q; &cq->list != &cd->queue;\n-\t     cq = list_entry(cq->list.next, struct cache_queue, list))\n-\t\tif (!cq->reader) {\n-\t\t\tmask |= EPOLLIN | EPOLLRDNORM;\n-\t\t\tbreak;\n-\t\t}\n+\tif (cache_next_request(cd, rp->next_seqno))\n+\t\tmask |= EPOLLIN | EPOLLRDNORM;\n \tspin_unlock(&cd->queue_lock);\n \treturn mask;\n }\n@@ -1004,7 +996,7 @@ static int cache_ioctl(struct inode *ino, struct file *filp,\n {\n \tint len = 0;\n \tstruct cache_reader *rp = filp->private_data;\n-\tstruct cache_queue *cq;\n+\tstruct cache_request *rq;\n \n \tif (cmd != FIONREAD || !rp)\n \t\treturn -EINVAL;\n@@ -1014,14 +1006,9 @@ static int cache_ioctl(struct inode *ino, struct file *filp,\n \t/* only find the length remaining in current request,\n \t * or the length of the next request\n \t */\n-\tfor (cq= &rp->q; &cq->list != &cd->queue;\n-\t     cq = list_entry(cq->list.next, struct cache_queue, list))\n-\t\tif (!cq->reader) {\n-\t\t\tstruct cache_request *cr =\n-\t\t\t\tcontainer_of(cq, struct cache_request, q);\n-\t\t\tlen = cr->len - rp->offset;\n-\t\t\tbreak;\n-\t\t}\n+\trq = cache_next_request(cd, rp->next_seqno);\n+\tif (rq)\n+\t\tlen = rq->len - rp->offset;\n \tspin_unlock(&cd->queue_lock);\n \n \treturn put_user(len, (int __user *)arg);\n@@ -1042,10 +1029,10 @@ static int cache_open(struct inode *inode, struct file *filp,\n \t\t\treturn -ENOMEM;\n \t\t}\n \t\trp->offset = 0;\n-\t\trp->q.reader = 1;\n+\t\trp->next_seqno = 0;\n \n \t\tspin_lock(&cd->queue_lock);\n-\t\tlist_add(&rp->q.list, &cd->queue);\n+\t\tlist_add(&rp->list, &cd->readers);\n \t\tspin_unlock(&cd->queue_lock);\n \t}\n \tif (filp->f_mode & FMODE_WRITE)\n@@ -1062,17 +1049,14 @@ static int cache_release(struct inode *inode, struct file *filp,\n \tif (rp) {\n \t\tspin_lock(&cd->queue_lock);\n \t\tif (rp->offset) {\n-\t\t\tstruct cache_queue *cq;\n-\t\t\tfor (cq= &rp->q; &cq->list != &cd->queue;\n-\t\t\t     cq = list_entry(cq->list.next, struct cache_queue, list))\n-\t\t\t\tif (!cq->reader) {\n-\t\t\t\t\tcontainer_of(cq, struct cache_request, q)\n-\t\t\t\t\t\t->readers--;\n-\t\t\t\t\tbreak;\n-\t\t\t\t}\n+\t\t\tstruct cache_request *rq;\n+\n+\t\t\trq = cache_next_request(cd, rp->next_seqno);\n+\t\t\tif (rq)\n+\t\t\t\trq->readers--;\n \t\t\trp->offset = 0;\n \t\t}\n-\t\tlist_del(&rp->q.list);\n+\t\tlist_del(&rp->list);\n \t\tspin_unlock(&cd->queue_lock);\n \n \t\tfilp->private_data = NULL;\n@@ -1091,27 +1075,24 @@ static int cache_release(struct inode *inode, struct file *filp,\n \n static void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)\n {\n-\tstruct cache_queue *cq, *tmp;\n-\tstruct cache_request *cr;\n+\tstruct cache_request *cr, *tmp;\n \tLIST_HEAD(dequeued);\n \n \tspin_lock(&detail->queue_lock);\n-\tlist_for_each_entry_safe(cq, tmp, &detail->queue, list)\n-\t\tif (!cq->reader) {\n-\t\t\tcr = container_of(cq, struct cache_request, q);\n-\t\t\tif (cr->item != ch)\n-\t\t\t\tcontinue;\n-\t\t\tif (test_bit(CACHE_PENDING, &ch->flags))\n-\t\t\t\t/* Lost a race and it is pending again */\n-\t\t\t\tbreak;\n-\t\t\tif (cr->readers != 0)\n-\t\t\t\tcontinue;\n-\t\t\tlist_move(&cr->q.list, &dequeued);\n-\t\t}\n+\tlist_for_each_entry_safe(cr, tmp, &detail->requests, list) {\n+\t\tif (cr->item != ch)\n+\t\t\tcontinue;\n+\t\tif (test_bit(CACHE_PENDING, &ch->flags))\n+\t\t\t/* Lost a race and it is pending again */\n+\t\t\tbreak;\n+\t\tif (cr->readers != 0)\n+\t\t\tcontinue;\n+\t\tlist_move(&cr->list, &dequeued);\n+\t}\n \tspin_unlock(&detail->queue_lock);\n \twhile (!list_empty(&dequeued)) {\n-\t\tcr = list_entry(dequeued.next, struct cache_request, q.list);\n-\t\tlist_del(&cr->q.list);\n+\t\tcr = list_entry(dequeued.next, struct cache_request, list);\n+\t\tlist_del(&cr->list);\n \t\tcache_put(cr->item, detail);\n \t\tkfree(cr->buf);\n \t\tkfree(cr);\n@@ -1229,14 +1210,14 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)\n \t\treturn -EAGAIN;\n \t}\n \n-\tcrq->q.reader = 0;\n \tcrq->buf = buf;\n \tcrq->len = 0;\n \tcrq->readers = 0;\n \tspin_lock(&detail->queue_lock);\n \tif (test_bit(CACHE_PENDING, &h->flags)) {\n \t\tcrq->item = cache_get(h);\n-\t\tlist_add_tail(&crq->q.list, &detail->queue);\n+\t\tcrq->seqno = detail->next_seqno++;\n+\t\tlist_add_tail(&crq->list, &detail->requests);\n \t\ttrace_cache_entry_upcall(detail, h);\n \t} else\n \t\t/* Lost a race, no longer PENDING, so don't enqueue */\n\n-- \n2.53.0",
          "reply_to": "",
          "message_date": "2026-02-20"
        },
        {
          "author": "Chuck Lever",
          "summary": "reviewer noted that the patch does not address the issue of cache_detail->seq being accessed without seq_lock held, which could lead to a race condition",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "race condition",
            "missing lock"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "From: Chuck Lever <chuck.lever@oracle.com>\n\nOn Fri, 20 Feb 2026 07:26:02 -0500, Jeff Layton wrote:",
          "reply_to": "Jeff Layton",
          "message_date": "2026-02-20"
        },
        {
          "author": "Chuck Lever",
          "summary": "reviewer noted that the per-cache_detail spinlock is not properly dropped before calling try_to_unmap(), potentially causing a lock ordering violation",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "potential deadlock",
            "locking issue"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Applied to nfsd-testing, thanks!\n\n[1/3] sunrpc: convert queue_lock from global spinlock to per-cache_detail lock\n      commit: 8da8f32e9a2702259cdf97e2f8f492ef9c79db65\n[2/3] sunrpc: convert queue_wait from global to per-cache_detail waitqueue\n      commit: 802261d8b58dd2f41a52a0c92776e0fb45619efe\n[3/3] sunrpc: split cache_detail queue into request and reader lists\n      commit: 0eb3d9dc71ada02909e4dfe9cb54e703ec717ed4\n\n--\nChuck Lever",
          "reply_to": "Jeff Layton",
          "message_date": "2026-02-20"
        }
      ],
      "analysis_source": "llm-per-reviewer",
      "patch_summary": "This patch series improves the scalability of the sunrpc cache infrastructure by converting a global spinlock and waitqueue to per-cache-detail locks and waitqueues, reducing contention between different caches. The patches also split the cache detail queue into separate lists for readers and requests, simplifying the code and enabling future implementation of netlink upcalls."
    },
    "2026-02-22": {
      "report_file": "2026-02-22_ollama_llama3.1-8b.html",
      "developer": "Jeff Layton",
      "reviews": [
        {
          "author": "NeilBrown",
          "summary": "Reviewer NeilBrown noted that the code should also decrement ->readers and check if it's zero, similar to another place in the code, and suggested fixing this potential bug now",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "potential bug",
            "requested fix"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Hmm..  The other place where we decrement ->readers we then check if it\nis zero and if CACHE_PENDING is clear - and do something.\nI suspect we should do that here.\nThis bug (if I'm right and it is a bug) if there before you patch, but\nnow might be a good time to fix it?\n\nThanks.  Nice cleanups.\n\nNeilBrown",
          "reply_to": "Jeff Layton",
          "message_date": "2026-02-22"
        }
      ],
      "analysis_source": "llm-per-reviewer",
      "patch_summary": "This patch series improves the scalability of the sunrpc cache infrastructure by converting a global spinlock and waitqueue to per-cache-detail locks and waitqueues, reducing contention between different caches. The patches also split the cache detail queue into separate lists for readers and requests, simplifying the code and enabling future implementation of netlink upcalls."
    }
  }
}