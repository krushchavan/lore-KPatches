{
  "thread_id": "aZw1DlKHaWvgOtm_@thinkstation",
  "subject": "Re: [LSF/MM/BPF TOPIC] 64k (or 16k) base page size on x86",
  "url": "https://lore.kernel.org/all/aZw1DlKHaWvgOtm_@thinkstation/",
  "dates": {
    "2026-02-19": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "Pedro Falcato",
          "summary": "Reviewer Pedro Falcato questioned the relevance of the proposed patch, citing that memory transparency can be achieved through mTHP (multi-THP) by simply toggling a kernel parameter in /sys/kernel/mm/transparent_hugepage.\n\nReviewer Pedro Falcato suggested adding a way to enforce a minimum allocation order globally on the page cache to address scalability issues, and noted that some points raised in the patch are not addressed by existing work (1G THPs) or are being addressed separately (memdesc work).",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "questioning relevance",
            "alternative solution",
            "requested changes",
            "suggested alternative solution"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Doesn't this idea make less sense these days, with mTHP? Simply by toggling one\nof the entries in /sys/kernel/mm/transparent_hugepage.\n\n---\n\nWe could perhaps add a way to enforce a min_order globally on the page cache,\nas a way to address it.\n\nThere are some points there which aren't addressed by mTHP work in any way\n(1G THPs for one), others which are being addressed separately (memdesc work\ntrying to cut down on struct page overhead).\n\n(I also don't understand your point about order-5 allocation, AFAIK pcp will\ncache up to COSTLY_ORDER (3) and PMD order, but I'm probably not seeing the\nfull picture)\n\n\n-- \nPedro",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-19"
        },
        {
          "author": "David (Arm)",
          "summary": "Reviewer suggested emulating a larger page size (64K) for user space on x86, while still using 4K pages internally to reduce zone lock contention and other issues.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "When discussing per-process page sizes with Ryan and Dev, I mentioned \nthat having a larger emulated page size could be interesting for other \narchitectures as well.\n\nThat is, we would emulate a 64K page size on Intel for user space as \nwell, but let the OS work with 4K pages.\n\nWe'd only allocate+map large folios into user space + pagecache, but \nstill allow for page tables etc. to not waste memory.\n\nSo \"most\" of your allocations in the system would actually be at least \n64k, reducing zone lock contention etc.\n\n\nIt doesn't solve all the problems you wanted to tackle on your list \n(e.g., \"struct page\" overhead, which will be sorted out by memdescs).\n\n-- \nCheers,\n\nDavid",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-19"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "Author responded to concerns about fragmentation by explaining that mTHP is a best effort mechanism that doesn't require considering fragmentation, and will allocate 64k pages as long as there's free memory.\n\nThe author addressed Pedro's concern about the increased work for the page allocator to merge/split buddy pages, explaining that it would be cheaper with a higher base page size.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarifying explanation",
            "no clear resolution signal",
            "acknowledged and explained the benefit"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "mTHP is still best effort. This is way you don't need to care about\nfragmentation, you will get your 64k page as long as you have free\nmemory.\n\n---\n\nWith higher base page size, page allocator doesn't need to do as much\nwork to merge/split buddy pages. So serving the same 2M as order-5 is\ncheaper than order-9.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
          "reply_to": "Pedro Falcato",
          "message_date": "2026-02-19"
        },
        {
          "author": "David (Arm)",
          "summary": "Reviewer David noted that the proposed change would lead to reduced page table merging and splitting due to larger allocation sizes, which is expected to naturally reduce fragmentation",
          "sentiment": "neutral",
          "sentiment_signals": [
            "no clear technical objection or suggestion"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "I think the idea is that if most of your allocations (anon + pagecache) \nare 64k instead of 4k, on average, you'll just naturally do less merging \nsplitting.\n\n-- \nCheers,\n\nDavid",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-19"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "Author pushed back on reviewer's suggestion that emulation could help reduce zone lock contention, arguing it would actually increase contention due to frequent splitting and merging of page requests.\n\nAuthor acknowledged that serving 1G pages from the buddy allocator is not feasible with a 4k order-0 allocation size, and expressed uncertainty about how to achieve viable 1G THPs without it.",
          "sentiment": "contentious",
          "sentiment_signals": [
            "pushed_back",
            "disagreed",
            "uncertainty",
            "acknowledgment of limitation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "I am not convinced emulation would help zone lock contention. I expect\ncontention to be higher if page allocator would see a mix of 4k and 64k\nrequests. It sounds like constant split/merge under the lock.\n\n---\n\nI don't think we can serve 1G pages out of buddy allocator with 4k\norder-0. And without it, I don't see how to get to a viable 1G THPs.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-19"
        },
        {
          "author": "David (Arm)",
          "summary": "Reviewer noted that if most allocations are larger than 64k, the benefits of splitting page size into PTE_SIZE and PG_SIZE may be minimal, as there would be less need for splitting/merging smaller allocations.\n\nReviewer David expressed skepticism about the proposed patch, suggesting that previous work by Zi Yan might provide a solution and implying that further discussion is needed.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "skepticism",
            "request for further discussion"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "If most your allocations are larger, then there isn't that much \nsplitting/merging.\n\nThere will be some for the < 64k allocations of course, but when all \nuser space+page cache is >= 64 then the split/merge + zone lock should \nbe heavily reduced.\n\n---\n\nZi Yan was one working on this, and I think we had ideas on how to make \nthat work in the long run.\n\n-- \nCheers,\n\nDavid",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-19"
        },
        {
          "author": "Dave Hansen",
          "summary": "Reviewer Dave Hansen noted that while splitting PAGE_SIZE into PTE_SIZE and PG_SIZE is a worthy endeavor for improving allocator performance and reducing 'struct page' overhead, the actual memory savings may be minimal due to increased RAM usage by the larger page cache. He provided empirical evidence from his kernel tree showing that a 64k page cache consumes around 5GB of extra memory.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "NEEDS_WORK",
            "POSITIVE"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "First of all, this looks like fun. Nice work! I'm not opposed at all in\nconcept to cleaning up things and doing the logical separation you\ndescribed to split buddy granularity and mapping granularity. That seems\nlike a worthy endeavor and some of the union/#define tricks look like a\nlikely viable way to do it incrementally.\n\nBut I don't think there's going to be a lot of memory savings in the\nend. Maybe this would bring the mem= hyperscalers back into the fold and\nhave them actually start using 'struct page' again for their VM memory.\nDunno.\n\nBut, let's look at my kernel directory and round the file sizes up to\n4k, 16k and 64k:\n\nfind .  -printf '%s\\n' | while read size; do echo\t\\\n\t\t$(((size + 0x0fff) & 0xfffff000))\t\\\n\t\t$(((size + 0x3fff) & 0xffffc000))\t\\\n\t\t$(((size + 0xffff) & 0xffff0000));\ndone\n\n... and add them all up:\n\n11,297,648 KB - on disk\n11,297,712 KB - in a 4k page cache\n12,223,488 KB - in a 16k page cache\n16,623,296 KB - in a 64k page cache\n\nSo a 64k page cache eats ~5GB of extra memory for a kernel tree (well,\n_my_ kernel tree). In other words, if you are looking for memory savings\non my laptop, you'll need ~300GB of RAM before 'struct page' overhead\noverwhelms the page cache bloat from a single kernel tree.\n\nThe whole kernel obviously isn't in the page cache all at the same time.\nThe page cache across the system is also obviously different than a\nkernel tree, but you get the point.\n\nThat's not to diminish how useful something like this might be,\nespecially for folks that are sensitive to 'struct page' overhead or\nallocator performance.\n\nBut, it will mostly be getting better performance at the _cost_ of\nconsuming more RAM, not saving RAM.",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-19"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "Author asks for clarification on whether the proposed page size change should enforce alignment of user-space mappings.\n\nThe author acknowledges that memory waste for page tables is a solvable issue and proposes using slab allocation to address it, indicating no immediate fix but a potential solution in the future.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "clarifying question",
            "acknowledges a problem",
            "proposes a solution"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Just to clarify, do you want it to be enforced on userspace ABI.\nLike, all mappings are 64k aligned?\n\n---\n\nWaste of memory for page table is solvable and pretty straight forward.\nMost of such cases can be solve mechanically by switching to slab.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-19"
        },
        {
          "author": "Dave Hansen",
          "summary": "Reviewer Dave Hansen noted that the patch does not handle the case where PTE_SIZE is less than PG_SIZE, including misaligned cases, and requested that the page fault handler be updated to handle this scenario.\n\nReviewer noted that the patch introduces a large number of changes across multiple files, primarily due to renaming, and requested further analysis on the actual code modifications.\n\nReviewer Dave Hansen noted that the patch requires auditing and suggested separating logic changes from purely mechanical modifications to improve review sanity.\n\nReviewer Dave Hansen noted that the proposed patch is not independent of other kernel changes, specifically the conversion to ptdescs, and suggested that the series should focus on identifying additional requirements before proceeding",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "large number of changes",
            "primarily due to renaming",
            "requested_changes",
            "gates",
            "other things need to get done"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On 2/19/26 07:08, Kiryl Shutsemau wrote:\n...\n\n---\n\nA few notes about the diffstats:\n\n$ git diff v6.17..HEAD arch/x86 | diffstat | tail -1\n 105 files changed, 874 insertions(+), 843 deletions(-)\n$ git diff v6.17..HEAD mm | diffstat | tail -1\n 53 files changed, 1136 insertions(+), 1069 deletions(-)\n\nThe vast, vast majority of this seems to be the renames. Stuff like:\n\n---\n\nThat stuff obviously needs to be audited but it's far less concerning\nthan the logic changes.\n\nSo just for review sanity, if you go forward with this, I'd very much\nappreciate a strong separation of the purely mechanical bits from any\nlogic changes.\n\n---\n\nOthers mentioned this, but I think this essentially gates what you are\ndoing behind a full tree conversion over to ptdescs.\n\nThe most useful thing we can do with this series is look at it and\ndecide what _other_ things need to get done before the tree could\npossibly go in that direction, like ptdesc or a the disambiguation\nbetween PTE_SIZE and PG_SIZE that you've kicked off here.",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-19"
        },
        {
          "author": "Matthew Wilcox",
          "summary": "Reviewer questioned the proposed slab allocation approach and suggested an alternative method of allocating a larger page size and using it for multiple entries",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Have you looked at the s390/ppc implementations (yes, they're different,\nno, that sucks)?  slab seems like the wrong approach to me.\n\nThere's a third approach that I've never looked at which is to allocate\nthe larger size, then just use it for N consecutive entries.",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-19"
        },
        {
          "author": "Pedro Falcato",
          "summary": "Reviewer Pedro Falcato noted that the proposed patch would result in 90%+ of allocations being 64k, which he hopes would be achieved by combining this change with an increase in slab_min_order.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "hopes",
            "depending"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Yep. That plus slab_min_order would hopefully yield a system where 90%+\n(depending on how your filesystem's buffer cache works) allocations are 64K.\n\n-- \nPedro",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-19"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "Author acknowledges that struct page memory consumption is a problem, but notes it's static and cannot be reclaimed, whereas page cache rounding overhead can be controlled by userspace",
          "sentiment": "neutral",
          "sentiment_signals": [
            "acknowledged the issue",
            "provided explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "That's fair.\n\nThe problem with struct page memory consumption is that it is static and\ncannot be reclaimed. You pay the struct page tax no matter what.\n\nPage cache rounding overhead can be large, but a motivated userspace can\nkeep it under control by avoiding splitting a dataset into many small\nfiles. And this memory is reclaimable.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
          "reply_to": "Dave Hansen",
          "message_date": "2026-02-19"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "Author acknowledged that packing of page tables is not necessary for correctness and plans to implement a proof-of-concept (PoC) without it, but needs to catch up on ptdescs first.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "acknowledged need to catch up on related code",
            "planning PoC implementation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "I have not followed ptdescs closely. Need to catch up.\n\nFor PoC, I will just waste full order-0 page for page table. Packing is\nnot required for correctness.",
          "reply_to": "Dave Hansen",
          "message_date": "2026-02-19"
        },
        {
          "author": "Dave Hansen",
          "summary": "Reviewer noted that the proposed change to separate PAGE_SIZE into PTE_SIZE and PG_SIZE would not significantly impact the KPTI pgd allocation size, as the current 8k allocation will fit within a 128k page.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "no clear technical objection or suggestion"
          ],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Yeah, I guess padding it out is ugly but effective.\n\nI was trying to figure out how it would apply to the KPTI pgd because we\njust flip bit 12 to switch between user and kernel PGDs. But I guess the\n8k of PGDs in the current allocation will fit fine in 128k, so it's\nweird but functional.",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-19"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author acknowledges that they haven't spent much time on the patch and implies it's still a work in progress, but doesn't commit to revising or addressing specific concerns.\n\nAuthor acknowledges that populating 16 page table entries of the parent page table is a possible way to handle the new page size, but doesn't address the specific concern about fragmentation within the page.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "acknowledges lack of effort",
            "implies ongoing work",
            "acknowledged alternative solution",
            "no clear resolution"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "I was the first thing that came to mind. I have not put much time into\nit\n\n---\n\nYeah, that's a possible way. We would need to populate 16 page table\nentries of the parent page table. But you don't need to care about\nfragmentation within the page.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
          "reply_to": "Matthew Wilcox",
          "message_date": "2026-02-19"
        },
        {
          "author": "Kalesh Singh",
          "summary": "Reviewer Kalesh Singh noted that the patch does not address the complexity of handling PTE_SIZE < PG_SIZE, including misaligned cases in the page fault handler, and requested further work to handle this scenario.\n\nReviewer Kalesh Singh noted that the current design does not enforce a larger granularity on VMAs to emulate a userspace page size, which is necessary for Android's use case of emulating 16KB devices on x86, and requested discussion on extending the design to cover this use case.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "discussion"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Thu, Feb 19, 2026 at 7:39 AM David Hildenbrand (Arm)\n<david@kernel.org> wrote:\n\n---\n\nHi Kiryl,\n\nI'd be interested to discuss this at LSFMM.\n\nOn Android, we have a separate but related use case: we emulate the\nuserspace page size on x86, primarily to enable app developers to\nconduct compatibility testing of their apps for 16KB Android devices.\n[1]\n\nIt mainly works by enforcing a larger granularity on the VMAs to\nemulate a userspace page size, somewhat similar to what David\nmentioned, while the underlying kernel still operates on a 4KB\ngranularity. [2]\n\nIIUC the current design would not enfore the larger granularity /\nalignment for VMAs to avoid breaking ABI. However, I'd be interest to\ndiscuss whether it can be extended to cover this usecase as well.\n\n[1]  https://developer.android.com/guide/practices/page-sizes#16kb-emulator\n[2] https://source.android.com/docs/core/architecture/16kb-page-size/getting-started-cf-x86-64-pgagnostic\n\nThanks,\nKalesh",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-19"
        },
        {
          "author": "Zi Yan",
          "summary": "Reviewer suggested adding a super pageblock concept that consists of N consecutive pageblocks, allowing anti-fragmentation to work at larger granularity (e.g., 1GB), and questioned whether free pages from memory compaction should go into the buddy allocator or not.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "debatable"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Right. The idea is to add super pageblock (or whatever name), which consists of N consecutive\npageblocks, so that anti fragmentation can work at larger granularity, e.g., 1GB, to create\nfree pages. Whether 1GB free pages from memory compaction need to go into buddy allocator\nor not is debatable.\n\n--\nBest Regards,\nYan, Zi",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-19"
        },
        {
          "author": "Liam Howlett",
          "summary": "Reviewer Liam Howlett noted that increasing page size may not be beneficial for systems under memory pressure due to increased reclaim overhead, which can degrade primary workloads and lead to more frequent OOMs.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "But we are in reclaim a lot more these days.  As I'm sure you are aware,\nwe are trying to maximize the resources (both cpu and ram) of any\nmachine powered on.  Entering reclaim will consume the cpu time and will\naffect other tasks.\n\nEspecially with multiple workload machines, the tendency is to have a\nprimary focus with the lower desired work being killed, if necessary.\nReducing the overhead just means more secondary tasks, or a bigger\nfootprint of the ones already executing.\n\nIncreasing the memory pressure will degrade the primary workload more\nfrequently, even if we recover enough to avoid OOMing the secondary.\n\nWhile in the struct page tax world, the secondary task would be killed\nafter a shorter (and less frequently executed) reclaim comes up short.\nSo, I would think that we would be degrading the primary workload in an\nattempt to keep the secondary alive?  Maybe I'm over-simplifying here?\n\nNear the other end of the spectrum, we have chromebooks that are\nconstantly in reclaim, even with 4k pages.  I guess these machines would\nbe destine to maintain the same page size they use today.  That is, this\nsolution for the struct page tax is only useful if you have a lot of\nmemory.  But then again, that's where the bookkeeping costs become hard\nto take.\n\nThanks,\nLiam",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-19"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-20": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "David Laight",
          "summary": "Reviewer David Laight noted that the patch does not address the issue of TLB coalescing on machines without native 64k pages, which could lead to performance issues.\n\nReviewer David Laight raised concerns about potential issues with PAGE_SIZE being larger than 4k, specifically mentioning 'random' buffers that are PAGE_SIZE and the impact on mmap of kernel memory and PCIe window alignment.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "performance",
            "TLB",
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Thu, 19 Feb 2026 15:08:51 +0000\nKiryl Shutsemau <kas@kernel.org> wrote:\n\n---\n\nAlso the 'random' buffers that are PAGE_SIZE rather than 4k.\n\nI also wonder how is affects mmap of kernel memory and the alignement\nof PCIe windows (etc).\n\n\tDavid",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-20"
        },
        {
          "author": "David (Arm)",
          "summary": "Reviewer noted that having a 64k page size could allow running emulated 64k processes alongside native 4k processes on the same machine, eliminating the need for 'vma crosses base pages' handling.\n\nReviewer David (Arm) expressed concerns that the proposed patch would reintroduce memory waste issues on x86, similar to those experienced by Arm users when they switched from 64k to 4k page size. He noted that achieving 64k native performance is hard and questioned why the proposal doesn't explore per-process page sizes like Arm does.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "no clear opinion or request",
            "requested changes",
            "concerns about memory waste"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Right, see the proposal from Dev on the list.\n\n From user-space POV, the pagesize would be 64K for these emulated \nprocesses. That is, VMAs must be suitable aligned etc.\n\nOne key thing I think is that you could run such emulated-64k process \n(that actually support it!) with 4k processes on the same machine, like \nArm is considering.\n\nYou would have no weird \"vma crosses base pages\" handling, which is just \nrather nasty and makes my head hurt.\n\n---\n\nWell, yes, like Willy says, there are already similar custom solutions \nfor s390x and ppc.\n\nPasha talked recently about the memory waste of 16k kernel stacks and \nhow we would want to reduce that to 4k. In your proposal, it would be \n64k, unless you somehow manage to allocate multiple kernel stacks from \nthe same 64k page. My head hurts thinking about whether that could work, \nmaybe it could (no idea about guard pages in there, though).\n\n\nLet's take a look at the history of page size usage on Arm (people can \nfeel free to correct me):\n\n(1) Most distros were using 64k on Arm.\n\n(2) People realized that 64k was suboptimal many use cases (memory\n     waste for stacks, pagecache, etc) and started to switch to 4k. I\n     remember that mostly HPC-centric users sticked to 64k, but there was\n     also demand from others to be able to stay on 64k.\n\n(3) Arm improved performance on a 4k kernel by adding cont-pte support,\n     trying to get closer to 64k native performance.\n\n(4) Achieving 64k native performance is hard, which is why per-process\n     page sizes are being explored to get the best out of both worlds\n     (use 64k page size only where it really matters for performance).\n\nArm clearly has the added benefit of actually benefiting from hardware \nsupport for 64k.\n\nIIUC, what you are proposing feels a bit like traveling back in time \nwhen it comes to the memory waste problem that Arm users encountered.\n\nWhere do you see the big difference to 64k on Arm in your proposal? \nWould you currently also be running 64k Arm in production and the memory \nwaste etc is acceptable?\n\n-- \nCheers,\n\nDavid",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-20"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "Author acknowledges that increasing the base page size to 64k or 16k would significantly limit the patch's adoption due to existing legacy code on x86, but no clear resolution or plan for addressing this issue is provided.\n\nAuthor responded to a concern about kernel stack allocation in the context of sub-page granularity mapping, suggesting that vmalloc-allocated stacks could work and questioning why slab-allocated stacks wouldn't be suitable for large base page sizes.\n\nAuthor responded to David's (Arm) feedback by stating that they don't see a significant difference between 64k pages on x86 and Arm, and are targeting the patch for machines with over 2TiB of RAM, where memory consumption can be traded for scalability.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "acknowledges limitation",
            "no clear resolution",
            "clarifying question",
            "explaining reasoning",
            "no clear resolution signal",
            "author is explaining their reasoning"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Well, it will drastically limit the adoption. We have too much legacy\nstuff on x86.\n\n---\n\nKernel stack is allocated from vmalloc. I think mapping them with\nsub-page granularity should be doable.\n\nBTW, do you see any reason why slab-allocated stack wouldn't work for\nlarge base page sizes? There's no requirement for it be aligned to page\nor PTE, right?\n\n---\n\nThat's the point. I don't see a big difference to 64k Arm. I want to\nbring this option to x86: at some machine size it makes sense trade\nmemory consumption for scalability. I am targeting it to machines with\nover 2TiB of RAM.\n\nBTW, we do run 64k Arm in our fleet. There's some growing pains, but it\nlooks good in general We have no plans to switch to 4k (or 16k) at the\nmoment. 512M THPs also look good on some workloads.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-20"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author is addressing Kalesh Singh's concern about breaking the ABI by introducing a new page size, and responds by suggesting adding a knob to enforce the new value without immediately breaking the ABI.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarifying question",
            "exploring options"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "I don't want to break ABI, but might add a knob (maybe personality(2) ?)\nfor enforcement to see what breaks.\n\nIn general, I would prefer to advertise a new value to userspace that\nwould mean preferred virtual address space granularity.",
          "reply_to": "Kalesh Singh",
          "message_date": "2026-02-20"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "Author responded to Liam's feedback by expressing uncertainty about the reviewer's point, suggesting that the trade-off between struct page size and page cache rounding overhead may not be the primary concern.\n\nThe author is addressing a concern about the applicability of 64k pages to smaller machines, stating that they will not benefit from this feature.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "uncertainty",
            "lack of clear resolution",
            "clarification",
            "explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "I am not sure I fully follow your point.\n\nSizing tasks and scheduling tasks between machines is hard in general.\nI don't think the balance between struct page tax and page cache\nrounding overhead is going to be the primary factor.\n\n---\n\nSmaller machines are not target for 64k pages. They will not benefit\nfrom them.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
          "reply_to": "Liam Howlett",
          "message_date": "2026-02-20"
        },
        {
          "author": "Liam Howlett",
          "summary": "Reviewer Liam Howlett noted that increasing page size could lead to increased reclaim penalties due to more frequent reclaim operations, which is a trade-off not initially considered in the patch.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "trade-offs",
            "reclaim penalty"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "I think there are more trade offs than what you listed.  It's still\nprobably worth doing, but I wanted to know if you though that this would\ncause us to spend more time in reclaim, which seems to be implied above.\nSo, another trade-off might be all the reclaim penalty being paid more\nfrequently?\n\n...\n\nThanks,\nLiam",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-20"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "The author is addressing Liam Howlett's concern about the potential benefits of increasing the base page size, specifically whether it would lead to less memory allocation and reclaim work in the kernel. The author acknowledges that their previous explanation was 'hand-wavy' and doesn't provide a clear answer.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "acknowledged lack of clarity",
            "admitted uncertainty"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "I am not sure.\n\nKernel would need to do less work in reclaim per unit of memory.\nDepending on workloads you might see less allocation events and\ntherefore less frequent reclaim.\n\nIt's all too hand-wavy at the stage.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
          "reply_to": "Liam Howlett",
          "message_date": "2026-02-20"
        },
        {
          "author": "David (Arm)",
          "summary": "Reviewer David (Arm) noted that many modern applications can handle differing page sizes, but legacy code or Intel-specific code may still hardcode PAGE_SIZE to 4k, and requested an assessment of the conversion effort required for Meta's fleet.\n\nReviewer David (Arm) expressed concern about sub-page mapping and its implications on the `mapcount` value, suggesting that if any part of a page is mapped, it should be considered as mapped.\n\nReviewer David (Arm) noted that splitting PAGE_SIZE into PTE_SIZE and PG_SIZE may require additional metadata in page tables, specifically a dedicated type for kernel stack pages, which was previously proposed but not upstream.\n\nThe reviewer, David from Arm, suggests simplifying the patch by removing or hiding the sub-page mapping part in the arch code to make it more digestible.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "conversion effort",
            "devil is in the detail"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "I'd assume that many applications nowadays can deal with differing page \nsizes (thanks to some other architectures paving the way).\n\nBut yes, some real legacy stuff, or stuff that ever only cared about \nintel still hardcodes pagesize=4k.\n\nIn Meta's fleet, I'd be quite interesting how much conversion there \nwould have to be done.\n\nFor legacy apps, you could still run them as 4k pagesize on the same \nsystem, of course.\n\n---\n\nI still have to wrap my head around the sub-page mapping here as well. \nIt's scary.\n\nRe mapcount: I think if any part of the page is mapped, it would be \nconsidered mapped -> mapcount += 1.\n\n---\n\nI'd assume that would work. Devil is in the detail with these things \nbefore we have memdescs.\n\nE.g., page table have a dedicated type (PGTY_table) and store separate \nmetadata in the ptdesc. For kernel stack there was once a proposal to \nhave a type but it is not upstream.\n\n---\n\nOkay, that's valuable information, thanks!\n\nBeing able to remove the sub-page mapping part (or being able to just \nhide it somewhere deep down in arch code) would make this a lot easier \nto digest.\n\n-- \nCheers,\n\nDavid",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-20"
        },
        {
          "author": "Kalesh Singh",
          "summary": "Reviewer Kalesh Singh noted that the personality(2) system call may be too late to enforce larger VMA alignment, as initial userspace mappings are already established by then, and suggested using an early_param for global enforcement and a prctl/personality flag for per-process opt-in.\n\nThe reviewer agrees that separating PAGE_SIZE into PTE_SIZE and PG_SIZE is beneficial for maintaining ABI compatibility, allowing userspace allocators to optimize their layouts to match the larger PG_SIZE while still being able to operate at the smaller PTE_SIZE when needed.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "agreement",
            "optimization"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "I think personality(2) may be too late? By the time a process invokes\nit, the initial userspace mappings (executable, linker for init, etc)\nare already established with the default granularity.\n\nTo handle this, I've been using an early_param to enforce the larger\nVMA alignment system-wide right from boot.\n\nPerhaps, something for global enforcement (Kconfig/early param) and a\nprctl/personality flag for per-process opt in?\n\n---\n\nThis makes sense for maintaining ABI compatibility. Userspace\nallocators might want to optimize their layouts to match PG_SIZE while\nstill being able to operate at PTE_SIZE when needed.\n\n-- Kalesh",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-20"
        },
        {
          "author": "Kalesh Singh",
          "summary": "Reviewer Kalesh Singh noted that the patch does not handle the case where PTE_SIZE is less than PG_SIZE, including misaligned cases in page faults, and requested that the page fault handler be updated to handle this scenario.\n\nReviewer noted that ELF segment alignment is set to 4096 by linkers, which would prevent ELFs from loading correctly or at all on a system using larger page sizes.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Fri, Feb 20, 2026 at 8:30 AM David Hildenbrand (Arm)\n<david@kernel.org> wrote:\n\n---\n\nI think most issues will stem from linkers setting the default ELF\nsegment alignment (max-page-size) for x86 to 4096. So those ELFs will\nnot load correctly or at all on the larger emulated granularity.\n\n-- Kalesh",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-20"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-23": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Kiryl Shutsemau",
      "reviews": [
        {
          "author": "David (Arm)",
          "summary": "Reviewer David noted that the patch requires consideration of potential issues with existing binaries and libraries, which may need to be recompiled.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "potential ABI changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Right, I assume that they will have to be thought about that, and \npossibly, some binaries/libraries recompiled.\n\n-- \nCheers,\n\nDavid",
          "reply_to": "Kalesh Singh",
          "message_date": "2026-02-23"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "Author acknowledges that ABI break is not necessary and cites x86-64 SysV ABI as an example, but notes it doesn't work in practice.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "acknowledges",
            "cites"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "I think backward compatibility is important and I believe we can get\nthere without ABI break. And optimize from there.\n\nBTW, x86-64 SysV ABI allows for 64k page size:\n\n\tSystems are permitted to use any power-of-two page size between\n\t4KB and 64KB, inclusive.\n\nBut it doesn't work in practice.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-23"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "Author acknowledged that the patch may not be suitable for all use cases, particularly desktops, and is willing to explore alternative page sizes such as 16k.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "acknowledged potential issue",
            "willing to explore alternatives"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "I have not invested much time into investigating this.\n\nI intentionally targeted compatible version assuming it will be better\nreceived by upstream. I want it to be usable outside specially cured\nuserspace. 64k might not be good fit for a desktop, but 16k can be a\ndifferent story.\n\n-- \n  Kiryl Shutsemau / Kirill A. Shutemov",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-23"
        },
        {
          "author": "Dave Hansen",
          "summary": "Reviewer Dave Hansen noted that the patch's approach to separating PTE_SIZE and PG_SIZE may not be feasible due to the need for a substantial rework of page fault and VMA handling, particularly when dealing with misaligned cases.\n\nReviewer questioned the feasibility of increasing the base page size on x86, citing potential breakage to userspace applications, similar to what occurred with 5-level paging. He suggested that a larger page size might be viable if it can be implemented as an out-of-tree hack or if its benefits outweigh the costs.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "uncertainty about feasibility"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On 2/23/26 03:27, David Hildenbrand (Arm) wrote:\n...\n\n---\n\nI think what Kirill is trying to say is that \"it breaks userspace\". ;)\n\nA hyperscaler (or other \"embedded\" environment) might be willing or able\nto go fix up userspace breakage. I would suspect our high frequency\ntrading friends would be all over this if it shaved a microsecond off\ntheir receive times.\n\nThe more important question is what it breaks and how badly it breaks\nthings. 5-level paging, for instance, broke some JITs that historically\nused the new (>48) upper virtual address bits for metadata. The gains\nfrom 5-level paging were big enough and the userspace breakage was\nconfined and fixable enough that 5-level paging was viable.\n\nI'm not sure which side a larger base page side will fall on, though. Is\nit going to be an out-of-tree hack that a few folks use, or will it be\nmore like 5-level paging and be good enough that it goes into mainline?",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-23"
        },
        {
          "author": "David (Arm)",
          "summary": "Reviewer David (Arm) expressed a neutral sentiment, stating that the proposal is expected and similar to previous proposals from other vendors.\n\nReviewer David expressed concerns about potential issues with VMAs spanning partial pages or a single page spanning multiple VMAs, and stated that he is certain he will not like the code without having seen it yet.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "expected",
            "similar",
            "uncertainty",
            "dislike"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Yes. Probably similar to Intel proposing an actual 64k page size. \nExpected. :)\n\n---\n\nJust thinking about VMAs spanning partial pages makes me shiver. Or A \nsingle page spanning multiple VMAs.\n\nI haven't seen the code yet, but I am certain that I will not like it.\n\nI'm happy to be proven wrong :)\n\n-- \nCheers,\n\nDavid",
          "reply_to": "Dave Hansen",
          "message_date": "2026-02-23"
        },
        {
          "author": "Kiryl Shutsemau (author)",
          "summary": "Author responded to reviewer's concern about the potential impact of allowing THP to span multiple VMAs, explaining that this is already allowed upstream and only the order-0 page size change introduces new behavior.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "clarification",
            "explanation"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Hate to break it to you, but we have it now upstream :P\n\nTHP can span multiple VMAs. And can be partially mapped.\n\nThe only new thing is that we allow this for order-0 page now. And you\ncannot realistically recover wasted memory -- no deferred split.",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-23"
        },
        {
          "author": "David (Arm)",
          "summary": "Reviewer David pointed out that the patch's approach to separating PAGE_SIZE into PTE_SIZE and PG_SIZE is fundamentally flawed, suggesting a completely different design direction.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Single mapcount, single anon-exclusive flag.\n\nCompletely different story :P\n\n-- \nCheers,\n\nDavid",
          "reply_to": "Kiryl Shutsemau",
          "message_date": "2026-02-23"
        },
        {
          "author": "Lorenzo Stoakes",
          "summary": "Reviewer Lorenzo Stoakes expressed strong disagreement with the patch, citing existing complexity in handling per-page anonexclusive vs. per-folio settings as a reason not to introduce further changes.\n\nReviewer Lorenzo Stoakes expressed concern that introducing sub-base-page metadata would add unnecessary complexity, citing existing difficulties in handling VMA vs. folio state coherently and warning against feature creep.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "strong disagreement",
            "existing complexity",
            "requested changes",
            "concerns about complexity"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "Yeah agree, we're not doing this.\n\nIt's already a nightmare to deal with per-page anonexclusive vs. per-folio\npretty much everything else, and we shouldn't have allowed that to be a thing,\nbut now we have to live with it.\n\n---\n\nIf the code tries to implement anything that even resembles some sub-base-page\nmetadata then that's just not something that's going to land.\n\nHandling VMA vs. folio state coherently is _already_ painful and difficult.\n\nPiling on more complexity because we theoretically could feels rather along the\nlines of 'let's just keep adding features and not worrying about where we end\nup', which is I think a bit of an anti-pattern in the kernel in general.",
          "reply_to": "David (Arm)",
          "message_date": "2026-02-23"
        },
        {
          "author": "David Laight",
          "summary": "Reviewer David Laight pointed out that the patch does not handle the case where PTE_SIZE is less than PG_SIZE, including misaligned cases, and requested that the page fault handler be updated to handle this scenario.\n\nReviewer questioned the need to split large physical pages into smaller structures, suggesting that dynamic splitting would solve alignment issues and potentially provide a net gain in TLB coalescing for most program binaries.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "alternative solution"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "On Mon, 23 Feb 2026 07:14:39 -0800\nDave Hansen <dave.hansen@intel.com> wrote:\n\n---\n\nWith a 4k physical page what stops you dynamically splitting the 64k a\n'struct page' references into 16 4k pages (using an extra dynamically\nallocated structure)?\nI'm not thinking it would happen that often, but it would solve the\nproblem of 4k aligned .data and (probably) mmap() of small files.\n\nIf the cpu supports TLB coalescing there could easily be a net gain\nusing 64k pages for most of a program binary.\n\n\tDavid",
          "reply_to": "Dave Hansen",
          "message_date": "2026-02-23"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    }
  }
}