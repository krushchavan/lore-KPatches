{
  "thread_id": "20260220191035.3703800-1-hannes@cmpxchg.org",
  "subject": "[PATCH 1/2] mm: vmalloc: streamline vmalloc memory accounting",
  "url": "https://lore.kernel.org/all/20260220191035.3703800-1-hannes@cmpxchg.org/",
  "dates": {
    "2026-02-20": {
      "report_file": "2026-02-20.html",
      "developer": "Johannes Weiner",
      "reviews": [
        {
          "author": "Shakeel Butt",
          "summary": "mod_node_page_state() takes 'struct pglist_data *pgdat', you need to use page_pgdat(page) as first param. Same here. With above fixes, you can add:",
          "sentiment": "neutral",
          "sentiment_signals": [],
          "has_inline_review": true,
          "tags_given": [
            "Acked-by"
          ],
          "analysis_source": "heuristic",
          "raw_body": "On Fri, Feb 20, 2026 at 02:10:34PM -0500, Johannes Weiner wrote:\n[...]\n>  static struct vmap_area *__find_vmap_area(unsigned long addr, struct rb_root *root)\n>  {\n>  \tstruct rb_node *n = root->rb_node;\n> @@ -3463,11 +3457,11 @@ void vfree(const void *addr)\n>  \t\t * High-order allocs for huge vmallocs are split, so\n>  \t\t * can be freed as an array of order-0 allocations\n>  \t\t */\n> +\t\tif (!(vm->flags & VM_MAP_PUT_PAGES))\n> +\t\t\tdec_node_page_state(page, NR_VMALLOC);\n>  \t\t__free_page(page);\n>  \t\tcond_resched();\n>  \t}\n> -\tif (!(vm->flags & VM_MAP_PUT_PAGES))\n> -\t\tatomic_long_sub(vm->nr_pages, &nr_vmalloc_pages);\n>  \tkvfree(vm->pages);\n>  \tkfree(vm);\n>  }\n> @@ -3655,6 +3649,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n>  \t\t\tcontinue;\n>  \t\t}\n>  \n> +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << large_order);\n\nmod_node_page_state() takes 'struct pglist_data *pgdat', you need to use\npage_pgdat(page) as first param.\n\n> +\n>  \t\tsplit_page(page, large_order);\n>  \t\tfor (i = 0; i < (1U << large_order); i++)\n>  \t\t\tpages[nr_allocated + i] = page + i;\n> @@ -3675,6 +3671,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n>  \tif (!order) {\n>  \t\twhile (nr_allocated < nr_pages) {\n>  \t\t\tunsigned int nr, nr_pages_request;\n> +\t\t\tint i;\n>  \n>  \t\t\t/*\n>  \t\t\t * A maximum allowed request is hard-coded and is 100\n> @@ -3698,6 +3695,9 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n>  \t\t\t\t\t\t\tnr_pages_request,\n>  \t\t\t\t\t\t\tpages + nr_allocated);\n>  \n> +\t\t\tfor (i = nr_allocated; i < nr_allocated + nr; i++)\n> +\t\t\t\tinc_node_page_state(pages[i], NR_VMALLOC);\n> +\n>  \t\t\tnr_allocated += nr;\n>  \n>  \t\t\t/*\n> @@ -3722,6 +3722,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n>  \t\tif (unlikely(!page))\n>  \t\t\tbreak;\n>  \n> +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << order);\n\nSame here.\n\nWith above fixes, you can add:\n\nAcked-by: Shakeel Butt <shakeel.butt@linux.dev>\n",
          "reply_to": "",
          "message_date": "2026-02-20",
          "message_id": ""
        }
      ],
      "analysis_source": "heuristic",
      "patch_summary": "Use a vmstat counter instead of a custom, open-coded atomic. This has the added benefit of making the data available per-node, and prepares for cleaning up the memcg accounting as well."
    },
    "2026-02-23": {
      "report_file": "2026-02-23.html",
      "developer": "Johannes Weiner",
      "reviews": [
        {
          "author": "Uladzislau Rezki",
          "summary": "Can we move *_node_page_stat() to the end of the vm_area_alloc_pages()? Or mod_node_page_state in first place should be invoked on high-order page before split(to avoid of looping over small pages afterword)? I mean it would be good to place to the one solid place. If it is possible of course.",
          "sentiment": "neutral",
          "sentiment_signals": [],
          "has_inline_review": true,
          "tags_given": [],
          "analysis_source": "heuristic",
          "raw_body": "On Fri, Feb 20, 2026 at 02:10:34PM -0500, Johannes Weiner wrote:\n> Use a vmstat counter instead of a custom, open-coded atomic. This has\n> the added benefit of making the data available per-node, and prepares\n> for cleaning up the memcg accounting as well.\n> \n> Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>\n> ---\n>  fs/proc/meminfo.c       |  3 ++-\n>  include/linux/mmzone.h  |  1 +\n>  include/linux/vmalloc.h |  3 ---\n>  mm/vmalloc.c            | 19 ++++++++++---------\n>  mm/vmstat.c             |  1 +\n>  5 files changed, 14 insertions(+), 13 deletions(-)\n> \n> diff --git a/fs/proc/meminfo.c b/fs/proc/meminfo.c\n> index a458f1e112fd..549793f44726 100644\n> --- a/fs/proc/meminfo.c\n> +++ b/fs/proc/meminfo.c\n> @@ -126,7 +126,8 @@ static int meminfo_proc_show(struct seq_file *m, void *v)\n>  \tshow_val_kb(m, \"Committed_AS:   \", committed);\n>  \tseq_printf(m, \"VmallocTotal:   %8lu kB\\n\",\n>  \t\t   (unsigned long)VMALLOC_TOTAL >> 10);\n> -\tshow_val_kb(m, \"VmallocUsed:    \", vmalloc_nr_pages());\n> +\tshow_val_kb(m, \"VmallocUsed:    \",\n> +\t\t    global_node_page_state(NR_VMALLOC));\n>  \tshow_val_kb(m, \"VmallocChunk:   \", 0ul);\n>  \tshow_val_kb(m, \"Percpu:         \", pcpu_nr_pages());\n>  \n> diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h\n> index fc5d6c88d2f0..64df797d45c6 100644\n> --- a/include/linux/mmzone.h\n> +++ b/include/linux/mmzone.h\n> @@ -220,6 +220,7 @@ enum node_stat_item {\n>  \tNR_KERNEL_MISC_RECLAIMABLE,\t/* reclaimable non-slab kernel pages */\n>  \tNR_FOLL_PIN_ACQUIRED,\t/* via: pin_user_page(), gup flag: FOLL_PIN */\n>  \tNR_FOLL_PIN_RELEASED,\t/* pages returned via unpin_user_page() */\n> +\tNR_VMALLOC,\n>  \tNR_KERNEL_STACK_KB,\t/* measured in KiB */\n>  #if IS_ENABLED(CONFIG_SHADOW_CALL_STACK)\n>  \tNR_KERNEL_SCS_KB,\t/* measured in KiB */\n> diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h\n> index e8e94f90d686..3b02c0c6b371 100644\n> --- a/include/linux/vmalloc.h\n> +++ b/include/linux/vmalloc.h\n> @@ -286,8 +286,6 @@ int unregister_vmap_purge_notifier(struct notifier_block *nb);\n>  #ifdef CONFIG_MMU\n>  #define VMALLOC_TOTAL (VMALLOC_END - VMALLOC_START)\n>  \n> -unsigned long vmalloc_nr_pages(void);\n> -\n>  int vm_area_map_pages(struct vm_struct *area, unsigned long start,\n>  \t\t      unsigned long end, struct page **pages);\n>  void vm_area_unmap_pages(struct vm_struct *area, unsigned long start,\n> @@ -304,7 +302,6 @@ static inline void set_vm_flush_reset_perms(void *addr)\n>  #else  /* !CONFIG_MMU */\n>  #define VMALLOC_TOTAL 0UL\n>  \n> -static inline unsigned long vmalloc_nr_pages(void) { return 0; }\n>  static inline void set_vm_flush_reset_perms(void *addr) {}\n>  #endif /* CONFIG_MMU */\n>  \n> diff --git a/mm/vmalloc.c b/mm/vmalloc.c\n> index e286c2d2068c..a49a46de9c4f 100644\n> --- a/mm/vmalloc.c\n> +++ b/mm/vmalloc.c\n> @@ -1063,14 +1063,8 @@ static BLOCKING_NOTIFIER_HEAD(vmap_notify_list);\n>  static void drain_vmap_area_work(struct work_struct *work);\n>  static DECLARE_WORK(drain_vmap_work, drain_vmap_area_work);\n>  \n> -static __cacheline_aligned_in_smp atomic_long_t nr_vmalloc_pages;\n>  static __cacheline_aligned_in_smp atomic_long_t vmap_lazy_nr;\n>  \n> -unsigned long vmalloc_nr_pages(void)\n> -{\n> -\treturn atomic_long_read(&nr_vmalloc_pages);\n> -}\n> -\n>  static struct vmap_area *__find_vmap_area(unsigned long addr, struct rb_root *root)\n>  {\n>  \tstruct rb_node *n = root->rb_node;\n> @@ -3463,11 +3457,11 @@ void vfree(const void *addr)\n>  \t\t * High-order allocs for huge vmallocs are split, so\n>  \t\t * can be freed as an array of order-0 allocations\n>  \t\t */\n> +\t\tif (!(vm->flags & VM_MAP_PUT_PAGES))\n> +\t\t\tdec_node_page_state(page, NR_VMALLOC);\n>  \t\t__free_page(page);\n>  \t\tcond_resched();\n>  \t}\n> -\tif (!(vm->flags & VM_MAP_PUT_PAGES))\n> -\t\tatomic_long_sub(vm->nr_pages, &nr_vmalloc_pages);\n>  \tkvfree(vm->pages);\n>  \tkfree(vm);\n>  }\n> @@ -3655,6 +3649,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n>  \t\t\tcontinue;\n>  \t\t}\n>  \n> +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << large_order);\n> +\n>  \t\tsplit_page(page, large_order);\n>  \t\tfor (i = 0; i < (1U << large_order); i++)\n>  \t\t\tpages[nr_allocated + i] = page + i;\n> @@ -3675,6 +3671,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n>  \tif (!order) {\n>  \t\twhile (nr_allocated < nr_pages) {\n>  \t\t\tunsigned int nr, nr_pages_request;\n> +\t\t\tint i;\n>  \n>  \t\t\t/*\n>  \t\t\t * A maximum allowed request is hard-coded and is 100\n> @@ -3698,6 +3695,9 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n>  \t\t\t\t\t\t\tnr_pages_request,\n>  \t\t\t\t\t\t\tpages + nr_allocated);\n>  \n> +\t\t\tfor (i = nr_allocated; i < nr_allocated + nr; i++)\n> +\t\t\t\tinc_node_page_state(pages[i], NR_VMALLOC);\n> +\n>  \t\t\tnr_allocated += nr;\n>  \n>  \t\t\t/*\n> @@ -3722,6 +3722,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n>  \t\tif (unlikely(!page))\n>  \t\t\tbreak;\n>  \n> +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << order);\n> +\n>  \t\t/*\nCan we move *_node_page_stat() to the end of the vm_area_alloc_pages()?\n\nOr mod_node_page_state in first place should be invoked on high-order\npage before split(to avoid of looping over small pages afterword)?\n\nI mean it would be good to place to the one solid place. If it is possible\nof course.\n\n--\nUladzislau Rezk\n",
          "reply_to": "",
          "message_date": "2026-02-23",
          "message_id": ""
        },
        {
          "author": "Johannes Weiner (author)",
          "summary": "Good catch, my apologies. Serves me right for not compiling incrementally.",
          "sentiment": "neutral",
          "sentiment_signals": [],
          "has_inline_review": true,
          "tags_given": [],
          "analysis_source": "heuristic",
          "raw_body": "On Fri, Feb 20, 2026 at 02:09:28PM -0800, Shakeel Butt wrote:\n> On Fri, Feb 20, 2026 at 02:10:34PM -0500, Johannes Weiner wrote:\n> [...]\n> >  static struct vmap_area *__find_vmap_area(unsigned long addr, struct rb_root *root)\n> >  {\n> >  \tstruct rb_node *n = root->rb_node;\n> > @@ -3463,11 +3457,11 @@ void vfree(const void *addr)\n> >  \t\t * High-order allocs for huge vmallocs are split, so\n> >  \t\t * can be freed as an array of order-0 allocations\n> >  \t\t */\n> > +\t\tif (!(vm->flags & VM_MAP_PUT_PAGES))\n> > +\t\t\tdec_node_page_state(page, NR_VMALLOC);\n> >  \t\t__free_page(page);\n> >  \t\tcond_resched();\n> >  \t}\n> > -\tif (!(vm->flags & VM_MAP_PUT_PAGES))\n> > -\t\tatomic_long_sub(vm->nr_pages, &nr_vmalloc_pages);\n> >  \tkvfree(vm->pages);\n> >  \tkfree(vm);\n> >  }\n> > @@ -3655,6 +3649,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> >  \t\t\tcontinue;\n> >  \t\t}\n> >  \n> > +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << large_order);\n> \n> mod_node_page_state() takes 'struct pglist_data *pgdat', you need to use\n> page_pgdat(page) as first param.\n\nGood catch, my apologies. Serves me right for not compiling\nincrementally.\n\n> With above fixes, you can add:\n> \n> Acked-by: Shakeel Butt <shakeel.butt@linux.dev>\n\nThanks! I'll send out v2.\n",
          "reply_to": "",
          "message_date": "2026-02-23",
          "message_id": ""
        },
        {
          "author": "Johannes Weiner (author)",
          "summary": "Note that the top one in the fast path IS called before the split. We're accounting in the same step size as the page allocator can give us. In the fallback paths (bulk allocator, and one-by-one loop), the issue is that the individual pages could be coming from different nodes, so they need to bump different counters. One possible solution would be to remember the last node and accumulate until it differs, then flush: fallback_loop() { page = alloc_pages(); nid = page_to_nid(page); if (nid != last_nid) { if (node_count) { mod_node_page_state(...); node_count = 0; } last_nid = nid; } } if (node_count) mod_node_page_state(...); But it IS the slow path, and these are fairly cheap per-cpu counters. Especially compared to the cost of calling into the allocator.",
          "sentiment": "neutral",
          "sentiment_signals": [],
          "has_inline_review": true,
          "tags_given": [],
          "analysis_source": "heuristic",
          "raw_body": "On Mon, Feb 23, 2026 at 04:30:32PM +0100, Uladzislau Rezki wrote:\n> On Fri, Feb 20, 2026 at 02:10:34PM -0500, Johannes Weiner wrote:\n> > @@ -3655,6 +3649,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> >  \t\t\tcontinue;\n> >  \t\t}\n> >  \n> > +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << large_order);\n> > +\n> >  \t\tsplit_page(page, large_order);\n> >  \t\tfor (i = 0; i < (1U << large_order); i++)\n> >  \t\t\tpages[nr_allocated + i] = page + i;\n> > @@ -3675,6 +3671,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> >  \tif (!order) {\n> >  \t\twhile (nr_allocated < nr_pages) {\n> >  \t\t\tunsigned int nr, nr_pages_request;\n> > +\t\t\tint i;\n> >  \n> >  \t\t\t/*\n> >  \t\t\t * A maximum allowed request is hard-coded and is 100\n> > @@ -3698,6 +3695,9 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> >  \t\t\t\t\t\t\tnr_pages_request,\n> >  \t\t\t\t\t\t\tpages + nr_allocated);\n> >  \n> > +\t\t\tfor (i = nr_allocated; i < nr_allocated + nr; i++)\n> > +\t\t\t\tinc_node_page_state(pages[i], NR_VMALLOC);\n> > +\n> >  \t\t\tnr_allocated += nr;\n> >  \n> >  \t\t\t/*\n> > @@ -3722,6 +3722,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> >  \t\tif (unlikely(!page))\n> >  \t\t\tbreak;\n> >  \n> > +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << order);\n> > +\n> >  \t\t/*\n> Can we move *_node_page_stat() to the end of the vm_area_alloc_pages()?\n> \n> Or mod_node_page_state in first place should be invoked on high-order\n> page before split(to avoid of looping over small pages afterword)?\n> \n> I mean it would be good to place to the one solid place. If it is possible\n> of course.\n\nNote that the top one in the fast path IS called before the\nsplit. We're accounting in the same step size as the page allocator\ncan give us.\n\nIn the fallback paths (bulk allocator, and one-by-one loop), the issue\nis that the individual pages could be coming from different nodes, so\nthey need to bump different counters. One possible solution would be\nto remember the last node and accumulate until it differs, then flush:\n\nfallback_loop() {\n\tpage = alloc_pages();\n\tnid = page_to_nid(page);\n\tif (nid != last_nid) {\n\t\tif (node_count) {\n\t\t\tmod_node_page_state(...);\n\t\t\tnode_count = 0;\n\t\t}\n\t\tlast_nid = nid;\n\t}\n}\n\nif (node_count)\n\tmod_node_page_state(...);\n\nBut it IS the slow path, and these are fairly cheap per-cpu\ncounters. Especially compared to the cost of calling into the\nallocator. So I'm not sure it's worth it... What do you think?\n",
          "reply_to": "",
          "message_date": "2026-02-23",
          "message_id": ""
        }
      ],
      "analysis_source": "heuristic",
      "patch_summary": "Use a vmstat counter instead of a custom, open-coded atomic. This has the added benefit of making the data available per-node, and prepares for cleaning up the memcg accounting as well."
    },
    "2026-02-24": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Johannes Weiner",
      "reviews": [
        {
          "author": "Uladzislau Rezki",
          "summary": "I see. I agree it is easier to keep original solution. I see that Andrew took it, but just in case:",
          "sentiment": "neutral",
          "sentiment_signals": [],
          "has_inline_review": true,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "heuristic",
          "raw_body": "On Mon, Feb 23, 2026 at 03:19:20PM -0500, Johannes Weiner wrote:\n> On Mon, Feb 23, 2026 at 04:30:32PM +0100, Uladzislau Rezki wrote:\n> > On Fri, Feb 20, 2026 at 02:10:34PM -0500, Johannes Weiner wrote:\n> > > @@ -3655,6 +3649,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> > >  \t\t\tcontinue;\n> > >  \t\t}\n> > >  \n> > > +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << large_order);\n> > > +\n> > >  \t\tsplit_page(page, large_order);\n> > >  \t\tfor (i = 0; i < (1U << large_order); i++)\n> > >  \t\t\tpages[nr_allocated + i] = page + i;\n> > > @@ -3675,6 +3671,7 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> > >  \tif (!order) {\n> > >  \t\twhile (nr_allocated < nr_pages) {\n> > >  \t\t\tunsigned int nr, nr_pages_request;\n> > > +\t\t\tint i;\n> > >  \n> > >  \t\t\t/*\n> > >  \t\t\t * A maximum allowed request is hard-coded and is 100\n> > > @@ -3698,6 +3695,9 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> > >  \t\t\t\t\t\t\tnr_pages_request,\n> > >  \t\t\t\t\t\t\tpages + nr_allocated);\n> > >  \n> > > +\t\t\tfor (i = nr_allocated; i < nr_allocated + nr; i++)\n> > > +\t\t\t\tinc_node_page_state(pages[i], NR_VMALLOC);\n> > > +\n> > >  \t\t\tnr_allocated += nr;\n> > >  \n> > >  \t\t\t/*\n> > > @@ -3722,6 +3722,8 @@ vm_area_alloc_pages(gfp_t gfp, int nid,\n> > >  \t\tif (unlikely(!page))\n> > >  \t\t\tbreak;\n> > >  \n> > > +\t\tmod_node_page_state(page, NR_VMALLOC, 1 << order);\n> > > +\n> > >  \t\t/*\n> > Can we move *_node_page_stat() to the end of the vm_area_alloc_pages()?\n> > \n> > Or mod_node_page_state in first place should be invoked on high-order\n> > page before split(to avoid of looping over small pages afterword)?\n> > \n> > I mean it would be good to place to the one solid place. If it is possible\n> > of course.\n> \n> Note that the top one in the fast path IS called before the\n> split. We're accounting in the same step size as the page allocator\n> can give us.\n> \n> In the fallback paths (bulk allocator, and one-by-one loop), the issue\n> is that the individual pages could be coming from different nodes, so\n> they need to bump different counters. One possible solution would be\n> to remember the last node and accumulate until it differs, then flush:\n> \n> fallback_loop() {\n> \tpage = alloc_pages();\n> \tnid = page_to_nid(page);\n> \tif (nid != last_nid) {\n> \t\tif (node_count) {\n> \t\t\tmod_node_page_state(...);\n> \t\t\tnode_count = 0;\n> \t\t}\n> \t\tlast_nid = nid;\n> \t}\n> }\n> \n> if (node_count)\n> \tmod_node_page_state(...);\n> \n> But it IS the slow path, and these are fairly cheap per-cpu\n> counters. Especially compared to the cost of calling into the\n> allocator. So I'm not sure it's worth it... What do you think?\n>\nI see. I agree it is easier to keep original solution. I see that\nAndrew took it, but just in case:\n\nReviewed-by: Uladzislau Rezki (Sony) <urezki@gmail.com>\n\n--\nUladzislau Rezki\n\n",
          "reply_to": "",
          "message_date": "2026-02-24",
          "message_id": ""
        }
      ],
      "analysis_source": "heuristic"
    }
  }
}