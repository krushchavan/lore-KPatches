<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Review Comments: [PATCH 0/3] sunrpc: cache infrastructure scalability improvements</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto,
                         "Helvetica Neue", Arial, sans-serif;
            background: #f5f5f5;
            color: #333;
            line-height: 1.6;
            padding: 20px;
            max-width: 900px;
            margin: 0 auto;
        }
        .home-link { margin-bottom: 12px; display: block; }
        .home-link a { color: #0366d6; text-decoration: none; font-size: 0.9em; }
        .home-link a:hover { text-decoration: underline; }

        h1 { font-size: 1.3em; margin-bottom: 2px; color: #1a1a1a; line-height: 1.3; }

        .lore-link { font-size: 0.85em; margin: 4px 0 6px; display: block; }
        .lore-link a { color: #0366d6; text-decoration: none; }
        .lore-link a:hover { text-decoration: underline; }

        .date-range {
            font-size: 0.8em;
            color: #888;
            margin-bottom: 16px;
        }
        .date-range a { color: #0366d6; text-decoration: none; }
        .date-range a:hover { text-decoration: underline; }

        /* thread-node scroll margin so the card isn't clipped at the top */
        .thread-node { scroll-margin-top: 8px; }

        /* ── Patch summary ──────────────────────────────────────────── */
        .patch-summary-block {
            background: #fff;
            border-radius: 8px;
            padding: 12px 16px;
            margin-bottom: 20px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            border-left: 3px solid #4a90d9;
        }
        .patch-summary-label {
            font-size: 0.72em;
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 0.06em;
            color: #4a90d9;
            margin-bottom: 4px;
        }
        .patch-summary-text {
            font-size: 0.88em;
            color: #444;
            line-height: 1.55;
        }

        /* ── Thread tree ────────────────────────────────────────────── */
        .thread-tree {
            display: flex;
            flex-direction: column;
            gap: 6px;
        }

        /* Depth indentation via left border */
        .thread-node { position: relative; }
        .thread-children {
            margin-left: 20px;
            padding-left: 12px;
            border-left: 2px solid #e0e0e0;
            margin-top: 6px;
            display: flex;
            flex-direction: column;
            gap: 6px;
        }

        /* ── Review comment card ────────────────────────────────────── */
        .review-comment {
            background: #fff;
            border-radius: 6px;
            padding: 10px 14px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.08);
            font-size: 0.88em;
        }
        .review-comment-header {
            display: flex;
            flex-wrap: wrap;
            align-items: center;
            gap: 6px;
            margin-bottom: 5px;
        }
        .review-author {
            font-weight: 700;
            color: #1a1a1a;
            font-size: 0.95em;
        }

        /* Date chip — links back to the daily report */
        .date-chip {
            font-size: 0.75em;
            color: #777;
            background: #f0f0f0;
            border-radius: 10px;
            padding: 1px 7px;
            text-decoration: none;
            white-space: nowrap;
        }
        a.date-chip:hover { background: #e0e8f5; color: #0366d6; }

        .badge {
            display: inline-block;
            padding: 1px 8px;
            border-radius: 10px;
            font-size: 0.75em;
            font-weight: 600;
        }
        .inline-review-badge {
            display: inline-block;
            padding: 0 6px;
            border-radius: 8px;
            font-size: 0.78em;
            font-weight: 500;
            background: #e3f2fd;
            color: #1565c0;
        }
        .review-tag-badge {
            display: inline-block;
            padding: 0 6px;
            border-radius: 8px;
            font-size: 0.78em;
            font-weight: 500;
            background: #e8f5e9;
            color: #2e7d32;
        }
        .analysis-source-badge {
            display: inline-block;
            padding: 1px 7px;
            border-radius: 10px;
            font-size: 0.72em;
            font-weight: 600;
            border: 1px solid rgba(0,0,0,0.1);
        }

        .review-comment-text {
            color: #444;
            line-height: 1.55;
            margin-bottom: 4px;
        }
        .review-comment-signals {
            margin-top: 3px;
            font-size: 0.85em;
            color: #aaa;
            font-style: italic;
        }

        /* ── Collapsible raw body ───────────────────────────────────── */
        .raw-body-toggle {
            margin-top: 5px;
            font-size: 0.85em;
        }
        .raw-body-toggle summary {
            cursor: pointer;
            color: #888;
            padding: 2px 0;
            font-weight: 500;
            font-size: 0.9em;
            list-style: none;
        }
        .raw-body-toggle summary::-webkit-details-marker { display: none; }
        .raw-body-toggle summary::before { content: "▶ "; font-size: 0.7em; }
        .raw-body-toggle[open] summary::before { content: "▼ "; }
        .raw-body-toggle summary:hover { color: #555; }
        .raw-body-text {
            white-space: pre-wrap;
            font-size: 0.95em;
            background: #f8f8f8;
            padding: 8px 10px;
            border-radius: 4px;
            max-height: 360px;
            overflow-y: auto;
            margin-top: 4px;
            line-height: 1.5;
            color: #444;
            border: 1px solid #e8e8e8;
        }

        .no-reviews {
            color: #aaa;
            font-size: 0.85em;
            font-style: italic;
            padding: 8px 0;
        }

        footer {
            text-align: center;
            color: #bbb;
            font-size: 0.78em;
            margin-top: 36px;
            padding: 16px;
        }
    </style>
</head>
<body>
    <div class="home-link"><a href="../">&larr; Back to reports</a></div>
    <h1>[PATCH 0/3] sunrpc: cache infrastructure scalability improvements</h1>
    <div class="lore-link"><a href="https://lore.kernel.org/all/20260220-sunrpc-cache-v1-0-47d04014c245@kernel.org/" target="_blank">View on lore.kernel.org &rarr;</a></div>
    <div class="date-range">Active on: <a href="#2026-02-22">2026-02-22</a> &bull; <a href="#2026-02-20">2026-02-20</a></div>
    <div class="patch-summary-block"><div class="patch-summary-label">Patch summary</div><div class="patch-summary-text">This patch series improves the scalability of the sunrpc cache infrastructure by converting a global spinlock and waitqueue to per-cache-detail locks and waitqueues, reducing contention between different caches. The patches also split the cache detail queue into separate lists for readers and requests, simplifying the code and enabling future implementation of netlink upcalls.</div></div>
    <div class="thread-tree">
<div class="thread-node depth-0" id="2026-02-20">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Jeff Layton (author)</span>
<a class="date-chip" href="../2026-02-20_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-20">2026-02-20</a>
<span class="badge" style="color:#155724;background:#d4edda">Positive</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed a concern about contention between different caches on queue operations by converting the global spinlock to per-cache_detail spinlocks, which will resolve the issue.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">The global queue_lock serializes all upcall queue operations across
every cache_detail instance. Convert it to a per-cache_detail spinlock
so that different caches (e.g. auth.unix.ip vs nfsd.fh) no longer
contend with each other on queue operations.

Signed-off-by: Jeff Layton &lt;jlayton@kernel.org&gt;
---
 include/linux/sunrpc/cache.h |  1 +
 net/sunrpc/cache.c           | 47 ++++++++++++++++++++++----------------------
 2 files changed, 24 insertions(+), 24 deletions(-)

diff --git a/include/linux/sunrpc/cache.h b/include/linux/sunrpc/cache.h
index e783132e481ff2593fdc5d323f7b3a08f85d4cd8..3d32dd1f7b05d35562d2064fed69877b3950fb51 100644
--- a/include/linux/sunrpc/cache.h
+++ b/include/linux/sunrpc/cache.h
@@ -113,6 +113,7 @@ struct cache_detail {
 
 	/* fields for communication over channel */
 	struct list_head	queue;
+	spinlock_t		queue_lock;
 
 	atomic_t		writers;		/* how many time is /channel open */
 	time64_t		last_close;		/* if no writers, when did last close */
diff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c
index 7c73d1c39687343db02d1f1423b58213b7a35f42..6add2fe311425dc3aec63efce2c4bed06a3d3ba5 100644
--- a/net/sunrpc/cache.c
+++ b/net/sunrpc/cache.c
@@ -400,6 +400,7 @@ void sunrpc_init_cache_detail(struct cache_detail *cd)
 {
 	spin_lock_init(&amp;cd-&gt;hash_lock);
 	INIT_LIST_HEAD(&amp;cd-&gt;queue);
+	spin_lock_init(&amp;cd-&gt;queue_lock);
 	spin_lock(&amp;cache_list_lock);
 	cd-&gt;nextcheck = 0;
 	cd-&gt;entries = 0;
@@ -803,8 +804,6 @@ void cache_clean_deferred(void *owner)
  *
  */
 
-static DEFINE_SPINLOCK(queue_lock);
-
 struct cache_queue {
 	struct list_head	list;
 	int			reader;	/* if 0, then request */
@@ -847,7 +846,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,
 	inode_lock(inode); /* protect against multiple concurrent
 			      * readers on this file */
  again:
-	spin_lock(&amp;queue_lock);
+	spin_lock(&amp;cd-&gt;queue_lock);
 	/* need to find next request */
 	while (rp-&gt;q.list.next != &amp;cd-&gt;queue &amp;&amp;
 	       list_entry(rp-&gt;q.list.next, struct cache_queue, list)
@@ -856,7 +855,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,
 		list_move(&amp;rp-&gt;q.list, next);
 	}
 	if (rp-&gt;q.list.next == &amp;cd-&gt;queue) {
-		spin_unlock(&amp;queue_lock);
+		spin_unlock(&amp;cd-&gt;queue_lock);
 		inode_unlock(inode);
 		WARN_ON_ONCE(rp-&gt;offset);
 		return 0;
@@ -865,7 +864,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,
 	WARN_ON_ONCE(rq-&gt;q.reader);
 	if (rp-&gt;offset == 0)
 		rq-&gt;readers++;
-	spin_unlock(&amp;queue_lock);
+	spin_unlock(&amp;cd-&gt;queue_lock);
 
 	if (rq-&gt;len == 0) {
 		err = cache_request(cd, rq);
@@ -876,9 +875,9 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,
 
 	if (rp-&gt;offset == 0 &amp;&amp; !test_bit(CACHE_PENDING, &amp;rq-&gt;item-&gt;flags)) {
 		err = -EAGAIN;
-		spin_lock(&amp;queue_lock);
+		spin_lock(&amp;cd-&gt;queue_lock);
 		list_move(&amp;rp-&gt;q.list, &amp;rq-&gt;q.list);
-		spin_unlock(&amp;queue_lock);
+		spin_unlock(&amp;cd-&gt;queue_lock);
 	} else {
 		if (rp-&gt;offset + count &gt; rq-&gt;len)
 			count = rq-&gt;len - rp-&gt;offset;
@@ -888,26 +887,26 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,
 		rp-&gt;offset += count;
 		if (rp-&gt;offset &gt;= rq-&gt;len) {
 			rp-&gt;offset = 0;
-			spin_lock(&amp;queue_lock);
+			spin_lock(&amp;cd-&gt;queue_lock);
 			list_move(&amp;rp-&gt;q.list, &amp;rq-&gt;q.list);
-			spin_unlock(&amp;queue_lock);
+			spin_unlock(&amp;cd-&gt;queue_lock);
 		}
 		err = 0;
 	}
  out:
 	if (rp-&gt;offset == 0) {
 		/* need to release rq */
-		spin_lock(&amp;queue_lock);
+		spin_lock(&amp;cd-&gt;queue_lock);
 		rq-&gt;readers--;
 		if (rq-&gt;readers == 0 &amp;&amp;
 		    !test_bit(CACHE_PENDING, &amp;rq-&gt;item-&gt;flags)) {
 			list_del(&amp;rq-&gt;q.list);
-			spin_unlock(&amp;queue_lock);
+			spin_unlock(&amp;cd-&gt;queue_lock);
 			cache_put(rq-&gt;item, cd);
 			kfree(rq-&gt;buf);
 			kfree(rq);
 		} else
-			spin_unlock(&amp;queue_lock);
+			spin_unlock(&amp;cd-&gt;queue_lock);
 	}
 	if (err == -EAGAIN)
 		goto again;
@@ -988,7 +987,7 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,
 	if (!rp)
 		return mask;
 
-	spin_lock(&amp;queue_lock);
+	spin_lock(&amp;cd-&gt;queue_lock);
 
 	for (cq= &amp;rp-&gt;q; &amp;cq-&gt;list != &amp;cd-&gt;queue;
 	     cq = list_entry(cq-&gt;list.next, struct cache_queue, list))
@@ -996,7 +995,7 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,
 			mask |= EPOLLIN | EPOLLRDNORM;
 			break;
 		}
-	spin_unlock(&amp;queue_lock);
+	spin_unlock(&amp;cd-&gt;queue_lock);
 	return mask;
 }
 
@@ -1011,7 +1010,7 @@ static int cache_ioctl(struct inode *ino, struct file *filp,
 	if (cmd != FIONREAD || !rp)
 		return -EINVAL;
 
-	spin_lock(&amp;queue_lock);
+	spin_lock(&amp;cd-&gt;queue_lock);
 
 	/* only find the length remaining in current request,
 	 * or the length of the next request
@@ -1024,7 +1023,7 @@ static int cache_ioctl(struct inode *ino, struct file *filp,
 			len = cr-&gt;len - rp-&gt;offset;
 			break;
 		}
-	spin_unlock(&amp;queue_lock);
+	spin_unlock(&amp;cd-&gt;queue_lock);
 
 	return put_user(len, (int __user *)arg);
 }
@@ -1046,9 +1045,9 @@ static int cache_open(struct inode *inode, struct file *filp,
 		rp-&gt;offset = 0;
 		rp-&gt;q.reader = 1;
 
-		spin_lock(&amp;queue_lock);
+		spin_lock(&amp;cd-&gt;queue_lock);
 		list_add(&amp;rp-&gt;q.list, &amp;cd-&gt;queue);
-		spin_unlock(&amp;queue_lock);
+		spin_unlock(&amp;cd-&gt;queue_lock);
 	}
 	if (filp-&gt;f_mode &amp; FMODE_WRITE)
 		atomic_inc(&amp;cd-&gt;writers);
@@ -1062,7 +1061,7 @@ static int cache_release(struct inode *inode, struct file *filp,
 	struct cache_reader *rp = filp-&gt;private_data;
 
 	if (rp) {
-		spin_lock(&amp;queue_lock);
+		spin_lock(&amp;cd-&gt;queue_lock);
 		if (rp-&gt;offset) {
 			struct cache_queue *cq;
 			for (cq= &amp;rp-&gt;q; &amp;cq-&gt;list != &amp;cd-&gt;queue;
@@ -1075,7 +1074,7 @@ static int cache_release(struct inode *inode, struct file *filp,
 			rp-&gt;offset = 0;
 		}
 		list_del(&amp;rp-&gt;q.list);
-		spin_unlock(&amp;queue_lock);
+		spin_unlock(&amp;cd-&gt;queue_lock);
 
 		filp-&gt;private_data = NULL;
 		kfree(rp);
@@ -1097,7 +1096,7 @@ static void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)
 	struct cache_request *cr;
 	LIST_HEAD(dequeued);
 
-	spin_lock(&amp;queue_lock);
+	spin_lock(&amp;detail-&gt;queue_lock);
 	list_for_each_entry_safe(cq, tmp, &amp;detail-&gt;queue, list)
 		if (!cq-&gt;reader) {
 			cr = container_of(cq, struct cache_request, q);
@@ -1110,7 +1109,7 @@ static void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)
 				continue;
 			list_move(&amp;cr-&gt;q.list, &amp;dequeued);
 		}
-	spin_unlock(&amp;queue_lock);
+	spin_unlock(&amp;detail-&gt;queue_lock);
 	while (!list_empty(&amp;dequeued)) {
 		cr = list_entry(dequeued.next, struct cache_request, q.list);
 		list_del(&amp;cr-&gt;q.list);
@@ -1235,7 +1234,7 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)
 	crq-&gt;buf = buf;
 	crq-&gt;len = 0;
 	crq-&gt;readers = 0;
-	spin_lock(&amp;queue_lock);
+	spin_lock(&amp;detail-&gt;queue_lock);
 	if (test_bit(CACHE_PENDING, &amp;h-&gt;flags)) {
 		crq-&gt;item = cache_get(h);
 		list_add_tail(&amp;crq-&gt;q.list, &amp;detail-&gt;queue);
@@ -1243,7 +1242,7 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)
 	} else
 		/* Lost a race, no longer PENDING, so don&#x27;t enqueue */
 		ret = -EAGAIN;
-	spin_unlock(&amp;queue_lock);
+	spin_unlock(&amp;detail-&gt;queue_lock);
 	wake_up(&amp;queue_wait);
 	if (ret == -EAGAIN) {
 		kfree(buf);

-- 
2.53.0</pre>
</details>
<div class="review-comment-signals">Signals: acknowledged fix needed, agreed to restructure</div>
</div>
<div class="thread-children">
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Chuck Lever</span>
<a class="date-chip" href="../2026-02-20_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-20">2026-02-20</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">reviewer noted that the patch does not address the issue of cache_detail-&gt;seq being accessed without seq_lock held, which could lead to a race condition</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">From: Chuck Lever &lt;chuck.lever@oracle.com&gt;

On Fri, 20 Feb 2026 07:26:02 -0500, Jeff Layton wrote:</pre>
</details>
<div class="review-comment-signals">Signals: race condition, missing lock</div>
</div>
</div>
<div class="thread-node depth-1">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Chuck Lever</span>
<a class="date-chip" href="../2026-02-20_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-20">2026-02-20</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">reviewer noted that the per-cache_detail spinlock is not properly dropped before calling try_to_unmap(), potentially causing a lock ordering violation</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Applied to nfsd-testing, thanks!

[1/3] sunrpc: convert queue_lock from global spinlock to per-cache_detail lock
      commit: 8da8f32e9a2702259cdf97e2f8f492ef9c79db65
[2/3] sunrpc: convert queue_wait from global to per-cache_detail waitqueue
      commit: 802261d8b58dd2f41a52a0c92776e0fb45619efe
[3/3] sunrpc: split cache_detail queue into request and reader lists
      commit: 0eb3d9dc71ada02909e4dfe9cb54e703ec717ed4

--
Chuck Lever</pre>
</details>
<div class="review-comment-signals">Signals: potential deadlock, locking issue</div>
</div>
</div>
<div class="thread-node depth-1" id="2026-02-22">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">NeilBrown</span>
<a class="date-chip" href="../2026-02-22_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-22">2026-02-22</a>
<span class="review-tag-badge">Reviewed-by</span>
<span class="badge" style="color:#856404;background:#fff3cd">Needs Work</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">Reviewer NeilBrown noted that the code should also decrement -&gt;readers and check if it&#x27;s zero, similar to another place in the code, and suggested fixing this potential bug now</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Hmm..  The other place where we decrement -&gt;readers we then check if it
is zero and if CACHE_PENDING is clear - and do something.
I suspect we should do that here.
This bug (if I&#x27;m right and it is a bug) if there before you patch, but
now might be a good time to fix it?

Thanks.  Nice cleanups.

NeilBrown</pre>
</details>
<div class="review-comment-signals">Signals: potential bug, requested fix</div>
</div>
</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Jeff Layton (author)</span>
<a class="date-chip" href="../2026-02-20_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-20">2026-02-20</a>
<span class="badge" style="color:#155724;background:#d4edda">Positive</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author is addressing a concern about the queue_wait waitqueue being global and waking pollers on all caches, instead of just the relevant one. The author agrees to convert it to a per-cache_detail field so that only pollers on the relevant cache are woken.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">The queue_wait waitqueue is currently a file-scoped global, so a
wake_up for one cache_detail wakes pollers on all caches. Convert it
to a per-cache_detail field so that only pollers on the relevant cache
are woken.

Signed-off-by: Jeff Layton &lt;jlayton@kernel.org&gt;
---
 include/linux/sunrpc/cache.h | 2 ++
 net/sunrpc/cache.c           | 7 +++----
 2 files changed, 5 insertions(+), 4 deletions(-)

diff --git a/include/linux/sunrpc/cache.h b/include/linux/sunrpc/cache.h
index 3d32dd1f7b05d35562d2064fed69877b3950fb51..031379efba24d40f64ce346cf1032261d4b98d05 100644
--- a/include/linux/sunrpc/cache.h
+++ b/include/linux/sunrpc/cache.h
@@ -16,6 +16,7 @@
 #include &lt;linux/atomic.h&gt;
 #include &lt;linux/kstrtox.h&gt;
 #include &lt;linux/proc_fs.h&gt;
+#include &lt;linux/wait.h&gt;
 
 /*
  * Each cache requires:
@@ -114,6 +115,7 @@ struct cache_detail {
 	/* fields for communication over channel */
 	struct list_head	queue;
 	spinlock_t		queue_lock;
+	wait_queue_head_t	queue_wait;
 
 	atomic_t		writers;		/* how many time is /channel open */
 	time64_t		last_close;		/* if no writers, when did last close */
diff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c
index 6add2fe311425dc3aec63efce2c4bed06a3d3ba5..aef2607b3d7ffb61a42b9ea2ec17947465c026dc 100644
--- a/net/sunrpc/cache.c
+++ b/net/sunrpc/cache.c
@@ -401,6 +401,7 @@ void sunrpc_init_cache_detail(struct cache_detail *cd)
 	spin_lock_init(&amp;cd-&gt;hash_lock);
 	INIT_LIST_HEAD(&amp;cd-&gt;queue);
 	spin_lock_init(&amp;cd-&gt;queue_lock);
+	init_waitqueue_head(&amp;cd-&gt;queue_wait);
 	spin_lock(&amp;cache_list_lock);
 	cd-&gt;nextcheck = 0;
 	cd-&gt;entries = 0;
@@ -970,8 +971,6 @@ static ssize_t cache_write(struct file *filp, const char __user *buf,
 	return ret;
 }
 
-static DECLARE_WAIT_QUEUE_HEAD(queue_wait);
-
 static __poll_t cache_poll(struct file *filp, poll_table *wait,
 			       struct cache_detail *cd)
 {
@@ -979,7 +978,7 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,
 	struct cache_reader *rp = filp-&gt;private_data;
 	struct cache_queue *cq;
 
-	poll_wait(filp, &amp;queue_wait, wait);
+	poll_wait(filp, &amp;cd-&gt;queue_wait, wait);
 
 	/* alway allow write */
 	mask = EPOLLOUT | EPOLLWRNORM;
@@ -1243,7 +1242,7 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)
 		/* Lost a race, no longer PENDING, so don&#x27;t enqueue */
 		ret = -EAGAIN;
 	spin_unlock(&amp;detail-&gt;queue_lock);
-	wake_up(&amp;queue_wait);
+	wake_up(&amp;detail-&gt;queue_wait);
 	if (ret == -EAGAIN) {
 		kfree(buf);
 		kfree(crq);

-- 
2.53.0</pre>
</details>
<div class="review-comment-signals">Signals: acknowledged fix needed, agreed to restructure</div>
</div>
</div>
<div class="thread-node depth-0">
<div class="review-comment">
<div class="review-comment-header">
<span class="review-author">Jeff Layton (author)</span>
<a class="date-chip" href="../2026-02-20_ollama_llama3.1-8b.html" title="First appeared in report for 2026-02-20">2026-02-20</a>
<span class="badge" style="color:#155724;background:#d4edda">Positive</span>
<span class="analysis-source-badge" style="color:#004085;background:#cce5ff" title="Analysis source: LLM">LLM</span>
</div>
<div class="review-comment-text">The author addressed a concern about the shared list of requests and readers in the sunrpc cache infrastructure. They replaced this single interleaved queue with two dedicated lists: cd-&gt;requests for upcall requests and cd-&gt;readers for open file handles, eliminating the need for the -&gt;reader flag. The sequence number (next_seqno) is now used to track position instead of the shared list. This change simplifies the reader-skipping loops in various functions and makes data flow easier to reason about.</div>
<details class="raw-body-toggle">
<summary>Show original comment</summary>
<pre class="raw-body-text">Replace the single interleaved queue (which mixed cache_request and
cache_reader entries distinguished by a -&gt;reader flag) with two
dedicated lists: cd-&gt;requests for upcall requests and cd-&gt;readers
for open file handles.

Readers now track their position via a monotonically increasing
sequence number (next_seqno) rather than by their position in the
shared list. Each cache_request is assigned a seqno when enqueued,
and a new cache_next_request() helper finds the next request at or
after a given seqno.

This eliminates the cache_queue wrapper struct entirely, simplifies
the reader-skipping loops in cache_read/cache_poll/cache_ioctl/
cache_release, and makes the data flow easier to reason about.

Also, remove an obsolete comment. CACHE_UPCALLING hasn&#x27;t existed
since before the git era started.

Signed-off-by: Jeff Layton &lt;jlayton@kernel.org&gt;
---
 include/linux/sunrpc/cache.h |   4 +-
 net/sunrpc/cache.c           | 125 ++++++++++++++++++-------------------------
 2 files changed, 56 insertions(+), 73 deletions(-)

diff --git a/include/linux/sunrpc/cache.h b/include/linux/sunrpc/cache.h
index 031379efba24d40f64ce346cf1032261d4b98d05..b1e595c2615bd4be4d9ad19f71a8f4d08bd74a9b 100644
--- a/include/linux/sunrpc/cache.h
+++ b/include/linux/sunrpc/cache.h
@@ -113,9 +113,11 @@ struct cache_detail {
 	int			entries;
 
 	/* fields for communication over channel */
-	struct list_head	queue;
+	struct list_head	requests;
+	struct list_head	readers;
 	spinlock_t		queue_lock;
 	wait_queue_head_t	queue_wait;
+	u64			next_seqno;
 
 	atomic_t		writers;		/* how many time is /channel open */
 	time64_t		last_close;		/* if no writers, when did last close */
diff --git a/net/sunrpc/cache.c b/net/sunrpc/cache.c
index aef2607b3d7ffb61a42b9ea2ec17947465c026dc..09389ce8b961fe0cb5a472bcf2d3dd0b3faa13a6 100644
--- a/net/sunrpc/cache.c
+++ b/net/sunrpc/cache.c
@@ -399,9 +399,11 @@ static struct delayed_work cache_cleaner;
 void sunrpc_init_cache_detail(struct cache_detail *cd)
 {
 	spin_lock_init(&amp;cd-&gt;hash_lock);
-	INIT_LIST_HEAD(&amp;cd-&gt;queue);
+	INIT_LIST_HEAD(&amp;cd-&gt;requests);
+	INIT_LIST_HEAD(&amp;cd-&gt;readers);
 	spin_lock_init(&amp;cd-&gt;queue_lock);
 	init_waitqueue_head(&amp;cd-&gt;queue_wait);
+	cd-&gt;next_seqno = 0;
 	spin_lock(&amp;cache_list_lock);
 	cd-&gt;nextcheck = 0;
 	cd-&gt;entries = 0;
@@ -796,29 +798,20 @@ void cache_clean_deferred(void *owner)
  * On read, you get a full request, or block.
  * On write, an update request is processed.
  * Poll works if anything to read, and always allows write.
- *
- * Implemented by linked list of requests.  Each open file has
- * a -&gt;private that also exists in this list.  New requests are added
- * to the end and may wakeup and preceding readers.
- * New readers are added to the head.  If, on read, an item is found with
- * CACHE_UPCALLING clear, we free it from the list.
- *
  */
 
-struct cache_queue {
-	struct list_head	list;
-	int			reader;	/* if 0, then request */
-};
 struct cache_request {
-	struct cache_queue	q;
+	struct list_head	list;
 	struct cache_head	*item;
-	char			* buf;
+	char			*buf;
 	int			len;
 	int			readers;
+	u64			seqno;
 };
 struct cache_reader {
-	struct cache_queue	q;
+	struct list_head	list;
 	int			offset;	/* if non-0, we have a refcnt on next request */
+	u64			next_seqno;
 };
 
 static int cache_request(struct cache_detail *detail,
@@ -833,6 +826,17 @@ static int cache_request(struct cache_detail *detail,
 	return PAGE_SIZE - len;
 }
 
+static struct cache_request *
+cache_next_request(struct cache_detail *cd, u64 seqno)
+{
+	struct cache_request *rq;
+
+	list_for_each_entry(rq, &amp;cd-&gt;requests, list)
+		if (rq-&gt;seqno &gt;= seqno)
+			return rq;
+	return NULL;
+}
+
 static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,
 			  loff_t *ppos, struct cache_detail *cd)
 {
@@ -849,20 +853,13 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,
  again:
 	spin_lock(&amp;cd-&gt;queue_lock);
 	/* need to find next request */
-	while (rp-&gt;q.list.next != &amp;cd-&gt;queue &amp;&amp;
-	       list_entry(rp-&gt;q.list.next, struct cache_queue, list)
-	       -&gt;reader) {
-		struct list_head *next = rp-&gt;q.list.next;
-		list_move(&amp;rp-&gt;q.list, next);
-	}
-	if (rp-&gt;q.list.next == &amp;cd-&gt;queue) {
+	rq = cache_next_request(cd, rp-&gt;next_seqno);
+	if (!rq) {
 		spin_unlock(&amp;cd-&gt;queue_lock);
 		inode_unlock(inode);
 		WARN_ON_ONCE(rp-&gt;offset);
 		return 0;
 	}
-	rq = container_of(rp-&gt;q.list.next, struct cache_request, q.list);
-	WARN_ON_ONCE(rq-&gt;q.reader);
 	if (rp-&gt;offset == 0)
 		rq-&gt;readers++;
 	spin_unlock(&amp;cd-&gt;queue_lock);
@@ -877,7 +874,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,
 	if (rp-&gt;offset == 0 &amp;&amp; !test_bit(CACHE_PENDING, &amp;rq-&gt;item-&gt;flags)) {
 		err = -EAGAIN;
 		spin_lock(&amp;cd-&gt;queue_lock);
-		list_move(&amp;rp-&gt;q.list, &amp;rq-&gt;q.list);
+		rp-&gt;next_seqno = rq-&gt;seqno + 1;
 		spin_unlock(&amp;cd-&gt;queue_lock);
 	} else {
 		if (rp-&gt;offset + count &gt; rq-&gt;len)
@@ -889,7 +886,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,
 		if (rp-&gt;offset &gt;= rq-&gt;len) {
 			rp-&gt;offset = 0;
 			spin_lock(&amp;cd-&gt;queue_lock);
-			list_move(&amp;rp-&gt;q.list, &amp;rq-&gt;q.list);
+			rp-&gt;next_seqno = rq-&gt;seqno + 1;
 			spin_unlock(&amp;cd-&gt;queue_lock);
 		}
 		err = 0;
@@ -901,7 +898,7 @@ static ssize_t cache_read(struct file *filp, char __user *buf, size_t count,
 		rq-&gt;readers--;
 		if (rq-&gt;readers == 0 &amp;&amp;
 		    !test_bit(CACHE_PENDING, &amp;rq-&gt;item-&gt;flags)) {
-			list_del(&amp;rq-&gt;q.list);
+			list_del(&amp;rq-&gt;list);
 			spin_unlock(&amp;cd-&gt;queue_lock);
 			cache_put(rq-&gt;item, cd);
 			kfree(rq-&gt;buf);
@@ -976,7 +973,6 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,
 {
 	__poll_t mask;
 	struct cache_reader *rp = filp-&gt;private_data;
-	struct cache_queue *cq;
 
 	poll_wait(filp, &amp;cd-&gt;queue_wait, wait);
 
@@ -988,12 +984,8 @@ static __poll_t cache_poll(struct file *filp, poll_table *wait,
 
 	spin_lock(&amp;cd-&gt;queue_lock);
 
-	for (cq= &amp;rp-&gt;q; &amp;cq-&gt;list != &amp;cd-&gt;queue;
-	     cq = list_entry(cq-&gt;list.next, struct cache_queue, list))
-		if (!cq-&gt;reader) {
-			mask |= EPOLLIN | EPOLLRDNORM;
-			break;
-		}
+	if (cache_next_request(cd, rp-&gt;next_seqno))
+		mask |= EPOLLIN | EPOLLRDNORM;
 	spin_unlock(&amp;cd-&gt;queue_lock);
 	return mask;
 }
@@ -1004,7 +996,7 @@ static int cache_ioctl(struct inode *ino, struct file *filp,
 {
 	int len = 0;
 	struct cache_reader *rp = filp-&gt;private_data;
-	struct cache_queue *cq;
+	struct cache_request *rq;
 
 	if (cmd != FIONREAD || !rp)
 		return -EINVAL;
@@ -1014,14 +1006,9 @@ static int cache_ioctl(struct inode *ino, struct file *filp,
 	/* only find the length remaining in current request,
 	 * or the length of the next request
 	 */
-	for (cq= &amp;rp-&gt;q; &amp;cq-&gt;list != &amp;cd-&gt;queue;
-	     cq = list_entry(cq-&gt;list.next, struct cache_queue, list))
-		if (!cq-&gt;reader) {
-			struct cache_request *cr =
-				container_of(cq, struct cache_request, q);
-			len = cr-&gt;len - rp-&gt;offset;
-			break;
-		}
+	rq = cache_next_request(cd, rp-&gt;next_seqno);
+	if (rq)
+		len = rq-&gt;len - rp-&gt;offset;
 	spin_unlock(&amp;cd-&gt;queue_lock);
 
 	return put_user(len, (int __user *)arg);
@@ -1042,10 +1029,10 @@ static int cache_open(struct inode *inode, struct file *filp,
 			return -ENOMEM;
 		}
 		rp-&gt;offset = 0;
-		rp-&gt;q.reader = 1;
+		rp-&gt;next_seqno = 0;
 
 		spin_lock(&amp;cd-&gt;queue_lock);
-		list_add(&amp;rp-&gt;q.list, &amp;cd-&gt;queue);
+		list_add(&amp;rp-&gt;list, &amp;cd-&gt;readers);
 		spin_unlock(&amp;cd-&gt;queue_lock);
 	}
 	if (filp-&gt;f_mode &amp; FMODE_WRITE)
@@ -1062,17 +1049,14 @@ static int cache_release(struct inode *inode, struct file *filp,
 	if (rp) {
 		spin_lock(&amp;cd-&gt;queue_lock);
 		if (rp-&gt;offset) {
-			struct cache_queue *cq;
-			for (cq= &amp;rp-&gt;q; &amp;cq-&gt;list != &amp;cd-&gt;queue;
-			     cq = list_entry(cq-&gt;list.next, struct cache_queue, list))
-				if (!cq-&gt;reader) {
-					container_of(cq, struct cache_request, q)
-						-&gt;readers--;
-					break;
-				}
+			struct cache_request *rq;
+
+			rq = cache_next_request(cd, rp-&gt;next_seqno);
+			if (rq)
+				rq-&gt;readers--;
 			rp-&gt;offset = 0;
 		}
-		list_del(&amp;rp-&gt;q.list);
+		list_del(&amp;rp-&gt;list);
 		spin_unlock(&amp;cd-&gt;queue_lock);
 
 		filp-&gt;private_data = NULL;
@@ -1091,27 +1075,24 @@ static int cache_release(struct inode *inode, struct file *filp,
 
 static void cache_dequeue(struct cache_detail *detail, struct cache_head *ch)
 {
-	struct cache_queue *cq, *tmp;
-	struct cache_request *cr;
+	struct cache_request *cr, *tmp;
 	LIST_HEAD(dequeued);
 
 	spin_lock(&amp;detail-&gt;queue_lock);
-	list_for_each_entry_safe(cq, tmp, &amp;detail-&gt;queue, list)
-		if (!cq-&gt;reader) {
-			cr = container_of(cq, struct cache_request, q);
-			if (cr-&gt;item != ch)
-				continue;
-			if (test_bit(CACHE_PENDING, &amp;ch-&gt;flags))
-				/* Lost a race and it is pending again */
-				break;
-			if (cr-&gt;readers != 0)
-				continue;
-			list_move(&amp;cr-&gt;q.list, &amp;dequeued);
-		}
+	list_for_each_entry_safe(cr, tmp, &amp;detail-&gt;requests, list) {
+		if (cr-&gt;item != ch)
+			continue;
+		if (test_bit(CACHE_PENDING, &amp;ch-&gt;flags))
+			/* Lost a race and it is pending again */
+			break;
+		if (cr-&gt;readers != 0)
+			continue;
+		list_move(&amp;cr-&gt;list, &amp;dequeued);
+	}
 	spin_unlock(&amp;detail-&gt;queue_lock);
 	while (!list_empty(&amp;dequeued)) {
-		cr = list_entry(dequeued.next, struct cache_request, q.list);
-		list_del(&amp;cr-&gt;q.list);
+		cr = list_entry(dequeued.next, struct cache_request, list);
+		list_del(&amp;cr-&gt;list);
 		cache_put(cr-&gt;item, detail);
 		kfree(cr-&gt;buf);
 		kfree(cr);
@@ -1229,14 +1210,14 @@ static int cache_pipe_upcall(struct cache_detail *detail, struct cache_head *h)
 		return -EAGAIN;
 	}
 
-	crq-&gt;q.reader = 0;
 	crq-&gt;buf = buf;
 	crq-&gt;len = 0;
 	crq-&gt;readers = 0;
 	spin_lock(&amp;detail-&gt;queue_lock);
 	if (test_bit(CACHE_PENDING, &amp;h-&gt;flags)) {
 		crq-&gt;item = cache_get(h);
-		list_add_tail(&amp;crq-&gt;q.list, &amp;detail-&gt;queue);
+		crq-&gt;seqno = detail-&gt;next_seqno++;
+		list_add_tail(&amp;crq-&gt;list, &amp;detail-&gt;requests);
 		trace_cache_entry_upcall(detail, h);
 	} else
 		/* Lost a race, no longer PENDING, so don&#x27;t enqueue */

-- 
2.53.0</pre>
</details>
<div class="review-comment-signals">Signals: acknowledged a design improvement, agreed with the approach</div>
</div>
</div>
</div>

    <footer>LKML Daily Activity Tracker</footer>
    <script>
    // When arriving via a date anchor (e.g. #2026-02-15 from a daily report),
    // scroll the anchor into view after a brief delay so layout is complete.
    (function () {
        var hash = window.location.hash;
        if (!hash) return;
        var target = document.getElementById(hash.slice(1));
        if (!target) return;
        setTimeout(function () {
            target.scrollIntoView({behavior: 'smooth', block: 'start'});
        }, 80);
    })();
    </script>
</body>
</html>