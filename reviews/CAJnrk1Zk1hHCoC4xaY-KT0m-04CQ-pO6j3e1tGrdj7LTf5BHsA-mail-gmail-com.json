{
  "thread_id": "CAJnrk1Zk1hHCoC4xaY_KT0m_04CQ=pO6j3e1tGrdj7LTf5BHsA@mail.gmail.com",
  "subject": "[PATCH v1 1/1] iomap: don't mark folio uptodate if read IO has bytes pending",
  "url": "https://lore.kernel.org/all/CAJnrk1Zk1hHCoC4xaY_KT0m_04CQ=pO6j3e1tGrdj7LTf5BHsA@mail.gmail.com/",
  "dates": {
    "2026-02-18": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Joanne Koong",
      "reviews": [
        {
          "author": "Joanne Koong (author)",
          "summary": "The author addressed a concern about the folio uptodate state being cleared when iomap_set_range_uptodate() is called after an async IO helper has successfully finished reading a partially read folio, and then post-EOF zeroing or inline data reads mark the folio as uptodate. The author explained that this is because folio_end_read(), which uses XOR semantics to set the uptodate bit, will clear it if it's already been set by iomap_set_range_uptodate(). To fix this, the author proposed not marking the folio as uptodate if the read IO has bytes pending.",
          "sentiment": "needs_work",
          "sentiment_signals": [],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "If a folio has ifs metadata attached to it and the folio is partially\nread in through an async IO helper with the rest of it then being read\nin through post-EOF zeroing or as inline data, and the helper\nsuccessfully finishes the read first, then post-EOF zeroing / reading\ninline will mark the folio as uptodate in iomap_set_range_uptodate().\n\nThis is a problem because when the read completion path later calls\niomap_read_end(), it will call folio_end_read(), which sets the uptodate\nbit using XOR semantics. Calling folio_end_read() on a folio that was\nalready marked uptodate clears the uptodate bit.\n\nFix this by not marking the folio as uptodate if the read IO has bytes\npending. The folio uptodate state will be set in the read completion\npath through iomap_end_read() -> folio_end_read().\n\nReported-by: Wei Gao <wegao@suse.com>\nSuggested-by: Sasha Levin <sashal@kernel.org>\nTested-by: Wei Gao <wegao@suse.com>\nSigned-off-by: Joanne Koong <joannelkoong@gmail.com>\nFixes: b2f35ac4146d (\"iomap: add caller-provided callbacks for read and readahead\")\n---\n fs/iomap/buffered-io.c | 15 ++++++++++++---\n 1 file changed, 12 insertions(+), 3 deletions(-)\n\ndiff --git a/fs/iomap/buffered-io.c b/fs/iomap/buffered-io.c\nindex 58887513b894..4fc5ce963feb 100644\n--- a/fs/iomap/buffered-io.c\n+++ b/fs/iomap/buffered-io.c\n@@ -80,18 +80,27 @@ static void iomap_set_range_uptodate(struct folio *folio, size_t off,\n {\n \tstruct iomap_folio_state *ifs = folio->private;\n \tunsigned long flags;\n-\tbool uptodate = true;\n+\tbool mark_uptodate = true;\n \n \tif (folio_test_uptodate(folio))\n \t\treturn;\n \n \tif (ifs) {\n \t\tspin_lock_irqsave(&ifs->state_lock, flags);\n-\t\tuptodate = ifs_set_range_uptodate(folio, ifs, off, len);\n+\t\t/*\n+\t\t * If a read with bytes pending is in progress, we must not call\n+\t\t * folio_mark_uptodate(). The read completion path\n+\t\t * (iomap_read_end()) will call folio_end_read(), which uses XOR\n+\t\t * semantics to set the uptodate bit. If we set it here, the XOR\n+\t\t * in folio_end_read() will clear it, leaving the folio not\n+\t\t * uptodate.\n+\t\t */\n+\t\tmark_uptodate = ifs_set_range_uptodate(folio, ifs, off, len) &&\n+\t\t\t\t!ifs->read_bytes_pending;\n \t\tspin_unlock_irqrestore(&ifs->state_lock, flags);\n \t}\n \n-\tif (uptodate)\n+\tif (mark_uptodate)\n \t\tfolio_mark_uptodate(folio);\n }\n \n-- \n2.47.3",
          "reply_to": "",
          "message_date": "2026-02-18"
        },
        {
          "author": "Darrick Wong",
          "summary": "Reviewer Darrick Wong suggested adding a link to a relevant discussion on linux-fsdevel and CC'ing stable@vger.kernel.org for the v6.19 release\n\nReviewer Darrick Wong noted that the patch's impact should be tested with an fstest, and provided a Reviewed-by tag.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "no clear signal"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "I would add:\n\nLink: https://lore.kernel.org/linux-fsdevel/aYbmy8JdgXwsGaPP@autotest-wegao.qe.prg2.suse.org/\nCc: <stable@vger.kernel.org> # v6.19\n\nsince the recent discussion around this was sort of buried in a\ndifferent thread, and the original patch is now in a released kernel.\n\n---\n\nYeah, that makes sense.  How difficult is this to write up as an fstest?\n\nReviewed-by: \"Darrick J. Wong\" <djwong@kernel.org>\n\n--D",
          "reply_to": "Joanne Koong",
          "message_date": "2026-02-18"
        },
        {
          "author": "Darrick Wong",
          "summary": "Reviewer Darrick Wong expressed confusion about an alternative approach mentioned in the patch description, indicating that he was not aware of a previous discussion and is seeking clarification.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "confusion",
            "lack of context"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "Well you could try explaining to me what that simpler way is?\n\n/me gets the sense he's missing a discussion somewhere...\n\n--D",
          "reply_to": "Matthew Wilcox",
          "message_date": "2026-02-18"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-19": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Joanne Koong",
      "reviews": [
        {
          "author": "Matthew Wilcox",
          "summary": "reviewer expressed frustration that the iomap code is overly complicated and difficult to understand, implying it needs a fundamental rework",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "This isn't \"the xor thing has come back to bite us\".  This is \"the iomap\ncode is now too complicated and I cannot figure out how to explain to\nJoanne that there's really a simple way to do this\".\n\nI'm going to have to set aside my current projects and redo the iomap\nreadahead/read_folio code myself, aren't I?",
          "reply_to": "Darrick Wong",
          "message_date": "2026-02-19"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-20": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Joanne Koong",
      "reviews": [
        {
          "author": "Joanne Koong (author)",
          "summary": "Author responded to a request for further context by providing the link to the prior discussion, indicating no specific action or resolution related to the original patch.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "no clear resolution signal",
            "request for clarification"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "This is the link to the prior discussion\nhttps://lore.kernel.org/linux-fsdevel/20251223223018.3295372-1-sashal@kernel.org/T/#mbd61eaa5fd1e8922caa479720232628e39b8c9da\n\nThanks,\nJoanne",
          "reply_to": "Darrick Wong",
          "message_date": "2026-02-20"
        },
        {
          "author": "Darrick Wong",
          "summary": "Reviewer Darrick Wong noted that the read_bytes_pending field in iomap has inconsistent behavior across different IO paths, and suggested consolidating the read code to simplify it. He also mentioned that Joanne's patch is a temporary fix for a bug in Linus' tree.",
          "sentiment": "needs_work",
          "sentiment_signals": [
            "requested changes",
            "suggested improvements"
          ],
          "has_inline_review": false,
          "tags_given": [
            "Reviewed-by"
          ],
          "analysis_source": "llm",
          "raw_body": "<willy and I had a chat; this is a clumsy non-AI summary of it>\n\nI started looking at folio read state management in iomap, and made a\nfew observations that (I hope) match what willy's grumpy about.\n\nThere are three ways that iomap can be reading into the pagecache:\na) async ->readahead,\nb) synchronous ->read_folio (page faults), and\nc) synchronous ->read_folio_range (pagecache write).\n\n(Note that (b) can call a different ->read_folio_range than (c), though\nall implementations seem to have the same function)\n\nAll three of these IO paths share the behavior that they try to fill out\nthe folio's contents and set the corresponding folio/ifs uptodate bits\nif that succeeds.  Folio contents can come from anywhere, whether it's:\n\ni) zeroing memory,\nii) copying from an inlinedata buffer, or\niii) asynchronously fetching the contents from somewhere\n\nIn the case of (c) above, if the read fails then we fail the write, and\nif the read succeeds then we start copying to the pagecache.\n\nHowever, (a) and (b) have this additional read_bytes_pending field in\nthe ifs that implements some extra tracking.  AFAICT the purpose of this\nfield is to ensure that we don't call folio_end_read prematurely if\nthere's an async read in progress.  This can happen if iomap_iter\nreturns a negative errno on a partially processed folio, I think?\n\nread_bytes_pending is initialized to the folio_size() at the start of a\nread and subtracted from when parts of the folio are supplied, whether\nthat's synchronous zeroing or asynchronous read ioend completion.  When\nthe field reaches zero, we can then call folio_end_read().\n\nBut then there are twists, like the fact that we only call\niomap_read_init() to set read_bytes_pending if we decide to do an\nasynchronous read.  Or that iomap_read_end and iomap_finish_folio_read\nhave awfully similar code.  I think in the case of (i) and (ii) we also\ndon't touch read_pending_bytes at all, and merely set the uptodate bits?\n\nThis is confusing to me.  It would be more straightforward (I think) if\nwe just did it for all cases instead of adding more conditionals.  IOWs,\nhow hard would it be to consolidate the read code so that there's one\nfunction that iomap calls when it has filled out part of a folio.  Is\nthat possible, even though we shouldn't be calling folio_end_read during\na pagecache write?\n\nAt the end of the day, however, there's a bug in Linus' tree and we need\nto fix it, so Joanne's patch is a sufficient bandaid until we can go\nclean this up.\n\n--D",
          "reply_to": "Matthew Wilcox",
          "message_date": "2026-02-20"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-23": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Joanne Koong",
      "reviews": [
        {
          "author": "Joanne Koong (author)",
          "summary": "The author acknowledged that the read IO has bytes pending issue is not unique to synchronous reads, explaining that the code for both synchronous and asynchronous reads (a) and b) are identical.\n\nAuthor clarified that synchronous zeroing does not update read_bytes_pending, explaining that only asynchronous read completions affect this value.\n\nAuthor Joanne Koong addressed Darrick Wong's feedback about consolidating synchronous buffered write logic with async read logic, arguing that it would add unnecessary overhead and complicate handling. She agreed that there are edge cases to consider for async reads but expressed concerns about introducing additional complexity through zeroing and inline read paths.",
          "sentiment": "neutral",
          "sentiment_signals": [
            "acknowledged a nuance",
            "provided clarification",
            "clarification",
            "explanation",
            "acknowledged a concern",
            "pushed back on an approach"
          ],
          "has_inline_review": false,
          "tags_given": [],
          "analysis_source": "llm",
          "raw_body": "b) is async as well. The code for b) and a) are exactly the same (the\nlogic in iomap_read_folio_iter())\n\n---\n\nSynchronous zeroing does not update read_bytes_pending, only async\nread completions do.\n\n---\n\nimo, I don't think the synchronous ->read_folio_range() for buffered\nwrites should be consolidated with the async read logic. If we have\nthe synchronous write path setting read_bytes_pending, that adds extra\noverhead with having to acquire/release the spinlock for every range\nread in. It also makes the handling more complicated (eg now having to\ndifferentiate whether the folio was read in for a read vs. a write).\nSynchronous ->read_folio_range() for buffered writes is extremely\nsimple and self-contained right now and I think it should be kept that\nway.\n\nFor async reads, I agree that there are a bunch of different edge\ncases that arise from i) ii) and iii), and from the fact that a folio\ncould be composed of a mixture of i) ii) and iii).\n\nThe motivation for adding read_bytes_pending was so we could know\nwhich async read finishes last. eg this example scenario: read a 64k\nfolio where the first and last page are not uptodate but everything in\nbetween is\n* ->read_folio_range() for 0 to 4k\n* ->read_folio_range() for 60k to 64k\nThese two async read calls may be two different I/O requests that\ncomplete at different times but only the last finisher should call\nfolio_end_read().\n\nI don't think having the zeroing and inline read paths also\nmanipulating read_bytes_pending helps here. This was discussed a bit\nin [1] but I think it runs into other edge cases / race conditions [2]\nthat would need to be accounted for and makes some paths more\nsuboptimal (eg unnecessary ifs allocations and spinlock acquires). But\nmaybe I'm missing something here and there is a better approach for\ndoing this?\n\nThanks,\nJoanne\n\n[1] https://lore.kernel.org/linux-fsdevel/CAJnrk1YcuhKwbZLo-11=umcTzH_OJ+bdwZq5=XjeJo8gb9e5ig@mail.gmail.com/T/#md09648082a96122ec1e541993872e0c43da5105f\n[2] https://lore.kernel.org/linux-fsdevel/CAJnrk1YcuhKwbZLo-11=umcTzH_OJ+bdwZq5=XjeJo8gb9e5ig@mail.gmail.com/T/#mdc49b649378798fa9e850c9c6914c8c6af5e2895",
          "reply_to": "Darrick Wong",
          "message_date": "2026-02-23"
        }
      ],
      "analysis_source": "llm-per-reviewer"
    },
    "2026-02-24": {
      "report_file": "2026-02-23_ollama_llama3.1-8b.html",
      "developer": "Joanne Koong",
      "reviews": [
        {
          "author": "Christoph Hellwig",
          "summary": "Yes.  I've been thinking about that on and off, but unfortunately so far I've not come up with a good idea how to merge the code.  Doing so would be very useful for many reasons. The problem with that isn't really async vs sync; ->read_folio clearly shows you you turn underlying asynchronous logic into a synchronous call. It's really about the range logic, where the writer preparation might want to only read the head and the tail segments of a folio.",
          "sentiment": "neutral",
          "sentiment_signals": [],
          "has_inline_review": true,
          "tags_given": [],
          "analysis_source": "heuristic",
          "raw_body": "On Mon, Feb 23, 2026 at 03:53:15PM -0800, Joanne Koong wrote:\n> > There are three ways that iomap can be reading into the pagecache:\n> > a) async ->readahead,\n> > b) synchronous ->read_folio (page faults), and\n> \n> b) is async as well. The code for b) and a) are exactly the same (the\n> logic in iomap_read_folio_iter())\n\nYes.\n\n> > This is confusing to me.  It would be more straightforward (I think) if\n> > we just did it for all cases instead of adding more conditionals.  IOWs,\n> > how hard would it be to consolidate the read code so that there's one\n> > function that iomap calls when it has filled out part of a folio.  Is\n> > that possible, even though we shouldn't be calling folio_end_read during\n> > a pagecache write?\n> \n> imo, I don't think the synchronous ->read_folio_range() for buffered\n> writes should be consolidated with the async read logic.\n\nYes.  I've been thinking about that on and off, but unfortunately so far\nI've not come up with a good idea how to merge the code.  Doing so would\nbe very useful for many reasons.\n\nThe problem with that isn't really async vs sync; ->read_folio clearly\nshows you you turn underlying asynchronous logic into a synchronous call.\nIt's really about the range logic, where the writer preparation might\nwant to only read the head and the tail segments of a folio.\n\nBut if we can merge that into the main implementation and have a single\ncore implementation we'd be much better off.\n\nAnyone looking for a \"little\" project? :)\n\n",
          "reply_to": "",
          "message_date": "2026-02-24",
          "message_id": ""
        }
      ],
      "analysis_source": "heuristic"
    }
  }
}